{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMSC 35440 Machine Learning in Biology and Medicine\n",
        "## Homework 1: Embedding Research Articles\n",
        "**Released**: Jan 14, 2025\n",
        "\n",
        "**Due**: Jan 24, 2025 at 11:59 PM Chicago Time on Gradescope\n",
        "\n",
        "**In this first homework, you'll generate embeddings for 20 provided research articles and visualize them.**\n",
        "\n",
        "At a high-level, embeddings are vectors computed by some algorithm or model that \"code\" information from data. Embeddings can be computed in a wide variety of different ways, from concatenating manually created features to using deep neural networks.\n",
        "\n",
        "For this homework, you will code text documents as vectors using the bag of words algorithm and normalize these vectors using the term-frequency inverse documentation frequency (TF-IDF) method. This method dates back over 50 years to 1972. Through this homework, hopefully we'll convince you that it's still very much relevant.\n",
        "\n",
        "Please carefully read through the instructions below. Also, while not required for the homework, the articles themselves are worth a read. They're some seminal papers across various domains around biomedical AI/ML.\n",
        "\n",
        "The starter notebook for this homework can be downloaded from GitHub:\n",
        "\n",
        "https://github.com/StevenSong/CMSC-35440-Source/blob/main/hw1/CMSC_35440_HW1_Student_Version.ipynb"
      ],
      "metadata": {
        "id": "K-gV2pmJcU79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "vfHBZQl0rauN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Download and open the starter notebook in your favorite Jupyter Notebook host. We recommend using [Google Colab](https://colab.research.google.com/).\n",
        "  * **NB:** We'll design all homeworks such that they can be run on the *free* tier of Colab. You're welcome to use any other host, but the benefit of Colab is that they offer free GPU-instances.\n",
        "  * Technically you don't need GPUs for any modeling but it can really speed it up. For homeworks where GPU-acceleration is recommended, we'll provide additional instructions on how to access GPU-instances on Colab.\n",
        "  * For this homework, we don't require the use of any GPUs.\n",
        "1. Download and unzip the research articles. We've provided them as a tarball that be downloaded from [https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz](https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz).\n",
        "  * You'll notice that there's a CSV of article metadata and a folder of article *PDFs*. While these articles are available elsewhere on the internet as extracted-text (come to office-hours if you're interested in using such a resource for other projects), real-world data is messy. One such way that data can be messy is that it only exists as PDFs - so **you must use the article PDFs for this assignment**.\n",
        "1. Extract the text from the articles. You should probably use some variables from the metadata at this step.\n",
        "1. Compute the term-document matrix and then normalize the term-document method using the TF-IDF method.  **You must implement TF-IDF yourself. You may not use any existing implementations for computing the TF-IDF matrix** (e.g. you can NOT use sklearn's function for TF-IDF).\n",
        "  * Defining what is a \"term\" is up to you but don't overcomplicate it. Splitting on whitespace characters works fine.\n",
        "  * The wikipedia is hopefully all you need to understand the formula: [https://en.wikipedia.org/wiki/Tf-idf](https://en.wikipedia.org/wiki/Tf-idf).\n",
        "1. Normalize your per-document embeddings. Normalization is an important step to make embeddings comparable across the data.\n",
        "1. Visualize your embeddings. Embeddings are typically used in some downstream application, but visualization at this stage can be a nice sanity check before proceeding with further usage - have your embeddings actually captured information that reflect the underlying data?\n",
        "  * Your embeddings are probably high-dimensional vectors. Humans have a hard time visualizing things beyond 3 dimensions and honestly we can get away with 2 dimensions in most cases.\n",
        "  * There are many methods to do unsupervised dimensionality reduction. Some of the classical methods include principal component analysis (PCA), uniform manifold approximation and projection (UMAP), and t-distributed stochastic neighbor embedding (t-SNE). These are all fine methods for this homework as they are provided by existing packages.\n",
        "  * However, beware of the pitfalls of methods such as UMAP and t-SNE which are highly succeptible to the hyperparameters used with the underlying data. This is a nice post detailing these pitfalls, check out the mammoth figure: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/).\n",
        "  * In your visualization plot, it may be helpful to incorporate aspects of the metadata. We'll leave that open ended; visualize in a way that you think will help your discussion.\n",
        "1. After you're happy with your work, analyze your results, writeup what you've done, and submit the homework to Gradescope.\n",
        "  * Your submission should include 2 things:\n",
        "    1. Your writeup containing a figure with your embedding visualization.\n",
        "    1. Your notebook with your code for computing TF-IDF and generating the figure.\n",
        "  * Your writeup should be 0.5 to 1 page long. This length should be *before* including your figure. The text should be size 12pt, single spaced, with 1 inch margins, and on letter size paper. Please submit either a PDF or Word document.\n",
        "  * Some guiding questions: Have your embeddings actually captured underlying information about the articles? How can you tell? Why are some articles embedded closer to each other while others are not?\n",
        "\n",
        "**Tips and Tricks:**\n",
        "1. In general, you're welcome to use any tools you need for this homework. The only exceptions have been noted in the instructions.\n",
        "1. Reading CSVs can be done with `pandas`. We'll use `pandas` plenty more in the future so be sure to familiarize yourself with it.\n",
        "1. Extracting text from PDFs is relatively simple these days with [`pypdf`](https://github.com/py-pdf/pypdf).\n",
        "    * If you've used PyPDF2 in the past, that package has been merged back into and development has resumed on the original pypdf project. So make the switch back! This change was made around the end of 2022. You can see the release notes [here](https://github.com/py-pdf/pypdf/releases/tag/3.1.0).\n",
        "1. `numpy` will probably be useful in the normalization step.\n",
        "1. You should probably use `matplotlib` or derivative (e.g. `seaborn`) for visualization.\n",
        "1. If you're looking for guidance on any part of the homework or related topics, email Steven (songs1@uchicago.edu) or come to office hours! JCL 205 Wed 11a - 12p. Also open to scheduling 1:1 meetings if this time does not work for you, just email to ask."
      ],
      "metadata": {
        "id": "qW_R-TgIriQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "w4aMfZ0krmzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!wget https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n",
        "!tar -xzf hw1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLCvyGgskxHa",
        "outputId": "1f4ab366-cbdf-48c1-ce18-969ca3a1c86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m297.0/298.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n",
            "--2025-01-12 15:52:15--  https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/915385537/280fd7f6-f4a2-4024-ba9b-2111f384e9df?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250112%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250112T155216Z&X-Amz-Expires=300&X-Amz-Signature=17be89d35a3048cb1fda65ad61caf76a62024a2892380155db39aeb8cf15c7b3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dhw1.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-01-12 15:52:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/915385537/280fd7f6-f4a2-4024-ba9b-2111f384e9df?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250112%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250112T155216Z&X-Amz-Expires=300&X-Amz-Signature=17be89d35a3048cb1fda65ad61caf76a62024a2892380155db39aeb8cf15c7b3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dhw1.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59861235 (57M) [application/octet-stream]\n",
            "Saving to: ‘hw1.tar.gz’\n",
            "\n",
            "hw1.tar.gz          100%[===================>]  57.09M  66.7MB/s    in 0.9s    \n",
            "\n",
            "2025-01-12 15:52:16 (66.7 MB/s) - ‘hw1.tar.gz’ saved [59861235/59861235]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6kMkUasW6Aw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}