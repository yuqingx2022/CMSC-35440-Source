{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMSC 35440 Machine Learning in Biology and Medicine\n",
        "## Homework 1: Embedding Research Articles\n",
        "**Released**: Jan 14, 2025\n",
        "\n",
        "**Due**: Jan 24, 2025 at 11:59 PM Chicago Time on Gradescope\n",
        "\n",
        "**In this first homework, you'll generate embeddings for 20 provided research articles and visualize them.**\n",
        "\n",
        "At a high-level, embeddings are vectors computed by some algorithm or model that \"code\" information from data. Embeddings can be computed in a wide variety of different ways, from concatenating manually created features to using deep neural networks.\n",
        "\n",
        "For this homework, you will code text documents as vectors using the bag of words algorithm and normalize these vectors using the term-frequency inverse documentation frequency (TF-IDF) method. This method dates back over 50 years to 1972. Through this homework, hopefully we'll convince you that it's still very much relevant.\n",
        "\n",
        "Please carefully read through the instructions below. Also, while not required for the homework, the articles themselves are worth a read. They're some seminal papers across various domains around biomedical AI/ML.\n",
        "\n",
        "The starter notebook for this homework can be downloaded from GitHub:\n",
        "\n",
        "https://github.com/StevenSong/CMSC-35440-Source/blob/main/hw1/CMSC_35440_HW1_Student_Version.ipynb"
      ],
      "metadata": {
        "id": "K-gV2pmJcU79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "vfHBZQl0rauN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Download and open the starter notebook in your favorite Jupyter Notebook host. We recommend using [Google Colab](https://colab.research.google.com/).\n",
        "  * **NB:** We'll design all homeworks such that they can be run on the *free* tier of Colab. You're welcome to use any other host, but the benefit of Colab is that they offer free GPU-instances.\n",
        "  * Technically you don't need GPUs for any modeling but it can really speed it up. For homeworks where GPU-acceleration is recommended, we'll provide additional instructions on how to access GPU-instances on Colab.\n",
        "  * For this homework, we don't require the use of any GPUs.\n",
        "1. Download and unzip the research articles. We've provided them as a tarball that be downloaded from [https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz](https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz).\n",
        "  * You'll notice that there's a CSV of article metadata and a folder of article *PDFs*. While these articles are available elsewhere on the internet as extracted-text (come to office-hours if you're interested in using such a resource for other projects), real-world data is messy. One such way that data can be messy is that it only exists as PDFs - so **you must use the article PDFs for this assignment**.\n",
        "1. Extract the text from the articles. You should probably use some variables from the metadata at this step.\n",
        "1. Compute the term-document matrix and then normalize the term-document method using the TF-IDF method.  **You must implement TF-IDF yourself. You may not use any existing implementations for computing the TF-IDF matrix** (e.g. you can NOT use sklearn's function for TF-IDF).\n",
        "  * Defining what is a \"term\" is up to you but don't overcomplicate it. Splitting on whitespace characters works fine.\n",
        "  * The wikipedia is hopefully all you need to understand the formula: [https://en.wikipedia.org/wiki/Tf-idf](https://en.wikipedia.org/wiki/Tf-idf).\n",
        "1. Normalize your per-document embeddings. Normalization is an important step to make embeddings comparable across the data.\n",
        "1. Visualize your embeddings. Embeddings are typically used in some downstream application, but visualization at this stage can be a nice sanity check before proceeding with further usage - have your embeddings actually captured information that reflect the underlying data?\n",
        "  * Your embeddings are probably high-dimensional vectors. Humans have a hard time visualizing things beyond 3 dimensions and honestly we can get away with 2 dimensions in most cases.\n",
        "  * There are many methods to do unsupervised dimensionality reduction. Some of the classical methods include principal component analysis (PCA), uniform manifold approximation and projection (UMAP), and t-distributed stochastic neighbor embedding (t-SNE). These are all fine methods for this homework as they are provided by existing packages.\n",
        "  * However, beware of the pitfalls of methods such as UMAP and t-SNE which are highly succeptible to the hyperparameters used with the underlying data. This is a nice post detailing these pitfalls, check out the mammoth figure: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/).\n",
        "  * In your visualization plot, it may be helpful to incorporate aspects of the metadata. We'll leave that open ended; visualize in a way that you think will help your discussion.\n",
        "1. After you're happy with your work, analyze your results, writeup what you've done, and submit the homework to Gradescope.\n",
        "  * Your submission should include 2 things:\n",
        "    1. Your writeup containing a figure with your embedding visualization.\n",
        "    1. Your notebook with your code for computing TF-IDF and generating the figure.\n",
        "  * Your writeup should be 0.5 to 1 page long. This length should be *before* including your figure. The text should be size 12pt, single spaced, with 1 inch margins, and on letter size paper. Please submit either a PDF or Word document.\n",
        "  * Some guiding questions: Have your embeddings actually captured underlying information about the articles? How can you tell? Why are some articles embedded closer to each other while others are not?\n",
        "\n",
        "**Tips and Tricks:**\n",
        "1. In general, you're welcome to use any tools you need for this homework. The only exceptions have been noted in the instructions.\n",
        "1. Reading CSVs can be done with `pandas`. We'll use `pandas` plenty more in the future so be sure to familiarize yourself with it.\n",
        "1. Extracting text from PDFs is relatively simple these days with [`pypdf`](https://github.com/py-pdf/pypdf).\n",
        "    * If you've used PyPDF2 in the past, that package has been merged back into and development has resumed on the original pypdf project. So make the switch back! This change was made around the end of 2022. You can see the release notes [here](https://github.com/py-pdf/pypdf/releases/tag/3.1.0).\n",
        "1. `numpy` will probably be useful in the normalization step.\n",
        "1. You should probably use `matplotlib` or derivative (e.g. `seaborn`) for visualization.\n",
        "1. If you're looking for guidance on any part of the homework or related topics, email Steven (songs1@uchicago.edu) or come to office hours! JCL 205 Wed 11a - 12p. Also open to scheduling 1:1 meetings if this time does not work for you, just email to ask."
      ],
      "metadata": {
        "id": "qW_R-TgIriQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "w4aMfZ0krmzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!wget https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n",
        "!tar -xzf hw1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLCvyGgskxHa",
        "outputId": "98a9dba4-180a-452c-8f2c-38bf67dbfc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n",
            "--2025-01-14 20:08:33--  https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/915385537/280fd7f6-f4a2-4024-ba9b-2111f384e9df?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250114%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250114T200834Z&X-Amz-Expires=300&X-Amz-Signature=a0a5cd08d93d25670a7fe6c658489de8b582b71e28a0aa799693a9d1c0544505&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dhw1.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-01-14 20:08:34--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/915385537/280fd7f6-f4a2-4024-ba9b-2111f384e9df?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250114%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250114T200834Z&X-Amz-Expires=300&X-Amz-Signature=a0a5cd08d93d25670a7fe6c658489de8b582b71e28a0aa799693a9d1c0544505&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dhw1.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59861235 (57M) [application/octet-stream]\n",
            "Saving to: ‘hw1.tar.gz’\n",
            "\n",
            "hw1.tar.gz          100%[===================>]  57.09M  51.2MB/s    in 1.1s    \n",
            "\n",
            "2025-01-14 20:08:35 (51.2 MB/s) - ‘hw1.tar.gz’ saved [59861235/59861235]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "id": "5_2CGIYL96FD",
        "outputId": "fe4e69b0-015f-402e-a5b1-875c89d1ffe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/298.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pypdf\n",
        "import os\n",
        "from pypdf import PdfReader\n",
        "from sklearn.decomposition import PCA\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "k6kMkUasW6Aw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQhsbwuG8-v4",
        "outputId": "a41909a2-71cc-47b8-d75d-8ead13627439"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "6hkR_X1S-Pnl",
        "outputId": "f4fa1d36-00bc-4287-e345-f9c6ee899e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read in and check meta data sheet\n",
        "meta = pd.read_csv('hw1/article-metadata.csv')\n",
        "pdf_path = 'hw1/articles/'\n",
        "print(meta.head())\n",
        "print(type(meta))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7vC20Iyf9cQ",
        "outputId": "943fa598-a444-4f4d-9f1e-96a61b8e11a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  year  journal  topic  \\\n",
            "0  Neural networks and physical systems with emer...  1982     PNAS  model   \n",
            "1  Learning representations by back-propagating e...  1986   Nature  model   \n",
            "2  ImageNet Classification with Deep Convolutiona...  2012  NeurIPS  model   \n",
            "3                                      Deep learning  2015   Nature  model   \n",
            "4       Deep Residual Learning for Image Recognition  2016     CVPR    vis   \n",
            "\n",
            "      short_name  main_pages  \n",
            "0    neural-nets           5  \n",
            "1       backprop           4  \n",
            "2            cnn           8  \n",
            "3  deep-learning           7  \n",
            "4         resnet           8  \n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add a file name to the metadata chart\n",
        "meta['file_name'] = meta['short_name'] + '.pdf'\n",
        "print(meta.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br3HkLkznY0v",
        "outputId": "cf448e21-3ca1-4452-8865-6c1e6805582b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  year  journal  topic  \\\n",
            "0  Neural networks and physical systems with emer...  1982     PNAS  model   \n",
            "1  Learning representations by back-propagating e...  1986   Nature  model   \n",
            "2  ImageNet Classification with Deep Convolutiona...  2012  NeurIPS  model   \n",
            "3                                      Deep learning  2015   Nature  model   \n",
            "4       Deep Residual Learning for Image Recognition  2016     CVPR    vis   \n",
            "\n",
            "      short_name  main_pages          file_name  \n",
            "0    neural-nets           5    neural-nets.pdf  \n",
            "1       backprop           4       backprop.pdf  \n",
            "2            cnn           8            cnn.pdf  \n",
            "3  deep-learning           7  deep-learning.pdf  \n",
            "4         resnet           8         resnet.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "texts = []\n",
        "\n",
        "for _, row in meta.iterrows():\n",
        "    pdf_file = os.path.join(pdf_path, row['file_name'])\n",
        "    file_text = extract_text_from_pdf(pdf_file)\n",
        "    texts.append({\"file_name\": row['short_name'], \"text\": file_text})\n",
        "\n",
        "print(len(texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64C85EN3GRFv",
        "outputId": "4a85621d-4241-42c0-f263-ecae4a038e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StHlwdndHp7e",
        "outputId": "60802bfc-da8d-44ae-ab87-a89798cae54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'file_name': 'neural-nets', 'text': 'Proc. NatL Acad. Sci. USAVol. 79, pp. 2554-2558, April 1982\\nBiophysics\\nNeural networks and physical systems with emergent collective\\ncomputational abilities(associative memory/parallel processing/categorization/content-addressable memory/fail-soft devices)\\nJ. J. HOPFIELD\\nDivision of Chemistry and Biology, California Institute of Technology, Pasadena, California 91125; and Bell Laboratories, Murray Hill, New Jersey 07974\\nContributed by John J. Hopfweld, January 15, 1982\\nABSTRACT Computational properties of use to biological or-\\nganisms or to the construction of computers can emerge as col-\\nlective properties of systems -having a large number of simple\\nequivalent components (or neurons). The physical meaning of con-tent-addressable memory is described by an appropriate phase\\nspace flow of the state of a system. A model of such a system is\\ngiven, based on aspects of neurobiology but readily adapted to in-\\ntegrated circuits. The collective properties of this model producea content-addressable memory which correctly yields an entire\\nmemory from any subpart of sufficient size. The algorithm for the\\ntime evolution of the state of the system is based on asynchronous\\nparallel processing. Additional emergent collective properties in-\\nclude some capacity for generalization, familiarity recognition,\\ncategorization, error correction, and time sequence retention.\\nThe collective properties are only weakly sensitive to details of themodeling or the failure of individual devices.\\nGiven the dynamical electrochemical properties of neurons and\\ntheir interconnections (synapses), we readily understand schemes\\nthat use a few neurons to obtain elementary useful biological\\nbehavior (1-3). Our understanding of such simple circuits in\\nelectronics allows us to plan larger and more complex circuits\\nwhich are essential to large computers. Because evolution has\\nno such plan, it becomes relevant to ask whether the ability of\\nlarge collections of neurons to perform \"computational\" tasks\\nmay in part be a spontaneous collective consequence of having\\na large number of interacting simple neurons.\\nIn physical systems made from a large number of simple ele-\\nments, interactions among large numbers of elementary com-\\nponents yield collective phenomena such as the stable magnetic\\norientations and domains in a magnetic system or the vortex\\npatterns in fluid flow. Do analogous collective phenomena in\\na system of simple interacting neurons have useful \"computa-tional\" correlates? For example, are the stability of memories,\\nthe construction of categories of generalization, or time-se-\\nquential memory also emergent properties and collective in\\norigin? This paper examines a new modeling of this old and fun-\\ndamental question (4-8) and shows that important computa-\\ntional properties spontaneously arise.\\nAll modeling is based on details, and the details of neuro-\\nanatomy and neural function are both myriad and incompletely\\nknown (9). In many physical systems, the nature of the emer-\\ngent collective properties is insensitive to the details inserted\\nin the model (e.g., collisions are essential to generate soundwaves, but any reasonable interatomic force law will yield ap-\\npropriate collisions). In the same spirit, I will seek collective\\nproperties that are robust against change in the model details.\\nThe model could be readily implemented by integrated cir-\\ncuit hardware. The conclusions suggest the design of a delo-\\ncalized content-addressable memory or categorizer using ex-\\ntensive asynchronous parallel processing.\\nThe general content-addressable memory of a physical\\nsystem\\nSuppose that an item stored in memory is \"H. A. Kramers &\\nG. H. Wannier Phys. Rev. 60, 252 (1941).\" A general content-\\naddressable memory would be capable of retrieving this entire\\nmemory item on the basis of sufficient partial information. The\\ninput \"& Wannier, (1941)\" might suffice. An ideal memory\\ncould deal with errors and retrieve this reference even from the\\ninput \"Vannier, (1941)\". In computers, only relatively simple\\nforms of content-addressable memory have been made in hard-\\nware (10, 11). Sophisticated ideas like error correction in ac-\\ncessing information are usually introduced as software (10).\\nThere are classes of physical systems whose spontaneous be-\\nhavior can be used as a form of general (and error-correcting)\\ncontent-addressable memory. Consider the time evolution of\\na physical system that can be described by a set of general co-\\nordinates. A point in state space then represents the instanta-\\nneous condition of the system. This state space may be either\\ncontinuous or discrete (as in the case of N Ising spins).\\nThe equations of motion of the system describe a flow in state\\nspace. Various classes of flow patterns are possible, but the sys-\\ntems of use for memory particularly include those that flow to-\\nward locally stable points from anywhere within regions around\\nthose points. A particle with frictional damping moving in a\\npotential well with two minima exemplifies such a dynamics.\\nIf the flow is not completely deterministic, the description\\nis more complicated. In the two-well problems above, if the\\nfrictional force is characterized by atemperature, it must also\\nproduce a random driving force. The limit points become small\\nlimiting regions, and the stability becomes not absolute. But\\nas long as the stochastic effects are small, the essence of local\\nstable points remains.\\nConsider a physical system described by many coordinates\\nX1 XN, the components of a state vector X. Let the system\\nhave locally stable limit points Xa, Xb, **. Then, if the system\\nis started sufficiently near any Xa, as at X = Xa + A, it will\\nproceed in time until X Xa. We can regard the information\\nstored in the system as the vectors Xa, Xb, . The starting\\npoint X = Xa + A represents a partial knowledge of the item\\nXa, and the system then generates the total information Xa.Any physical system whose dynamics in phase space is dom-\\ninated by a substantial number of locally stable states to which\\nit is attracted can therefore be regarded as a general content-\\naddressable memory. The physical system will be a potentiallyuseful memory if, in addition, any prescribed set of states can\\nreadily be made the stable states of the system.\\nThe model system\\nThe processing devices will be called neurons. Each neuron i\\nhas two states like those of McCullough and Pitts (12): Vi = 0\\n2554\\nThe publication costs of this article were defrayed in part by page chargepayment. This article must therefore be hereby marked \"advertise-\\nment\" in accordance with 18 U. S. C. §1734 solely to indicate this fact.\\nDownloaded from https://www.pnas.org by 73.36.173.158 on January 11, 2025 from IP address 73.36.173.158.\\n\\nProc. Natl. Acad. Sci. USA 79 (1982) 2555\\n(\"not firing\") and Vi = 1 (\"firing at maximum rate\"). When neu-\\nron i has a connection made to it from neuron j, the strength\\nof connection is defined as Tij. (Nonconnected neurons have Tij0.) The instantaneous state of the system is specified by listing\\nthe N values of Vi, so it is represented by a binary word of N\\nbits.\\nThe state changes in time according to the following algo-\\nrithm. For each neuron i there is a fixed threshold U,. Each\\nneuron i readjusts its state randomly in time but with a mean\\nattempt rate W, setting\\nVi °1 < Ui ]Vi0if I T.,V.joi\\nThus, each neuron randomly and asynchronously evaluates\\nwhether it is above or below threshold and readjusts accord-\\ningly. (Unless otherwise stated, we choose Ui = 0.)\\nAlthough this model has superficial similarities to the Per-\\nceptron (13, 14) the essential differences are responsible for the\\nnew results. First, Perceptrons were modeled chiefly with\\nneural connections in a \"forward\" direction A -> B -* C -- D.\\nThe analysis of networks with strong backward coupling\\nproved intractable. All our interesting results arise\\nas consequences of the strong back-coupling. Second, Percep-\\ntron studies usually made a random net of neurons deal directly\\nwith a real physical world and did not ask the questions essential\\nto finding the more abstract emergent computational proper-\\nties. Finally, Perceptron modeling required synchronous neu-\\nrons like a conventional digital computer. There is no evidence\\nfor such global synchrony and, given the delays of nerve signal\\npropagation, there would be no way to use global synchrony\\neffectively. Chiefly computational properties which can exist\\nin spite of asynchrony have interesting implications in biology.\\nThe information storage algorithm\\nSuppose we wish to store the set of states V8, s = 1 n. We\\nuse the storage prescription (15, 16)\\nTij= (2V - 1)(2Vj - 1) [2]\\nS\\nbut with Tii = 0. From this definition\\nTijjs =E (2V, - 1) I VJ(2Vj-1) Hjs. [3]\\nThe mean value of the bracketed term in Eq. 3 is 0 unless s\\n- s\\', for which the mean is N/2. This pseudoorthogonality\\nyields\\n> TiVs (Hs\\') (2Vs\\' - 1) N/2\\ni\\n[4]\\nand is positive if VW\\' = 1 and negative if Vf\\' = 0. Except for the\\nnoise coming from the s # s\\' terms, the stored state would al-\\nways be stable under our processing algorithm.\\nSuch matrices T,. have been used in theories of linear asso-ciative nets (15-19) to produce an output pattern from a paired\\ninput stimulus, S1 -* 01. A second association S2 -° 02 can be\\nsimultaneously stored in the same network. But the confusing\\nsimulus 0.6 Si + 0.4 S2 will produce a generally meaningless\\nmixed output 0.6 01 + 0.4 02 Our model, in contrast, will use\\nits strong nonlinearity to make choices, produce categories, and\\nregenerate information and, with high probability, will generate\\nthe output 01 from such a confusing mixed stimulus.\\nA linear associative net must be connected in a complex way\\nwith an external nonlinear logic processor in order to yield true\\ncomputation (20, 21). Complex circuitry is easy to plan but more\\ndifficult to discuss in evolutionary terms. In contrast, our model\\nobtains its emergent computational properties from simpleproperties of many cells rather than circuitry.\\nThe biological interpretation of the model\\nMost neurons are capable of generating a train of action poten-\\ntials-propagating pulses of electrochemical activity-when the\\naverage potential across their membrane is held well above its\\nnormal resting value. The mean rate at which action potentialsare generated is a smooth function of the mean membrane po-\\ntential, having the general form shown in Fig. 1.\\nThe biological information sent to other neurons often lies\\nin a short-time average of the firing rate (22). When this is so,\\none can neglect the details of individual action potentials and\\nregard Fig. 1 as a smooth input-output relationship. [Parallelpathways carrying the same information would enhance the\\nability of the system to extract a short-term average firing rate\\n(23, 24).]\\nA study of emergent collective effects and spontaneous com-\\nputation must necessarily focus on the nonlinearity of the in-\\nput-output relationship. The essence of computation is nonlin-\\near logical operations. The particle interactions that produce\\ntrue collective effects in particle dynamics come from a nonlin-\\near dependence of forces on positions of the particles. Whereas\\nlinear associative networks have emphasized the linear central\\nregion (14-19) of Fig. 1, we will replace the input-output re-\\nlationship by the dot-dash step. Those neurons whose operation\\nis dominantly linear merely provide a pathway of communica-\\ntion between nonlinear neurons. Thus, we consider a network\\nof \"on or off\" neurons, granting that some of the interconnec-\\ntions may be by way of neurons operating in the linear regime.\\nDelays in synaptic transmission (of partially stochastic char-\\nacter) and in the transmission of impulses along axons and den-\\ndrites produce a delay between the input of a neuron and the\\ngeneration of an effective output. All such delays have been\\nmodeled by a single parameter, the stochastic mean processing\\ntime 1/W.\\nThe input to a particular neuron arises from the current leaks\\nof the synapses to that neuron, which influence the cell mean\\npotential. The synapses are activated by arriving action poten-\\ntials. The input signal to a cell i can be taken to be\\n[5]I Tijvj\\nwhere Tij represents the effectiveness of a synapse. Fig. 1 thus\\n/\\nQ ~~~~~~~~~/0 ,\\n P° I I\\'\\n0 a)-jaz -Present Model\\nW t --Linear Modelingw .\\'C\\nE -0.1 / 0\\nMembrane Potential (Volts) or \"Input\"\\nFIG. 1. Firing rate versus membrane voltage for a typical neuron\\n(solid line), dropping to 0 for large negative potentials and saturating\\nfor positive potentials. The broken lines show approximations used in\\nmodeling.\\nBiophysics: Hopfield\\nDownloaded from https://www.pnas.org by 73.36.173.158 on January 11, 2025 from IP address 73.36.173.158.\\n\\nProc. NatL Acad. Sci. USA 79 (1982)\\nbecomes an input-output relationship for a neuron.\\nLittle, Shaw, and Roney (8, 25, 26) have developed ideas on\\nthe collective functioning of neural nets based on \"on/off\" neu-\\nrons and synchronous processing. However, in their model the\\nrelative timing of action potential spikes was central and re-\\nsulted in reverberating action potential trains. Our model and\\ntheirs have limited formal similarity, although there may be\\nconnections at a deeper level.\\nMost modeling of neural learning networks has been based\\non synapses of a general type described by Hebb (27) and Eccles\\n(28). The essential ingredient is the modification of T; by cor-\\nrelations like\\nAT41 = [Vi(t)Vj(t)]average [6]\\nwhere the average is some appropriate calculation over past\\nhistory. Decay in time and effects of [Vi(t)]avg or [Vj(t)]avg are also\\nallowed. Model networks with such synapses (16, 20, 21) can\\nconstruct the associative T., of Eq. 2. We will therefore initiallyassume that such a Ty1has been produced by previous experi-\\nence (or inheritance). The Hebbian property need not reside\\nin single synapses; small groups of cells which produce such a\\nnet effect would suffice.\\nThe network of cells we describe performs an abstract cal-\\nculation and, for applications, the inputs should be appropri-\\nately coded. In visual processing, for example, feature extrac-\\ntion should previously have been done. The present modeling\\nmight then be related to how an entity or Gestalt is remembered\\nor categorized on the basis of inputs representing a collection\\nof its features.\\nStudies of the collective behaviors of the model\\nThe model has stable limit points. Consider the special case T\\n= Tji, and define\\nE =-2 TijVjVj [7]\\nioj\\nAE due to AV1 is given by\\nAE = -AVi Tij Vj [8]joi\\'\\nThus, the algorithm for altering Vi causes E to be a monotoni-\\ncally decreasing function. State changes will continue until a\\nleast (local) E is reached. This case is isomorphic with an Ising\\nmodel. Tij provides the role of the exchange coupling, and there\\nis also an external local field at each site. When T.jis symmetric\\nbut has a random character (the spin glass) there are known to\\nbe many (locally) stable states (29).\\nMonte Carlo calculations were made on systems of N = 30\\nand N = 100, to examine the effect of removing the T.1= T.\\nrestriction. Each element of T., was chosen as a random number\\nbetween -1 and 1. The neural architecture of typical cortical\\nregions (30, 31) and also of simple ganglia of invertebrates (32)\\nsuggests the importance of 100-10,000 cells with intense mu-\\ntual interconnections in elementary processing, so our scale of\\nN is slightly small.\\nThe dynamics algorithm was initiated from randomly chosen\\ninitial starting configurations. For N = 30 the system never\\ndisplayed an ergodic wandering through state space. Within a\\ntime of about 4/W it settled into limiting behaviors, the com-\\nmonest being a stable state. When 50 trials were examined for\\na particular such random matrix, all would result in one of two\\nor three end states. A few stable states thus collect the flow from\\nmost of the initial state space. A simple cycle also occurred oc-\\ncasionally-for example, . A -* B -- A -- B\\nThe third behavior seen was chaotic wandering in a small\\nregion of state space. The Hamming distance between two bi-\\nnary states A and B is defined as the number of places in which\\nthe digits are different. The chaotic wandering occurred within\\na short Hamming distance of one particular state. Statistics were\\ndone on the probability pi of the occurrence of a state in a time\\nof wandering around this minimum, and an entropic measure\\nof the available states M was taken\\n[9]\\nA value of M = 25 was found for N = 30. Theflow in phase space\\nproduced by this model algorithm has the properties necessary\\nfor a physical content-addressable memory whether or not T\\nis symmetric.\\nSimulations with N = 100 were much slower and not quan-\\ntitatively pursued. They showed qualitative similarity to N =\\n30.\\nWhy should stable limit points or regions persist when Tij# Tjj? If the algorithm at some time changes Vi from 0 to 1 or\\nvice versa, the change of the energy defined in Eq. 7 can be\\nsplit into two terms, one of which is always negative. The second\\nis identical if Ty1is symmetric and is \"stochastic\" with mean 0\\nif Tijand Tji are randomly chosen. The algorithm for Tij # Tj,\\ntherefore changes E in a fashion similar to the way E would\\nchange in time for a symmetric Tij but with an algorithm cor-\\nresponding to a finite temperature.\\nAbout 0.15 N states can be simultaneously remembered be-\\nfore error in recall is severe. Computer modeling of memory\\nstorage according to Eq. 2 was carried out for N = 30 and N\\n= 100. n random memory states were chosen and the corre-\\nsponding T.9 was generated. If a nervous system preprocessedsignals for efficient storage, the preprocessed information\\nwould appear random (e.g., the coding sequences of DNA have\\na random character). The random memory vectors thus simulate\\nefficiently encoded real information, as well as representing our\\nignorance. The system was started at each assigned nominal\\nmemory state, and the state was allowed to evolve until\\nstationary.\\nTypical results are shown in Fig. 2. The statistics are averages\\nover both the states in a given matrix and different matrices.\\nWith n = 5, the assigned memory states are almost always stable\\n(and exactly recallable). For n = 15, about half of the nominallyremembered states evolved to stable states with less than 5 er-\\nrors, but the rest evolved to states quite different from the start-\\ning points.\\nThese results can be understood from an analysis of the effect\\nof the noise terms. In Eq. 3, H\\' is the \"effective field\" on neuron\\ni when the state of the system is s\\', one of the nominal memory\\nstates. The expectation value of this sum, Eq. 4, is ±N/2 as\\nappropriate. The s # s\\' summation in Eq. 2 contributes no\\nmean, but has a rms noise of [(n - 1)N/2]\\'2- a. For nN large,this noise is approximately Gaussian and the probability of an\\nerror in a single particular bit of a particular memory will be\\nP = 1 e-x2/2a2 dx.\\n2 N/2\\n[10]\\nFor the case n = 10, N = 100, P = 0.0091, the probability that\\na state had no errors in its 100 bits should be about eC0O9\\' 0.40.\\nIn the simulation of Fig. 2, the experimental number was 0.6.\\nThe theoretical scaling of n with N at fixed P was demon-\\nstrated in the simulations going between N = 30 and N = 100.\\nThe experimental results of half the memories being well re-\\ntained at n = 0.15 N and the rest badly retained is expected to\\n2556 Biophysics: Hopfield\\nIn M= -2 pi In pi.\\nDownloaded from https://www.pnas.org by 73.36.173.158 on January 11, 2025 from IP address 73.36.173.158.\\n\\nProc. Natl. Acad. Sci. USA 79 (1982) 2557\\n1.0\\n0.5 N- 100\\n 0.5 n= 10\\nN= 100\\n2& 0.2\\n0.2 n=1\\n0.2 N= 100\\n3 6 9 10- 20- 30- 40- >49\\n19 29 39 49\\nNerr = Number of Errors in S tate\\nFIG. 2. The probability distribution of the occurrence of errors in\\nthe location of the stable states obtained from nominally assigned\\nmemories.\\nbe true for all large N. The information storage at a given level\\nof accuracy can be increased by a factor of 2 by a judicious choice\\nof individual neuron thresholds. This choice is equivalent to\\nusing variables ip = ±1, Tij = 1,,u4,4j, and a threshold level\\nof 0.\\nGiven some arbitrary starting state, what is the resulting final\\nstate (or statistically, states)? To study this, evolutions from ran-\\ndomly chosen initial states were tabulated for N = 30 and n\\n= 5. From the (inessential) symmetry of the algorithm, if\\n(101110 ) is an assigned stable state, (010001 .) is also stable.\\nTherefore, the matrices had 10 nominal stable states. Approx-\\nimately 85% of the trials ended in assigned memories, and 10%\\nended in stable states of no obvious meaning. An ambiguous 5%\\nlanded in stable states very near assigned memories. There was\\na range of a factor of 20 of the likelihood of finding these 10\\nstates.\\nThe algorithm leads to memories near the starting state. For\\nN = 30, n = 5, partially random starting states were generated\\nby random modification of known memories. The probability\\nthat the final state was that closest to the initial state was studied\\nas a function of the distance between the initial state and the\\nnearest memory state. For distance c 5, the nearest state was\\nreached more than 90% of the time. Beyond that distance, the\\nprobability fell off smoothly, dropping to a level of 0.2 (2 times\\nrandom chance) for a distance of 12.\\nThe phase space flow is apparently dominated by attractors\\nwhich are the nominally assigned memories, each of which dom-\\ninates a substantial region around it. The flow is not entirely\\ndeterministic, and the system responds to an ambiguous start-\\ning state by a statistical choice between the memory states it\\nmost resembles.\\nWere it desired to use such a system in an Si-based content-\\naddressable memory, the algorithm should be used and modi-\\nfied to hold the known bits of information while letting the oth-\\ners adjust.\\nThe model was studied by using a \"clipped\" Tij, replacing T4,in Eq. 3 by ± 1, the algebraic sign of Tij. The purposes were to\\nexamine the necessity of a linear synapse supposition (by makinga highly nonlinear one) and to examine the efficiency of storage.\\nOnly N(N/2) bits of information can possibly be stored in this\\nsymmetric matrix. Experimentally, for N = 100, n = 9, the level\\nof errors was similar to that for the ordinary algorithm at n =\\n12. The signal-to-noise ratio can be evaluated analytically for\\nthis clipped algorithm and is reduced by a factor of (2/r)1\"2 com-\\npared with the unclipped case. For a fixed error probability, the\\nnumber of memories must be reduced by 2/ir.With the 4 algorithm and the clipped Tij, both analysis and\\nmodeling showed that the maximal information stored for N\\n= 100 occurred at about n = 13. Some errors were present, and\\nthe Shannon information stored corresponded to about N(N/\\n8) bits.\\nNew memories can be continually added to Ti.. The addition\\nof new memories beyond the capacity overloads the system and\\nmakes all memory states irretrievable unless there is a provision\\nfor forgetting old memories (16, 27, 28).The saturation of the possible size of Tij will itself cause for-\\ngetting. Let the possible values of TY be 0, ± 1, ±2, ±3, and\\nTt, be freely incremented within this range. If Tij = 3, a next\\nincrement of +1 would be ignored and a next increment of\\n-1 would reduce Tij to 2. When Ty, is so constructed, only the\\nrecent memory states are retained, with a slightly increased\\nnoise level. Memories from the distant past are no longer stable.\\nHow far into the past are states remembered depends on the\\ndigitizing depth of T., and 0, , ±3 is an appropriate level for\\nN = 100. Other schemes can be used to keep too many mem-\\nories from being simultaneously written, but this particular one\\nis attractive because it requires no delicate balances and is a\\nconsequence of natural hardware.\\nReal neurons need not make synapses both of i -- j and j\\ni. Particular synapses are restricted to one sign of output. We\\ntherefore asked whether Tij = Tjjis important. Simulations were\\ncarried out with only one ij connection: if T- $0, T.i = 0. The\\nprobability of making errors increased, but the algorithm con-\\ntinued to generate stable minima. A Gaussian noise descriptionof the error rate shows that the signal-to-noise ratio for given\\nn and N should be decreased by the factor 1/F2, and the sim-\\nulations were consistent with such a factor. This same analysisshows that the system generally fails in a \"soft\" fashion, with\\nsignal-to-noise ratio and error rate increasing slowly as more\\nsynapses fail.\\nMemories too close to each other are confused and tend to\\nmerge. For N = 100, a pair of random memories should be sep-\\narated by 50 ± 5 Hamming units. The case N = 100, n = 8,was studied with seven random memories and the eighth made\\nup a Hamming distance of only 30, 20, or 10 from one of the\\nother seven memories. At a distance of 30, both similar mem-\\nories were usually stable. At a distance of 20, the minima were\\nusually distinct but displaced. At a distance of 10, the minima\\nwere often fused.\\nThe algorithm categorizes initial states according to the sim-\\nilarity to memory states. With a threshold of 0, the system be-\\nhaves as a forced categorizer.\\nThe state 00000 ... is always stable. For a threshold of 0, this\\nstable state is much higher in energy than the stored memory\\nstates and very seldom occurs. Adding a uniform threshold in\\nthe algorithm is equivalent to raising the effective energy of the\\nstored memories compared to the 0000 state, and 0000 also\\nbecomes a likely stable state. The 0000 state is then generatedby any initial state that does not resemble adequately closely\\none of the assigned memories and represents positive recog-\\nnition that the starting state is not familiar.\\nBiophysics: Hopfield\\nDownloaded from https://www.pnas.org by 73.36.173.158 on January 11, 2025 from IP address 73.36.173.158.\\n\\nProc. Natl. Acad. Sci. USA 79 (1982)\\nFamiliarity can be recognized by other means when the\\nmemory is drastically overloaded. We examined the case N\\n= 100, n = 500, in which there is a memory overload of a factor\\nof 25. None of the memory states assigned were stable. The ini-\\ntial rate of processing of a starting state is defined as the number\\nof neuron state readjustments that occur in a time 1/2W. Fa-\\nmiliar and unfamiliar states were distinguishable most of the\\ntime at this level of overload on the basis of the initial processing\\nrate, which was faster for unfamiliar states. This kind of famil-\\niarity can only be read out of the system by a class of neurons\\nor devices abstracting average properties of the processing\\ngroup.\\nFor the cases so far considered, the expectation value of Tijwas 0 for i # j. A set of memories can be stored with average\\ncorrelations, and Ty = Cij # 0 because there is a consistent in-\\nternal correlation in the memories. If now a partial new state\\nX is stored\\nATUj = (2Xi -1)(2Xj -1) Q~ -- k < N [ill\\nusing only k of the neurons rather than N, an attempt to re-\\nconstruct it will generate a stable point for all N neurons. The\\nvalues of Xk+l- *.XN that result will be determined primarily\\nfrom the sign of\\nk\\nE C ij Xj [12]j=1\\nand X is completed according to the mean correlations of the\\nother memories. The most effective implementation of this ca-\\npacity stores a large number of correlated matrices weakly fol-\\nlowed by a normal storage of X.\\nA nonsymmetric T.. can lead to the possibility that a minimum\\nwill be only metastable and will be replaced in time by another\\nminimum. Additional nonsymmetric terms which could be eas-\\nily generated by a minor modification of Hebb synapses\\nATU = A > (2Vs+1 - 1)(2Vj - 1) [13]\\nS\\nwere added to T . When A was judiciously adjusted, the system\\nwould spend a while near V. and then leave and go to a point\\nnear V,+,. But sequences longer than four states proved im-\\npossible to generate, and even these were not faithfullyfollowed.\\nDiscussion\\nIn the model network each \"neuron\" has elementary properties,\\nand the network has little structure. Nonetheless, collective\\ncomputational properties spontaneously arose. Memories are\\nretained as stable entities or Gestalts and can be correctly re-\\ncalled from any reasonably sized subpart. Ambiguities are re-\\nsolved on a statistical basis. Some capacity for generalization is\\npresent, and time ordering of memories can also be encoded.\\nThese properties follow from the nature of the flow in phase\\nspace produced by the processing algorithm, which does not\\nappear to be strongly dependent on precise details of the mod-\\neling. This robustness suggests that similar effects will obtain\\neven when more neurobiological details are added.\\nMuch of the architecture of regions of the brains of higher\\nanimals must be made from a proliferation of simple local cir-\\ncuits with well-defined functions. The bridge between simple\\ncircuits and the complex computational properties of higher\\nnervous systems may be the spontaneous emergence of new\\ncomputational capabilities from the collective behavior of large\\nnumbers of simple processing elements.\\nImplementation of a similar model by using integrated cir-\\ncuits would lead to chips which are much less sensitive to ele-\\nment failure and soft-failure than are normal circuits. Such chips\\nwould be wasteful of gates but could be made many times larger\\nthan standard designs at a given yield. Their asynchronous par-\\nallel processing capability would provide rapid solutions to some\\nspecial classes of computational problems.\\nThe work at California Institute of Technology was supported in part\\nby National Science Foundation Grant DMR-8107494. This is contri-\\nbution no. 6580 from the Division of Chemistry and Chemical\\nEngineering.\\n1. Willows, A. 0. D., Dorsett, D. A. & Hoyle, G. (1973) J. Neu-\\nrobiol 4, 207-237, 255-285.\\n2. Kristan, W. B. (1980) in Information Processing in the Nervous\\nSystem, eds. Pinsker, H. M. & Willis, W. D. (Raven, New York),\\n241-261.\\n3. Knight, B. W. (1975) Lect. Math. Life Sci. 5, 111-144.\\n4. Smith, D. R. & Davidson, C. H. (1962)J. Assoc. Comput. Mach.\\n9, 268-279.\\n5. Harmon, L. D. (1964) in Neural Theory and Modeling, ed. Reiss,R. F. (Stanford Univ. Press, Stanford, CA), pp. 23-24.\\n6. Amari, S.-I. (1977) Bio. Cybern. 26, 175-185.\\n7. Amari, S.-I. & Akikazu, T. (1978) Biol Cybern. 29, 127-136.\\n8. Little, W. A. (1974) Math. Biosci. 19, 101-120.\\n9. Marr, J. (1969) J. Physiol 202, 437-470.\\n10. Kohonen, T. (1980) Content Addressable Memories (Springer,New York).11. Palm, G. (1980) Biol Cybern. 36, 19-31.\\n12. McCulloch, W. S. & Pitts, W. (1943) BulL Math Biophys. 5,\\n115-133.\\n13. Minsky, M. & Papert, S. (1969) Perceptrons: An Introduction to\\nComputational Geometry (MIT Press, Cambridge, MA).14. Rosenblatt, F. (1962) Principles of Perceptrons (Spartan, Wash-\\nington, DC).15. Cooper, L. N. (1973) in Proceedings of the Nobel Symposium on\\nCollective Properties of Physical Systems, eds. Lundqvist, B. &\\nLundqvist, S. (Academic, New York), 252-264.\\n16. Cooper, L. N., Liberman, F. & Oja, E. (1979) Biol Cybern. 33,9-28.\\n17. Longuet-Higgins, J. C. (1968) Proc. Roy. Soc. London Ser. B 171,327-334.\\n18. Longuet-Higgins, J. C. (1968) Nature (London) 217, 104-105.\\n19. Kohonen, T. (1977) Associative Memory-A System-TheoreticApproach (Springer, New York).20. Willwacher, G. (1976) Biol Cybern. 24, 181-198.\\n21. Anderson, J. A. (1977) Psych. Rev. 84, 413-451.\\n22. Perkel, D. H. & Bullock, T. H. (1969) Neurosci. Res. Symp.Summ. 3, 405-527.\\n23. John, E. R. (1972) Science 177, 850-864.\\n24. Roney, K. J., Scheibel, A. B. & Shaw, G. L. (1979) Brain Res.\\nRev. 1, 225-271.\\n25. Little, W. A. & Shaw, G. L. (1978) Math. Biosci. 39, 281-289.\\n26. Shaw, G. L. & Roney, K. J. (1979) Phys. Rev. Lett. 74, 146-150.\\n27. Hebb, D. 0. (1949) The Organization of Behavior (Wiley, New\\nYork).28. Eccles, J. G. (1953) The Neurophysiological Basis of Mind (Clar-endon, Oxford).29. Kirkpatrick, S. & Sherrington, D. (1978) Phys. Rev. 17, 4384-4403.\\n30. Mountcastle, V. B. (1978) in The Mindful Brain, eds. Edelman,G. M. & Mountcastle, V. B. (MIT Press, Cambridge, MA), pp.\\n36-41.\\n31. Goldman, P. S. & Nauta, W. J. H. (1977) Brain Res. 122,393-413.32. Kandel, E. R. (1979) Sci. Am. 241, 61-70.\\n0 rl,2558 Biophysics: Hopfield\\nDownloaded from https://www.pnas.org by 73.36.173.158 on January 11, 2025 from IP address 73.36.173.158.\\n\\n'}, {'file_name': 'backprop', 'text': '©          Nature Publishing Group1986\\n_NA~TU_R_E_V_O_L_. 3_2_3 _9_0_CT_O_B_E_R_1_98_6 _________ LETTERSTONATURE ------------------=533 \\ndelineating the absolute indigeneity of amino acids in fossils. \\nAs AMS techniques are refined to handle smaller samples, it \\nmay also become possible to date individual amino acid enan\\xad\\ntiomers by the 14 C method. If one enantiomer is entirely derived \\nfrom the other by racemization during diagenesis, the individual \\nD- and L-enantiomers for a given amino acid should have \\nidentical 14 C ages. \\nOlder, more poorly preserved fossils may not always prove \\namenable to the determination of amino acid indigeneity by the \\nstable isotope method, as the prospects for complete replace\\xad\\nment of indigenous amino acids with non-indigenous amino \\nacids increases with time. As non-indigenous amino acids \\nundergo racemization, the enantiomers may have identical \\nisotopic compositions and still not be related to the original \\norganisms. Such a circumstance may, however, become easier \\nto recognize as more information becomes available concerning \\nthe distribution and stable isotopic composition of the amino \\nacid constituents of modern representatives of fossil organisms. \\nAlso, AMS dates on individual amino acid enantiomers may, \\nin some cases, help to clarify indigeneity problems, in particular \\nwhen stratigraphic controls can be used to estimate a general \\nage range for the fossil in question. \\nFinally, the development of techniques for determining the \\nstable isotopic composition of amino acid enantiomers may \\nenable us to establish whether non-racemic amino acids in some \\ncarbonaceous meteorites 27 are indigenous, or result in part from \\nterrestrial contamination. \\nM.H.E. thanks the NSF, Division of Earth Sciences (grant \\nEAR-8352055) and the following contributors to his Presidential \\nYoung Investigator Award for partial support of this research: \\nLearning representations \\nby back-propagating errors \\nDavid E. Rumelhart*, Geoffrey E. Hintont \\n& Ronald J. Williams* \\n* Institute for Cognitive Science, C-015, University of California, \\nSan Diego, La Jolla, California 92093, USA \\nt Department of Computer Science, Carnegie-Mellon University, \\nPittsburgh, Philadelphia 15213, USA \\nWe describe a new learning procedure, back-propagation, for \\nnetworks of neurone-like units. The procedure repeatedly adjusts \\nthe weights of the connections in the network so as to minimize a \\nmeasure of the difference between the actual output vector of the \\nnet and the desired output vector. As a result of the weight \\nadjustments, internal \\'hidden\\' units which are not part of the input \\nor output come to represent important features of the task domain, \\nand the regularities in the task are captured by the interactions \\nof these units. The ability to create useful new features distin\\xad\\nguishes back-propagation from earlier, simpler methods such as \\nthe perceptron-convergence procedure 1 • \\nThere have been many attempts to design self-organizing \\nneural networks. The aim is to find a powerful synaptic \\nmodification rule that will allow an arbitrarily connected neural \\nnetwork to develop an internal structure that is appropriate for \\na particular task domain. The task is specified by giving the \\ndesired state vector of the output units for each state vector of \\nthe input units. If the input units are directly connected to the \\noutput units it is relatively easy to find learning rules that \\niteratively adjust the relative strengths of the connections so as \\nto progressively reduce the difference between the actual and \\ndesired output vectors 2• Learning becomes more interesting but \\nt To whom correspondence should be addressed. \\nArco, Exxon, Phillips Petroleum, Texaco Inc., The Upjohn Co. \\nWe also acknowledge the donors of the Petroleum Research \\nFund, administered by the American Chemical Society (grant \\n16144-AC2 to M.H.E., grant 14805-AC2 to S.A.M.) for support. \\nS.A.M. acknowledges NSERC (grant A2644) for partial support. \\nReceived 19 May; accepted 15 July 1986. \\nI. Bada, J. L. & Protsch, R. Proc. natn. Acad. Set U.S.A 70, 1331-1334 (1973). \\n2. Bada, J. L., Schroeder, R. A. & Carter, G. F. Science 184, 791-793 (1974). \\n3. Boulton, G. S. et al. Nature 298, 437-441 (1982). \\n4. Wehmiller, J. F. in Quaternary Dating Methods (ed. Mahaney, W. C.) 171-193 (Elsevier, \\nAmsterdam, 1984). \\n5. Engel, M. H., Zumberge, J.E. & Nagy. B. Analyt. Biochem. 82, 415-422 (1977). \\n6. Bada, J. L. A Rev. Earth planet. Sci. 13, 241-268 (1985). \\n7. Chisholm, B. S., Nelson, D. E. & Schwarcz, H.P. Science 216, 1131-1132 (1982). \\n8. Ambrose, S. H. & DeNiro, M. J. Nature 319, 321-324 (1986). \\n9. Macko, S. A., Estep, M. L. F., Hare, P. E. & Haering, T. C. Yb. Carnegie lnstn Wash. 82, \\n404-410 (1983). \\n10. Hare, P. E. & Estep, M. L. F. Yb. Carnegie Instn Wash. 82, 410-414 (1983). \\n11. Engel, M. H. & Hare, P. E. in Chemistry and Biochemistry of the Ami,,o Acids ( ed. Barrett, \\nG. C.) 462-479 (Chapman and Hall, London, 1985). \\n12. Johnstone, R. A. W. & Rose, M. E. in Chemistry and Biochemistry of the Amino Acids (ed. \\nBarrett, G. C.) 480-524 (Chapman and Hall, London, 1985). \\n13. Weinstein, S., Engel, M. H. & Hare, P. E. in Practical Protein Chemistry-A Handbook ( ed. \\nDarbre, A.) 337-344 (Wiley, New York, 1986). \\n14. Bada, J. L., Gillespie, R., Gowlett, J. A. J. & Hedges, R. E. M. Nature 312, 442-444 (1984). \\n15. Mitterer, R. M. & Kriausakul, N. Org. Geochem. 7, 91-98 (1984). \\n16. Williams, K. M. & Smith, G. G. Origins Life 8, 91-144 (1977). \\n17. Engel, M. H. & Hare, P. E. Yb. Carnegie Instn Wash. 81, 425-430 (1982). \\n18. Hare, P. E. Yb. Carnegie Instn Wash. 73, 576-581 (1974). \\n19. Pillinger, C. T. Nature 296, 802 (1982). \\n20. Neuberger, A. Adv. Protein Chem. 4, 298-383 (1948). \\n21. Engel, M. H. & Macko, S. A. Analyt. Chem. 56, 2598-2600 (1984). \\n22. Dungworth, G. Chem. Geo/. 17, 135.-153 (1976). \\n23. Weinstein, S., Engel, M. H. & Hare, P. E. Analyt. Biochem. 121, 370-377 (1982). \\n24. Macko, S. A., Lee, W. Y. & Parker, P. L. J. exp. mar. Biol. Ecol 63, 145-149 (1982). \\n25. Macko, S. A., Estep, M. L. F. & Haering, T. C. Yb. Carnegie Instn Wash. 81, 413-417 (1982). \\n26. Vallentync, J. R. Geochim. cosmochim. Acta 28, 157-188 (1964). \\n27. Engel, M. H. & Nagy, B. Nature 296, 837-840 (1982). \\nmore difficult when we introduce hidden units whose actual or \\ndesired states are not specified by the task. (In perceptrons, \\nthere are \\'feature analysers\\' between the input and output that \\nare not true hidden units because their input connections are \\nfixed by hand, so their states are completely determined by the \\ninput vector: they do not learn representations.) The learning \\nprocedure must decide under what circumstances the hidden \\nunits should be active in order to help achieve the desired \\ninput-output behaviour. This amounts to deciding what these \\nunits should represent. We demonstrate that a general purpose \\nand relatively simple procedure is powerful enough to construct \\nappropriate internal representations. \\nThe simplest form of the learning procedure is for layered \\nnetworks which have a layer of input units at the bottom; any \\nnumber of intermediate layers; and a layer of output units at \\nthe top. Connections within a layer or from higher to lower \\nlayers are forbidden, but connections can skip intermediate \\nlayers. An input vector is presented to the network by setting \\nthe states of the input units. Then the states of the units in each \\nlayer are determined by applying equations (1) and (2) to the \\nconnections coming from lower layers. All units within a layer \\nhave their states set in parallel, but different layers have their \\nstates set sequentially, starting at the bottom and working \\nupwards until the states of the output units are determined. \\nThe total input, xi, to unitj is a linear function of the outputs, \\nYi, of the units that are connected to j and of the weights, wii• \\non these connections \\n(1) \\nUnits can be given biases by introducing an extra input to each \\nunit which always has a value of 1. The weight on this extra \\ninput is called the bias and is equivalent to a threshold of the \\nopposite sign. It can be treated just like the other weights. \\nA unit has a real-valued output, Yi, which is a non-linear \\nfunction of its total input \\n(2) \\n©          Nature Publishing Group1986\\n~534~----------------- LETTERS TO NATURE--------_:_:N::.:ATU=R=E-\\'Y\\'---=0-=L:....:. 3=23\\'---\\'--9 --=-OCT-=-=-=0-=B=ER\"-\\'-\\'19--\\'-\"86 \\n~-------.::..8·:_::8_~ f>.4 \"\"----=-8.:.::.8~------, \\nOutput unit \\n14.2 -14.2 \\n-3.f> 3.f> \\n7.2 -7.1 \\n-7.2 7.1 \\n3.f> -3.f> \\n-14.2 14.2 \\nInput units \\nFig. 1 A network that has learned to detect mirror symmetry in \\nthe input vector. The numbers on the arcs are weights and the \\nnumbers inside the nodes are biases. The learning required 1,425 \\nsweeps through the set of 64 possible input vectors, with the weights \\nbeing adjusted on the basis of the accumulated gradient after each \\nsweep. The values of the parameters in equation (9) were e = 0.1 \\nand a= 0.9. The initial weights were random and were uniformly \\ndistributed between -0.3 and 0.3. The key property of this solution \\nis that for a given hidden unit, weights that are symmetric about \\nthe middle of the input vector are equal in magnitude and opposite \\nin sign. So if a symmetrical pattern is presented, both hidden units \\nwill receive a net input of 0 from the input units, and, because the \\nhidden units have a negative bias, both will be off. In this case the \\noutput unit, having a positive bias, will be on. Note that the weights \\non each side of the midpoint are in the ratio 1 : 2: 4. This ensures \\nthat each of the eight patterns that can occur above the midpoint \\nsends a unique activation sum to each hidden unit, so the only \\npattern below the midpoint that can exactly balance this sum is \\nthe symmetrical one. For all non-symmetrical patterns, both hidden \\nunits will receive non-zero activations from the input units. The \\ntwo hidden units have identical patterns of weights but with \\nopposite signs, so for every non-symmetric pattern one hidden unit \\nwill come on and suppress the output unit. \\nIt is not necessary to use exactly the functions given in equations \\n(1) and (2). Any input-output function which has a bounded \\nderivative will do. However, the use of a linear function for \\ncombining the inputs to a unit before applying the nonlinearity \\ngreatly simplifies the learning procedure. \\nThe aim is to find a set of weights that ensure that for each \\ninput vector the output vector produced by the network is the \\nsame as (or sufficiently close to) the desired output vector. If \\nthere is a fixed, finite set of input-output cases, the total error \\nin the performance of the network with a particular set of weights \\ncan be computed by comparing the actual and desired output \\nvectors for every case. The total error, E, is defined as \\nE =½2: L (Yj,c-dj,c)2 (3) \\nC j \\nwhere c is an index over cases (input-output pairs), j is an \\nindex over output units, y is the actual state of an output unit \\nand d is its desired state. To minimize E by gradient descent \\nit is necessary to compute the partial derivative of E with respect \\nto each weight in the network. This is simply the sum of the \\npartial derivatives for each of the input-output cases, For a \\ngiven case, the partial derivatives of the error with respect to \\neach weight are computed in two passes. We have already \\ndescribed the forward pass in which the units in each layer have \\ntheir states determined by the input they receive from units in \\nlower layers using equations (1) and (2). The backward pass \\nwhich propagates derivatives from the top layer back to the \\nbottom one is more complicated. \\nChristopher = Penelope Andrew = Christine \\nI \\nMargaret = Arthur Victoria = James \\nI \\nJennifer = Charles \\nColin Charlotte \\nRoberto = Maria Pierro= Francesca \\nGina= Emilio Lucia = Marco Angela= Tomaso \\nI \\nAlfonso \\nI \\nI \\nSophia \\nFig. 2 Two isomorphic family trees. The information can be \\nexpressed as a set of triples of the form (person !)(relationship) \\n(person 2), where the possible relationships are {father, mother, \\nhusband, wife, son, daughter, uncle, aunt, brother, sister, nephew, \\nniece}. A layered net can be said to \\'know\\' these triples if it can \\nproduce the third term of each triple when given the first two. The \\nfirst two terms are encoded by activating two of the input units, \\nand the network must then complete the proposition by activating \\nthe output unit that represents the third term. \\nFig. 3 Activity levels in a five-layer network after it has learned. \\nThe bottom layer has 24 input units on the left for representing \\n(person 1) and 12 input units on the right for representing the \\nrelationship. The white squares inside these two groups show the \\nactivity levels of the units. There is one active unit in the first group \\nrepresenting Colin and one in the second group representing the \\nrelationship \\'has-aunt\\'. Each of the two input groups is totally \\nconnected to its own group of 6 units in the second layer. These \\ngroups learn to encode people and relationships as distributed \\npatterns of activity. The second layer is totally connected to the \\ncentral layer of 12 units, and these are connected to the penultimate \\nlayer of 6 units. The activity in the penultimate layer must activate \\nthe correct output units, each of which stands for a particular \\n(person 2). In this case, there are two correct answers (marked by \\nblack dots) because Colin has two aunts. Both the input units and \\nthe output units are laid out spatially with the English people in \\none row and the isomorphic Italians immediately below. \\nThe backward pass starts by computing aE/ay for each of \\nthe output units. Differentiating equation (3) for a particular \\ncase, c, and suppressing the index c gives \\naE/ay,=y,-d, (4) \\nWe can then apply the chain rule to compute aE/ax, \\naE/axi =aE/ayi·dy/dxi \\nDifferentiating equation (2) to get the value of dyi/ dx, and \\nsubstituting gives \\n(5) \\nThis means that we know how a change in the total input x to \\nan output unit will affect the error. But this total input is just a \\nlinear function of the states of the lower level units and it is \\nalso a linear function of the weights on the connections, so it \\nis easy to compute how the error will be affected by changing \\nthese states and weights. For a weight w,;, from i to j the \\nderivative is \\naE/aw,; =aE/ax,·axi/aw,; \\n=iJE/ax,· Y; (6) \\nand for the output of the i\\'h unit the contribution to aE/ay; \\n©          Nature Publishing Group1986\\nNc..cAc.ccTUc.c..ccR=E_V..ccO-=-L---\\'. 3=2c....3 --\\'---9---\"\\'0---\"-CT--\\'---O\"--\\'B=E=R\\'---\\'1\"---98\\'--\"6---------LETTERSTONATURE------------------- 53 _ 5 \\n.···• = - · - • -• °- a _--; ~ \" • -:__E\\': • \\n~ · - _- .- . • =- -= • -:;:__~a -;, ~-== \\n-~_;:_ • _-= =- \\'-~ --~_=--.=- ~ =- --• \\n----_0- - ~ _:: :..:- • •= :..=.. ::.• • - • • \\nFig. 4 The weights from the 24 input units that represent people \\nto the 6 units in the second layer that learn distributed representa\\xad\\ntions of people. White rectangles, excitatory weights; black rec\\xad\\ntangles, inhibitory weights; area of the rectangle encodes the mag\\xad\\nnitude of the weight. The weights from the 12 English people are \\nin the top row of each unit. Unit 1 is primarily concerned with the \\ndistinction between English and Italian and most of the other units \\nignore this distinction. This means that the representation of an \\nEnglish person is very similar to the representation of their Italian \\nequivalent. The network is making use of the isomorphism between \\nthe two family trees to allow it to share structure and it will therefore \\ntend to generalize sensibly from one tree to the other. Unit 2 \\nencodes which generation a person belongs to, and unit 6 encodes \\nwhich branch of the family they come from. The features captured \\nby the hidden units are not at all explicit in the input and output \\nencodings, since these use a separate unit for each person. Because \\nthe hidden features capture the underlying structure of the task \\ndomain, the network generalizes correctly to the four triples on \\nwhich it was not trained. We trained the network for 1500 sweeps, \\nusing e = 0.005 and a = 0.5 for the first 20 sweeps and E = 0.01 and \\na = 0.9 for the remaining sweeps. To make it easier to interpret \\nthe weights we introduced \\'weight-decay\\' by decrementing every \\nweight by 0.2 % after each weight change. After prolonged learning, \\nthe decay was balanced by aE I aw, so the final magnitude of each \\nweight indicates its usefulness in reducing the error. To prevent \\nthe network needing large weights to drive the outputs to 1 or 0, \\nthe error was considered to be zero if output units that should be \\non had activities above 0.8 and output units that should be off had \\nactivities below 0.2. \\nA set of \\ncorresponding \\nweights \\nFig. S A synchronous iterative net that is run for three iterations \\nand the equivalent layered net. Each time-step in the recurrent net \\ncorresponds to a layer in the layered net. The learning procedure \\nfor layered nets can be mapped into a learning procedure for \\niterative nets. Two complications arise in performing this mapping: \\nfirst, in a layered net the output levels of the units in the intermedi\\xad\\nate layers during the forward pass are required for performing the \\nbackward pass (see equations (5) and (6)). So in an iterative net \\nit is necessary to store the history of output states of each unit. \\nSecond, for a layered net to be equivalent to an iterative net, \\ncorresponding weights between different layers must have the same \\nvalue. To preserve this property, we average i!E/aw for all the \\nweights in each set of corresponding weights and then change each \\nweight in the set by an amount proportional to this average gradient. \\nWith these two provisos, the learning procedure can be applied \\ndirectly to iterative nets. These nets can then either learn to perform \\niterative searches or learn sequential structures 4• \\n3 \\nresulting from the effect of i on j is simply \\naE/avaxjjayj =a-E/axj· wji \\nso taking into account all the connections emanating from unit \\ni we have \\n(7) \\nWe have now seen how to compute aE/ay for any unit in the \\npenultimate layer when given aE/ay for all units in the last \\nlayer. We can therefore repeat this procedure to compute this \\nterm for successively earlier layers, computing aE / aw for the \\nweights as we go. \\nOne way of using aE / aw is to change the weights after every \\ninput-output case. This has the advantage that no separate \\nmemory is required for the derivatives. An alternative scheme, \\nwhich we used in the research reported here, is to accumulate \\naE/aw over all the input-output cases before changing the \\nweights. The simplest version of gradient descent is to change \\neach weight by an amount proportional to the accumulated \\naE/aw \\nA.w=-eaE/aw (8) \\nThis method does not converge as rapidly as methods which \\nmake use of the second derivatives, but it is much simpler and \\ncan easily be implemented by local computations in parallel \\nhardware. It can be significantly improved, without sacrificing \\nthe simplicity and locality, by using an acceleration method in \\nwhich the current gradient is used to modify the velocity of the \\npoint in weight space instead of its position \\nA. w(t) = -eaEjaw(t) + aA.w(t -1) (9) \\nwhere t is incremented by 1 for each sweep through the whole \\nset of input-output cases, and a is an exponential decay factor \\nbetween O and 1 that determines the relative contribution of the \\ncurrent gradient and earlier gradients to the weight change. \\nTo break symmetry we start with small random weights. \\nVariants on the learning procedure have been discovered \\nindependently by David Parker (personal communication) and \\nby Yann Le Cun3. \\nOne simple task that cannot be done by just connecting the \\ninput units to the output units is the detection of symmetry. To \\ndetect whether the binary activity levels of a one-dimensional \\narray of input units are symmetrical about the centre point, it \\nis essential to use an intermediate layer because the activity in \\nan individual input unit, considered alone, provides no evidence \\nabout the symmetry or non-symmetry of the whole input vector, \\nso simply adding up the evidence from the individual input \\nunits is insufficient. (A more formal proof that intermediate \\nunits are required is given in ref. 2.) The learning procedure \\ndiscovered an elegant solution using just two intermediate units, \\nas shown in Fig. 1. \\nAnother interesting task is to store the information in the two \\nfamily trees (Fig. 2). Figure 3 shows the network we used, and \\nFig. 4 shows the \\'receptive fields\\' of some of the hidden units \\nafter the network was trained on 100 of the 104 possible triples. \\nSo far, we have only dealt with layered, feed-forward \\nnetworks. The equivalence between layered networks and recur\\xad\\nrent networks that are run iteratively is shown in Fig. 5. \\nThe most obvious drawback of the learning procedure is that \\nthe error-surface may contain local minima so that gradient \\ndescent is not guaranteed to find a global minimum. However, \\nexperience with many tasks shows that the network very rarely \\ngets stuck in poor local minima that are significantly worse than \\nthe global minimum. We have only encountered this undesirable \\nbehaviour in networks that have just enough connections to \\nperform the task. Adding a few more connections creates extra \\ndimensions in weight-space and these dimensions provide paths \\naround the barriers that create poor local minima in the lower \\ndimensional subspaces. \\n©          Nature Publishing Group1986\\n_S 36 __________________ LETTERSTO NATURE _________ N_ATU_R_E_V_O_L._3_23_9_0C_T_O_B_ER_19_86 \\nThe learning procedure, in its current form, is not a plausible \\nmodel of learning in brains. However, applying the procedure \\nto various tasks shows that interesting internal representations \\ncan be constructed by gradient descent in weight-space, and \\nthis suggests that it is worth looking for more biologically \\nplausible ways of doing gradient descent in neural networks. \\nWe thank the System Development Foundation and the Office \\nof Naval Research for financial support. \\nReceived I May; accepted 31 July 1986. \\n1. Rosenblatt, F. Principles of Neurodynamics {Spartan, Washington, DC, 1961). \\n2. Minsky. M. L. & Papert, S. Perceptrons (MIT, Cambridge, 1969). \\n3. Le Cun, Y. Proc. Cognitiva 85, 599-604 (1985). \\n4. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. in Parallel Distributed Processing: \\nExplorations in the Microstructure of Cognition. Vol. 1: Foundations (eds Rumclhart, D. E. \\n& McClelland, J. L.) 318-362 (MIT, Cambridge, 1986). \\nBilateral amblyopia after a short \\nperiod of reverse occlusion in kittens \\nKathryn M. Murphy* & Donald E. Mitchell \\nDepartment of Psychology, Dalhousie University, \\nHalifax Nova Scotia, Canada B3H 4Jl \\nThe majority of neurones in the visual cortex of both adult cats \\nand kittens can be excited by visual stimulation of either eye. \\nNevertheless, if one eye is deprived of patterned vision early in \\nlife, most cortical cells can only be activated by visual stimuli \\npresented to the nondeprived eye and behaviourally the deprived \\neye is apparently useless 1 \\'2 , Although the consequences of \\nmonocular deprivation can be severe, they can in many circum\\xad\\nstances be rapidly reversed with the early implementation of reverse \\nocclusion which forces the use of the initially deprived eye 3•4• \\nHowever, by itself reverse occlusion does not restore a normal \\ndistribution of cortical occular dominance 3 and only promotes \\nvisual recovery in one eye 5 •6 • In an effort to find a procedure that \\nmight restore good binocular vision, we have examined the effects \\non acuity and cortical ocular dominance of a short, but physiologi\\xad\\ncally optimal period of reverse occlusion, followed by a period of \\nbinocular vision beginning at 7.5 weeks of age. Surprisingly, despite \\nthe early introduction of binocular vision, both eyes attained \\nacuities that were only approximately 1/3 of normal acuity levels. \\nDespite the severe bilateral amblyopia, cortical ocular dominance \\nappeared similar to that of normal cats. This is the first demonstra\\xad\\ntion of severe bilateral amblyopia following consecutive periods \\nof monocular occlusion. \\nNine kittens were used, of which eight were monocularly \\ndeprived by eyelid suture from about the time of natural eye \\nopening (6 to 11 days) until 5 weeks of age, at which time the \\ninitially deprived eye was opened and the other eye was sutured \\nclosed for 18 days. Physiological recordings from area 17 were \\nmade from one normal control and from five monocularly\\xad\\ndeprived kittens, one immediately after reverse occlusion (as a \\ncontrol); the remaining four after a further 4 weeks at least \\n(range 4-8 weeks) of normal binocular vision. Grating acuity \\nthresholds were determined for both eyes of a further three \\nkittens (subjected to the same regime-monocular deprivation, \\n18 days reverse suturing, followed by normal binocular vision) \\nby use of a jumping stand 5 • 7 • None of the kittens tested \\nbehaviourally were examined physiologically. Single unit \\nrecordings were made in area 17 of the anaesthetized, paralysed \\nkittens ( one normal, five experimental) with glass coated \\nplatinum-iridium electrodes. Anaesthesia was induced by \\n• Present address: School of Optometry, Univen1ity of California, Berkeley, California 94720, \\nUSA. \\n6 \\n5 \\n4 \\n3 \\n4 \\n3 \\n2 \\n0 \\n0 \\n• 0 \\n0 \\n00 \\n:, OOC9 • \\n: \"· .... \\n\\'o \\'O - - - -\\nC155 \\n00 \\n• • • • \\n0 10 20 30 40 50 60 70 \\nC164 \\n• 0 \\n\\'0 1()- - - ------------- -------- ._ ________ _ \\n0 10 20 30 40 50 60 70 \\nDays since termination \\nof reverse occlusion \\nFig. 1. Changes in visual acuity during the period of binocular \\nvision for two kittens (C155 and Cl64) that were previously \\nmonocularly deprived until 5 weeks of age, and then reverse \\noccluded for 18 days. e, Acuity of the initially deprived eye; 0, \\nacuity of the initially nondeprived eye. \\nintravenous pentothal and maintained by artificial respiration \\nwith 70% N 20 and 30% 0 2 supplemented with intravenous \\nNembutal; EEG, EKG, body temperature, and expired CO 2 \\nlevels were monitored.The eyes were brought to focus on a \\ntangent screen 137 cm distant from the kitten using contact \\nlenses with 3 mm artificial pupils. Single units were recorded \\nalong one long penetration in area 17 down the medial bank of \\nthe postlateral gyrus in each hemisphere, always beginning in \\nthe hemisphere contralateral to the initially open eye. Receptive \\nfields were sampled according to established procedures 8, every \\n100 µm along the penetration in a cortical region corresponding \\nto the horizontal meridian of visual space. All units were located \\nwithin 15° of the area centralis, with the majority within 5°. \\nThe longitudinal changes in visual acuity of both eyes follow\\xad\\ning introduction of binocular vision are shown in Fig. 1 for two \\nrepresentative kittens. At the end of 18 days of reverse occlusion \\nthe vision of the initially deprived eye had recovered to only \\nrudimentary levels (1-2.5 cycles per degree) while at the same \\ntime the initially nondeprived eye had been rendered blind. \\nDuring the subsequent period of binocular visual exposure the \\nvision of both eyes improved slightly, but only to a very limited \\nextent (to between 1.7 and 3.4 cycles per degree). The results \\nfrom the third animal were very similar. After more than 2 \\nmonths of binocular exposure the acuities of the initially \\ndeprived and nondeprived eyes were respectively, 2.54 and 3.35 \\ncycles per degree. Surprisingly, after 2 months of binocular \\nvision, the acuity of both eyes of these animals remained at \\nabout one-third to one-half of normal levels 6 • Although the \\ninitially deprived eye was opened at the peak of the sensitive \\nperiod (5 weeks of age) and the initially nondeprived eye was \\nclosed for a relatively brief period of time (18 days), this depriva\\xad\\ntion regimen had a devastiating and permanent effect upon the \\nvisual acuity of both eyes. \\n'}, {'file_name': 'cnn', 'text': 'ImageNet Classiﬁcation with Deep Convolutional\\nNeural Networks\\nAlex Krizhevsky\\nUniversity of Toronto\\nkriz@cs.utoronto.ca\\nIlya Sutskever\\nUniversity of Toronto\\nilya@cs.utoronto.ca\\nGeoffrey E. Hinton\\nUniversity of Toronto\\nhinton@cs.utoronto.ca\\nAbstract\\nWe trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof ﬁve convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a ﬁnal 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efﬁcient GPU implemen-\\ntation of the convolution operation. To reduce overﬁtting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n1 Introduction\\nCurrent approaches to object recognition make essential use of machine learning methods. To im-\\nprove their performance, we can collect larger datasets, learn more powerful models, and use bet-\\nter techniques for preventing overﬁtting. Until recently, datasets of labeled images were relatively\\nsmall — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and\\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\\nespecially if they are augmented with label-preserving transformations. For example, the current-\\nbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].\\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to col-\\nlect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\\nover 15 million labeled high-resolution images in over 22,000 categories.\\nTo learn about thousands of objects from millions of images, we need a model with a large learning\\ncapacity. However, the immense complexity of the object recognition task means that this prob-\\nlem cannot be speciﬁed even by a dataset as large as ImageNet, so our model should also have lots\\nof prior knowledge to compensate for all the data we don’t have. Convolutional neural networks\\n(CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\\ntrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\\nabout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\\nmuch fewer connections and parameters and so they are easier to train, while their theoretically-best\\nperformance is likely to be only slightly worse.\\n1\\nDespite the attractive qualities of CNNs, and despite the relative efﬁciency of their local architecture,\\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\\ncontain enough labeled examples to train such models without severe overﬁtting.\\nThe speciﬁc contributions of this paper are as follows: we trained one of the largest convolutional\\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\\ntraining convolutional neural networks, which we make available publicly 1. Our network contains\\na number of new and unusual features which improve its performance and reduce its training time,\\nwhich are detailed in Section 3. The size of our network made overﬁtting a signiﬁcant problem, even\\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\\noverﬁtting, which are described in Section 4. Our ﬁnal network contains ﬁve convolutional and\\nthree fully-connected layers, and this depth seems to be important: we found that removing any\\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\\ninferior performance.\\nIn the end, the network’s size is limited mainly by the amount of memory available on current GPUs\\nand by the amount of training time that we are willing to tolerate. Our network takes between ﬁve\\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\\n2 The Dataset\\nImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000\\ncategories. The images were collected from the web and labeled by human labelers using Ama-\\nzon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object\\nChallenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge\\n(ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of\\n1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and\\n150,000 testing images.\\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is\\nthe version on which we performed most of our experiments. Since we also entered our model in\\nthe ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as\\nwell, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates:\\ntop-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label\\nis not among the ﬁve labels considered most probable by the model.\\nImageNet consists of variable-resolution images, while our system requires a constant input dimen-\\nsionality. Therefore, we down-sampled the images to a ﬁxed resolution of 256 ×256. Given a\\nrectangular image, we ﬁrst rescaled the image such that the shorter side was of length 256, and then\\ncropped out the central256×256 patch from the resulting image. We did not pre-process the images\\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\\nwe trained our network on the (centered) raw RGB values of the pixels.\\n3 The Architecture\\nThe architecture of our network is summarized in Figure 2. It contains eight learned layers —\\nﬁve convolutional and three fully-connected. Below, we describe some of the novel or unusual\\nfeatures of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of\\ntheir importance, with the most important ﬁrst.\\n1http://code.google.com/p/cuda-convnet/\\n2\\n3.1 ReLU Nonlinearity\\nFigure 1: A four-layer convolutional neural\\nnetwork with ReLUs (solid line) reaches a 25%\\ntraining error rate on CIFAR-10 six times faster\\nthan an equivalent network with tanh neurons\\n(dashed line). The learning rates for each net-\\nwork were chosen independently to make train-\\ning as fast as possible. No regularization of\\nany kind was employed. The magnitude of the\\neffect demonstrated here varies with network\\narchitecture, but networks with ReLUs consis-\\ntently learn several times faster than equivalents\\nwith saturating neurons.\\nThe standard way to model a neuron’s output f as\\na function of its input x is with f(x) = tanh( x)\\nor f(x) = (1 +e−x)−1. In terms of training time\\nwith gradient descent, these saturating nonlinearities\\nare much slower than the non-saturating nonlinearity\\nf(x) = max(0,x). Following Nair and Hinton [20],\\nwe refer to neurons with this nonlinearity as Rectiﬁed\\nLinear Units (ReLUs). Deep convolutional neural net-\\nworks with ReLUs train several times faster than their\\nequivalents with tanh units. This is demonstrated in\\nFigure 1, which shows the number of iterations re-\\nquired to reach 25% training error on the CIFAR-10\\ndataset for a particular four-layer convolutional net-\\nwork. This plot shows that we would not have been\\nable to experiment with such large neural networks for\\nthis work if we had used traditional saturating neuron\\nmodels.\\nWe are not the ﬁrst to consider alternatives to tradi-\\ntional neuron models in CNNs. For example, Jarrett\\net al. [11] claim that the nonlinearityf(x) =|tanh(x)|\\nworks particularly well with their type of contrast nor-\\nmalization followed by local average pooling on the\\nCaltech-101 dataset. However, on this dataset the pri-\\nmary concern is preventing overﬁtting, so the effect\\nthey are observing is different from the accelerated\\nability to ﬁt the training set which we report when us-\\ning ReLUs. Faster learning has a great inﬂuence on the\\nperformance of large models trained on large datasets.\\n3.2 Training on Multiple GPUs\\nA single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks\\nthat can be trained on it. It turns out that 1.2 million training examples are enough to train networks\\nwhich are too big to ﬁt on one GPU. Therefore we spread the net across two GPUs. Current GPUs\\nare particularly well-suited to cross-GPU parallelization, as they are able to read from and write to\\none another’s memory directly, without going through host machine memory. The parallelization\\nscheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one\\nadditional trick: the GPUs communicate only in certain layers. This means that, for example, the\\nkernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input\\nonly from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of\\nconnectivity is a problem for cross-validation, but this allows us to precisely tune the amount of\\ncommunication until it is an acceptable fraction of the amount of computation.\\nThe resultant architecture is somewhat similar to that of the “columnar” CNN employed by Cire¸ san\\net al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1\\nand top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many\\nkernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time\\nto train than the one-GPU net2.\\n2The one-GPU net actually has the same number of kernels as the two-GPU net in the ﬁnal convolutional\\nlayer. This is because most of the net’s parameters are in the ﬁrst fully-connected layer, which takes the last\\nconvolutional layer as input. So to make the two nets have approximately the same number of parameters, we\\ndid not halve the size of the ﬁnal convolutional layer (nor the fully-conneced layers which follow). Therefore\\nthis comparison is biased in favor of the one-GPU net, since it is bigger than “half the size” of the two-GPU\\nnet.\\n3\\n3.3 Local Response Normalization\\nReLUs have the desirable property that they do not require input normalization to prevent them\\nfrom saturating. If at least some training examples produce a positive input to a ReLU, learning will\\nhappen in that neuron. However, we still ﬁnd that the following local normalization scheme aids\\ngeneralization. Denoting by ai\\nx,y the activity of a neuron computed by applying kernel iat position\\n(x,y) and then applying the ReLU nonlinearity, the response-normalized activity bi\\nx,y is given by\\nthe expression\\nbi\\nx,y = ai\\nx,y/\\n\\uf8eb\\n\\uf8edk+ α\\nmin(N−1,i+n/2)∑\\nj=max(0,i−n/2)\\n(aj\\nx,y)2\\n\\uf8f6\\n\\uf8f8\\nβ\\nwhere the sum runs over n“adjacent” kernel maps at the same spatial position, and N is the total\\nnumber of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined\\nbefore training begins. This sort of response normalization implements a form of lateral inhibition\\ninspired by the type found in real neurons, creating competition for big activities amongst neuron\\noutputs computed using different kernels. The constants k,n,α , and βare hyper-parameters whose\\nvalues are determined using a validation set; we used k = 2, n= 5, α= 10−4, and β = 0.75. We\\napplied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).\\nThis scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11],\\nbut ours would be more correctly termed “brightness normalization”, since we do not subtract the\\nmean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%,\\nrespectively. We also veriﬁed the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer\\nCNN achieved a 13% test error rate without normalization and 11% with normalization3.\\n3.4 Overlapping Pooling\\nPooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel\\nmap. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g.,\\n[17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling\\nunits spaced spixels apart, each summarizing a neighborhood of size z×zcentered at the location\\nof the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed\\nin CNNs. If we set s < z, we obtain overlapping pooling. This is what we use throughout our\\nnetwork, with s = 2and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and\\n0.3%, respectively, as compared with the non-overlapping scheme s = 2,z = 2, which produces\\noutput of equivalent dimensions. We generally observe during training that models with overlapping\\npooling ﬁnd it slightly more difﬁcult to overﬁt.\\n3.5 Overall Architecture\\nNow we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net\\ncontains eight layers with weights; the ﬁrst ﬁve are convolutional and the remaining three are fully-\\nconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces\\na distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression\\nobjective, which is equivalent to maximizing the average across training cases of the log-probability\\nof the correct label under the prediction distribution.\\nThe kernels of the second, fourth, and ﬁfth convolutional layers are connected only to those kernel\\nmaps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\\nconvolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-\\nconnected layers are connected to all neurons in the previous layer. Response-normalization layers\\nfollow the ﬁrst and second convolutional layers. Max-pooling layers, of the kind described in Section\\n3.4, follow both response-normalization layers as well as the ﬁfth convolutional layer. The ReLU\\nnon-linearity is applied to the output of every convolutional and fully-connected layer.\\nThe ﬁrst convolutional layer ﬁlters the224×224×3 input image with 96 kernels of size11×11×3\\nwith a stride of 4 pixels (this is the distance between the receptive ﬁeld centers of neighboring\\n3We cannot describe this network in detail due to space constraints, but it is speciﬁed precisely by the code\\nand parameter ﬁles provided here: http://code.google.com/p/cuda-convnet/.\\n4\\nFigure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities\\nbetween the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-parts\\nat the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and\\nthe number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264–\\n4096–4096–1000.\\nneurons in a kernel map). The second convolutional layer takes as input the (response-normalized\\nand pooled) output of the ﬁrst convolutional layer and ﬁlters it with 256 kernels of size 5 ×5 ×48.\\nThe third, fourth, and ﬁfth convolutional layers are connected to one another without any intervening\\npooling or normalization layers. The third convolutional layer has 384 kernels of size 3 ×3 ×\\n256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth\\nconvolutional layer has 384 kernels of size 3 ×3 ×192 , and the ﬁfth convolutional layer has 256\\nkernels of size 3 ×3 ×192. The fully-connected layers have 4096 neurons each.\\n4 Reducing Overﬁtting\\nOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC\\nmake each training example impose 10 bits of constraint on the mapping from image to label, this\\nturns out to be insufﬁcient to learn so many parameters without considerable overﬁtting. Below, we\\ndescribe the two primary ways in which we combat overﬁtting.\\n4.1 Data Augmentation\\nThe easiest and most common method to reduce overﬁtting on image data is to artiﬁcially enlarge\\nthe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms\\nof data augmentation, both of which allow transformed images to be produced from the original\\nimages with very little computation, so the transformed images do not need to be stored on disk.\\nIn our implementation, the transformed images are generated in Python code on the CPU while the\\nGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,\\ncomputationally free.\\nThe ﬁrst form of data augmentation consists of generating image translations and horizontal reﬂec-\\ntions. We do this by extracting random224 ×224 patches (and their horizontal reﬂections) from the\\n256×256 images and training our network on these extracted patches4. This increases the size of our\\ntraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-\\ndependent. Without this scheme, our network suffers from substantial overﬁtting, which would have\\nforced us to use much smaller networks. At test time, the network makes a prediction by extracting\\nﬁve 224 ×224 patches (the four corner patches and the center patch) as well as their horizontal\\nreﬂections (hence ten patches in all), and averaging the predictions made by the network’s softmax\\nlayer on the ten patches.\\nThe second form of data augmentation consists of altering the intensities of the RGB channels in\\ntraining images. Speciﬁcally, we perform PCA on the set of RGB pixel values throughout the\\nImageNet training set. To each training image, we add multiples of the found principal components,\\n4This is the reason why the input images in Figure 2 are 224 × 224 × 3-dimensional.\\n5\\nwith magnitudes proportional to the corresponding eigenvalues times a random variable drawn from\\na Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel Ixy =\\n[IR\\nxy,IG\\nxy,IB\\nxy]T we add the following quantity:\\n[p1,p2,p3][α1λ1,α2λ2,α3λ3]T\\nwhere pi and λi are ith eigenvector and eigenvalue of the 3 ×3 covariance matrix of RGB pixel\\nvalues, respectively, and αi is the aforementioned random variable. Each αi is drawn only once\\nfor all the pixels of a particular training image until that image is used for training again, at which\\npoint it is re-drawn. This scheme approximately captures an important property of natural images,\\nnamely, that object identity is invariant to changes in the intensity and color of the illumination. This\\nscheme reduces the top-1 error rate by over 1%.\\n4.2 Dropout\\nCombining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efﬁcient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\nWe use dropout in the ﬁrst two fully-connected layers of Figure 2. Without dropout, our network ex-\\nhibits substantial overﬁtting. Dropout roughly doubles the number of iterations required to converge.\\nFigure 3: 96 convolutional kernels of size\\n11×11×3 learned by the ﬁrst convolutional\\nlayer on the224×224×3 input images. The\\ntop 48 kernels were learned on GPU 1 while\\nthe bottom 48 kernels were learned on GPU\\n2. See Section 6.1 for details.\\n5 Details of learning\\nWe trained our models using stochastic gradient descent\\nwith a batch size of 128 examples, momentum of 0.9, and\\nweight decay of 0.0005. We found that this small amount\\nof weight decay was important for the model to learn. In\\nother words, weight decay here is not merely a regularizer:\\nit reduces the model’s training error. The update rule for\\nweight wwas\\nvi+1 := 0 .9 ·vi −0.0005 ·ϵ·wi −ϵ·\\n⟨∂L\\n∂w\\n⏐⏐\\nwi\\n⟩\\nDi\\nwi+1 := wi + vi+1\\nwhere iis the iteration index, vis the momentum variable, ϵis the learning rate, and\\n⣨\\n∂L\\n∂w\\n⏐⏐\\nwi\\n⟩\\nDi\\nis\\nthe average over the ith batch Di of the derivative of the objective with respect to w, evaluated at\\nwi.\\nWe initialized the weights in each layer from a zero-mean Gaussian distribution with standard de-\\nviation 0.01. We initialized the neuron biases in the second, fourth, and ﬁfth convolutional layers,\\nas well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates\\nthe early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron\\nbiases in the remaining layers with the constant 0.\\nWe used an equal learning rate for all layers, which we adjusted manually throughout training.\\nThe heuristic which we followed was to divide the learning rate by 10 when the validation error\\nrate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and\\n6\\nreduced three times prior to termination. We trained the network for roughly 90 cycles through the\\ntraining set of 1.2 million images, which took ﬁve to six days on two NVIDIA GTX 580 3GB GPUs.\\n6 Results\\nOur results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5\\ntest set error rates of 37.5% and 17.0%5. The best performance achieved during the ILSVRC-\\n2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced\\nfrom six sparse-coding models trained on different features [2], and since then the best pub-\\nlished results are 45.7% and 25.7% with an approach that averages the predictions of two classi-\\nﬁers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\\nModel Top-1 Top-5\\nSparse coding [2] 47.1% 28.2%\\nSIFT + FVs [24] 45.7% 25.7%\\nCNN 37.5% 17.0%\\nTable 1: Comparison of results on ILSVRC-\\n2010 test set. In italics are best results\\nachieved by others.\\nWe also entered our model in the ILSVRC-2012 com-\\npetition and report our results in Table 2. Since the\\nILSVRC-2012 test set labels are not publicly available,\\nwe cannot report test error rates for all the models that\\nwe tried. In the remainder of this paragraph, we use\\nvalidation and test error rates interchangeably because\\nin our experience they do not differ by more than 0.1%\\n(see Table 2). The CNN described in this paper achieves\\na top-5 error rate of 18.2%. Averaging the predictions\\nof ﬁve similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth con-\\nvolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release\\n(15M images, 22K categories), and then “ﬁne-tuning” it on ILSVRC-2012 gives an error rate of\\n16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 re-\\nlease with the aforementioned ﬁve CNNs gives an error rate of 15.3%. The second-best con-\\ntest entry achieved an error rate of 26.2% with an approach that averages the predictions of sev-\\neral classiﬁers trained on FVs computed from different types of densely-sampled features [7].\\nModel Top-1 (val) Top-5 (val) Top-5 (test)\\nSIFT + FVs [7] — — 26.2%\\n1 CNN 40.7% 18.2% —\\n5 CNNs 38.1% 16.4% 16.4%\\n1 CNN* 39.0% 16.6% —\\n7 CNNs* 36.7% 15.4% 15.3%\\nTable 2: Comparison of error rates on ILSVRC-2012 validation and\\ntest sets. In italics are best results achieved by others. Models with an\\nasterisk* were “pre-trained” to classify the entire ImageNet 2011 Fall\\nrelease. See Section 6 for details.\\nFinally, we also report our error\\nrates on the Fall 2009 version of\\nImageNet with 10,184 categories\\nand 8.9 million images. On this\\ndataset we follow the convention\\nin the literature of using half of\\nthe images for training and half\\nfor testing. Since there is no es-\\ntablished test set, our split neces-\\nsarily differs from the splits used\\nby previous authors, but this does\\nnot affect the results appreciably.\\nOur top-1 and top-5 error rates\\non this dataset are 67.4% and\\n40.9%, attained by the net described above but with an additional, sixth convolutional layer over the\\nlast pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].\\n6.1 Qualitative Evaluations\\nFigure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The\\nnetwork has learned a variety of frequency- and orientation-selective kernels, as well as various col-\\nored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connec-\\ntivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels\\non on GPU 2 are largely color-speciﬁc. This kind of specialization occurs during every run and is\\nindependent of any particular random weight initialization (modulo a renumbering of the GPUs).\\n5The error rates without averaging predictions over ten patches as described in Section 4.1 are 39.0% and\\n18.3%.\\n7\\nFigure 4: (Left) Eight ILSVRC-2010 test images and the ﬁve labels considered most probable by our model.\\nThe correct label is written under each image, and the probability assigned to the correct label is also shown\\nwith a red bar (if it happens to be in the top 5).(Right) Five ILSVRC-2010 test images in the ﬁrst column. The\\nremaining columns show the six training images that produce feature vectors in the last hidden layer with the\\nsmallest Euclidean distance from the feature vector for the test image.\\nIn the left panel of Figure 4 we qualitatively assess what the network has learned by computing its\\ntop-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the\\ntop-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example,\\nonly other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry)\\nthere is genuine ambiguity about the intended focus of the photograph.\\nAnother way to probe the network’s visual knowledge is to consider the feature activations induced\\nby an image at the last, 4096-dimensional hidden layer. If two images produce feature activation\\nvectors with a small Euclidean separation, we can say that the higher levels of the neural network\\nconsider them to be similar. Figure 4 shows ﬁve images from the test set and the six images from\\nthe training set that are most similar to each of them according to this measure. Notice that at the\\npixel level, the retrieved training images are generally not close in L2 to the query images in the ﬁrst\\ncolumn. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\\nresults for many more test images in the supplementary material.\\nComputing similarity by using Euclidean distance between two 4096-dimensional, real-valued vec-\\ntors is inefﬁcient, but it could be made efﬁcient by training an auto-encoder to compress these vectors\\nto short binary codes. This should produce a much better image retrieval method than applying auto-\\nencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency\\nto retrieve images with similar patterns of edges, whether or not they are semantically similar.\\n7 Discussion\\nOur results show that a large, deep convolutional neural network is capable of achieving record-\\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\\nthat our network’s performance degrades if a single convolutional layer is removed. For example,\\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\\nnetwork. So the depth really is important for achieving our results.\\nTo simplify our experiments, we did not use any unsupervised pre-training even though we expect\\nthat it will help, especially if we obtain enough computational power to signiﬁcantly increase the\\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\\nfar, our results have improved as we have made our network larger and trained it longer but we still\\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\\nsequences where the temporal structure provides very helpful information that is missing or far less\\nobvious in static images.\\n8\\nReferences\\n[1] R.M. Bell and Y . Koren. Lessons from the netﬂix prize challenge.ACM SIGKDD Explorations Newsletter,\\n9(2):75–79, 2007.\\n[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.image-\\nnet.org/challenges. 2010.\\n[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\\n[4] D. Cire¸ san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\\nArxiv preprint arXiv:1202.2745, 2012.\\n[5] D.C. Cire¸ san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural\\nnetworks for visual object classiﬁcation. Arxiv preprint arXiv:1102.0183, 2011.\\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09, 2009.\\n[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL\\nhttp://www.image-net.org/challenges/LSVRC/2012/.\\n[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An\\nincremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-\\ning, 106(1):59–70, 2007.\\n[9] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-\\nfornia Institute of Technology, 2007. URL http://authors.library.caltech.edu/7694.\\n[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural net-\\nworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\\n[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y . LeCun. What is the best multi-stage architecture for\\nobject recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.\\n[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of\\nComputer Science, University of Toronto, 2009.\\n[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.\\n[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In\\nESANN, 2011.\\n[15] Y . Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-\\nwritten digit recognition with a back-propagation network. In Advances in neural information processing\\nsystems, 1990.\\n[16] Y . LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to\\npose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the\\n2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.\\n[17] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In\\nCircuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256.\\nIEEE, 2010.\\n[18] H. Lee, R. Grosse, R. Ranganath, and A.Y . Ng. Convolutional deep belief networks for scalable unsuper-\\nvised learning of hierarchical representations. InProceedings of the 26th Annual International Conference\\non Machine Learning, pages 609–616. ACM, 2009.\\n[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classiﬁ-\\ncation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer\\nVision, Florence, Italy, October 2012.\\n[20] V . Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. 27th\\nInternational Conference on Machine Learning, 2010.\\n[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computa-\\ntional biology, 4(1):e27, 2008.\\n[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering\\ngood forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,\\n2009.\\n[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for\\nimage annotation. International journal of computer vision, 77(1):157–173, 2008.\\n[24] J. Sánchez and F. Perronnin. High-dimensional signature compression for large-scale image classiﬁcation.\\nIn Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,\\n2011.\\n[25] P.Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to\\nvisual document analysis. In Proceedings of the Seventh International Conference on Document Analysis\\nand Recognition, volume 2, pages 958–962, 2003.\\n[26] S.C. Turaga, J.F. Murray, V . Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-\\nvolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computation,\\n22(2):511–538, 2010.\\n9\\n'}, {'file_name': 'deep-learning', 'text': '1Facebook AI Research, 770 Broadway, New York, New York 10003 USA. 2New York University, 715 Broadway, New York, New York 10003, USA. 3Department of Computer Science and Operations \\nResearch Université de Montréal, Pavillon André-Aisenstadt, PO Box 6128  Centre-Ville STN Montréal, Quebec H3C 3J7, Canada. 4Google, 1600 Amphitheatre Parkway, Mountain View, California \\n94043, USA. 5Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada.\\nM\\nachine-learning technology powers many aspects of modern \\nsociety: from web searches to content filtering on social net-\\nworks to recommendations on e-commerce websites, and \\nit is increasingly present in consumer products such as cameras and \\nsmartphones. Machine-learning systems are used to identify objects \\nin images, transcribe speech into text, match news items, posts or \\nproducts with users’ interests, and select relevant results of search. \\nIncreasingly, these applications make use of a class of techniques called \\ndeep learning. \\nConventional machine-learning techniques were limited in their \\nability to process natural data in their raw form. For decades, con -\\nstructing a pattern-recognition or machine-learning system required \\ncareful engineering and considerable domain expertise to design a fea-\\nture extractor that transformed the raw data (such as the pixel values \\nof an image) into a suitable internal representation or feature vector \\nfrom which the learning subsystem, often a classifier, could detect or \\nclassify patterns in the input. \\nRepresentation learning is a set of methods that allows a machine to \\nbe fed with raw data and to automatically discover the representations \\nneeded for detection or classification. Deep-learning methods are \\nrepresentation-learning methods with multiple levels of representa-\\ntion, obtained by composing simple but non-linear modules that each \\ntransform the representation at one level (starting with the raw input) \\ninto a representation at a higher, slightly more abstract level. With the \\ncomposition of enough such transformations, very complex functions \\ncan be learned. For classification tasks, higher layers of representation \\namplify aspects of the input that are important for discrimination and \\nsuppress irrelevant variations. An image, for example, comes in the \\nform of an array of pixel values, and the learned features in the first \\nlayer of representation typically represent the presence or absence of \\nedges at particular orientations and locations in the image. The second \\nlayer typically detects motifs by spotting particular arrangements of \\nedges, regardless of small variations in the edge positions. The third \\nlayer may assemble motifs into larger combinations that correspond \\nto parts of familiar objects, and subsequent layers would detect objects \\nas combinations of these parts. The key aspect of deep learning is that \\nthese layers of features are not designed by human engineers: they \\nare learned from data using a general-purpose learning procedure. \\nDeep learning is making major advances in solving problems that \\nhave resisted the best attempts of the artificial intelligence commu-\\nnity for many years. It has turned out to be very good at discovering \\nintricate structures in high-dimensional data and is therefore applica-\\nble to many domains of science, business and government. In addition \\nto beating records in image recognition1–4 and speech recognition5–7, it \\nhas beaten other machine-learning techniques at predicting the activ-\\nity of potential drug molecules8, analysing particle accelerator data9,10, \\nreconstructing brain circuits11, and predicting the effects of mutations \\nin non-coding DNA on gene expression and disease12,13. Perhaps more \\nsurprisingly, deep learning has produced extremely promising results \\nfor various tasks in natural language understanding 14, particularly \\ntopic classification, sentiment analysis, question answering15 and lan-\\nguage translation16,17. \\nWe think that deep learning will have many more successes in the \\nnear future because it requires very little engineering by hand, so it \\ncan easily take advantage of increases in the amount of available com-\\nputation and data. New learning algorithms and architectures that are \\ncurrently being developed for deep neural networks will only acceler-\\nate this progress. \\nSupervised learning \\nThe most common form of machine learning, deep or not, is super-\\nvised learning. Imagine that we want to build a system that can classify \\nimages as containing, say, a house, a car, a person or a pet. We first \\ncollect a large data set of images of houses, cars, people and pets, each \\nlabelled with its category. During training, the machine is shown an \\nimage and produces an output in the form of a vector of scores, one \\nfor each category. We want the desired category to have the highest \\nscore of all categories, but this is unlikely to happen before training. \\nWe compute an objective function that measures the error (or dis-\\ntance) between the output scores and the desired pattern of scores. The \\nmachine then modifies its internal adjustable parameters to reduce \\nthis error. These adjustable parameters, often called weights, are real \\nnumbers that can be seen as ‘knobs’ that define the input–output func-\\ntion of the machine. In a typical deep-learning system, there may be \\nhundreds of millions of these adjustable weights, and hundreds of \\nmillions of labelled examples with which to train the machine. \\nTo properly adjust the weight vector, the learning algorithm com-\\nputes a gradient vector that, for each weight, indicates by what amount \\nthe error would increase or decrease if the weight were increased by a \\ntiny amount. The weight vector is then adjusted in the opposite direc-\\ntion to the gradient vector. \\nThe objective function, averaged over all the training examples, can \\nDeep learning allows computational models that are composed of multiple processing layers to learn representations of \\ndata with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech rec-\\nognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep \\nlearning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine \\nshould change its internal parameters that are used to compute the representation in each layer from the representation in \\nthe previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and \\naudio, whereas recurrent nets have shone light on sequential data such as text and speech. \\nDeep learning\\nY ann LeCun1,2, Y oshua Bengio3 & Geoffrey Hinton4,5\\n436 | NATURE | VOL 521 | 28 MAY 2015\\nREVIEW\\ndoi:10.1038/nature14539\\n© 2015 Macmillan Publishers Limited. All rights reserved\\nbe seen as a kind of hilly landscape in the high-dimensional space of \\nweight values. The negative gradient vector indicates the direction \\nof steepest descent in this landscape, taking it closer to a minimum, \\nwhere the output error is low on average. \\nIn practice, most practitioners use a procedure called stochastic \\ngradient descent (SGD). This consists of showing the input vector \\nfor a few examples, computing the outputs and the errors, computing \\nthe average gradient for those examples, and adjusting the weights \\naccordingly. The process is repeated for many small sets of examples \\nfrom the training set until the average of the objective function stops \\ndecreasing. It is called stochastic because each small set of examples \\ngives a noisy estimate of the average gradient over all examples. This \\nsimple procedure usually finds a good set of weights surprisingly \\nquickly when compared with far more elaborate optimization tech-\\nniques18. After training, the performance of the system is measured \\non a different set of examples called a test set. This serves to test the \\ngeneralization ability of the machine — its ability to produce sensible \\nanswers on new inputs that it has never seen during training. \\nMany of the current practical applications of machine learning use \\nlinear classifiers on top of hand-engineered features. A two-class linear \\nclassifier computes a weighted sum of the feature vector components. \\nIf the weighted sum is above a threshold, the input is classified as \\nbelonging to a particular category. \\nSince the 1960s we have known that linear classifiers can only carve \\ntheir input space into very simple regions, namely half-spaces sepa-\\nrated by a hyperplane19. But problems such as image and speech recog-\\nnition require the input–output function to be insensitive to irrelevant \\nvariations of the input, such as variations in position, orientation or \\nillumination of an object, or variations in the pitch or accent of speech, \\nwhile being very sensitive to particular minute variations (for example, \\nthe difference between a white wolf and a breed of wolf-like white \\ndog called a Samoyed). At the pixel level, images of two Samoyeds in \\ndifferent poses and in different environments may be very different \\nfrom each other, whereas two images of a Samoyed and a wolf in the \\nsame position and on similar backgrounds may be very similar to each \\nother. A linear classifier, or any other ‘shallow’ classifier operating on \\nFigure 1 | Multilayer neural networks and backpropagation. a, A multi-\\nlayer neural network (shown by the connected dots) can distort the input \\nspace to make the classes of data (examples of which are on the red and \\nblue lines) linearly separable. Note how a regular grid (shown on the left) \\nin input space is also transformed (shown in the middle panel) by hidden \\nunits. This is an illustrative example with only two input units, two hidden \\nunits and one output unit, but the networks used for object recognition \\nor natural language processing contain tens or hundreds of thousands of \\nunits. Reproduced with permission from C. Olah (http://colah.github.io/). \\nb, The chain rule of derivatives tells us how two small effects (that of a small \\nchange of x on y, and that of y on z) are composed. A small change Δx in \\nx gets transformed first into a small change Δy in y by getting multiplied \\nby ∂y/∂x (that is, the definition of partial derivative). Similarly, the change \\nΔy creates a change Δz in z. Substituting one equation into the other \\ngives the chain rule of derivatives — how Δx gets turned into Δz through \\nmultiplication by the product of ∂y/∂x and ∂z/∂x. It also works when x, \\ny and z are vectors (and the derivatives are Jacobian matrices). c, The \\nequations used for computing the forward pass in a neural net with two \\nhidden layers and one output layer, each constituting a module through \\nwhich one can backpropagate gradients. At each layer, we first compute \\nthe total input z to each unit, which is a weighted sum of the outputs of \\nthe units in the layer below. Then a non-linear function f(.) is applied to \\nz to get the output of the unit. For simplicity, we have omitted bias terms. \\nThe non-linear functions used in neural networks include the rectified \\nlinear unit (ReLU) f(z) = max(0,z), commonly used in recent years, as \\nwell as the more conventional sigmoids, such as the hyberbolic tangent, \\nf(z) = (exp(z) − exp(−z))/(exp(z) + exp(−z)) and logistic function logistic, \\nf(z) = 1/(1 + exp(−z)). d, The equations used for computing the backward \\npass. At each hidden layer we compute the error derivative with respect to \\nthe output of each unit, which is a weighted sum of the error derivatives \\nwith respect to the total inputs to the units in the layer above. We then \\nconvert the error derivative with respect to the output into the error \\nderivative with respect to the input by multiplying it by the gradient of f(z). \\nAt the output layer, the error derivative with respect to the output of a unit \\nis computed by differentiating the cost function. This gives yl − tl if the cost \\nfunction for unit l is 0.5(yl − tl)2, where tl is the target value. Once the ∂E/∂zk \\nis known, the error-derivative for the weight wjk on the connection from \\nunit j in the layer below is just yj ∂E/∂zk.\\nInput\\n(2)\\nOutput\\n(1 sigmoid)\\nHidden\\n(2 sigmoid)\\na b\\ndc\\ny\\ny x\\ny x\\uf0b6\\n\\uf0b6=y\\nz\\n\\uf0b6\\n\\uf0b6\\nx\\ny\\n\\uf0b6\\n\\uf0b6\\nz y\\nzz y\\uf0b6\\n\\uf0b6=/uni0394/uni0394\\n/uni0394/uni0394\\n/uni0394/uni0394z y\\nz\\nx\\ny x\\uf0b6\\n\\uf0b6\\n\\uf0b6\\n\\uf0b6=\\nx\\nz\\ny\\nz\\nxx\\ny\\n\\uf0b6\\n\\uf0b6\\n\\uf0b6\\n\\uf0b6\\n\\uf0b6\\n\\uf0b6=\\nCompare outputs with correct \\nanswer to get error derivatives\\nj\\nk\\nE\\nyl\\n=yl tl\\nE\\nzl\\n= E\\nyl\\nyl\\nzl\\nl\\nE\\nyj\\n= wjk\\nE\\nzk\\nE\\nzj\\n= E\\nyj\\nyj\\nzj\\nE\\nyk\\n= wkl\\nE\\nzl\\nE\\nzk\\n= E\\nyk\\nyk\\nzk\\nwkl\\nwjk\\nwij\\ni\\nj\\nk\\nyl = f (zl )\\nzl = wkl yk\\nl\\nyj = f (zj )\\nzj = wij xi\\nyk = f (zk )\\nzk = wjk yj\\nOutput units \\nInput units \\nHidden units H2 \\nHidden units H1 \\nwkl\\nwjk\\nwij\\nk \\uf065 H2\\nk \\uf065 H2\\nI \\uf065 out\\nj \\uf065 H1\\ni \\uf065 Input\\ni\\n28 MAY 2015 | VOL 521 | NATURE | 437\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\nraw pixels could not possibly distinguish the latter two, while putting \\nthe former two in the same category. This is why shallow classifiers \\nrequire a good feature extractor that solves the selectivity–invariance \\ndilemma — one that produces representations that are selective to \\nthe aspects of the image that are important for discrimination, but \\nthat are invariant to irrelevant aspects such as the pose of the animal. \\nTo make classifiers more powerful, one can use generic non-linear \\nfeatures, as with kernel methods20, but generic features such as those \\narising with the Gaussian kernel do not allow the learner to general-\\nize well far from the training examples21. The conventional option is \\nto hand design good feature extractors, which requires a consider -\\nable amount of engineering skill and domain expertise. But this can \\nall be avoided if good features can be learned automatically using a \\ngeneral-purpose learning procedure. This is the key advantage of \\ndeep learning. \\nA deep-learning architecture is a multilayer stack of simple mod -\\nules, all (or most) of which are subject to learning, and many of which \\ncompute non-linear input–output mappings. Each module in the \\nstack transforms its input to increase both the selectivity and the \\ninvariance of the representation. With multiple non-linear layers, say \\na depth of 5 to 20, a system can implement extremely intricate func-\\ntions of its inputs that are simultaneously sensitive to minute details \\n— distinguishing Samoyeds from white wolves — and insensitive to \\nlarge irrelevant variations such as the background, pose, lighting and \\nsurrounding objects. \\nBackpropagation to train multilayer architectures \\nFrom the earliest days of pattern recognition22,23, the aim of research-\\ners has been to replace hand-engineered features with trainable \\nmultilayer networks, but despite its simplicity, the solution was not \\nwidely understood until the mid 1980s. As it turns out, multilayer \\narchitectures can be trained by simple stochastic gradient descent. \\nAs long as the modules are relatively smooth functions of their inputs \\nand of their internal weights, one can compute gradients using the \\nbackpropagation procedure. The idea that this could be done, and \\nthat it worked, was discovered independently by several different \\ngroups during the 1970s and 1980s24–27.  \\nThe backpropagation procedure to compute the gradient of an \\nobjective function with respect to the weights of a multilayer stack \\nof modules is nothing more than a practical application of the chain \\nrule for derivatives. The key insight is that the derivative (or gradi -\\nent) of the objective with respect to the input of a module can be \\ncomputed by working backwards from the gradient with respect to \\nthe output of that module (or the input of the subsequent module) \\n(Fig.\\xa01). The backpropagation equation can be applied repeatedly to \\npropagate gradients through all modules, starting from the output \\nat the top (where the network produces its prediction) all the way to \\nthe bottom (where the external input is fed). Once these gradients \\nhave been computed, it is straightforward to compute the gradients \\nwith respect to the weights of each module. \\nMany applications of deep learning use feedforward neural net -\\nwork architectures (Fig. 1), which learn to map a fixed-size input \\n(for example, an image) to a fixed-size output (for example, a prob-\\nability for each of several categories). To go from one layer to the \\nnext, a set of units compute a weighted sum of their inputs from the \\nprevious layer and pass the result through a non-linear function. At \\npresent, the most popular non-linear function is the rectified linear \\nunit (ReLU), which is simply the half-wave rectifier f(z) = max(z, 0). \\nIn past decades, neural nets used smoother non-linearities, such as \\ntanh(z) or 1/(1 + exp(−z)), but the ReLU typically learns much faster \\nin networks with many layers, allowing training of a deep supervised \\nnetwork without unsupervised pre-training 28. Units that are not in \\nthe input or output layer are conventionally called hidden units. The \\nhidden layers can be seen as distorting the input in a non-linear way \\nso that categories become linearly separable by the last layer (Fig.\\xa01). \\nIn the late 1990s, neural nets and backpropagation were largely \\nforsaken by the machine-learning community and ignored by the \\ncomputer-vision and speech-recognition communities. It was widely \\nthought that learning useful, multistage, feature extractors with lit -\\ntle prior knowledge was infeasible. In particular, it was commonly \\nthought that simple gradient descent would get trapped in poor local \\nminima — weight configurations for which no small change would \\nreduce the average error. \\nIn practice, poor local minima are rarely a problem with large net-\\nworks. Regardless of the initial conditions, the system nearly always \\nreaches solutions of very similar quality. Recent theoretical and \\nempirical results strongly suggest that local minima are not a serious \\nissue in general. Instead, the landscape is packed with a combinato -\\nrially large number of saddle points where the gradient is zero, and \\nthe surface curves up in most dimensions and curves down in the \\nFigure 2 | Inside a convolutional network. The outputs (not the filters) \\nof each layer (horizontally) of a typical convolutional network architecture \\napplied to the image of a Samoyed dog (bottom left; and RGB (red, green, \\nblue) inputs, bottom right). Each rectangular image is a feature map \\ncorresponding to the output for one of the learned features, detected at each \\nof the image positions. Information flows bottom up, with lower-level features \\nacting as oriented edge detectors, and a score is computed for each image class \\nin output. ReLU, rectified linear unit.\\nRed Green Blue\\nSamoyed (16); Papillon (5.7); Pomeranian (2.7); Arctic fox (1.0); Eskimo dog (0.6); white wolf (0.4); Siberian husky (0.4)\\nConvolutions and ReLU\\nMax pooling\\nMax pooling\\nConvolutions and ReLU\\nConvolutions and ReLU\\n438 | NATURE | VOL 521 | 28 MAY 2015\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\nremainder29,30. The analysis seems to show that saddle points with \\nonly a few downward curving directions are present in very large \\nnumbers, but almost all of them have very similar values of the objec-\\ntive function. Hence, it does not much matter which of these saddle \\npoints the algorithm gets stuck at. \\nInterest in deep feedforward networks was revived around 2006 \\n(refs\\xa031–34) by a group of researchers brought together by the Cana-\\ndian Institute for Advanced Research (CIFAR). The researchers intro-\\nduced unsupervised learning procedures that could create layers of \\nfeature detectors without requiring labelled data. The objective in \\nlearning each layer of feature detectors was to be able to reconstruct \\nor model the activities of feature detectors (or raw inputs) in the layer \\nbelow. By ‘pre-training’ several layers of progressively more complex \\nfeature detectors using this reconstruction objective, the weights of a \\ndeep network could be initialized to sensible values. A final layer of \\noutput units could then be added to the top of the network and the \\nwhole deep system could be fine-tuned using standard backpropaga-\\ntion33–35. This worked remarkably well for recognizing handwritten \\ndigits or for detecting pedestrians, especially when the amount of \\nlabelled data was very limited 36. \\nThe first major application of this pre-training approach was in \\nspeech recognition, and it was made possible by the advent of fast \\ngraphics processing units (GPUs) that were convenient to program37 \\nand allowed researchers to train networks 10 or 20 times faster. In \\n2009, the approach was used to map short temporal windows of coef-\\nficients extracted from a sound wave to a set of probabilities for the \\nvarious fragments of speech that might be represented by the frame \\nin the centre of the window. It achieved record-breaking results on a \\nstandard speech recognition benchmark that used a small vocabu -\\nlary38 and was quickly developed to give record-breaking results on \\na large vocabulary task39. By 2012, versions of the deep net from 2009 \\nwere being developed by many of the major speech groups6 and were \\nalready being deployed in Android phones. For smaller data sets, \\nunsupervised pre-training helps to prevent overfitting 40, leading to \\nsignificantly better generalization when the number of labelled exam-\\nples is small, or in a transfer setting where we have lots of examples \\nfor some ‘source’ tasks but very few for some ‘target’ tasks. Once deep \\nlearning had been rehabilitated, it turned out that the pre-training \\nstage was only needed for small data sets. \\nThere was, however, one particular type of deep, feedforward net-\\nwork that was much easier to train and generalized much better than \\nnetworks with full connectivity between adjacent layers. This was \\nthe convolutional neural network (ConvNet) 41,42. It achieved many \\npractical successes during the period when neural networks were out \\nof favour and it has recently been widely adopted by the computer-\\nvision community. \\nConvolutional neural networks \\nConvNets are designed to process data that come in the form of \\nmultiple arrays, for example a colour image composed of three 2D \\narrays containing pixel intensities in the three colour channels. Many \\ndata modalities are in the form of multiple arrays: 1D for signals and \\nsequences, including language; 2D for images or audio spectrograms; \\nand 3D for video or volumetric images. There are four key ideas \\nbehind ConvNets that take advantage of the properties of natural \\nsignals: local connections, shared weights, pooling and the use of \\nmany layers. \\nThe architecture of a typical ConvNet (Fig. 2) is structured as a \\nseries of stages. The first few stages are composed of two types of \\nlayers: convolutional layers and pooling layers. Units in a convolu -\\ntional layer are organized in feature maps, within which each unit \\nis connected to local patches in the feature maps of the previous \\nlayer through a set of weights called a filter bank. The result of this \\nlocal weighted sum is then passed through a non-linearity such as a \\nReLU. All units in a feature map share the same filter bank. Differ -\\nent feature maps in a layer use different filter banks. The reason for \\nthis architecture is twofold. First, in array data such as images, local \\ngroups of values are often highly correlated, forming distinctive local \\nmotifs that are easily detected. Second, the local statistics of images \\nand other signals are invariant to location. In other words, if a motif \\ncan appear in one part of the image, it could appear anywhere, hence \\nthe idea of units at different locations sharing the same weights and \\ndetecting the same pattern in different parts of the array. Mathemati-\\ncally, the filtering operation performed by a feature map is a discrete \\nconvolution, hence the name. \\nAlthough the role of the convolutional layer is to detect local con-\\njunctions of features from the previous layer, the role of the pooling \\nlayer is to merge semantically similar features into one. Because the \\nrelative positions of the features forming a motif can vary somewhat, \\nreliably detecting the motif can be done by coarse-graining the posi-\\ntion of each feature. A typical pooling unit computes the maximum  \\nof a local patch of units in one feature map (or in a few feature maps). \\nNeighbouring pooling units take input from patches that are shifted \\nby more than one row or column, thereby reducing the dimension of \\nthe representation and creating an invariance to small shifts and dis-\\ntortions. Two or three stages of convolution, non-linearity and pool-\\ning are stacked, followed by more convolutional and fully-connected \\nlayers. Backpropagating gradients through a ConvNet is as simple as \\nthrough a regular deep network, allowing all the weights in all the \\nfilter banks to be trained. \\nDeep neural networks exploit the property that many natural sig-\\nnals are compositional hierarchies, in which higher-level features \\nare obtained by composing lower-level ones. In images, local combi-\\nnations of edges form motifs, motifs assemble into parts, and parts \\nform objects. Similar hierarchies exist in speech and text from sounds \\nto phones, phonemes, syllables, words and sentences. The pooling \\nallows representations to vary very little when elements in the previ-\\nous layer vary in position and appearance. \\nThe convolutional and pooling layers in ConvNets are directly \\ninspired by the classic notions of simple cells and complex cells in \\nvisual neuroscience 43, and the overall architecture is reminiscent of \\nthe LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral path-\\nway44. When ConvNet models and monkeys are shown the same pic-\\nture, the activations of high-level units in the ConvNet explains half \\nof the variance of random sets of 160\\xa0neurons in the monkey’s infer-\\notemporal cortex45. ConvNets have their roots in the neocognitron46, \\nthe architecture of which was somewhat similar, but did not have an \\nend-to-end supervised-learning algorithm such as backpropagation. \\nA primitive 1D ConvNet called a time-delay neural net was used for \\nthe recognition of phonemes and simple words47,48. \\nThere have been numerous applications of convolutional net -\\nworks going back to the early 1990s, starting with time-delay neu -\\nral networks for speech recognition 47 and document reading42. The \\ndocument reading system used a ConvNet trained jointly with a \\nprobabilistic model that implemented language constraints. By the \\nlate 1990s this system was reading over 10% of all the cheques in the \\nUnited States. A number of ConvNet-based optical character recog-\\nnition and handwriting recognition systems were later deployed by \\nMicrosoft49. ConvNets were also experimented with in the early 1990s \\nfor object detection in natural images, including faces and hands50,51, \\nand for face recognition52. \\nImage understanding with deep convolutional networks \\nSince the early 2000s, ConvNets have been applied with great success to \\nthe detection, segmentation and recognition of objects and regions in \\nimages. These were all tasks in which labelled data was relatively abun-\\ndant, such as traffic sign recognition53, the segmentation of biological \\nimages54 particularly for connectomics55, and the detection of faces, \\ntext, pedestrians and human bodies in natural images36,50,51,56–58. A major \\nrecent practical success of ConvNets is face recognition59. \\nImportantly, images can be labelled at the pixel level, which will have \\napplications in technology, including autonomous mobile robots and \\n28 MAY 2015 | VOL 521 | NATURE | 439\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\nself-driving cars 60,61. Companies such as Mobileye and NVIDIA are \\nusing such ConvNet-based methods in their upcoming vision sys -\\ntems for cars. Other applications gaining importance involve natural \\nlanguage understanding 14 and speech recognition7. \\nDespite these successes, ConvNets were largely forsaken by the \\nmainstream computer-vision and machine-learning communities \\nuntil the ImageNet competition in 2012. When deep convolutional \\nnetworks were applied to a data set of about a million images from \\nthe web that contained 1,000 different classes, they achieved spec -\\ntacular results, almost halving the error rates of the best compet -\\ning approaches1. This success came from the efficient use of GPUs, \\nReLUs, a new regularization technique called dropout 62, and tech-\\nniques to generate more training examples by deforming the existing \\nones. This success has brought about a revolution in computer vision; \\nConvNets are now the dominant approach for almost all recognition \\nand detection tasks 4,58,59,63–65  and approach human performance on \\nsome tasks. A recent stunning demonstration combines ConvNets \\nand recurrent net modules for the generation of image captions \\n(Fig.\\xa03). \\nRecent ConvNet architectures have 10 to 20 layers of ReLUs, hun-\\ndreds of millions of weights, and billions of connections between \\nunits. Whereas training such large networks could have taken weeks \\nonly two years ago, progress in hardware, software and algorithm \\nparallelization have reduced training times to a few hours. \\nThe performance of ConvNet-based vision systems has caused \\nmost major technology companies, including Google, Facebook, \\nMicrosoft, IBM, Y ahoo!, Twitter and Adobe, as well as a quickly \\ngrowing number of start-ups to initiate research and development \\nprojects and to deploy ConvNet-based image understanding products \\nand services. \\nConvNets are easily amenable to efficient hardware implemen -\\ntations in chips or field-programmable gate arrays 66,67. A number \\nof companies such as NVIDIA, Mobileye, Intel, Qualcomm and \\nSamsung are developing ConvNet chips to enable real-time vision \\napplications in smartphones, cameras, robots and self-driving cars. \\nDistributed representations and language processing \\nDeep-learning theory shows that deep nets have two different expo-\\nnential advantages over classic learning algorithms that do not use \\ndistributed representations21. Both of these advantages arise from the \\npower of composition and depend on the underlying data-generating \\ndistribution having an appropriate componential structure 40. First, \\nlearning distributed representations enable generalization to new \\ncombinations of the values of learned features beyond those seen \\nduring training (for example, 2 n combinations are possible with n \\nbinary features) 68,69. Second, composing layers of representation in \\na deep net brings the potential for another exponential advantage 70 \\n(exponential in the depth). \\nThe hidden layers of a multilayer neural network learn to repre -\\nsent the network’s inputs in a way that makes it easy to predict the \\ntarget outputs. This is nicely demonstrated by training a multilayer \\nneural network to predict the next word in a sequence from a local \\nFigure 3 | From image to text. Captions generated by a recurrent neural \\nnetwork (RNN) taking, as extra input, the representation extracted by a deep \\nconvolution neural network (CNN) from a test image, with the RNN trained to \\n‘translate’ high-level representations of images into captions (top). Reproduced \\nwith permission from ref. 102. When the RNN is given the ability to focus its \\nattention on a different location in the input image (middle and bottom; the \\nlighter patches were given more attention) as it generates each word (bold), we \\nfound86 that it exploits this to achieve better ‘translation’ of images into captions.\\nVision\\nDeep CNN\\nLanguage\\nGenerating RNN\\nA group of people \\nshopping at an outdoor \\nmarket.\\nThere are many \\nvegetables at the \\nfruit stand.\\nA woman is throwing a frisbee in a park.\\nA little girl sitting on a bed with a teddy bear. A group of people sitting on a boat in the water. A giraﬀe standing in a forest with\\ntrees in the background.\\nA dog is standing on a hardwood /f_loor. A stop sign is on a road with a\\nmountain in the background\\n440 | NATURE | VOL 521 | 28 MAY 2015\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\ncontext of earlier words71. Each word in the context is presented to \\nthe network as a one-of-N vector, that is, one component has a value \\nof 1 and the rest are\\xa00. In the first layer, each word creates a different \\npattern of activations, or word vectors (Fig.\\xa04). In a language model, \\nthe other layers of the network learn to convert the input word vec -\\ntors into an output word vector for the predicted next word, which \\ncan be used to predict the probability for any word in the vocabulary \\nto appear as the next word. The network learns word vectors that \\ncontain many active components each of which can be interpreted \\nas a separate feature of the word, as was first demonstrated 27 in the \\ncontext of learning distributed representations for symbols. These \\nsemantic features were not explicitly present in the input. They were \\ndiscovered by the learning procedure as a good way of factorizing \\nthe structured relationships between the input and output symbols \\ninto multiple ‘micro-rules’ . Learning word vectors turned out to also \\nwork very well when the word sequences come from a large corpus \\nof real text and the individual micro-rules are unreliable 71. When \\ntrained to predict the next word in a news story, for example, the \\nlearned word vectors for Tuesday and Wednesday are very similar, as \\nare the word vectors for Sweden and Norway. Such representations \\nare called distributed representations because their elements (the \\nfeatures) are not mutually exclusive and their many configurations \\ncorrespond to the variations seen in the observed data. These word \\nvectors are composed of learned features that were not determined \\nahead of time by experts, but automatically discovered by the neural \\nnetwork. Vector representations of words learned from text are now \\nvery widely used in natural language applications 14,17,72–76. \\nThe issue of representation lies at the heart of the debate between \\nthe logic-inspired and the neural-network-inspired paradigms for \\ncognition. In the logic-inspired paradigm, an instance of a symbol is \\nsomething for which the only property is that it is either identical or \\nnon-identical to other symbol instances. It has no internal structure \\nthat is relevant to its use; and to reason with symbols, they must be \\nbound to the variables in judiciously chosen rules of inference. By \\ncontrast, neural networks just use big activity vectors, big weight \\nmatrices and scalar non-linearities to perform the type of fast ‘intui-\\ntive’ inference that underpins effortless commonsense reasoning. \\nBefore the introduction of neural language models71, the standard \\napproach to statistical modelling of language did not exploit distrib-\\nuted representations: it was based on counting frequencies of occur-\\nrences of short symbol sequences of length up to N (called N-grams). \\nThe number of possible N-grams is on the order of V N, where V is \\nthe vocabulary size, so taking into account a context of more than a \\nhandful of words would require very large training corpora. N-grams \\ntreat each word as an atomic unit, so they cannot generalize across \\nsemantically related sequences of words, whereas neural language \\nmodels can because they associate each word with a vector of real \\nvalued features, and semantically related words end up close to each \\nother in that vector space (Fig.\\xa04). \\nRecurrent neural networks \\nWhen backpropagation was first introduced, its most exciting use was \\nfor training recurrent neural networks (RNNs). For tasks that involve \\nsequential inputs, such as speech and language, it is often better to \\nuse RNNs (Fig. 5). RNNs process an input sequence one element at a \\ntime, maintaining in their hidden units a ‘state vector’ that implicitly \\ncontains information about the history of all the past elements of \\nthe sequence. When we consider the outputs of the hidden units at \\ndifferent discrete time steps as if they were the outputs of different \\nneurons in a deep multilayer network (Fig.\\xa05, right), it becomes clear \\nhow we can apply backpropagation to train RNNs. \\nRNNs are very powerful dynamic systems, but training them has \\nproved to be problematic because the backpropagated gradients \\neither grow or shrink at each time step, so over many time steps they \\ntypically explode or vanish77,78. \\nThanks to advances in their architecture 79,80 and ways of training \\nthem81,82, RNNs have been found to be very good at predicting the \\nnext character in the text83 or the next word in a sequence75, but they \\ncan also be used for more complex tasks. For example, after reading \\nan English sentence one word at a time, an English ‘encoder’ network \\ncan be trained so that the final state vector of its hidden units is a good \\nrepresentation of the thought expressed by the sentence. This thought \\nvector can then be used as the initial hidden state of (or as extra input \\nto) a jointly trained French ‘decoder’ network, which outputs a prob-\\nability distribution for the first word of the French translation. If a \\nparticular first word is chosen from this distribution and provided \\nas input to the decoder network it will then output a probability dis-\\ntribution for the second word of the translation and so on until a \\nfull stop is chosen17,72,76. Overall, this process generates sequences of \\nFrench words according to a probability distribution that depends on \\nthe English sentence. This rather naive way of performing machine \\ntranslation has quickly become competitive with the state-of-the-art, \\nand this raises serious doubts about whether understanding a sen -\\ntence requires anything like the internal symbolic expressions that are \\nmanipulated by using inference rules. It is more compatible with the \\nview that everyday reasoning involves many simultaneous analogies \\nFigure 4 | Visualizing the learned word vectors. On the left is an illustration \\nof word representations learned for modelling language, non-linearly projected \\nto 2D for visualization using the t-SNE algorithm103. On the right is a 2D \\nrepresentation of phrases learned by an English-to-French encoder–decoder \\nrecurrent neural network75. One can observe that semantically similar words \\nor sequences of words are mapped to nearby representations. The distributed \\nrepresentations of words are obtained by using backpropagation to jointly learn \\na representation for each word and a function that predicts a target quantity \\nsuch as the next word in a sequence (for language modelling) or a whole \\nsequence of translated words (for machine translation)18,75.\\n−37 −36 −35 −34 −33 −32 −31 −30 −29\\n9\\n10\\n10.5\\n11\\n11.5\\n12\\n12.5\\n13\\n13.5\\n14\\n community\\n organizations institutions\\n society\\n industry company\\n organization\\n school\\n companies\\n Community\\n oﬃce\\n Agency\\n communities\\n Association\\n body\\n schools\\n agencies\\n−5.5 −5 −4.5 −4 −3.5 −3 −2.5 −2\\n−4.2\\n−4\\n−3.8\\n−3.6\\n−3.4\\n−3.2\\n−3\\n−2.8\\n−2.6\\n−2.4\\n−2.2\\nover the past few months\\nthat a few days\\nIn the last few daysthe past few days\\nIn a few months\\nin the coming months\\na few months ago\\n&quot; the two groups\\nof the two groups\\nover the last few months\\ndispute between the two\\nthe last two decades\\nthe next six months\\ntwo months before being\\nfor nearly two months\\nover the last two decades\\nwithin a few months\\n28 MAY 2015 | VOL 521 | NATURE | 441\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\nthat each contribute plausibility to a conclusion84,85. \\nInstead of translating the meaning of a French sentence into  an \\nEnglish sentence, one can learn to ‘translate’ the meaning of an image \\ninto an English sentence (Fig. 3). The encoder here is a deep Con -\\nvNet that converts the pixels into an activity vector in its last hidden \\nlayer. The decoder is an RNN similar to the ones used for machine \\ntranslation and neural language modelling. There has been a surge of \\ninterest in such systems recently (see examples mentioned in ref. 86). \\nRNNs, once unfolded in time (Fig. 5), can be seen as very deep \\nfeedforward networks in which all the layers share the same weights. \\nAlthough their main purpose is to learn long-term dependencies, \\ntheoretical and empirical evidence shows that it is difficult to learn \\nto store information for very long78.  \\nTo correct for that, one idea is to augment the network with an \\nexplicit memory. The first proposal of this kind is the long short-term \\nmemory (LSTM) networks that use special hidden units, the natural \\nbehaviour of which is to remember inputs for a long time79. A special \\nunit called the memory cell acts like an accumulator or a gated leaky \\nneuron: it has a connection to itself at the next time step that has a \\nweight of one, so it copies its own real-valued state and accumulates \\nthe external signal, but this self-connection is multiplicatively gated \\nby another unit that learns to decide when to clear the content of the \\nmemory. \\nLSTM networks have subsequently proved to be more effective \\nthan conventional RNNs, especially when they have several layers for \\neach time step87, enabling an entire speech recognition system that \\ngoes all the way from acoustics to the sequence of characters in the \\ntranscription. LSTM networks or related forms of gated units are also \\ncurrently used for the encoder and decoder networks that perform \\nso well at machine translation17,72,76. \\nOver the past year, several authors have made different proposals to \\naugment RNNs with a memory module. Proposals include the Neural \\nTuring Machine in which the network is augmented by a ‘tape-like’ \\nmemory that the RNN can choose to read from or write to 88, and \\nmemory networks, in which a regular network is augmented by a \\nkind of associative memory89. Memory networks have yielded excel-\\nlent performance on standard question-answering benchmarks. The \\nmemory is used to remember the story about which the network is \\nlater asked to answer questions. \\nBeyond simple memorization, neural Turing machines and mem-\\nory networks are being used for tasks that would normally require \\nreasoning and symbol manipulation. Neural Turing machines can \\nbe taught ‘algorithms’ . Among other things, they can learn to output \\na sorted list of symbols when their input consists of an unsorted \\nsequence in which each symbol is accompanied by a real value that \\nindicates its priority in the list 88. Memory networks can be trained \\nto keep track of the state of the world in a setting similar to a text \\nadventure game and after reading a story, they can answer questions \\nthat require complex inference90. In one test example, the network is \\nshown a 15-sentence version of the The Lord of the Rings and correctly \\nanswers questions such as “where is Frodo now?”89.  \\nThe future of deep learning \\nUnsupervised learning91–98 had a catalytic effect in reviving interest in \\ndeep learning, but has since been overshadowed by the successes of \\npurely supervised learning. Although we have not focused on it in this \\nReview, we expect unsupervised learning to become far more important \\nin the longer term. Human and animal learning is largely unsupervised: \\nwe discover the structure of the world by observing it, not by being told \\nthe name of every object. \\nHuman vision is an active process that sequentially samples the optic \\narray in an intelligent, task-speciﬁc way using a small, high-resolution \\nfovea with a large, low-resolution surround. We expect much of the \\nfuture progress in vision to come from systems that are trained end-to-\\nend and combine ConvNets with RNNs that use reinforcement learning \\nto decide where to look. Systems combining deep learning and rein-\\nforcement learning are in their infancy, but they already outperform \\npassive vision systems99 at classification tasks and produce impressive \\nresults in learning to play many different video games100. \\nNatural language understanding is another area in which deep learn-\\ning is poised to make a large impact over the next few years. We expect \\nsystems that use RNNs to understand sentences or whole documents \\nwill become much better when they learn strategies for selectively \\nattending to one part at a time76,86. \\nUltimately, major progress in artificial intelligence will come about \\nthrough systems that combine representation learning with complex \\nreasoning. Although deep learning and simple reasoning have been \\nused for speech and handwriting recognition for a long time, new \\nparadigms are needed to replace rule-based manipulation of symbolic \\nexpressions by operations on large vectors101. ■\\nReceived 25 February; accepted 1 May 2015.\\n1. Krizhevsky, A., Sutskever, I. & Hinton, G. ImageNet classification with deep \\nconvolutional neural networks. In Proc. Advances in Neural Information \\nProcessing Systems 25 1090–1098 (2012).\\n This report was a breakthrough that used convolutional nets to almost halve \\nthe error rate for object recognition, and precipitated the rapid adoption of \\ndeep learning by the computer vision community.\\n2. Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Learning hierarchical features for \\nscene labeling. IEEE Trans. Pattern Anal. Mach. Intell. 35, 1915–1929 (2013). \\n3. Tompson, J., Jain, A., LeCun, Y. & Bregler, C. Joint training of a convolutional \\nnetwork and a graphical model for human pose estimation. In Proc. Advances in \\nNeural Information Processing Systems 27 1799–1807 (2014). \\n4. Szegedy, C. et al. Going deeper with convolutions. Preprint at http://arxiv.org/\\nabs/1409.4842 (2014). \\n5. Mikolov, T., Deoras, A., Povey, D., Burget, L. & Cernocky, J. Strategies for training \\nlarge scale neural network language models. In Proc. Automatic Speech \\nRecognition and Understanding 196–201 (2011). \\n6. Hinton, G. et al. Deep neural networks for acoustic modeling in speech \\nrecognition. IEEE Signal Processing Magazine 29, 82–97 (2012).\\n This joint paper from the major speech recognition laboratories, summarizing \\nthe breakthrough achieved with deep learning on the task of phonetic \\nclassification for automatic speech recognition, was the first major industrial \\napplication of deep learning.\\n7. Sainath, T., Mohamed, A.-R., Kingsbury, B. & Ramabhadran, B. Deep \\nconvolutional neural networks for LVCSR. In Proc. Acoustics, Speech and Signal \\nProcessing 8614–8618 (2013). \\n8. Ma, J., Sheridan, R. P., Liaw, A., Dahl, G. E. & Svetnik, V. Deep neural nets as a \\nmethod for quantitative structure-activity relationships. J. Chem. Inf. Model. 55, \\n263–274 (2015). \\n9. Ciodaro, T., Deva, D., de Seixas, J. & Damazio, D. Online particle detection with \\nneural networks based on topological calorimetry information. J. Phys. Conf. \\nSeries 368, 012030 (2012). \\n10. Kaggle. Higgs boson machine learning challenge. Kaggle https://www.kaggle.\\ncom/c/higgs-boson (2014). \\n11. Helmstaedter, M. et al. Connectomic reconstruction of the inner plexiform layer \\nin the mouse retina. Nature 500, 168–174 (2013). \\nxtxt−1 xt+1x\\nUnfold\\nV W W\\nW W W\\nV V V\\nU U U U\\ns\\no\\nst−1\\not−1 ot\\nst st+1\\not+1\\nFigure 5 | A recurrent neural network and the unfolding in time of the \\ncomputation involved in its forward computation. The artificial neurons \\n(for example, hidden units grouped under node s with values st at time t) get \\ninputs from other neurons at previous time steps (this is represented with the \\nblack square, representing a delay of one time step, on the left). In this way, a \\nrecurrent neural network can map an input sequence with elements xt into an \\noutput sequence with elements ot, with each ot depending on all the previous \\nxtʹ (for tʹ ≤ t). The same parameters (matrices U,V,W ) are used at each time \\nstep. Many other architectures are possible, including a variant in which the \\nnetwork can generate a sequence of outputs (for example, words), each of \\nwhich is used as inputs for the next time step. The backpropagation algorithm \\n(Fig. 1) can be directly applied to the computational graph of the unfolded \\nnetwork on the right, to compute the derivative of a total error (for example, \\nthe log-probability of generating the right sequence of outputs) with respect to \\nall the states st and all the parameters.\\n442 | NATURE | VOL 521 | 28 MAY 2015\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\n12. Leung, M. K., Xiong, H. Y., Lee, L. J. & Frey, B. J. Deep learning of the tissue-\\nregulated splicing code. Bioinformatics 30, i121–i129 (2014). \\n13. Xiong, H. Y. et al. The human splicing code reveals new insights into the genetic \\ndeterminants of disease. Science 347, 6218 (2015). \\n14. Collobert, R., et al. Natural language processing (almost) from scratch. J. Mach. \\nLearn. Res. 12, 2493–2537 (2011). \\n15. Bordes, A., Chopra, S. & Weston, J. Question answering with subgraph \\nembeddings. In Proc. Empirical Methods in Natural Language Processing http://\\narxiv.org/abs/1406.3676v3 (2014). \\n16. Jean, S., Cho, K., Memisevic, R. & Bengio, Y. On using very large target \\nvocabulary for neural machine translation. In Proc. ACL-IJCNLP http://arxiv.org/\\nabs/1412.2007 (2015).\\n17. Sutskever, I. Vinyals, O. & Le. Q. V. Sequence to sequence learning with neural \\nnetworks. In Proc. Advances in Neural Information Processing Systems 27 \\n3104–3112 (2014). \\n This paper showed state-of-the-art machine translation results with the \\narchitecture introduced in ref. 72, with a recurrent network trained to read a \\nsentence in one language, produce a semantic representation of its meaning, \\nand generate a translation in another language.\\n18. Bottou, L. & Bousquet, O. The tradeoffs of large scale learning. In Proc. Advances \\nin Neural Information Processing Systems 20 161–168 (2007). \\n19. Duda, R. O. & Hart, P. E. Pattern Classiﬁcation and Scene Analysis (Wiley, 1973). \\n20. Schölkopf, B. & Smola, A. Learning with Kernels (MIT Press, 2002). \\n21. Bengio, Y., Delalleau, O. & Le Roux, N. The curse of highly variable functions \\nfor local kernel machines. In Proc. Advances in Neural Information Processing \\nSystems 18 107–114 (2005). \\n22. Selfridge, O. G. Pandemonium: a paradigm for learning in mechanisation of \\nthought processes. In Proc. Symposium on Mechanisation of Thought Processes \\n513–526 (1958). \\n23. Rosenblatt, F. The Perceptron — A Perceiving and Recognizing Automaton. Tech. \\nRep. 85-460-1 (Cornell Aeronautical Laboratory, 1957). \\n24. Werbos, P. Beyond Regression: New Tools for Prediction and Analysis in the \\nBehavioral Sciences. PhD thesis, Harvard Univ. (1974). \\n25. Parker, D. B. Learning Logic Report TR–47 (MIT Press, 1985). \\n26. LeCun, Y. Une procédure d’apprentissage pour Réseau à seuil assymétrique \\nin Cognitiva 85: a la Frontière de l’Intelligence Artiﬁcielle, des Sciences de la \\nConnaissance et des Neurosciences [in French] 599–604 (1985). \\n27. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by \\nback-propagating errors. Nature 323, 533–536 (1986). \\n28. Glorot, X., Bordes, A. & Bengio. Y. Deep sparse rectiﬁer neural networks. In Proc. \\n14th International Conference on Artificial Intelligence and Statistics 315–323 \\n(2011). \\n This paper showed that supervised training of very deep neural networks is \\nmuch faster if the hidden layers are composed of ReLU.\\n29. Dauphin, Y. et al. Identifying and attacking the saddle point problem in high-\\ndimensional non-convex optimization. In Proc. Advances in Neural Information \\nProcessing Systems 27 2933–2941 (2014). \\n30. Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. & LeCun, Y. The loss \\nsurface of multilayer networks. In Proc. Conference on AI and Statistics http://\\narxiv.org/abs/1412.0233 (2014). \\n31. Hinton, G. E. What kind of graphical model is the brain? In Proc. 19th \\nInternational Joint Conference on Artificial intelligence 1765–1775 (2005). \\n32. Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief \\nnets. Neural Comp. 18, 1527–1554 (2006).\\n This paper introduced a novel and effective way of training very deep neural \\nnetworks by pre-training one hidden layer at a time using the unsupervised \\nlearning procedure for restricted Boltzmann machines. \\n33. Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training \\nof deep networks. In Proc. Advances in Neural Information Processing Systems 19 \\n153–160 (2006). \\n This report demonstrated that the unsupervised pre-training method \\nintroduced in ref. 32 significantly improves performance on test data and \\ngeneralizes the method to other unsupervised representation-learning \\ntechniques, such as auto-encoders.\\n34. Ranzato, M., Poultney, C., Chopra, S. & LeCun, Y. Efﬁcient learning of sparse \\nrepresentations with an energy-based model. In Proc. Advances in Neural \\nInformation Processing Systems 19 1137–1144 (2006). \\n35. Hinton, G. E. & Salakhutdinov, R. Reducing the dimensionality of data with \\nneural networks. Science 313, 504–507 (2006). \\n36. Sermanet, P., Kavukcuoglu, K., Chintala, S. & LeCun, Y. Pedestrian detection with \\nunsupervised multi-stage feature learning. In Proc. International Conference \\non Computer Vision and Pattern Recognition http://arxiv.org/abs/1212.0142 \\n(2013). \\n37. Raina, R., Madhavan, A. & Ng, A. Y. Large-scale deep unsupervised learning \\nusing graphics processors. In Proc. 26th Annual International Conference on \\nMachine Learning 873–880 (2009). \\n38. Mohamed, A.-R., Dahl, G. E. & Hinton, G. Acoustic modeling using deep belief \\nnetworks. IEEE Trans. Audio Speech Lang. Process. 20, 14–22 (2012). \\n39. Dahl, G. E., Yu, D., Deng, L. & Acero, A. Context-dependent pre-trained deep \\nneural networks for large vocabulary speech recognition. IEEE Trans. Audio \\nSpeech Lang. Process. 20, 33–42 (2012). \\n40. Bengio, Y., Courville, A. & Vincent, P. Representation learning: a review and new \\nperspectives. IEEE Trans. Pattern Anal. Machine Intell. 35, 1798–1828 (2013). \\n41. LeCun, Y. et al. Handwritten digit recognition with a back-propagation network. \\nIn Proc. Advances in Neural Information Processing Systems 396–404 (1990). \\n This is the first paper on convolutional networks trained by backpropagation \\nfor the task of classifying low-resolution images of handwritten digits.\\n42. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to \\ndocument recognition. Proc. IEEE 86, 2278–2324 (1998). \\n This overview paper on the principles of end-to-end training of modular \\nsystems such as deep neural networks using gradient-based optimization \\nshowed how neural networks (and in particular convolutional nets) can be \\ncombined with search or inference mechanisms to model complex outputs \\nthat are interdependent, such as sequences of characters associated with the \\ncontent of a document.\\n43. Hubel, D. H. & Wiesel, T. N. Receptive ﬁelds, binocular interaction, and functional \\narchitecture in the cat’s visual cortex. J. Physiol. 160, 106–154 (1962). \\n44. Felleman, D. J. & Essen, D. C. V. Distributed hierarchical processing in the \\nprimate cerebral cortex. Cereb. Cortex 1, 1–47 (1991). \\n45. Cadieu, C. F. et al. Deep neural networks rival the representation of primate \\nit cortex for core visual object recognition. PLoS Comp. Biol. 10, e1003963 \\n(2014). \\n46. Fukushima, K. & Miyake, S. Neocognitron: a new algorithm for pattern \\nrecognition tolerant of deformations and shifts in position. Pattern Recognition \\n15, 455–469 (1982). \\n47. Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K. & Lang, K. Phoneme \\nrecognition using time-delay neural networks. IEEE Trans. Acoustics Speech \\nSignal Process. 37, 328–339 (1989). \\n48. Bottou, L., Fogelman-Soulié, F., Blanchet, P. & Lienard, J. Experiments with time \\ndelay networks and dynamic time warping for speaker independent isolated \\ndigit recognition. In Proc. EuroSpeech 89 537–540 (1989). \\n49. Simard, D., Steinkraus, P. Y. & Platt, J. C. Best practices for convolutional neural \\nnetworks. In Proc. Document Analysis and Recognition 958–963 (2003). \\n50. Vaillant, R., Monrocq, C. & LeCun, Y. Original approach for the localisation of \\nobjects in images. In Proc. Vision, Image, and Signal Processing 141, 245–250 \\n(1994). \\n51. Nowlan, S. & Platt, J. in Neural Information Processing Systems 901–908 (1995). \\n52. Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition: a \\nconvolutional neural-network approach. IEEE Trans. Neural Networks 8, 98–113 \\n(1997). \\n53. Ciresan, D., Meier, U. Masci, J. & Schmidhuber, J. Multi-column deep neural \\nnetwork for trafﬁc sign classiﬁcation. Neural Networks 32, 333–338 (2012). \\n54. Ning, F. et al. Toward automatic phenotyping of developing embryos from \\nvideos. IEEE Trans. Image Process. 14, 1360–1371 (2005). \\n55. Turaga, S. C. et al. Convolutional networks can learn to generate afﬁnity graphs \\nfor image segmentation. Neural Comput. 22, 511–538 (2010). \\n56. Garcia, C. & Delakis, M. Convolutional face ﬁnder: a neural architecture for \\nfast and robust face detection. IEEE Trans. Pattern Anal. Machine Intell. 26, \\n1408–1423 (2004). \\n57. Osadchy, M., LeCun, Y. & Miller, M. Synergistic face detection and pose \\nestimation with energy-based models. J. Mach. Learn. Res. 8, 1197–1215 \\n(2007). \\n58. Tompson, J., Goroshin, R. R., Jain, A., LeCun, Y. Y. & Bregler, C. C. Efﬁcient object \\nlocalization using convolutional networks. In Proc. Conference on Computer \\nVision and Pattern Recognition http://arxiv.org/abs/1411.4280 (2014). \\n59. Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: closing the gap to \\nhuman-level performance in face veriﬁcation. In Proc. Conference on Computer \\nVision and Pattern Recognition 1701–1708 (2014). \\n60. Hadsell, R. et al. Learning long-range vision for autonomous off-road driving. \\nJ. Field Robot. 26, 120–144 (2009). \\n61. Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Scene parsing with multiscale \\nfeature learning, purity trees, and optimal covers. In Proc. International \\nConference on Machine Learning http://arxiv.org/abs/1202.2160 (2012). \\n62. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. \\nDropout: a simple way to prevent neural networks from overﬁtting. J. Machine \\nLearning Res. 15, 1929–1958 (2014). \\n63. Sermanet, P. et al. Overfeat: integrated recognition, localization and detection \\nusing convolutional networks. In Proc. International Conference on Learning \\nRepresentations http://arxiv.org/abs/1312.6229 (2014). \\n64. Girshick, R., Donahue, J., Darrell, T. & Malik, J. Rich feature hierarchies for \\naccurate object detection and semantic segmentation. In Proc. Conference on \\nComputer Vision and Pattern Recognition 580–587 (2014). \\n65. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale \\nimage recognition. In Proc. International Conference on Learning Representations \\nhttp://arxiv.org/abs/1409.1556 (2014). \\n66. Boser, B., Sackinger, E., Bromley, J., LeCun, Y. & Jackel, L. An analog neural \\nnetwork processor with programmable topology. J. Solid State Circuits 26, \\n2017–2025 (1991). \\n67. Farabet, C. et al. Large-scale FPGA-based convolutional networks. In Scaling \\nup Machine Learning: Parallel and Distributed Approaches (eds Bekkerman, R., \\nBilenko, M. & Langford, J.) 399–419 (Cambridge Univ. Press, 2011). \\n68. Bengio, Y. Learning Deep Architectures for AI (Now, 2009). \\n69. Montufar, G. & Morton, J. When does a mixture of products contain a product of \\nmixtures? J. Discrete Math. 29, 321–347 (2014). \\n70. Montufar, G. F., Pascanu, R., Cho, K. & Bengio, Y. On the number of linear regions \\nof deep neural networks. In Proc. Advances in Neural Information Processing \\nSystems 27 2924–2932 (2014). \\n71. Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model. In \\nProc. Advances in Neural Information Processing Systems 13 932–938 (2001). \\n This paper introduced neural language models, which learn to convert a word \\nsymbol into a word vector or word embedding composed of learned semantic \\nfeatures in order to predict the next word in a sequence.\\n72. Cho, K. et al. Learning phrase representations using RNN encoder-decoder \\n28 MAY 2015 | VOL 521 | NATURE | 443\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\nfor statistical machine translation. In Proc. Conference on Empirical Methods in \\nNatural Language Processing 1724–1734 (2014).  \\n73. Schwenk, H. Continuous space language models. Computer Speech Lang. 21, \\n492–518 (2007). \\n74. Socher, R., Lin, C. C-Y., Manning, C. & Ng, A. Y. Parsing natural scenes and \\nnatural language with recursive neural networks. In Proc. International \\nConference on Machine Learning 129–136 (2011). \\n75. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed \\nrepresentations of words and phrases and their compositionality. In Proc. \\nAdvances in Neural Information Processing Systems 26 3111–3119 (2013). \\n76. Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly \\nlearning to align and translate. In Proc. International Conference on Learning \\nRepresentations http://arxiv.org/abs/1409.0473 (2015).\\n77. Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen [in \\nGerman] Diploma thesis, T.U. Münich (1991). \\n78. Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with \\ngradient descent is difﬁcult. IEEE Trans. Neural Networks 5, 157–166 (1994). \\n79. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, \\n1735–1780 (1997). \\n This paper introduced LSTM recurrent networks, which have become a crucial \\ningredient in recent advances with recurrent networks because they are good \\nat learning long-range dependencies. \\n80. ElHihi, S. & Bengio, Y. Hierarchical recurrent neural networks for long-term \\ndependencies. In Proc. Advances in Neural Information Processing Systems 8 \\nhttp://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-\\nlong-term-dependencies (1995). \\n81. Sutskever, I. Training Recurrent Neural Networks. PhD thesis, Univ. Toronto \\n(2012). \\n82. Pascanu, R., Mikolov, T. & Bengio, Y. On the difﬁculty of training recurrent neural \\nnetworks. In Proc. 30th International Conference on Machine Learning 1310–\\n1318 (2013). \\n83. Sutskever, I., Martens, J. & Hinton, G. E. Generating text with recurrent neural \\nnetworks. In Proc. 28th International Conference on Machine Learning 1017–\\n1024 (2011). \\n84. Lakoff, G. & Johnson, M. Metaphors We Live By (Univ. Chicago Press, 2008). \\n85. Rogers, T. T. & McClelland, J. L. Semantic Cognition: A Parallel Distributed \\nProcessing Approach (MIT Press, 2004). \\n86. Xu, K. et al. Show, attend and tell: Neural image caption generation with visual \\nattention. In Proc. International Conference on Learning Representations http://\\narxiv.org/abs/1502.03044 (2015). \\n87. Graves, A., Mohamed, A.-R. & Hinton, G. Speech recognition with deep recurrent \\nneural networks. In Proc. International Conference on Acoustics, Speech and \\nSignal Processing 6645–6649 (2013). \\n88. Graves, A., Wayne, G. & Danihelka, I. Neural Turing machines. http://arxiv.org/\\nabs/1410.5401 (2014). \\n89. Weston, J. Chopra, S. & Bordes, A. Memory networks. http://arxiv.org/\\nabs/1410.3916 (2014). \\n90. Weston, J., Bordes, A., Chopra, S. & Mikolov, T. Towards AI-complete question \\nanswering: a set of prerequisite toy tasks. http://arxiv.org/abs/1502.05698 \\n(2015). \\n91. Hinton, G. E., Dayan, P., Frey, B. J. & Neal, R. M. The wake-sleep algorithm for \\nunsupervised neural networks. Science 268, 1558–1161 (1995). \\n92. Salakhutdinov, R. & Hinton, G. Deep Boltzmann machines. In Proc. International \\nConference on Artificial Intelligence and Statistics 448–455 (2009). \\n93. Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.-A. Extracting and composing \\nrobust features with denoising autoencoders. In Proc. 25th International \\nConference on Machine Learning 1096–1103 (2008). \\n94. Kavukcuoglu, K. et al. Learning convolutional feature hierarchies for visual \\nrecognition. In Proc. Advances in Neural Information Processing Systems 23 \\n1090–1098 (2010). \\n95. Gregor, K. & LeCun, Y. Learning fast approximations of sparse coding. In Proc. \\nInternational Conference on Machine Learning 399–406 (2010). \\n96. Ranzato, M., Mnih, V., Susskind, J. M. & Hinton, G. E. Modeling natural images \\nusing gated MRFs. IEEE Trans. Pattern Anal. Machine Intell. 35, 2206–2222 \\n(2013). \\n97. Bengio, Y., Thibodeau-Laufer, E., Alain, G. & Yosinski, J. Deep generative \\nstochastic networks trainable by backprop. In Proc. 31st International \\nConference on Machine Learning 226–234 (2014). \\n98. Kingma, D., Rezende, D., Mohamed, S. & Welling, M. Semi-supervised learning \\nwith deep generative models. In Proc. Advances in Neural Information Processing \\nSystems 27 3581–3589 (2014). \\n99. Ba, J., Mnih, V. & Kavukcuoglu, K. Multiple object recognition with visual \\nattention. In Proc. International Conference on Learning Representations http://\\narxiv.org/abs/1412.7755 (2014). \\n100. Mnih, V. et al. Human-level control through deep reinforcement learning. Nature  \\n518, 529–533 (2015).\\n101. Bottou, L. From machine learning to machine reasoning. Mach. Learn. 94, \\n133–149 (2014). \\n102. Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image \\ncaption generator. In Proc. International Conference on Machine Learning http://\\narxiv.org/abs/1502.03044 (2014).\\n103. van der Maaten, L. & Hinton, G. E. Visualizing data using t-SNE. J. Mach. Learn.\\nResearch 9, 2579–2605 (2008).\\nAcknowledgements The authors would like to thank the Natural Sciences and \\nEngineering Research Council of Canada, the Canadian Institute For Advanced \\nResearch (CIFAR), the National Science Foundation and Office of Naval Research \\nfor support. Y.L. and Y.B. are CIFAR fellows.\\nAuthor Information Reprints and permissions information is available at \\nwww.nature.com/reprints. The authors declare no competing financial \\ninterests. Readers are welcome to comment on the online version of this \\npaper at go.nature.com/7cjbaa. Correspondence should be addressed to Y.L. \\n(yann@cs.nyu.edu).\\n444 | NATURE | VOL 521 | 28 MAY 2015\\nREVIEWINSIGHT\\n© 2015 Macmillan Publishers Limited. All rights reserved\\n'}, {'file_name': 'resnet', 'text': 'Deep Residual Learning for Image Recognition\\nKaiming He Xiangyu Zhang Shaoqing Ren Jian Sun\\nMicrosoft Research\\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\\nAbstract\\nDeeper neural networks are more difﬁcult to train. We\\npresent a residual learning framework to ease the training\\nof networks that are substantially deeper than those used\\npreviously. We explicitly reformulate the layers as learn-\\ning residual functions with reference to the layer inputs, in-\\nstead of learning unreferenced functions. We provide com-\\nprehensive empirical evidence showing that these residual\\nnetworks are easier to optimize, and can gain accuracy from\\nconsiderably increased depth. On the ImageNet dataset we\\nevaluate residual nets with a depth of up to 152 layers—8×\\ndeeper than VGG nets [40] but still having lower complex-\\nity. An ensemble of these residual nets achieves 3.57% error\\non the ImageNettest set. This result won the 1st place on the\\nILSVRC 2015 classiﬁcation task. We also present analysis\\non CIF AR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance\\nfor many visual recognition tasks. Solely due to our ex-\\ntremely deep representations, we obtain a 28% relative im-\\nprovement on the COCO object detection dataset. Deep\\nresidual nets are foundations of our submissions to ILSVRC\\n& COCO 2015 competitions1, where we also won the 1st\\nplaces on the tasks of ImageNet detection, ImageNet local-\\nization, COCO detection, and COCO segmentation.\\n1. Introduction\\nDeep convolutional neural networks [22, 21] have led\\nto a series of breakthroughs for image classiﬁcation [21,\\n49, 39]. Deep networks naturally integrate low/mid/high-\\nlevel features [49] and classiﬁers in an end-to-end multi-\\nlayer fashion, and the “levels” of features can be enriched\\nby the number of stacked layers (depth). Recent evidence\\n[40, 43] reveals that network depth is of crucial importance,\\nand the leading results [40, 43, 12, 16] on the challenging\\nImageNet dataset [35] all exploit “very deep” [40] models,\\nwith a depth of sixteen [40] to thirty [16]. Many other non-\\ntrivial visual recognition tasks [7, 11, 6, 32, 27] have also\\n1http://image-net.org/challenges/LSVRC/2015/ and\\nhttp://mscoco.org/dataset/#detections-challenge2015 .\\n0 1 2 3 4 5 60\\n10\\n20\\niter. (1e4)\\ntraining error (%)\\n0 1 2 3 4 5 60\\n10\\n20\\niter. (1e4)\\ntest error (%)\\n56-layer\\n20-layer\\n56-layer\\n20-layer\\nFigure 1. Training error (left) and test error (right) on CIFAR-10\\nwith 20-layer and 56-layer “plain” networks. The deeper network\\nhas higher training error, and thus test error. Similar phenomena\\non ImageNet is presented in Fig. 4.\\ngreatly beneﬁted from very deep models.\\nDriven by the signiﬁcance of depth, a question arises:Is\\nlearning better networks as easy as stacking more layers?\\nAn obstacle to answering this question was the notorious\\nproblem of vanishing/exploding gradients [14, 1, 8], which\\nhamper convergence from the beginning. This problem,\\nhowever, has been largely addressed by normalized initial-\\nization [23, 8, 36, 12] and intermediate normalization layers\\n[16], which enable networks with tens of layers to start con-\\nverging for stochastic gradient descent (SGD) with back-\\npropagation [22].\\nWhen deeper networks are able to start converging, a\\ndegradation problem has been exposed: with the network\\ndepth increasing, accuracy gets saturated (which might be\\nunsurprising) and then degrades rapidly. Unexpectedly,\\nsuch degradation is not caused by overﬁtting, and adding\\nmore layers to a suitably deep model leads tohigher train-\\ning error, as reported in [10, 41] and thoroughly veriﬁed by\\nour experiments. Fig. 1 shows a typical example.\\nThe degradation (of training accuracy) indicates that not\\nall systems are similarly easy to optimize. Let us consider a\\nshallower architecture and its deeper counterpart that adds\\nmore layers onto it. There exists a solutionby construction\\nto the deeper model: the added layers areidentity mapping,\\nand the other layers are copied from the learned shallower\\nmodel. The existence of this constructed solution indicates\\nthat a deeper model should produce no higher training error\\nthan its shallower counterpart. But experiments show that\\nour current solvers on hand are unable to ﬁnd solutions that\\n2016 IEEE Conference on Computer Vision and Pattern Recognition\\n1063-6919/16 $31.00 © 2016 IEEE\\nDOI 10.1109/CVPR.2016.90\\n770\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\nidentity\\nweight layer\\nweight layer\\nrelu\\nreluF(x)\\x01+\\x01x\\nx\\nF(x) x\\nFigure 2. Residual learning: a building block.\\nare comparably good or better than the constructed solution\\n(or unable to do so in feasible time).\\nIn this paper, we address the degradation problem by\\nintroducing a deep residual learning framework. In-\\nstead of hoping each few stacked layers directly ﬁt a\\ndesired underlying mapping, we explicitly let these lay-\\ners ﬁt a residual mapping. Formally, denoting the desired\\nunderlying mapping as H(x), we let the stacked nonlinear\\nlayers ﬁt another mapping ofF(x): =H(x) − x. The orig-\\ninal mapping is recast intoF(x)+ x. We hypothesize that it\\nis easier to optimize the residual mapping than to optimize\\nthe original, unreferenced mapping. To the extreme, if an\\nidentity mapping were optimal, it would be easier to push\\nthe residual to zero than to ﬁt an identity mapping by a stack\\nof nonlinear layers.\\nThe formulation ofF(x)+ x can be realized by feedfor-\\nward neural networks with “shortcut connections” (Fig. 2).\\nShortcut connections [2, 33, 48] are those skipping one or\\nmore layers. In our case, the shortcut connections simply\\nperform identity mapping, and their outputs are added to\\nthe outputs of the stacked layers (Fig. 2). Identity short-\\ncut connections add neither extra parameter nor computa-\\ntional complexity. The entire network can still be trained\\nend-to-end by SGD with backpropagation, and can be eas-\\nily implemented using common libraries (e.g., Caffe [19])\\nwithout modifying the solvers.\\nWe present comprehensive experiments on ImageNet\\n[35] to show the degradation problem and evaluate our\\nmethod. We show that: 1) Our extremely deep residual nets\\nare easy to optimize, but the counterpart “plain” nets (that\\nsimply stack layers) exhibit higher training error when the\\ndepth increases; 2) Our deep residual nets can easily enjoy\\naccuracy gains from greatly increased depth, producing re-\\nsults substantially better than previous networks.\\nSimilar phenomena are also shown on the CIFAR-10 set\\n[20], suggesting that the optimization difﬁculties and the\\neffects of our method are not just akin to a particular dataset.\\nWe present successfully trained models on this dataset with\\nover 100 layers, and explore models with over 1000 layers.\\nOn the ImageNet classiﬁcation dataset [35], we obtain\\nexcellent results by extremely deep residual nets. Our 152-\\nlayer residual net is the deepest network ever presented on\\nImageNet, while still having lower complexity than VGG\\nnets [40]. Our ensemble has 3.57% top-5 error on the\\nImageNet test set, and won the 1st place in the ILSVRC\\n2015 classiﬁcation competition. The extremely deep rep-\\nresentations also have excellent generalization performance\\non other recognition tasks, and lead us to further win the\\n1st places on: ImageNet detection, ImageNet localization,\\nCOCO detection, and COCO segmentation in ILSVRC &\\nCOCO 2015 competitions. This strong evidence shows that\\nthe residual learning principle is generic, and we expect that\\nit is applicable in other vision and non-vision problems.\\n2. Related Work\\nResidual Representations. In image recognition, VLAD\\n[18] is a representation that encodes by the residual vectors\\nwith respect to a dictionary, and Fisher V ector [30] can be\\nformulated as a probabilistic version [18] of VLAD. Both\\nof them are powerful shallow representations for image re-\\ntrieval and classiﬁcation [4, 47]. For vector quantization,\\nencoding residual vectors [17] is shown to be more effec-\\ntive than encoding original vectors.\\nIn low-level vision and computer graphics, for solv-\\ning Partial Differential Equations (PDEs), the widely used\\nMultigrid method [3] reformulates the system as subprob-\\nlems at multiple scales, where each subproblem is respon-\\nsible for the residual solution between a coarser and a ﬁner\\nscale. An alternative to Multigrid is hierarchical basis pre-\\nconditioning [44, 45], which relies on variables that repre-\\nsent residual vectors between two scales. It has been shown\\n[3, 44, 45] that these solvers converge much faster than stan-\\ndard solvers that are unaware of the residual nature of the\\nsolutions. These methods suggest that a good reformulation\\nor preconditioning can simplify the optimization.\\nShortcut Connections. Practices and theories that lead to\\nshortcut connections [2, 33, 48] have been studied for a long\\ntime. An early practice of training multi-layer perceptrons\\n(MLPs) is to add a linear layer connected from the network\\ninput to the output [33, 48]. In [43, 24], a few interme-\\ndiate layers are directly connected to auxiliary classiﬁers\\nfor addressing vanishing/exploding gradients. The papers\\nof [38, 37, 31, 46] propose methods for centering layer re-\\nsponses, gradients, and propagated errors, implemented by\\nshortcut connections. In [43], an “inception” layer is com-\\nposed of a shortcut branch and a few deeper branches.\\nConcurrent with our work, “highway networks” [41, 42]\\npresent shortcut connections with gating functions [15].\\nThese gates are data-dependent and have parameters, in\\ncontrast to our identity shortcuts that are parameter-free.\\nWhen a gated shortcut is “closed” (approaching zero), the\\nlayers in highway networks represent non-residual func-\\ntions. On the contrary, our formulation always learns\\nresidual functions; our identity shortcuts are never closed,\\nand all information is always passed through, with addi-\\ntional residual functions to be learned. In addition, high-\\n771\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\nway networks have not demonstrated accuracy gains with\\nextremely increased depth (e.g., over 100 layers).\\n3. Deep Residual Learning\\n3.1. Residual Learning\\nLet us consider H(x) as an underlying mapping to be\\nﬁt by a few stacked layers (not necessarily the entire net),\\nwith x denoting the inputs to the ﬁrst of these layers. If one\\nhypothesizes that multiple nonlinear layers can asymptoti-\\ncally approximate complicated functions2, then it is equiv-\\nalent to hypothesize that they can asymptotically approxi-\\nmate the residual functions, i.e., H(x) − x (assuming that\\nthe input and output are of the same dimensions). So\\nrather than expect stacked layers to approximateH(x),w e\\nexplicitly let these layers approximate a residual function\\nF(x): = H(x) − x. The original function thus becomes\\nF(x)+ x. Although both forms should be able to asymptot-\\nically approximate the desired functions (as hypothesized),\\nthe ease of learning might be different.\\nThis reformulation is motivated by the counterintuitive\\nphenomena about the degradation problem (Fig. 1, left). As\\nwe discussed in the introduction, if the added layers can\\nbe constructed as identity mappings, a deeper model should\\nhave training error no greater than its shallower counter-\\npart. The degradation problem suggests that the solvers\\nmight have difﬁculties in approximating identity mappings\\nby multiple nonlinear layers. With the residual learning re-\\nformulation, if identity mappings are optimal, the solvers\\nmay simply drive the weights of the multiple nonlinear lay-\\ners toward zero to approach identity mappings.\\nIn real cases, it is unlikely that identity mappings are op-\\ntimal, but our reformulation may help to precondition the\\nproblem. If the optimal function is closer to an identity\\nmapping than to a zero mapping, it should be easier for the\\nsolver to ﬁnd the perturbations with reference to an identity\\nmapping, than to learn the function as a new one. We show\\nby experiments (Fig. 7) that the learned residual functions in\\ngeneral have small responses, suggesting that identity map-\\npings provide reasonable preconditioning.\\n3.2. Identity Mapping by Shortcuts\\nWe adopt residual learning to every few stacked layers.\\nA building block is shown in Fig. 2. Formally, in this paper\\nwe consider a building block deﬁned as:\\ny = F(x,{Wi})+ x. (1)\\nHere x and y are the input and output vectors of the lay-\\ners considered. The function F(x,{Wi}) represents the\\nresidual mapping to be learned. For the example in Fig. 2\\nthat has two layers, F = W2σ(W1x) in which σ denotes\\n2This hypothesis, however, is still an open question. See [28].\\nReLU [29] and the biases are omitted for simplifying no-\\ntations. The operation F + x is performed by a shortcut\\nconnection and element-wise addition. We adopt the sec-\\nond nonlinearity after the addition (i.e., σ(y), see Fig. 2).\\nThe shortcut connections in Eqn.(1) introduce neither ex-\\ntra parameter nor computation complexity. This is not only\\nattractive in practice but also important in our comparisons\\nbetween plain and residual networks. We can fairly com-\\npare plain/residual networks that simultaneously have the\\nsame number of parameters, depth, width, and computa-\\ntional cost (except for the negligible element-wise addition).\\nThe dimensions of x and F must be equal in Eqn.(1).\\nIf this is not the case (e.g., when changing the input/output\\nchannels), we can perform a linear projection Ws by the\\nshortcut connections to match the dimensions:\\ny = F(x,{Wi})+ Wsx. (2)\\nWe can also use a square matrixWs in Eqn.(1). But we will\\nshow by experiments that the identity mapping is sufﬁcient\\nfor addressing the degradation problem and is economical,\\nand thus Ws is only used when matching dimensions.\\nThe form of the residual functionF is ﬂexible. Exper-\\niments in this paper involve a function F that has two or\\nthree layers (Fig. 5), while more layers are possible. But if\\nF has only a single layer, Eqn.(1) is similar to a linear layer:\\ny = W1x + x, for which we have not observed advantages.\\nWe also note that although the above notations are about\\nfully-connected layers for simplicity, they are applicable to\\nconvolutional layers. The function F(x,{Wi}) can repre-\\nsent multiple convolutional layers. The element-wise addi-\\ntion is performed on two feature maps, channel by channel.\\n3.3. Network Architectures\\nWe have tested various plain/residual nets, and have ob-\\nserved consistent phenomena. To provide instances for dis-\\ncussion, we describe two models for ImageNet as follows.\\nPlain Network. Our plain baselines (Fig. 3, middle) are\\nmainly inspired by the philosophy of VGG nets [40] (Fig. 3,\\nleft). The convolutional layers mostly have 3×3 ﬁlters and\\nfollow two simple design rules: (i) for the same output\\nfeature map size, the layers have the same number of ﬁl-\\nters; and (ii) if the feature map size is halved, the num-\\nber of ﬁlters is doubled so as to preserve the time com-\\nplexity per layer. We perform downsampling directly by\\nconvolutional layers that have a stride of 2. The network\\nends with a global average pooling layer and a 1000-way\\nfully-connected layer with softmax. The total number of\\nweighted layers is 34 in Fig. 3 (middle).\\nIt is worth noticing that our model hasfewer ﬁlters and\\nlower complexity than VGG nets [40] (Fig. 3, left). Our 34-\\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\\nis only 18% of VGG-19 (19.6 billion FLOPs).\\n772\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\n7x7 conv, 64, /2\\npool, /2\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 128, /2\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 256, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 512, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\navg pool\\nfc 1000\\nimage\\n3x3 conv, 512\\n3x3 conv, 64\\n3x3 conv, 64\\npool, /2\\n3x3 conv, 128\\n3x3 conv, 128\\npool, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\npool, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\npool, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\npool, /2\\nfc 4096\\nfc 4096\\nfc 1000\\nimage\\noutput \\nsize: 112\\noutput \\nsize: 224\\noutput \\nsize: 56\\noutput \\nsize: 28\\noutput \\nsize: 14\\noutput \\nsize: 7\\noutput \\nsize: 1\\nVGG-19 34-layer plain\\n7x7 conv, 64, /2\\npool, /2\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 128, /2\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 256, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 512, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\navg pool\\nfc 1000\\nimage\\n34-layer residual\\nFigure 3. Example network architectures for ImageNet.Left: the\\nVGG-19 model [40] (19.6 billion FLOPs) as a reference. Mid-\\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\\nRight: a residual network with 34 parameter layers (3.6 billion\\nFLOPs). The dotted shortcuts increase dimensions.Table 1shows\\nmore details and other variants.\\nResidual Network. Based on the above plain network, we\\ninsert shortcut connections (Fig. 3, right) which turn the\\nnetwork into its counterpart residual version. The identity\\nshortcuts (Eqn.(1)) can be directly used when the input and\\noutput are of the same dimensions (solid line shortcuts in\\nFig. 3). When the dimensions increase (dotted line shortcuts\\nin Fig. 3), we consider two options: (A) The shortcut still\\nperforms identity mapping, with extra zero entries padded\\nfor increasing dimensions. This option introduces no extra\\nparameter; (B) The projection shortcut in Eqn.(2) is used to\\nmatch dimensions (done by 1×1 convolutions). For both\\noptions, when the shortcuts go across feature maps of two\\nsizes, they are performed with a stride of 2.\\n3.4. Implementation\\nOur implementation for ImageNet follows the practice\\nin [21, 40]. The image is resized with its shorter side ran-\\ndomly sampled in [256,480] for scale augmentation [40].\\nA 224×224 crop is randomly sampled from an image or its\\nhorizontal ﬂip, with the per-pixel mean subtracted [21]. The\\nstandard color augmentation in [21] is used. We adopt batch\\nnormalization (BN) [16] right after each convolution and\\nbefore activation, following [16]. We initialize the weights\\nas in [12] and train all plain/residual nets from scratch. We\\nuse SGD with a mini-batch size of 256. The learning rate\\nstarts from 0.1 and is divided by 10 when the error plateaus,\\nand the models are trained for up to60 × 104 iterations. We\\nuse a weight decay of 0.0001 and a momentum of 0.9. We\\ndo not use dropout [13], following the practice in [16].\\nIn testing, for comparison studies we adopt the standard\\n10-crop testing [21]. For best results, we adopt the fully-\\nconvolutional form as in [40, 12], and average the scores\\nat multiple scales (images are resized such that the shorter\\nside is in{224,256,384,480,640}).\\n4. Experiments\\n4.1. ImageNet Classiﬁcation\\nWe evaluate our method on the ImageNet 2012 classiﬁ-\\ncation dataset [35] that consists of 1000 classes. The models\\nare trained on the 1.28 million training images, and evalu-\\nated on the 50k validation images. We also obtain a ﬁnal\\nresult on the 100k test images, reported by the test server.\\nWe evaluate both top-1 and top-5 error rates.\\nPlain Networks. We ﬁrst evaluate 18-layer and 34-layer\\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\\n18-layer plain net is of a similar form. See Table 1 for de-\\ntailed architectures.\\nThe results in Table 2 show that the deeper 34-layer plain\\nnet has higher validation error than the shallower 18-layer\\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\\npare their training/validation errors during the training pro-\\ncedure. We have observed the degradation problem - the\\n773\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\nlayer name output size 18-layer 34-layer 50-layer 101-layer 152-layer\\nconv1 112×112 7×7, 64, stride 2\\nconv2 x 56×56\\n3×3 max pool, stride 2\\n[\\n3×3, 64\\n3×3, 64\\n]\\n×2\\n[\\n3×3, 64\\n3×3, 64\\n]\\n×3\\n⎡\\n⎣\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n⎤\\n⎦×3\\n⎡\\n⎣\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n⎤\\n⎦×3\\n⎡\\n⎣\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n⎤\\n⎦×3\\nconv3 x 28×28\\n[\\n3×3, 128\\n3×3, 128\\n]\\n×2\\n[\\n3×3, 128\\n3×3, 128\\n]\\n×4\\n⎡\\n⎣\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n⎤\\n⎦×4\\n⎡\\n⎣\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n⎤\\n⎦×4\\n⎡\\n⎣\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n⎤\\n⎦×8\\nconv4 x 14×14\\n[\\n3×3, 256\\n3×3, 256\\n]\\n×2\\n[\\n3×3, 256\\n3×3, 256\\n]\\n×6\\n⎡\\n⎣\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n⎤\\n⎦×6\\n⎡\\n⎣\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n⎤\\n⎦×23\\n⎡\\n⎣\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n⎤\\n⎦×36\\nconv5 x 7×7\\n[\\n3×3, 512\\n3×3, 512\\n]\\n×2\\n[\\n3×3, 512\\n3×3, 512\\n]\\n×3\\n⎡\\n⎣\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n⎤\\n⎦×3\\n⎡\\n⎣\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n⎤\\n⎦×3\\n⎡\\n⎣\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n⎤\\n⎦×3\\n1×1 average pool, 1000-d fc, softmax\\nFLOPs 1.8×109 3.6×109 3.8×109 7.6×109 11.3×109\\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\\nsampling is performed by conv31, conv4 1, and conv51 with a stride of 2.\\n0 10 20 30 40 5020\\n30\\n40\\n50\\n60\\niter. (1e4)\\nerror (%)\\nplain-18\\nplain-34\\n0 10 20 30 40 5020\\n30\\n40\\n50\\n60\\niter. (1e4)\\nerror (%)\\nResNet-18\\nResNet-34\\n18-layer\\n34-layer\\n18-layer\\n34-layer\\nFigure 4. Training onImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\\ntheir plain counterparts.\\nplain ResNet\\n18 layers 27.94 27.88\\n34 layers 28.54 25.03\\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\\nHere the ResNets have no extra parameter compared to their plain\\ncounterparts. Fig. 4 shows the training procedures.\\n34-layer plain net has highertraining error throughout the\\nwhole training procedure, even though the solution space\\nof the 18-layer plain network is a subspace of that of the\\n34-layer one.\\nWe argue that this optimization difﬁculty isunlikely to\\nbe caused by vanishing gradients. These plain networks are\\ntrained with BN [16], which ensures forward propagated\\nsignals to have non-zero variances. We also verify that the\\nbackward propagated gradients exhibit healthy norms with\\nBN. So neither forward nor backward signals vanish. In\\nfact, the 34-layer plain net is still able to achieve compet-\\nitive accuracy (Table 3), suggesting that the solver works\\nto some extent. We conjecture that the deep plain nets may\\nhave exponentially low convergence rates, which impact the\\nreducing of the training error3. The reason for such opti-\\nmization difﬁculties will be studied in the future.\\nResidual Networks. Next we evaluate 18-layer and 34-\\nlayer residual nets (ResNets). The baseline architectures\\nare the same as the above plain nets, expect that a shortcut\\nconnection is added to each pair of 3×3 ﬁlters as in Fig. 3\\n(right). In the ﬁrst comparison (Table 2 and Fig. 4 right),\\nwe use identity mapping for all shortcuts and zero-padding\\nfor increasing dimensions (option A). So they haveno extra\\nparameter compared to the plain counterparts.\\nWe have three major observations from Table 2 and\\nFig. 4. First, the situation is reversed with residual learn-\\ning – the 34-layer ResNet is better than the 18-layer ResNet\\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\\nconsiderably lower training error and is generalizable to the\\nvalidation data. This indicates that the degradation problem\\nis well addressed in this setting and we manage to obtain\\naccuracy gains from increased depth.\\nSecond, compared to its plain counterpart, the 34-layer\\n3We have experimented with more training iterations (3×) and still ob-\\nserved the degradation problem, suggesting that this problem cannot be\\nfeasibly addressed by simply using more iterations.\\n774\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\nmodel top-1 err. top-5 err.\\nVGG-16 [40] 28.07 9.33\\nGoogLeNet [43] - 9.15\\nPReLU-net [12] 24.27 7.38\\nplain-34 28.54 10.02\\nResNet-34 A 25.03 7.76\\nResNet-34 B 24.52 7.46\\nResNet-34 C 24.19 7.40\\nResNet-50 22.85 6.71\\nResNet-101 21.75 6.05\\nResNet-152 21.43 5.71\\nTable 3. Error rates (%,10-crop testing) on ImageNet validation.\\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\\nthat only uses projections for increasing dimensions.\\nmethod top-1 err. top-5 err.\\nVGG [40] (ILSVRC’14) - 8.43 †\\nGoogLeNet [43] (ILSVRC’14) - 7.89\\nVGG [40] (v5) 24.4 7.1\\nPReLU-net [12] 21.59 5.71\\nBN-inception [16] 21.99 5.81\\nResNet-34 B 21.84 5.71\\nResNet-34 C 21.53 5.60\\nResNet-50 20.74 5.25\\nResNet-101 19.87 4.60\\nResNet-152 19.38 4.49\\nTable 4. Error rates (%) ofsingle-model results on the ImageNet\\nvalidation set (except† reported on the test set).\\nmethod top-5 err. (test)\\nVGG [40] (ILSVRC’14) 7.32\\nGoogLeNet [43] (ILSVRC’14) 6.66\\nVGG [40] (v5) 6.8\\nPReLU-net [12] 4.94\\nBN-inception [16] 4.82\\nResNet (ILSVRC’15) 3.57\\nTable 5. Error rates (%) ofensembles. The top-5 error is on the\\ntest set of ImageNet and reported by the test server.\\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\\nfrom the successfully reduced training error (Fig. 4 rightvs.\\nleft). This comparison veriﬁes the effectiveness of residual\\nlearning on extremely deep systems.\\nLast, we also note that the 18-layer plain/residual nets\\nare comparably accurate (Table 2), but the 18-layer ResNet\\nconverges faster (Fig. 4 rightvs. left). When the net is “not\\noverly deep” (18 layers here), the current SGD solver is still\\nable to ﬁnd good solutions to the plain net. In this case, the\\nResNet eases the optimization by providing faster conver-\\ngence at the early stage.\\nIdentity vs. Projection Shortcuts. We have shown that\\n3x3, 64\\n1x1, 64\\nrelu\\n1x1, 256\\nrelu\\nrelu\\n3x3, 64\\n3x3, 64\\nrelu\\nrelu\\n64-d 256-d\\nFigure 5. A deeper residual function F for ImageNet. Left: a\\nbuilding block (on 56×56 feature maps) as in Fig. 3 for ResNet-\\n34. Right: a “bottleneck” building block for ResNet-50/101/152.\\nparameter-free, identity shortcuts help with training. Next\\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\\ncompare three options: (A) zero-padding shortcuts are used\\nfor increasing dimensions, and all shortcuts are parameter-\\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\\ntion shortcuts are used for increasing dimensions, and other\\nshortcuts are identity; and (C) all shortcuts are projections.\\nTable 3 shows that all three options are considerably bet-\\nter than the plain counterpart. B is slightly better than A. We\\nargue that this is because the zero-padded dimensions in A\\nindeed have no residual learning. C is marginally better than\\nB, and we attribute this to the extra parameters introduced\\nby many (thirteen) projection shortcuts. But the small dif-\\nferences among A/B/C indicate that projection shortcuts are\\nnot essential for addressing the degradation problem. So we\\ndo not use option C in the rest of this paper, to reduce mem-\\nory/time complexity and model sizes. Identity shortcuts are\\nparticularly important for not increasing the complexity of\\nthe bottleneck architectures that are introduced below.\\nDeeper Bottleneck Architectures. Next we describe our\\ndeeper nets for ImageNet. Because of concerns on the train-\\ning time that we can afford, we modify the building block\\nas a bottleneck design4. For each residual function F,w e\\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\\nare 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers\\nare responsible for reducing and then increasing (restoring)\\ndimensions, leaving the 3×3 layer a bottleneck with smaller\\ninput/output dimensions. Fig. 5 shows an example, where\\nboth designs have similar time complexity.\\nThe parameter-free identity shortcuts are particularly im-\\nportant for the bottleneck architectures. If the identity short-\\ncut in Fig. 5 (right) is replaced with projection, one can\\nshow that the time complexity and model size are doubled,\\nas the shortcut is connected to the two high-dimensional\\nends. So identity shortcuts lead to more efﬁcient models\\nfor the bottleneck designs.\\n50-layer ResNet: We replace each 2-layer block in the\\n4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy\\nfrom increased depth (as shown on CIFAR-10), but are not as economical\\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\\nto practical considerations. We further note that the degradation problem\\nof plain nets is also witnessed for the bottleneck designs.\\n775\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\n34-layer net with this 3-layer bottleneck block, resulting in\\na 50-layer ResNet (Table 1). We use option B for increasing\\ndimensions. This model has 3.8 billion FLOPs.\\n101-layer and 152-layer ResNets: We construct 101-\\nlayer and 152-layer ResNets by using more 3-layer blocks\\n(Table 1). Remarkably, although the depth is signiﬁcantly\\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\\nhas lower complexity than VGG-16/19 nets (15.3/19.6 bil-\\nlion FLOPs).\\nThe 50/101/152-layer ResNets are more accurate than\\nthe 34-layer ones by considerable margins (Table 3 and 4).\\nWe do not observe the degradation problem and thus en-\\njoy signiﬁcant accuracy gains from considerably increased\\ndepth. The beneﬁts of depth are witnessed for all evaluation\\nmetrics (Table 3 and 4).\\nComparisons with State-of-the-art Methods. In Table 4\\nwe compare with the previous best single-model results.\\nOur baseline 34-layer ResNets have achieved very compet-\\nitive accuracy. Our 152-layer ResNet has a single-model\\ntop-5 validation error of 4.49%. This single-model result\\noutperforms all previous ensemble results (Table 5). We\\ncombine six models of different depth to form an ensemble\\n(only with two 152-layer ones at the time of submitting).\\nThis leads to 3.57% top-5 error on the test set (Table 5).\\nThis entry won the 1st place in ILSVRC 2015.\\n4.2. CIFAR-10 and Analysis\\nWe conducted more studies on the CIFAR-10 dataset\\n[20], which consists of 50k training images and 10k test-\\ning images in 10 classes. We present experiments trained\\non the training set and evaluated on the test set. Our focus\\nis on the behaviors of extremely deep networks, but not on\\npushing the state-of-the-art results, so we intentionally use\\nsimple architectures as follows.\\nThe plain/residual architectures follow the form in Fig. 3\\n(middle/right). The network inputs are 32×32 images, with\\nthe per-pixel mean subtracted. The ﬁrst layer is 3×3 convo-\\nlutions. Then we use a stack of6n layers with 3×3 convo-\\nlutions on the feature maps of sizes{32,16,8} respectively,\\nwith 2n layers for each feature map size. The numbers of\\nﬁlters are{16,32,64} respectively. The subsampling is per-\\nformed by convolutions with a stride of 2. The network ends\\nwith a global average pooling, a 10-way fully-connected\\nlayer, and softmax. There are totally 6n+2 stacked weighted\\nlayers. The following table summarizes the architecture:\\noutput map size 32×32 16×16 8×8\\n# layers 1+2n 2n 2n\\n# ﬁlters 16 32 64\\nWhen shortcut connections are used, they are connected\\nto the pairs of 3×3 layers (totally 3n shortcuts). On this\\ndataset we use identity shortcuts in all cases (i.e., option A),\\nmethod error (%)\\nMaxout [9] 9.38\\nNIN [25] 8.81\\nDSN [24] 8.22\\n# layers # params\\nFitNet [34] 19 2.5M 8.39\\nHighway [41, 42] 19 2.3M 7.54 (7.72±0.16)\\nHighway [41, 42] 32 1.25M 8.80\\nResNet 20 0.27M 8.75\\nResNet 32 0.46M 7.51\\nResNet 44 0.66M 7.17\\nResNet 56 0.85M 6.97\\nResNet 110 1.7M 6.43 (6.61±0.16)\\nResNet 1202 19.4M 7.93\\nTable 6. Classiﬁcation error on theCIFAR-10 test set. All meth-\\nods are with data augmentation. For ResNet-110, we run it 5 times\\nand show “best (mean±std)” as in [42].\\nso our residual models have exactly the same depth, width,\\nand number of parameters as the plain counterparts.\\nWe use a weight decay of 0.0001 and momentum of 0.9,\\nand adopt the weight initialization in [12] and BN [16] but\\nwith no dropout. These models are trained with a mini-\\nbatch size of 128 on two GPUs. We start with a learning\\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\\nterminate training at 64k iterations, which is determined on\\na 45k/5k train/val split. We follow the simple data augmen-\\ntation in [24] for training: 4 pixels are padded on each side,\\na n da3 2×32 crop is randomly sampled from the padded\\nimage or its horizontal ﬂip. For testing, we only evaluate\\nthe single view of the original 32×32 image.\\nWe compare n = {3,5,7,9}, leading to 20, 32, 44, and\\n56-layer networks. Fig. 6 (left) shows the behaviors of the\\nplain nets. The deep plain nets suffer from increased depth,\\nand exhibit higher training error when going deeper. This\\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\\non MNIST (see [41]), suggesting that such an optimization\\ndifﬁculty is a fundamental problem.\\nFig. 6 (middle) shows the behaviors of ResNets. Also\\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\\nmanage to overcome the optimization difﬁculty and demon-\\nstrate accuracy gains when the depth increases.\\nWe further explore n =1 8 that leads to a 110-layer\\nResNet. In this case, we ﬁnd that the initial learning rate\\nof 0.1 is slightly too large to start converging5. So we use\\n0.01 to warm up the training until the training error is below\\n80% (about 400 iterations), and then go back to 0.1 and con-\\ntinue training. The rest of the learning schedule is as done\\npreviously. This 110-layer network converges well (Fig. 6,\\nmiddle). It has fewer parameters than other deep and thin\\n5With an initial learning rate of 0.1, it starts converging (<90% error)\\nafter several epochs, but still reaches similar accuracy.\\n776\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\n0 1 2 3 4 5 60\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\nplain-20\\nplain-32\\nplain-44\\nplain-56\\n0 1 2 3 4 5 60\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\nResNet-20\\nResNet-32\\nResNet-44\\nResNet-56\\nResNet-11056-layer\\n20-layer\\n110-layer\\n20-layer\\n4 5 60\\n1\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\nresidual-110\\nresidual-1202\\nFigure 6. Training onCIFAR-10. Dashed lines denote training error, and bold lines denote testing error.Left: plain networks. The error\\nof plain-110 is higher than 60% and not displayed.Middle: ResNets. Right: ResNets with 110 and 1202 layers.\\n0 20 40 60 80 100\\n1\\n2\\n3\\nlayer index (sorted by magnitude)\\nstd\\nplain-20\\nplain-56\\nResNet-20\\nResNet-56\\nResNet-110\\n0 20 40 60 80 100\\n1\\n2\\n3\\nlayer index (original)\\nstd\\nplain-20\\nplain-56\\nResNet-20\\nResNet-56\\nResNet-110\\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\\n10. The responses are the outputs of each 3×3 layer, after BN and\\nbefore nonlinearity. Top: the layers are shown in their original\\norder. Bottom: the responses are ranked in descending order.\\nnetworks such as FitNet [34] and Highway [41] (Table 6),\\nyet is among the state-of-the-art results (6.43%, Table 6).\\nAnalysis of Layer Responses. Fig. 7 shows the standard\\ndeviations (std) of the layer responses. The responses are\\nthe outputs of each 3×3 layer, after BN and before other\\nnonlinearity (ReLU/addition). For ResNets, this analy-\\nsis reveals the response strength of the residual functions.\\nFig. 7 shows that ResNets have generally smaller responses\\nthan their plain counterparts. These results support our ba-\\nsic motivation (Sec.3.1) that the residual functions might\\nbe generally closer to zero than the non-residual functions.\\nWe also notice that the deeper ResNet has smaller magni-\\ntudes of responses, as evidenced by the comparisons among\\nResNet-20, 56, and 110 in Fig. 7. When there are more\\nlayers, an individual layer of ResNets tends to modify the\\nsignal less.\\nExploring Over 1000 layers. We explore an aggressively\\ndeep model of over 1000 layers. We set n = 200 that\\nleads to a 1202-layer network, which is trained as described\\nabove. Our method shows no optimization difﬁculty, and\\nthis 103-layer network is able to achieve training error\\n<0.1% (Fig. 6, right). Its test error is still fairly good\\n(7.93%, Table 6).\\nBut there are still open problems on such aggressively\\ndeep models. The testing result of this 1202-layer network\\nis worse than that of our 110-layer network, although both\\ntraining data 07+12 07++12\\ntest data VOC 07 test VOC 12 test\\nVGG-16 73.2 70.4\\nResNet-101 76.4 73.8\\nTable 7. Object detection mAP (%) on the PASCAL VOC\\n2007/2012 test sets using baseline Faster R-CNN. See also ap-\\npendix for better results.\\nmetric mAP@.5 mAP@[.5, .95]\\nVGG-16 41.5 21.2\\nResNet-101 48.4 27.2\\nTable 8. Object detection mAP (%) on the COCO validation set\\nusing baseline Faster R-CNN. See also appendix for better results.\\nhave similar training error. We argue that this is because of\\noverﬁtting. The 1202-layer network may be unnecessarily\\nlarge (19.4M) for this small dataset. Strong regularization\\nsuch as maxout [9] or dropout [13] is applied to obtain the\\nbest results ([9, 25, 24, 34]) on this dataset. In this paper, we\\nuse no maxout/dropout and just simply impose regulariza-\\ntion via deep and thin architectures by design, without dis-\\ntracting from the focus on the difﬁculties of optimization.\\nBut combining with stronger regularization may improve\\nresults, which we will study in the future.\\n4.3. Object Detection on PASCAL and MS COCO\\nOur method has good generalization performance on\\nother recognition tasks. Table 7 and 8 show the object de-\\ntection baseline results on PASCAL VOC 2007 and 2012\\n[5] and COCO [26]. We adoptFaster R-CNN[32] as the de-\\ntection method. Here we are interested in the improvements\\nof replacing VGG-16 [40] with ResNet-101. The detection\\nimplementation (see appendix) of using both models is the\\nsame, so the gains can only be attributed to better networks.\\nMost remarkably, on the challenging COCO dataset we ob-\\ntain a 6.0% increase in COCO’s standard metric (mAP@[.5,\\n.95]), which is a 28% relative improvement. This gain is\\nsolely due to the learned representations.\\nBased on deep residual nets, we won the 1st places in\\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\\nageNet detection, ImageNet localization, COCO detection,\\nand COCO segmentation. The details are in the appendix.\\n777\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\nReferences\\n[1] Y . Bengio, P . Simard, and P . Frasconi. Learning long-term dependen-\\ncies with gradient descent is difﬁcult.IEEE Transactions on Neural\\nNetworks, 5(2):157–166, 1994.\\n[2] C. M. Bishop. Neural networks for pattern recognition. Oxford\\nuniversity press, 1995.\\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\\n2000.\\n[4] K. Chatﬁeld, V . Lempitsky, A. V edaldi, and A. Zisserman. The devil\\nis in the details: an evaluation of recent feature encoding methods.\\nIn BMVC, 2011.\\n[5] M. Everingham, L. V an Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman. The Pascal Visual Object Classes (VOC) Challenge.IJCV,\\npages 303–338, 2010.\\n[6] R. Girshick. Fast R-CNN. In ICCV, 2015.\\n[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\\narchies for accurate object detection and semantic segmentation. In\\nCVPR, 2014.\\n[8] X. Glorot and Y . Bengio. Understanding the difﬁculty of training\\ndeep feedforward neural networks. InAISTATS, 2010.\\n[9] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\\nY . Bengio. Maxout networks.arXiv:1302.4389, 2013.\\n[10] K. He and J. Sun. Convolutional neural networks at constrained time\\ncost. In CVPR, 2015.\\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition. InECCV, 2014.\\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁcation. In\\nICCV, 2015.\\n[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. Improving neural networks by preventing co-\\nadaptation of feature detectors.arXiv:1207.0580, 2012.\\n[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen.\\nDiploma thesis, TU Munich, 1991.\\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory.Neural\\ncomputation, 9(8):1735–1780, 1997.\\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. InICML, 2015.\\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\\nneighbor search. TPAMI, 33, 2011.\\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P . Perez, and\\nC. Schmid. Aggregating local image descriptors into compact codes.\\nTPAMI, 2012.\\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\\nfast feature embedding. arXiv:1408.5093, 2014.\\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\\nages. Tech Report, 2009.\\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation\\nwith deep convolutional neural networks. InNIPS, 2012.\\n[22] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\\nwritten zip code recognition.Neural computation, 1989.\\n[23] Y . LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop.\\nIn Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.\\n[24] C.-Y . Lee, S. Xie, P . Gallagher, Z. Zhang, and Z. Tu. Deeply-\\nsupervised nets. arXiv:1409.5185, 2014.\\n[25] M. Lin, Q. Chen, and S. Y an. Network in network.arXiv:1312.4400,\\n2013.\\n[26] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\\nP . Doll´ar, and C. L. Zitnick. Microsoft COCO: Common objects in\\ncontext. In ECCV. 2014.\\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\\nfor semantic segmentation. InCVPR, 2015.\\n[28] G. Mont ´ufar, R. Pascanu, K. Cho, and Y . Bengio. On the number of\\nlinear regions of deep neural networks. InNIPS, 2014.\\n[29] V . Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\\nboltzmann machines. In ICML, 2010.\\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\\nimage categorization. In CVPR, 2007.\\n[31] T. Raiko, H. V alpola, and Y . LeCun. Deep learning made easier by\\nlinear transformations in perceptrons. InAISTATS, 2012.\\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\\nreal-time object detection with region proposal networks. In NIPS,\\n2015.\\n[33] B. D. Ripley. Pattern recognition and neural networks. Cambridge\\nuniversity press, 1996.\\n[34] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio. Fitnets: Hints for thin deep nets. InICLR, 2015.\\n[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\\nlarge scale visual recognition challenge.arXiv:1409.0575, 2014.\\n[36] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\\nthe nonlinear dynamics of learning in deep linear neural networks.\\narXiv:1312.6120, 2013.\\n[37] N. N. Schraudolph. Accelerated gradient descent by factor-centering\\ndecomposition. Technical report, 1998.\\n[38] N. N. Schraudolph. Centering neural network gradient factors. In\\nNeural Networks: Tricks of the Trade , pages 207–226. Springer,\\n1998.\\n[39] P . Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . Le-\\nCun. Overfeat: Integrated recognition, localization and detection\\nusing convolutional networks. InICLR, 2014.\\n[40] K. Simonyan and A. Zisserman. V ery deep convolutional networks\\nfor large-scale image recognition. InICLR, 2015.\\n[41] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\\narXiv:1505.00387, 2015.\\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\\nnetworks. 1507.06228, 2015.\\n[43] C. Szegedy, W. Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelov, D. Er-\\nhan, V . V anhoucke, and A. Rabinovich. Going deeper with convolu-\\ntions. In CVPR, 2015.\\n[44] R. Szeliski. Fast surface interpolation using hierarchical basis func-\\ntions. TPAMI, 1990.\\n[45] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\\nSIGGRAPH, 2006.\\n[46] T. V atanen, T. Raiko, H. V alpola, and Y . LeCun. Pushing stochas-\\ntic gradient towards second-order methods–backpropagation learn-\\ning with transformations in nonlinearities. In Neural Information\\nProcessing, 2013.\\n[47] A. V edaldi and B. Fulkerson. VLFeat: An open and portable library\\nof computer vision algorithms, 2008.\\n[48] W. V enables and B. Ripley. Modern applied statistics with s-plus.\\n1999.\\n[49] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\\ntional neural networks. In ECCV, 2014.\\n778\\nAuthorized licensed use limited to: UNIV OF CHICAGO LIBRARY. Downloaded on January 11,2025 at 17:05:44 UTC from IEEE Xplore.  Restrictions apply. \\n'}, {'file_name': 'attention', 'text': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2\\nFigure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3\\nScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\nMultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11\\n'}, {'file_name': 'chexnet', 'text': 'CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays\\nwith Deep Learning\\nPranav Rajpurkar* 1 Jeremy Irvin* 1 Kaylie Zhu1 Brandon Yang1 Hershel Mehta1\\nTony Duan1 Daisy Ding1 Aarti Bagul1 Robyn L. Ball2 Curtis Langlotz3 Katie Shpanskaya3\\nMatthew P. Lungren3 Andrew Y. Ng1\\nAbstract\\nWe develop an algorithm that can detect\\npneumonia from chest X-rays at a level ex-\\nceeding practicing radiologists. Our algo-\\nrithm, CheXNet, is a 121-layer convolutional\\nneural network trained on ChestX-ray14, cur-\\nrently the largest publicly available chest X-\\nray dataset, containing over 100,000 frontal-\\nview X-ray images with 14 diseases. Four\\npracticing academic radiologists annotate a\\ntest set, on which we compare the perfor-\\nmance of CheXNet to that of radiologists.\\nWe ﬁnd that CheXNet exceeds average ra-\\ndiologist performance on the F1 metric. We\\nextend CheXNet to detect all 14 diseases in\\nChestX-ray14 and achieve state of the art re-\\nsults on all 14 diseases.\\n1. Introduction\\nMore than 1 million adults are hospitalized with pneu-\\nmonia and around 50,000 die from the disease every\\nyear in the US alone (CDC, 2017). Chest X-rays\\nare currently the best available method for diagnosing\\npneumonia (WHO, 2001), playing a crucial role in clin-\\nical care (Franquet, 2001) and epidemiological studies\\n(Cherian et al., 2005). However, detecting pneumo-\\nnia in chest X-rays is a challenging task that relies on\\nthe availability of expert radiologists. In this work, we\\npresent a model that can automatically detect pneu-\\nmonia from chest X-rays at a level exceeding practicing\\nradiologists.\\n*Equal contribution 1Stanford University Depart-\\nment of Computer Science 2Stanford University De-\\npartment of Medicine 3Stanford University Depart-\\nment of Radiology. Correspondence to: Pranav\\nRajpurkar <pranavsr@cs.stanford.edu>, Jeremy Irvin\\n<jirvin16@cs.stanford.edu>.\\nProject website at https://stanfordmlgroup.\\ngithub.io/projects/chexnet\\nOutput\\nPneumonia Positive (85%)\\nInput\\nChest X-Ray Image\\nCheXNet\\n121-layer CNN\\nFigure 1.CheXNet is a 121-layer convolutional neural net-\\nwork that takes a chest X-ray image as input, and outputs\\nthe probability of a pathology. On this example, CheXnet\\ncorrectly detects pneumonia and also localizes areas in the\\nimage most indicative of the pathology.\\nOur model, ChexNet (shown in Figure 1), is a 121-\\nlayer convolutional neural network that inputs a chest\\nX-ray image and outputs the probability of pneumonia\\nalong with a heatmap localizing the areas of the im-\\nage most indicative of pneumonia. We train CheXNet\\non the recently released ChestX-ray14 dataset (Wang\\net al., 2017), which contains 112,120 frontal-view chest\\nX-ray images individually labeled with up to 14 diﬀer-\\nent thoracic diseases, including pneumonia. We use\\narXiv:1711.05225v3  [cs.CV]  25 Dec 2017\\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\\nF1 Score (95% CI)\\nRadiologist 1 0.383 (0.309, 0.453)\\nRadiologist 2 0.356 (0.282, 0.428)\\nRadiologist 3 0.365 (0.291, 0.435)\\nRadiologist 4 0.442 (0.390, 0.492)\\nRadiologist Avg. 0.387 (0.330, 0.442)\\nCheXNet 0.435 (0.387, 0.481)\\nTable 1.We compare radiologists and our model on the F1\\nmetric, which is the harmonic average of the precision and\\nrecall of the models. CheXNet achieves an F1 score of 0.435\\n(95% CI 0.387, 0.481), higher than the radiologist average\\nof 0.387 (95% CI 0.330, 0.442). We use the bootstrap to\\nﬁnd that the diﬀerence in performance is statistically sig-\\nniﬁcant.\\ndense connections (Huang et al., 2016) and batch nor-\\nmalization (Ioﬀe & Szegedy, 2015) to make the opti-\\nmization of such a deep network tractable.\\nDetecting pneumonia in chest radiography can be diﬃ-\\ncult for radiologists. The appearance of pneumonia in\\nX-ray images is often vague, can overlap with other di-\\nagnoses, and can mimic many other benign abnormal-\\nities. These discrepancies cause considerable variabil-\\nity among radiologists in the diagnosis of pneumonia\\n(Neuman et al., 2012; Davies et al., 1996; Hopstaken\\net al., 2004). To estimate radiologist performance, we\\ncollect annotations from four practicing academic radi-\\nologists on a subset of 420 images from ChestX-ray14.\\nOn these 420 images, we measure performance of in-\\ndividual radiologists and the model.\\nWe ﬁnd that the model exceeds the average ra-\\ndiologist performance on the pneumonia detection\\ntask. To compare CheXNet against previous work us-\\ning ChestX-ray14, we make simple modiﬁcations to\\nCheXNet to detect all 14 diseases in ChestX-ray14,\\nand ﬁnd that we outperform best published results on\\nall 14 diseases. Automated detection of diseases from\\nchest X-rays at the level of expert radiologists would\\nnot only have tremendous beneﬁt in clinical settings,\\nit would also be invaluable in delivery of health care\\nto populations with inadequate access to diagnostic\\nimaging specialists.\\n2. CheXNet\\n2.1. Problem Formulation\\nThe pneumonia detection task is a binary classiﬁcation\\nproblem, where the input is a frontal-view chest X-\\nray image X and the output is a binary label y ∈\\n{0,1}indicating the absence or presence of pneumonia\\nrespectively. For a single example in the training set,\\nwe optimize the weighted binary cross entropy loss\\nL(X,y) = −w+ ·ylog p(Y = 1|X)\\n−w− ·(1 −y) logp(Y = 0|X),\\nwhere p(Y = i|X) is the probability that the network\\nassigns to the label i, w+ = |N|/(|P|+|N|), and w− =\\n|P|/(|P|+|N|) with |P|and |N|the number of positive\\ncases and negative cases of pneumonia in the training\\nset respectively.\\n2.2. Model Architecture and Training\\nCheXNet is a 121-layer Dense Convolutional Net-\\nwork (DenseNet) (Huang et al., 2016) trained on the\\nChestX-ray 14 dataset. DenseNets improve ﬂow of in-\\nformation and gradients through the network, making\\nthe optimization of very deep networks tractable. We\\nreplace the ﬁnal fully connected layer with one that\\nhas a single output, after which we apply a sigmoid\\nnonlinearity.\\nThe weights of the network are initialized with weights\\nfrom a model pretrained on ImageNet (Deng et al.,\\n2009). The network is trained end-to-end using Adam\\nwith standard parameters ( β1 = 0.9 and β2 = 0.999)\\n(Kingma & Ba, 2014). We train the model using mini-\\nbatches of size 16. We use an initial learning rate of\\n0.001 that is decayed by a factor of 10 each time the\\nvalidation loss plateaus after an epoch, and pick the\\nmodel with the lowest validation loss.\\n3. Data\\n3.1. Training\\nWe use the ChestX-ray14 dataset released by Wang\\net al. (2017) which contains 112,120 frontal-view X-ray\\nimages of 30,805 unique patients. Wang et al. (2017)\\nannotate each image with up to 14 diﬀerent thoracic\\npathology labels using automatic extraction methods\\non radiology reports. We label images that have pneu-\\nmonia as one of the annotated pathologies as positive\\nexamples and label all other images as negative exam-\\nples. For the pneumonia detection task, we randomly\\nsplit the dataset into training (28744 patients, 98637\\nimages), validation (1672 patients, 6351 images), and\\ntest (389 patients, 420 images). There is no patient\\noverlap between the sets.\\nBefore inputting the images into the network, we\\ndownscale the images to 224×224 and normalize based\\non the mean and standard deviation of images in the\\nImageNet training set. We also augment the training\\ndata with random horizontal ﬂipping.\\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\\nPathology Wang et al. (2017) Yao et al. (2017) CheXNet (ours)\\nAtelectasis 0.716 0.772 0.8094\\nCardiomegaly 0.807 0.904 0.9248\\nEﬀusion 0.784 0.859 0.8638\\nInﬁltration 0.609 0.695 0.7345\\nMass 0.706 0.792 0.8676\\nNodule 0.671 0.717 0.7802\\nPneumonia 0.633 0.713 0.7680\\nPneumothorax 0.806 0.841 0.8887\\nConsolidation 0.708 0.788 0.7901\\nEdema 0.835 0.882 0.8878\\nEmphysema 0.815 0.829 0.9371\\nFibrosis 0.769 0.767 0.8047\\nPleural Thickening 0.708 0.765 0.8062\\nHernia 0.767 0.914 0.9164\\nTable 2.CheXNet outperforms the best published results on all 14 pathologies in the ChestX-ray14 dataset. In detecting\\nMass, Nodule, Pneumonia, and Emphysema, CheXNet has a margin of >0.05 AUROC over previous state of the art\\nresults.\\n3.2. Test\\nWe collected a test set of 420 frontal chest X-rays. An-\\nnotations were obtained independently from four prac-\\nticing radiologists at Stanford University, who were\\nasked to label all 14 pathologies in Wang et al. (2017).\\nThe radiologists had 4, 7, 25, and 28 years of experi-\\nence, and one of the radiologists is a sub-specialty fel-\\nlowship trained thoracic radiologist. Radiologists did\\nnot have access to any patient information or knowl-\\nedge of disease prevalence in the data. Labels were\\nentered into a standardized data entry program.\\n4. CheXNet vs. Radiologist\\nPerformance\\n4.1. Comparison\\nWe assess the performance of both radiologists and\\nCheXNet on the test set for the pneumonia detection\\ntask. Recall that for each of the images in the test set,\\nwe have 4 labels from four practicing radiologists and\\n1 label from CheXNet. We compute the F1 score for\\neach individual radiologist and for CheXNet against\\neach of the other 4 labels as ground truth. We report\\nthe mean of the 4 resulting F1 scores for each radi-\\nologist and for CheXNet, along with the average F1\\nacross the radiologists. We use the bootstrap to con-\\nstruct 95% bootstrap conﬁdence intervals (CIs), cal-\\nculating the average F1 score for both the radiologists\\nand CheXNet on 10,000 bootstrap samples, sampled\\nwith replacement from the test set. We take the 2.5th\\nand 97.5th percentiles of the F1 scores as the 95%\\nbootstrap CI. We ﬁnd that CheXNet achieves an F1\\nscore of 0.435 (95% CI 0.387, 0.481), higher than the\\nradiologist average of 0.387 (95% CI 0.330, 0.442). Ta-\\nble 1 summarizes the performance of each radiologist\\nand of CheXNet.\\nTo determine whether CheXNet’s performance is sta-\\ntistically signiﬁcantly higher than radiologist perfor-\\nmance, we also calculate the diﬀerence between the\\naverage F1 score of CheXNet and the average F1 score\\nof the radiologists on the same bootstrap samples. If\\nthe 95% CI on the diﬀerence does not include zero,\\nwe conclude there was a signiﬁcant diﬀerence between\\nthe F1 score of CheXNet and the F1 score of the ra-\\ndiologists. We ﬁnd that the diﬀerence in F1 scores —\\n0.051 (95% CI 0.005, 0.084) — does not contain 0, and\\ntherefore conclude that the performance of CheXNet\\nis statistically signiﬁcantly higher than radiologist per-\\nformance.\\n4.2. Limitations\\nWe identify three limitations of this comparison. First,\\nonly frontal radiographs were presented to the radi-\\nologists and model during diagnosis, but it has been\\nshown that up to 15% of accurate diagnoses require\\nthe lateral view (Raoof et al., 2012); we thus expect\\nthat this setup provides a conservative estimate of per-\\nformance. Third, neither the model nor the radiolo-\\ngists were not permitted to use patient history, which\\nhas been shown to decrease radiologist diagnostic per-\\nformance in interpreting chest radiographs (Berbaum\\net al., 1985; Potchen et al., 1979); for example, given\\na pulmonary abnormality with a history of fever and\\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\\n(a) Patient with multifocal com-\\nmunity acquired pneumonia. The\\nmodel correctly detects the airspace\\ndisease in the left lower and right up-\\nper lobes to arrive at the pneumonia\\ndiagnosis.\\n(b) Patient with a left lung nodule.\\nThe model identiﬁes the left lower\\nlobe lung nodule and correctly clas-\\nsiﬁes the pathology.\\n(c) Patient with primary lung ma-\\nlignancy and two large masses, one\\nin the left lower lobe and one in\\nthe right upper lobe adjacent to the\\nmediastinum. The model correctly\\nidentiﬁes both masses in the X-ray.\\n(d) Patient with a right-sided pneu-\\nmothroax and chest tube. The\\nmodel detects the abnormal lung\\nto correctly predict the presence of\\npneumothorax (collapsed lung).\\n(e) Patient with a large right pleural\\neﬀusion (ﬂuid in the pleural space).\\nThe model correctly labels the eﬀu-\\nsion and focuses on the right lower\\nchest.\\n(f) Patient with congestive heart\\nfailure and cardiomegaly (enlarged\\nheart). The model correctly identi-\\nﬁes the enlarged cardiac silhouette.\\nFigure 2.CheXNet localizes pathologies it identiﬁes using Class Activation Maps, which highlight the areas of the X-ray\\nthat are most important for making a particular pathology classiﬁcation. The captions for each image are provided by\\none of the practicing radiologists.\\ncough, pneumonia would be appropriate rather than\\nless speciﬁc terms such as inﬁltration or consolidation)\\n(Potchen et al., 1979).\\n5. ChexNet vs. Previous State of the\\nArt on the ChestX-ray14 Dataset\\nWe extend the algorithm to classify multiple thoracic\\npathologies by making three changes. First, instead of\\noutputting one binary label, ChexNet outputs a vec-\\ntor tof binary labels indicating the absence or presence\\nof each of the following 14 pathology classes: Atelec-\\ntasis, Cardiomegaly, Consolidation, Edema, Eﬀusion,\\nEmphysema, Fibrosis, Hernia, Inﬁltration, Mass, Nod-\\nule, Pleural Thickening, Pneumonia, and Pneumotho-\\nrax. Second, we replace the ﬁnal fully connected layer\\nin CheXNet with a fully connected layer producing a\\n14-dimensional output, after which we apply an ele-\\nmentwise sigmoid nonlinearity. The ﬁnal output is the\\npredicted probability of the presence of each pathology\\nclass. Third, we modify the loss function to optimize\\nthe sum of unweighted binary cross entropy losses\\nL(X,y) =\\n14∑\\nc=1\\n[−yc log p(Yc = 1|X)\\n−(1 −yc) logp(Yc = 0|X)],\\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\\nwhere p(Yc = 1 |X) is the predicted probability that\\nthe image contains the pathology c and p(Yc = 0|X)\\nis the predicted probability that the image does not\\ncontain the pathology c.\\nWe randomly split the dataset into training (70%), val-\\nidation (10%), and test (20%) sets, following previous\\nwork on ChestX-ray14 (Wang et al., 2017; Yao et al.,\\n2017). We ensure that there is no patient overlap be-\\ntween the splits. We compare the per-class AUROC of\\nthe model against the previous state of the art held by\\nYao et al. (2017) on 13 classes and Wang et al. (2017)\\non the remaining 1 class.\\nWe ﬁnd that CheXNet achieves state of the art results\\non all 14 pathology classes. Table 2 illustrates the per-\\nclass AUROC comparison on the test set. On Mass,\\nNodule, Pneumonia, and Emphysema, we outperform\\nprevious state of the art considerably ( >0.05 increase\\nin AUROC).\\n6. Model Interpretation\\nTo interpret the network predictions, we also produce\\nheatmaps to visualize the areas of the image most in-\\ndicative of the disease using class activation mappings\\n(CAMs) (Zhou et al., 2016). To generate the CAMs,\\nwe feed an image into the fully trained network and\\nextract the feature maps that are output by the ﬁnal\\nconvolutional layer. Let fk be the kth feature map and\\nlet wc,k be the weight in the ﬁnal classiﬁcation layer\\nfor feature map k leading to pathology c. We obtain\\na map Mc of the most salient features used in classi-\\nfying the image as having pathology c by taking the\\nweighted sum of the feature maps using their associ-\\nated weights. Formally,\\nMc =\\n∑\\nk\\nwc,kfk.\\nWe identify the most important features used by the\\nmodel in its prediction of the pathology c by upscal-\\ning the map Mc to the dimensions of the image and\\noverlaying the image.\\nFigure 2 shows several examples of CAMs on the pneu-\\nmonia detection task as well as the 14-class pathology\\nclassiﬁcation task.\\n7. Related Work\\nRecent advancements in deep learning and large\\ndatasets have enabled algorithms to surpass the per-\\nformance of medical professionals in a wide variety of\\nmedical imaging tasks, including diabetic retinopathy\\ndetection (Gulshan et al., 2016), skin cancer classiﬁca-\\ntion (Esteva et al., 2017), arrhythmia detection (Ra-\\njpurkar et al., 2017), and hemorrhage identiﬁcation\\n(Grewal et al., 2017).\\nAutomated diagnosis from chest radiographs has re-\\nceived increasing attention with algorithms for pul-\\nmonary tuberculosis classiﬁcation (Lakhani & Sun-\\ndaram, 2017) and lung nodule detection (Huang et al.,\\n2017). Islam et al. (2017) studied the performance\\nof various convolutional architectures on diﬀerent ab-\\nnormalities using the publicly available OpenI dataset\\n(Demner-Fushman et al., 2015). Wang et al. (2017)\\nreleased ChestX-ray-14, an order of magnitude larger\\nthan previous datasets of its kind, and also bench-\\nmarked diﬀerent convolutional neural network archi-\\ntectures pre-trained on ImageNet. Recently Yao et al.\\n(2017) exploited statistical dependencies between la-\\nbels in order make more accurate predictions, outper-\\nforming Wang et al. (2017) on 13 of 14 classes.\\n8. Conclusion\\nPneumonia accounts for a signiﬁcant proportion of\\npatient morbidity and mortality (Gon¸ calves-Pereira\\net al., 2013). Early diagnosis and treatment of pneu-\\nmonia is critical to preventing complications including\\ndeath (Aydogdu et al., 2010). With approximately 2\\nbillion procedures per year, chest X-rays are the most\\ncommon imaging examination tool used in practice,\\ncritical for screening, diagnosis, and management of a\\nvariety of diseases including pneumonia (Raoof et al.,\\n2012). However, two thirds of the global population\\nlacks access to radiology diagnostics, according to an\\nestimate by the World Health Organization (Mollura\\net al., 2010). There is a shortage of experts who can in-\\nterpret X-rays, even when imaging equipment is avail-\\nable, leading to increased mortality from treatable dis-\\neases (Kesselman et al., 2016).\\nWe develop an algorithm which detects pneumonia\\nfrom frontal-view chest X-ray images at a level ex-\\nceeding practicing radiologists. We also show that\\na simple extension of our algorithm to detect multi-\\nple diseases outperforms previous state of the art on\\nChestX-ray14, the largest publicly available chest X-\\nray dataset. With automation at the level of experts,\\nwe hope that this technology can improve healthcare\\ndelivery and increase access to medical imaging ex-\\npertise in parts of the world where access to skilled\\nradiologists is limited.\\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\\n9. Acknowledgements\\nWe would like to acknowledge the Stanford Center for\\nArtiﬁcial Intelligence in Medicine and Imaging for clin-\\nical dataset infrastructure support ( AIMI.stanford.\\nedu).\\nReferences\\nAydogdu, M, Ozyilmaz, E, Aksoy, Handan, Gursel,\\nG, and Ekim, Numan. Mortality prediction in\\ncommunity-acquired pneumonia requiring mechan-\\nical ventilation; values of pneumonia and intensive\\ncare unit severity scores. Tuberk Toraks, 58(1):25–\\n34, 2010.\\nBerbaum, K, Franken Jr, EA, and Smith, WL. The\\neﬀect of comparison ﬁlms upon resident interpre-\\ntation of pediatric chest radiographs. Investigative\\nradiology, 20(2):124–128, 1985.\\nCDC, 2017. URL https://www.cdc.gov/features/\\npneumonia/index.html.\\nCherian, Thomas, Mulholland, E Kim, Carlin, John B,\\nOstensen, Harald, Amin, Ruhul, Campo, Mar-\\ngaret de, Greenberg, David, Lagos, Rosanna,\\nLucero, Marilla, Madhi, Shabir A, et al. Standard-\\nized interpretation of paediatric chest radiographs\\nfor the diagnosis of pneumonia in epidemiological\\nstudies. Bulletin of the World Health Organization,\\n83(5):353–359, 2005.\\nDavies, H Dele, Wang, Elaine E-l, Manson, David,\\nBabyn, Paul, and Shuckett, Bruce. Reliability of\\nthe chest radiograph in the diagnosis of lower res-\\npiratory infections in young children. The Pediatric\\ninfectious disease journal, 15(7):600–604, 1996.\\nDemner-Fushman, Dina, Kohli, Marc D, Rosenman,\\nMarc B, Shooshan, Sonya E, Rodriguez, Laritza,\\nAntani, Sameer, Thoma, George R, and McDonald,\\nClement J. Preparing a collection of radiology ex-\\naminations for distribution and retrieval. Journal of\\nthe American Medical Informatics Association, 23\\n(2):304–310, 2015.\\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li,\\nKai, and Fei-Fei, Li. Imagenet: A large-scale hier-\\narchical image database. In Computer Vision and\\nPattern Recognition, 2009. CVPR 2009. IEEE Con-\\nference on, pp. 248–255. IEEE, 2009.\\nEsteva, Andre, Kuprel, Brett, Novoa, Roberto A,\\nKo, Justin, Swetter, Susan M, Blau, Helen M, and\\nThrun, Sebastian. Dermatologist-level classiﬁcation\\nof skin cancer with deep neural networks. Nature,\\n542(7639):115–118, 2017.\\nFranquet, T. Imaging of pneumonia: trends and algo-\\nrithms. European Respiratory Journal, 18(1):196–\\n208, 2001.\\nGon¸ calves-Pereira, Jo˜ ao, Concei¸ c˜ ao, Catarina, and\\nP´ ovoa, Pedro. Community-acquired pneumo-\\nnia: identiﬁcation and evaluation of nonresponders.\\nTherapeutic advances in infectious disease, 1(1):5–\\n17, 2013.\\nGrewal, Monika, Srivastava, Muktabh Mayank, Ku-\\nmar, Pulkit, and Varadarajan, Srikrishna. Radnet:\\nRadiologist level accuracy using deep learning for\\nhemorrhage detection in ct scans. arXiv preprint\\narXiv:1710.04934, 2017.\\nGulshan, Varun, Peng, Lily, Coram, Marc, Stumpe,\\nMartin C, Wu, Derek, Narayanaswamy, Arunacha-\\nlam, Venugopalan, Subhashini, Widner, Kasumi,\\nMadams, Tom, Cuadros, Jorge, et al. Development\\nand validation of a deep learning algorithm for de-\\ntection of diabetic retinopathy in retinal fundus pho-\\ntographs. Jama, 316(22):2402–2410, 2016.\\nHopstaken, RM, Witbraad, T, Van Engelshoven,\\nJMA, and Dinant, GJ. Inter-observer variation in\\nthe interpretation of chest radiographs for pneumo-\\nnia in community-acquired lower respiratory tract\\ninfections. Clinical radiology, 59(8):743–752, 2004.\\nHuang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and\\nvan der Maaten, Laurens. Densely connected convo-\\nlutional networks. arXiv preprint arXiv:1608.06993,\\n2016.\\nHuang, Peng, Park, Seyoun, Yan, Rongkai, Lee,\\nJunghoon, Chu, Linda C, Lin, Cheng T, Hussien,\\nAmira, Rathmell, Joshua, Thomas, Brett, Chen,\\nChen, et al. Added value of computer-aided ct image\\nfeatures for early lung cancer diagnosis with small\\npulmonary nodules: A matched case-control study.\\nRadiology, pp. 162725, 2017.\\nIoﬀe, Sergey and Szegedy, Christian. Batch normaliza-\\ntion: Accelerating deep network training by reduc-\\ning internal covariate shift. In International Confer-\\nence on Machine Learning, pp. 448–456, 2015.\\nIslam, Mohammad Tariqul, Aowal, Md Abdul, Min-\\nhaz, Ahmed Tahseen, and Ashraf, Khalid. Ab-\\nnormality detection and localization in chest x-rays\\nusing deep convolutional neural networks. arXiv\\npreprint arXiv:1705.09850, 2017.\\nKesselman, Andrew, Soroosh, Garshasb, Mollura,\\nDaniel J, and Group, RAD-AID Conference Writ-\\ning. 2015 rad-aid conference on international radi-\\nology for developing countries: The evolving global\\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\\nradiology landscape. Journal of the American Col-\\nlege of Radiology, 13(9):1139–1144, 2016.\\nKingma, Diederik and Ba, Jimmy. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\nLakhani, Paras and Sundaram, Baskaran. Deep learn-\\ning at chest radiography: Automated classiﬁcation\\nof pulmonary tuberculosis by using convolutional\\nneural networks. Radiology, pp. 162326, 2017.\\nMollura, Daniel J, Azene, Ezana M, Starikovsky,\\nAnna, Thelwell, Aduke, Iosifescu, Sarah, Kimble,\\nCary, Polin, Ann, Garra, Brian S, DeStigter, Kris-\\nten K, Short, Brad, et al. White paper report of\\nthe rad-aid conference on international radiology for\\ndeveloping countries: identifying challenges, oppor-\\ntunities, and strategies for imaging services in the\\ndeveloping world. Journal of the American College\\nof Radiology, 7(7):495–500, 2010.\\nNeuman, Mark I, Lee, Edward Y, Bixby, Sarah,\\nDiperna, Stephanie, Hellinger, Jeﬀrey, Markowitz,\\nRichard, Servaes, Sabah, Monuteaux, Michael C,\\nand Shah, Samir S. Variability in the interpretation\\nof chest radiographs for the diagnosis of pneumo-\\nnia in children. Journal of hospital medicine, 7(4):\\n294–298, 2012.\\nPotchen, EJ, Gard, JW, Lazar, P, Lahaie, P, and\\nAndary, M. Eﬀect of clinical history data on chest\\nﬁlm interpretation-direction or distraction. InInves-\\ntigative Radiology, volume 14, pp. 404–404, 1979.\\nRajpurkar, Pranav, Hannun, Awni Y, Haghpanahi,\\nMasoumeh, Bourn, Codie, and Ng, Andrew Y.\\nCardiologist-level arrhythmia detection with con-\\nvolutional neural networks. arXiv preprint\\narXiv:1707.01836, 2017.\\nRaoof, Suhail, Feigin, David, Sung, Arthur,\\nRaoof, Sabiha, Irugulpati, Lavanya, and Rosenow,\\nEdward C. Interpretation of plain chest\\nroentgenogram. CHEST Journal, 141(2):545–558,\\n2012.\\nWang, Xiaosong, Peng, Yifan, Lu, Le, Lu, Zhiyong,\\nBagheri, Mohammadhadi, and Summers, Ronald M.\\nChestx-ray8: Hospital-scale chest x-ray database\\nand benchmarks on weakly-supervised classiﬁcation\\nand localization of common thorax diseases. arXiv\\npreprint arXiv:1705.02315, 2017.\\nWHO. Standardization of interpretation of chest ra-\\ndiographs for the diagnosis of pneumonia in chil-\\ndren. 2001.\\nYao, Li, Poblenz, Eric, Dagunts, Dmitry, Covington,\\nBen, Bernard, Devon, and Lyman, Kevin. Learning\\nto diagnose from scratch by exploiting dependen-\\ncies among labels. arXiv preprint arXiv:1710.10501,\\n2017.\\nZhou, Bolei, Khosla, Aditya, Lapedriza, Agata, Oliva,\\nAude, and Torralba, Antonio. Learning deep fea-\\ntures for discriminative localization. In Proceedings\\nof the IEEE Conference on Computer Vision and\\nPattern Recognition, pp. 2921–2929, 2016.\\n'}, {'file_name': 'densenet', 'text': 'Densely Connected Convolutional Networks\\nGao Huang ∗\\nCornell University\\ngh349@cornell.edu\\nZhuang Liu ∗\\nTsinghua University\\nliuzhuang13@mails.tsinghua.edu.cn\\nLaurens van der Maaten\\nFacebook AI Research\\nlvdmaaten@fb.com\\nKilian Q. W einberger\\nCornell University\\nkqw4@cornell.edu\\nAbstract\\nRecent work has shown that convolutional networks can\\nbe substantially deeper , more accurate, and efﬁcient to train\\nif they contain shorter connections between layers close to\\nthe input and those close to the output. In this paper , we\\nembrace this observation and introduce the Dense Convo-\\nlutional Network (DenseNet), which connects each layer\\nto every other layer in a feed-forward fashion. Whereas\\ntraditional convolutional networks withL layers have L\\nconnections—one between each layer and its subsequent\\nlayer—our network hasL(L+1)\\n2 direct connections. F or\\neach layer , the feature-maps of all preceding layers are\\nused as inputs, and its own feature-maps are used as inputs\\ninto all subsequent layers. DenseNets have several com-\\npelling advantages: they alleviate the vanishing-gradient\\nproblem, strengthen feature propagation, encourage fea-\\nture reuse, and substantially reduce the number of parame-\\nters. W e evaluate our proposed architecture on four highly\\ncompetitive object recognition benchmark tasks (CIF AR-10,\\nCIF AR-100, SVHN, and ImageNet). DenseNets obtain sig-\\nniﬁcant improvements over the state-of-the-art on most of\\nthem, whilst requiring less computation to achieve high per-\\nformance. Code and pre-trained models are available at\\nhttps://github.com/liuzhuang13/DenseNet.\\n1. Introduction\\nConvolutional neural networks (CNNs) have become\\nthe dominant machine learning approach for visual object\\nrecognition. Although they were originally introduced over\\n20 years ago [\\n18], improvements in computer hardware and\\nnetwork structure have enabled the training of truly deep\\nCNNs only recently . The original LeNet5 [\\n19] consisted of\\n5 layers, VGG featured 19 [ 28], and only last year Highway\\n∗ Authors contributed equally\\nx0\\nx1\\nH1\\nx2\\nH2\\nH3\\nH4\\nx3\\nx4\\nFigure 1: A 5-layer dense block with a growth rate of k = 4.\\nEach layer takes all preceding feature-maps as input.\\nNetworks [\\n33] and Residual Networks (ResNets) [ 11] have\\nsurpassed the 100-layer barrier.\\nAs CNNs become increasingly deep, a new research\\nproblem emerges: as information about the input or gra-\\ndient passes through many layers, it can vanish and “wash\\nout” by the time it reaches the end (or beginning) of the\\nnetwork. Many recent publications address this or related\\nproblems. ResNets [\\n11] and Highway Networks [ 33] by-\\npass signal from one layer to the next via identity connec-\\ntions. Stochastic depth [\\n13] shortens ResNets by randomly\\ndropping layers during training to allow better information\\nand gradient ﬂow . FractalNets [\\n17] repeatedly combine sev-\\neral parallel layer sequences with different number of con-\\nvolutional blocks to obtain a large nominal depth, while\\nmaintaining many short paths in the network. Although\\nthese different approaches vary in network topology and\\ntraining procedure, they all share a key characteristic: they\\ncreate short paths from early layers to later layers.\\n1\\n4700\\n\\nIn this paper, we propose an architecture that distills this\\ninsight into a simple connectivity pattern: to ensure maxi-\\nmum information ﬂow between layers in the network, we\\nconnectall layers (with matching feature-map sizes) di-\\nrectly with each other. T o preserve the feed-forward nature,\\neach layer obtains additional inputs from all preceding lay-\\ners and passes on its own feature-maps to all subsequent\\nlayers. Figure\\n1 illustrates this layout schematically . Cru-\\ncially , in contrast to ResNets, we never combine features\\nthrough summation before they are passed into a layer; in-\\nstead, we combine features by concatenating them. Hence,\\ntheℓth layer has ℓ inputs, consisting of the feature-maps\\nof all preceding convolutional blocks. Its own feature-maps\\nare passed on to allL−ℓ subsequent layers. This introduces\\nL(L+1)\\n2 connections in an L-layer network, instead of just\\nL, as in traditional architectures. Because of its dense con-\\nnectivity pattern, we refer to our approach as Dense Convo-\\nlutional Network (DenseNet) .\\nA possibly counter-intuitive effect of this dense connec-\\ntivity pattern is that it requires fewer parameters than tra-\\nditional convolutional networks, as there is no need to re-\\nlearn redundant feature-maps. Traditional feed-forward ar-\\nchitectures can be viewed as algorithms with a state, which\\nis passed on from layer to layer. Each layer reads the state\\nfrom its preceding layer and writes to the subsequent layer.\\nIt changes the state but also passes on information that needs\\nto be preserved. ResNets [\\n11] make this information preser-\\nvation explicit through additive identity transformations.\\nRecent variations of ResNets [\\n13] show that many layers\\ncontribute very little and can in fact be randomly dropped\\nduring training. This makes the state of ResNets similar\\nto (unrolled) recurrent neural networks [\\n21], but the num-\\nber of parameters of ResNets is substantially larger because\\neach layer has its own weights. Our proposed DenseNet ar-\\nchitecture explicitly differentiates between information that\\nis added to the network and information that is preserved.\\nDenseNet layers are very narrow (e.g., 12 ﬁlters per layer),\\nadding only a small set of feature-maps to the “collective\\nknowledge” of the network and keep the remaining feature-\\nmaps unchanged—and the ﬁnal classiﬁer makes a decision\\nbased on all feature-maps in the network.\\nBesides better parameter efﬁciency , one big advantage of\\nDenseNets is their improved ﬂow of information and gra-\\ndients throughout the network, which makes them easy to\\ntrain. Each layer has direct access to the gradients from the\\nloss function and the original input signal, leading to an im-\\nplicit deep supervision [\\n20]. This helps training of deeper\\nnetwork architectures. Further, we also observe that dense\\nconnections have a regularizing effect, which reduces over-\\nﬁtting on tasks with smaller training set sizes.\\nW e evaluate DenseNets on four highly competitive\\nbenchmark datasets (CIF AR-10, CIF AR-100, SVHN, and\\nImageNet). Our models tend to require much fewer param-\\neters than existing algorithms with comparable accuracy .\\nFurther, we signiﬁcantly outperform the current state-of-\\nthe-art results on most of the benchmark tasks.\\n2. Related W ork\\nThe exploration of network architectures has been a part\\nof neural network research since their initial discovery . The\\nrecent resurgence in popularity of neural networks has also\\nrevived this research domain. The increasing number of lay-\\ners in modern networks ampliﬁes the differences between\\narchitectures and motivates the exploration of different con-\\nnectivity patterns and the revisiting of old research ideas.\\nA cascade structure similar to our proposed dense net-\\nwork layout has already been studied in the neural networks\\nliterature in the 1980s [\\n3]. Their pioneering work focuses on\\nfully connected multi-layer perceptrons trained in a layer-\\nby-layer fashion. More recently , fully connected cascade\\nnetworks to be trained with batch gradient descent were\\nproposed [\\n39]. Although effective on small datasets, this\\napproach only scales to networks with a few hundred pa-\\nrameters. In [\\n9, 23, 30, 40], utilizing multi-level features\\nin CNNs through skip-connnections has been found to be\\neffective for various vision tasks. Parallel to our work, [\\n1]\\nderived a purely theoretical framework for networks with\\ncross-layer connections similar to ours.\\nHighway Networks [\\n33] were amongst the ﬁrst architec-\\ntures that provided a means to effectively train end-to-end\\nnetworks with more than 100 layers. Using bypassing paths\\nalong with gating units, Highway Networks with hundreds\\nof layers can be optimized without difﬁculty . The bypass-\\ning paths are presumed to be the key factor that eases the\\ntraining of these very deep networks. This point is further\\nsupported by ResNets [\\n11], in which pure identity mappings\\nare used as bypassing paths. ResNets have achieved im-\\npressive, record-breaking performance on many challeng-\\ning image recognition, localization, and detection tasks,\\nsuch as ImageNet and COCO object detection [\\n11]. Re-\\ncently , stochastic depth was proposed as a way to success-\\nfully train a 1202-layer ResNet [ 13]. Stochastic depth im-\\nproves the training of deep residual networks by dropping\\nlayers randomly during training. This shows that not all\\nlayers may be needed and highlights that there is a great\\namount of redundancy in deep (residual) networks. Our pa-\\nper was partly inspired by that observation. ResNets with\\npre-activationalso facilitate the training of state-of-the-art\\nnetworks with > 1000 layers [\\n12].\\nAn orthogonal approach to making networks deeper\\n(e.g., with the help of skip connections) is to increase the\\nnetwork width. The GoogLeNet [ 35, 36] uses an “Incep-\\ntion module” which concatenates feature-maps produced\\nby ﬁlters of different sizes. In [\\n37], a variant of ResNets\\nwith wide generalized residual blocks was proposed. In\\nfact, simply increasing the number of ﬁlters in each layer of\\n4701\\n\\nConvolution\\nPooling\\nDense Block 1\\nConvolution\\nPooling\\nPooling\\nLinear\\nConvolution\\nInput\\nPrediction\\nಯhorseರ\\nDense Block 2 Dense Block 3\\nFigure 2: A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change\\nfeature-map sizes via convolution and pooling.\\nResNets can improve its performance provided the depth is\\nsufﬁcient [\\n41]. FractalNets also achieve competitive results\\non several datasets using a wide network structure [ 17].\\nInstead of drawing representational power from ex-\\ntremely deep or wide architectures, DenseNets exploit the\\npotential of the network throughfeature reuse, yielding con-\\ndensed models that are easy to train and highly parameter-\\nefﬁcient. Concatenating feature-maps learned bydifferent\\nlayers increases variation in the input of subsequent layers\\nand improves efﬁciency . This constitutes a major difference\\nbetween DenseNets and ResNets. Compared to Inception\\nnetworks [\\n35, 36], which also concatenate features from dif-\\nferent layers, DenseNets are simpler and more efﬁcient.\\nThere are other notable network architecture innovations\\nwhich have yielded competitive results. The Network in\\nNetwork (NIN) [\\n22] structure includes micro multi-layer\\nperceptrons into the ﬁlters of convolutional layers to ex-\\ntract more complicated features. In Deeply Supervised Net-\\nwork (DSN) [\\n20], internal layers are directly supervised\\nby auxiliary classiﬁers, which can strengthen the gradients\\nreceived by earlier layers. Ladder Networks [\\n26, 25] in-\\ntroduce lateral connections into autoencoders, producing\\nimpressive accuracies on semi-supervised learning tasks.\\nIn [\\n38], Deeply-Fused Nets (DFNs) were proposed to im-\\nprove information ﬂow by combining intermediate layers\\nof different base networks. The augmentation of networks\\nwith pathways that minimize reconstruction losses was also\\nshown to improve image classiﬁcation models [\\n42].\\n3. DenseNets\\nConsider a single image x0 that is passed through a con-\\nvolutional network. The network comprises L layers, each\\nof which implements a non-linear transformation Hℓ(·),\\nwhere ℓ indexes the layer. Hℓ(·) can be a composite func-\\ntion of operations such as Batch Normalization (BN) [ 14],\\nrectiﬁed linear units (ReLU) [ 6], Pooling [ 19], or Convolu-\\ntion (Conv). W e denote the output of the ℓth layer as xℓ.\\nResNets. Traditional convolutional feed-forward net-\\nworks connect the output of the ℓth layer as input to the\\n(ℓ + 1)th layer [ 16], which gives rise to the following\\nlayer transition: xℓ = Hℓ(xℓ− 1). ResNets [ 11] add a\\nskip-connection that bypasses the non-linear transforma-\\ntions with an identity function:\\nxℓ = Hℓ(xℓ− 1) +xℓ− 1. (1)\\nAn advantage of ResNets is that the gradient can ﬂow di-\\nrectly through the identity function from later layers to the\\nearlier layers. However, the identity function and the output\\nofHℓ are combined by summation, which may impede the\\ninformation ﬂow in the network.\\nDense connectivity .T o further improve the information\\nﬂow between layers we propose a different connectivity\\npattern: we introduce direct connections from any layer\\nto all subsequent layers. Figure\\n1 illustrates the layout of\\nthe resulting DenseNet schematically . Consequently , the\\nℓth layer receives the feature-maps of all preceding layers,\\nx0, . . . , xℓ− 1, as input:\\nxℓ = Hℓ([x0, x1, . . . , xℓ− 1]), (2)\\nwhere [x0, x1, . . . , xℓ− 1] refers to the concatenation of the\\nfeature-maps produced in layers 0, . . . , ℓ −1. Because of its\\ndense connectivity we refer to this network architecture as\\nDense Convolutional Network (DenseNet). For ease of im-\\nplementation, we concatenate the multiple inputs of Hℓ(·)\\nin eq. (\\n2) into a single tensor.\\nComposite function. Motivated by [ 12], we deﬁne Hℓ(·)\\nas a composite function of three consecutive operations:\\nbatch normalization (BN) [\\n14], followed by a rectiﬁed lin-\\near unit (ReLU) [ 6] and a 3 × 3 convolution (Conv).\\nPooling layers. The concatenation operation used in\\nEq. ( 2) is not viable when the size of feature-maps changes.\\nHowever, an essential part of convolutional networks is\\ndown-sampling layers that change the size of feature-maps.\\nT o facilitate down-sampling in our architecture we divide\\nthe network into multiple densely connecteddense blocks ;\\nsee Figure\\n2. W e refer to layers between blocks as transition\\nlayers, which do convolution and pooling. The transition\\nlayers used in our experiments consist of a batch normal-\\nization layer and an 1×1 convolutional layer followed by a\\n2×2 average pooling layer.\\nGrowth rate. If each function Hℓ produces k feature-\\nmaps, it follows that the ℓth layer has k0 + k × (ℓ − 1) input\\nfeature-maps, where k0 is the number of channels in the in-\\nput layer. An important difference between DenseNet and\\nexisting network architectures is that DenseNet can have\\nvery narrow layers,e.g., k = 12. W e refer to the hyper-\\nparameter k as the growth rate of the network. W e show in\\nSection\\n4 that a relatively small growth rate is sufﬁcient to\\n4702\\n\\nLayers Output Size DenseNet-121(k = 32) DenseNet-169(k = 32) DenseNet-201(k = 32) DenseNet-161(k = 48)\\nConvolution 112 × 112 7 × 7 conv , stride 2\\nPooling 56 × 56 3 × 3 max pool, stride 2\\nDense Block\\n(1) 56 × 56\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 6\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 6\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 6\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 6\\nTransition Layer\\n(1)\\n56 × 56 1 × 1 conv\\n28 × 28 2 × 2 average pool, stride 2\\nDense Block\\n(2) 28 × 28\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 12\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 12\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 12\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 12\\nTransition Layer\\n(2)\\n28 × 28 1 × 1 conv\\n14 × 14 2 × 2 average pool, stride 2\\nDense Block\\n(3) 14 × 14\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 24\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 32\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 48\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 36\\nTransition Layer\\n(3)\\n14 × 14 1 × 1 conv\\n7 × 7 2 × 2 average pool, stride 2\\nDense Block\\n(4) 7 × 7\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 16\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 32\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 32\\n[ 1 × 1 conv\\n3 × 3 conv\\n]\\n× 24\\nClassiﬁcation\\nLayer\\n1 × 1 7 × 7 global average pool\\n1000D fully-connected, softmax\\nT able 1: DenseNet architectures for ImageNet. The growth rate for the ﬁrst 3 networks is k = 32, and k = 48 for DenseNet-161. Note\\nthat each “conv” layer shown in the table corresponds the sequence BN-ReLU-Conv .\\nobtain state-of-the-art results on the datasets that we tested\\non. One explanation for this is that each layer has access\\nto all the preceding feature-maps in its block and, therefore,\\nto the network’s “collective knowledge”. One can view the\\nfeature-maps as the global state of the network. Each layer\\naddsk feature-maps of its own to this state. The growth\\nrate regulates how much new information each layer con-\\ntributes to the global state. The global state, once written,\\ncan be accessed from everywhere within the network and,\\nunlike in traditional network architectures, there is no need\\nto replicate it from layer to layer.\\nBottleneck layers.Although each layer only produces k\\noutput feature-maps, it typically has many more inputs. It\\nhas been noted in [\\n36, 11] that a 1 ×1 convolution can be in-\\ntroduced as bottleneck layer before each 3 ×3 convolution\\nto reduce the number of input feature-maps, and thus to\\nimprove computational efﬁciency . W e ﬁnd this design es-\\npecially effective for DenseNet and we refer to our network\\nwith such a bottleneck layer,i.e., to the BN-ReLU-Conv(1 ×\\n1)-BN-ReLU-Conv(3×3) version of Hℓ, as DenseNet-B. In\\nour experiments, we let each 1 ×1 convolution produce 4k\\nfeature-maps.\\nCompression.T o further improve model compactness,\\nwe can reduce the number of feature-maps at transition\\nlayers. If a dense block containsm feature-maps, we let\\nthe following transition layer generate ⌊θm⌋ output feature-\\nmaps, where 0 <θ ≤1 is referred to as the compression fac-\\ntor. When θ = 1, the number of feature-maps across transi-\\ntion layers remains unchanged. W e refer the DenseNet with\\nθ <1 as DenseNet-C, and we set θ = 0.5 in our experiment.\\nWhen both the bottleneck and transition layers with θ < 1\\nare used, we refer to our model as DenseNet-BC.\\nImplementation Details. On all datasets except Ima-\\ngeNet, the DenseNet used in our experiments has three\\ndense blocks that each has an equal number of layers. Be-\\nfore entering the ﬁrst dense block, a convolution with 16 (or\\ntwice the growth rate for DenseNet-BC) output channels is\\nperformed on the input images. For convolutional layers\\nwith kernel size 3×3, each side of the inputs is zero-padded\\nby one pixel to keep the feature-map size ﬁxed. W e use 1 ×1\\nconvolution followed by 2 ×2 average pooling as transition\\nlayers between two contiguous dense blocks. At the end of\\nthe last dense block, a global average pooling is performed\\nand then a softmax classiﬁer is attached. The feature-map\\nsizes in the three dense blocks are 32× 32, 16 ×16, and\\n8×8, respectively . W e experiment with the basic DenseNet\\nstructure with conﬁgurations {L = 40, k = 12}, {L =\\n100, k = 12} and {L = 100, k = 24}. For DenseNet-\\nBC, the networks with conﬁgurations {L = 100, k = 12},\\n{L=250, k =24} and {L=190, k =40} are evaluated.\\nIn our experiments on ImageNet, we use a DenseNet-BC\\nstructure with 4 dense blocks on 224 ×224 input images.\\nThe initial convolution layer comprises 2k convolutions of\\nsize 7 ×7 with stride 2; the number of feature-maps in all\\nother layers also follow from setting k. The exact network\\nconﬁgurations we used on ImageNet are shown in T able\\n1.\\n4. Experiments\\nW e empirically demonstrate DenseNet’s effectiveness on\\nseveral benchmark datasets and compare with state-of-the-\\nart architectures, especially with ResNet and its variants.\\n4.1. Datasets\\nCIF AR.The two CIF AR datasets [\\n15] consist of colored\\nnatural images with 32 ×32 pixels. CIF AR-10 (C10) con-\\n4703\\n\\nMethod Depth Params C10 C10+ C100 C100+ SVHN\\nNetwork in Network [ 22] - - 10.41 8.81 35.68 - 2.35\\nAll-CNN [ 31] - - 9.08 7.25 - 33.71 -\\nDeeply Supervised Net [ 20] - - 9.69 7.97 - 34.57 1.92\\nHighway Network [ 33] - - - 7.72 - 32.39 -\\nFractalNet [ 17] 21 38.6M 10.18 5.22 35.34 23.30 2.01\\nwith Dropout/Drop-path 21 38.6M 7.33 4.60 28.20 23.73 1.87\\nResNet [ 11] 110 1.7M - 6.61 - - -\\nResNet (reported by [ 13]) 110 1.7M 13.63 6.41 44.74 27.22 2.01\\nResNet with Stochastic Depth [ 13] 110 1.7M 11.66 5.23 37.80 24.58 1.75\\n1202 10.2M - 4.91 - - -\\nWide ResNet [ 41] 16 11.0M - 4.81 - 22.07 -\\n28 36.5M - 4.17 - 20.50 -\\nwith Dropout 16 2.7M - - - - 1.64\\nResNet (pre-activation) [ 12] 164 1.7M 11.26∗ 5.46 35.58∗ 24.33 -\\n1001 10.2M 10.56∗ 4.62 33.47∗ 22.71 -\\nDenseNet (k = 12) 40 1.0M 7.00 5.24 27.55 24.42 1.79\\nDenseNet (k = 12) 100 7.0M 5.77 4.10 23.79 20.20 1.67\\nDenseNet (k = 24) 100 27.2M 5.83 3.74 23.42 19.25 1.59\\nDenseNet-BC (k = 12) 100 0.8M 5.92 4.51 24.15 22.27 1.76\\nDenseNet-BC (k = 24) 250 15.3M 5.19 3.62 19.64 17.60 1.74\\nDenseNet-BC (k = 40) 190 25.6M - 3.46 - 17.18 -\\nT able 2: Error rates (%) on CIF AR and SVHN datasets. k denotes network’s growth rate. Results that surpass all competing methods are\\nbold and the overall best results are blue. “+” indicates standard data augmentation (translation and/or mirroring). ∗ indicates results run\\nby ourselves. All the results of DenseNets without data augmentation (C10, C100, SVHN) are obtained using Dropout. DenseNets achieve\\nlower error rates while using fewer parameters than ResNet. Without data augmentation, DenseNet performs better by a large margin.\\nsists of images drawn from 10 and CIF AR-100 (C100) from\\n100 classes. The training and test sets contain 50,000 and\\n10,000 images respectively , and we hold out 5,000 training\\nimages as a validation set. W e adopt a standard data aug-\\nmentation scheme (mirroring/shifting) that is widely used\\nfor these two datasets [\\n11, 13, 17, 22, 27, 20, 31, 33]. W e\\ndenote this data augmentation scheme by a “+” mark at the\\nend of the dataset name (e.g., C10+). For preprocessing,\\nwe normalize the data using the channel means and stan-\\ndard deviations. For the ﬁnal run we use all 50,000 training\\nimages and report the ﬁnal test error at the end of training.\\nSVHN.The Street V iew House Numbers (SVHN) dataset\\n[\\n24] contains 32 ×32 colored digit images. There are 73,257\\nimages in the training set, 26,032 images in the test set, and\\n531,131 images for additional training. Following common\\npractice [\\n7, 13, 20, 22, 29] we use all the training data with-\\nout any data augmentation, and a validation set with 6,000\\nimages is split from the training set. W e select the model\\nwith the lowest validation error during training and report\\nthe test error. W e follow [\\n41] and divide the pixel values by\\n255 so they are in the [0, 1] range.\\nImageNet. The ILSVRC 2012 classiﬁcation dataset [ 2]\\nconsists 1.2 million images for training, and 50,000 for val-\\nidation, from1, 000 classes. W e adopt the same data aug-\\nmentation scheme for training images as in [\\n8, 11, 12], and\\napply a single-crop or 10-crop with size 224 ×224 at test\\ntime. Following [ 11, 12, 13], we report classiﬁcation errors\\non the validation set.\\n4.2. T raining\\nAll the networks are trained using stochastic gradient de-\\nscent (SGD). On CIF AR and SVHN we train using batch\\nsize 64 for 300 and 40 epochs, respectively . The initial\\nlearning rate is set to 0.1, and is divided by 10 at 50% and\\n75% of the total number of training epochs. On ImageNet,\\nwe train models for 90 epochs with a batch size of 256. The\\nlearning rate is set to 0.1 initially , and is lowered by 10 times\\nat epoch 30 and 60. Due to GPU memory constraints, our\\nlargest model (DenseNet-161) is trained with a mini-batch\\nsize 128. T o compensate for the smaller batch size, we train\\nthis model for 100 epochs, and divide the learning rate by\\n10 at epoch 90.\\nFollowing [\\n8], we use a weight decay of 10− 4 and a\\nNesterov momentum [ 34] of 0.9 without dampening. W e\\nadopt the weight initialization introduced by [ 10]. For the\\nthree datasets without data augmentation, i.e., C10, C100\\nand SVHN, we add a dropout layer [ 32] after each convolu-\\ntional layer (except the ﬁrst one) and set the dropout rate to\\n0.2. The test errors were only evaluated once for each task\\nand model setting.\\n4704\\n\\nModel top-1 top-5\\nDenseNet-121 (k =32) 25.02 (23.61) 7.71 (6.66)\\nDenseNet-169 (k =32) 23.80 (22.08) 6.85 (5.92)\\nDenseNet-201 (k =32) 22.58 (21.46) 6.34 (5.54)\\nDenseNet-161 (k =48) 22.33 (20.85) 6.15 (5.30)\\nT able 3: The top-1 and top-5 error rates on the\\nImageNet validation set, with single-crop (10-\\ncrop) testing.\\n0 1 2 3 4 5 6 7 8\\nx 10\\n7\\n21.5\\n22.5\\n23.5\\n24.5\\n25.5\\n26.5\\n27.5\\n#parameters\\nvalidation error\\nResNet−34\\nResNet−101\\nResNet−152\\nDenseNet−121\\nResNet−50\\nDenseNet−169\\nDenseNet−201\\nDenseNet−161(k=48)\\nResNets\\nDenseNets−BC\\n0.5 0.75 1 1.25 1.5 1.75 2 2.25 2.5\\nx 10\\n10\\n21.5\\n22.5\\n23.5\\n24.5\\n25.5\\n26.5\\n27.5\\n#FLOPs\\nvalidation error\\nResNet−34\\nDenseNet−121\\nResNet−50\\nDenseNet−169\\nDenseNet−201\\nResNets\\nDenseNets−BC\\nResNet−152\\nDenseNet−161(k=48)\\nResNet−101\\nFigure 3: Comparison of the DenseNets and ResNets top-1 error rates (single-crop\\ntesting) on the ImageNet validation dataset as a function of learned parameters ( left)\\nand FLOPs during test-time ( right).\\n4.3. Classiﬁcation Results on CIF AR and SVHN\\nW e train DenseNets with different depths, L, and growth\\nrates, k. The main results on CIF AR and SVHN are shown\\nin T able 2. T o highlight general trends, we mark all results\\nthat outperform the existing state-of-the-art in boldface and\\nthe overall best result in blue.\\nAccuracy . Possibly the most noticeable trend may orig-\\ninate from the bottom row of T able 2, which shows that\\nDenseNet-BC with L = 190 and k = 40 outperforms\\nthe existing state-of-the-art consistently on all the CIF AR\\ndatasets. Its error rates of 3.46% on C10+ and 17.18% on\\nC100+ are signiﬁcantly lower than the error rates achieved\\nby wide ResNet architecture [\\n41]. Our best results on\\nC10 and C100 (without data augmentation) are even more\\nencouraging: both are close to 30% lower than Fractal-\\nNet with drop-path regularization [\\n17]. On SVHN, with\\ndropout, the DenseNet with L = 100 and k = 24 also\\nsurpasses the current best result achieved by wide ResNet.\\nHowever, the 250-layer DenseNet-BC doesn’t further im-\\nprove the performance over its shorter counterpart. This\\nmay be explained by that SVHN is a relatively easy task,\\nand extremely deep models may overﬁt to the training set.\\nCapacity .Without compression or bottleneck layers,\\nthere is a general trend that DenseNets perform better as\\nLand k increase. W e attribute this primarily to the corre-\\nsponding growth in model capacity . This is best demon-\\nstrated by the column of C10+ and C100+. On C10+, the\\nerror drops from 5.24% to 4.10% and ﬁnally to 3.74% as\\nthe number of parameters increases from 1.0M, over 7.0M\\nto 27.2M. On C100+, we observe a similar trend. This sug-\\ngests that DenseNets can utilize the increased representa-\\ntional power of bigger and deeper models. It also indicates\\nthat they do not suffer from overﬁtting or the optimization\\ndifﬁculties of residual networks [\\n11].\\nParameter Efﬁciency . The results in T able 2 indicate that\\nDenseNets utilize parameters more efﬁciently than alterna-\\ntive architectures (in particular, ResNets). The DenseNet-\\nBC with bottleneck structure and dimension reduction at\\ntransition layers is particularly parameter-efﬁcient. For ex-\\nample, our 250-layer model only has 15.3M parameters, but\\nit consistently outperforms other models such as FractalNet\\nand Wide ResNets that have more than 30M parameters. W e\\nalso highlight that DenseNet-BC withL = 100and k = 12\\nachieves comparable performance ( e.g., 4.51% vs 4.62% er-\\nror on C10+, 22.27% vs 22.71% error on C100+) as the\\n1001-layer pre-activation ResNet using 90% fewer parame-\\nters. Figure\\n4 (right panel) shows the training loss and test\\nerrors of these two networks on C10+. The 1001-layer deep\\nResNet converges to a lower training loss value but a similar\\ntest error. W e analyze this effect in more detail below .\\nOverﬁtting.One positive side-effect of the more efﬁcient\\nuse of parameters is a tendency of DenseNets to be less\\nprone to overﬁtting. W e observe that on the datasets without\\ndata augmentation, the improvements of DenseNet architec-\\ntures over prior work are particularly pronounced. On C10,\\nthe improvement denotes a 29% relative reduction in error\\nfrom 7.33% to 5.19%. On C100, the reduction is about 30%\\nfrom 28.20% to 19.64%. In our experiments, we observed\\npotential overﬁtting in a single setting: on C10, a 4× growth\\nof parameters produced by increasing k = 12to k = 24lead\\nto a modest increase in error from 5.77% to 5.83%. The\\nDenseNet-BC bottleneck and compression layers appear to\\nbe an effective way to counter this trend.\\n4.4. Classiﬁcation Results on ImageNet\\nW e evaluate DenseNet-BC with different depths and\\ngrowth rates on the ImageNet classiﬁcation task, and com-\\npare it with state-of-the-art ResNet architectures. T o ensure\\na fair comparison between the two architectures, we elimi-\\nnate all other factors such as differences in data preprocess-\\ning and optimization settings by adopting the publicly avail-\\nable T orch implementation for ResNet by [\\n8]1 . W e simply\\nreplace the ResNet model with the DenseNet-BC network,\\nand keep all the experiment settingsexactly the same as\\nthose used for ResNet. The only exception is our largest\\nDenseNet model is trained with a mini-batch size of 128\\n1 https://github.com/facebook/fb.resnet.torch\\n4705\\n\\n0\\n 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n#parameters\\n× 10\\n5\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\ntest error (%)\\nDenseNet  \\nDenseNet-C\\nDenseNet-B  \\nDenseNet-BC\\n0\\n 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n#parameters × 105\\n4\\n6\\n8\\n10\\n12\\n14\\n16test error (%)\\nResNet\\nDenseNet-BC\\n3x fewer parameters\\n0\\n 50\\n 100\\n 150\\n 200\\n 250\\n 300\\nepoch\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\ntest error (%)\\nT est error: ResNet-1001 (10.2M)\\nT est error: DenseNet-BC-100 (0.8M)\\nT raining loss: ResNet-1001 (10.2M)\\nT raining loss: DenseNet-BC-100 (0.8M)\\n10\\n− 3\\n10\\n− 2\\n10\\n− 1\\n10\\n0\\ntraining loss\\nFigure 4: Left: Comparison of the parameter efﬁciency on C10+ between DenseNet variations. Middle: Comparison of the parameter\\nefﬁciency between DenseNet-BC and (pre-activation) ResNets. DenseNet-BC requires about 1/3 of the parameters as ResNet to achieve\\ncomparable accuracy .Right: Training and testing curves of the 1001-layer pre-activation ResNet [\\n12] with more than 10M parameters and\\na 100-layer DenseNet with only 0.8M parameters.\\nbecause of GPU memory limitations; we train this model\\nfor 100 epochs with a third learning rate drop after epoch\\n90 to compensate for the smaller batch size.\\nW e report the single-crop and 10-crop validation errors\\nof DenseNets on ImageNet in T able\\n3. Figure 3 shows\\nthe single-crop top-1 validation errors of DenseNets and\\nResNets as a function of the number of parameters (left) and\\nFLOPs (right). The results presented in the ﬁgure reveal that\\nDenseNets perform on par with the state-of-the-art ResNets,\\nwhilst requiring signiﬁcantly fewer parameters and compu-\\ntation to achieve comparable performance. For example, a\\nDenseNet-201 with 20M parameters model yields similar\\nvalidation error as a 101-layer ResNet with more than 40M\\nparameters. Similar trends can be observed from the right\\npanel, which plots the validation error as a function of the\\nnumber of FLOPs: a DenseNet that requires as much com-\\nputation as a ResNet-50 performs on par with a ResNet-101,\\nwhich requires twice as much computation.\\nIt is worth noting that our experimental setup implies\\nthat we use hyperparameter settings that are optimized for\\nResNets but not for DenseNets. It is conceivable that more\\nextensive hyper-parameter searches may further improve\\nthe performance of DenseNet on ImageNet.\\n2\\n5. Discussion\\nSuperﬁcially , DenseNets are quite similar to ResNets:\\nEq. ( 2) differs from Eq. ( 1) only in that the inputs to Hℓ(·)\\nare concatenated instead of summed. However, the implica-\\ntions of this seemingly small modiﬁcation lead to substan-\\ntially different behaviors of the two network architectures.\\nModel compactness.As a direct consequence of the in-\\nput concatenation, the feature-maps learned by any of the\\nDenseNet layers can be accessed by all subsequent layers.\\nThis encourages feature reuse throughout the network, and\\nleads to more compact models.\\n2 Our DenseNet implementation contains some memory inefﬁciencies\\nwhich temporarily precludes experiments with over 30M parameters.\\nThe left two plots in Figure 4 show the result of an\\nexperiment that aims to compare the parameter efﬁciency\\nof all variants of DenseNets (left) and also a comparable\\nResNet architecture (middle). W e train multiple small net-\\nworks with varying depths on C10+ and plot their test ac-\\ncuracies as a function of network parameters. In com-\\nparison with other popular network architectures, such as\\nAlexNet [\\n16] or VGG-net [ 28], ResNets with pre-activation\\nuse fewer parameters while typically achieving better re-\\nsults [\\n12]. Hence, we compare DenseNet ( k = 12) against\\nthis architecture. The training setting for DenseNet is kept\\nthe same as in the previous section.\\nThe graph shows that DenseNet-BC is consistently the\\nmost parameter efﬁcient variant of DenseNet. Further, to\\nachieve the same level of accuracy , DenseNet-BC only re-\\nquires around 1/3 of the parameters of ResNets (middle\\nplot). This result is in line with the results on ImageNet\\nwe presented in Figure\\n3. The right plot in Figure 4 shows\\nthat a DenseNet-BC with only 0.8M trainable parameters\\nis able to achieve comparable accuracy as the 1001-layer\\n(pre-activation) ResNet [\\n12] with 10.2M parameters.\\nImplicit Deep Supervision. One explanation for the im-\\nproved accuracy of dense convolutional networks may be\\nthat individual layers receive additional supervision from\\nthe loss function through the shorter connections. One can\\ninterpret DenseNets to perform a kind of “deep supervi-\\nsion”. The beneﬁts of deep supervision have previously\\nbeen shown in deeply-supervised nets (DSN; [\\n20]), which\\nhave classiﬁers attached to every hidden layer, enforcing the\\nintermediate layers to learn discriminative features.\\nDenseNets perform a similar deep supervision in an im-\\nplicit fashion: a single classiﬁer on top of the network pro-\\nvides direct supervision to all layers through at most two or\\nthree transition layers. However, the loss function and gra-\\ndient of DenseNets are substantially less complicated, as the\\nsame loss function is shared between all layers.\\nStochastic vs. deterministic connection.There is an\\ninteresting connection between dense convolutional net-\\n4706\\n\\nworks and stochastic depth regularization of residual net-\\nworks [\\n13]. In stochastic depth, layers in residual networks\\nare randomly dropped, which creates direct connections be-\\ntween the surrounding layers. As the pooling layers are\\nnever dropped, the network results in a similar connectiv-\\nity pattern as DenseNet: there is a small probability for\\nany two layers, between the same pooling layers, to be di-\\nrectly connected—if all intermediate layers are randomly\\ndropped. Although the methods are ultimately quite dif-\\nferent, the DenseNet interpretation of stochastic depth may\\nprovide insights into the success of this regularizer.\\nFeature Reuse.By design, DenseNets allow layers ac-\\ncess to feature-maps from all of its preceding layers (al-\\nthough sometimes through transition layers). W e conduct\\nan experiment to investigate if a trained network takes ad-\\nvantage of this opportunity . W e ﬁrst train a DenseNet on\\nC10+ withL = 40 and k = 12. For each convolutional\\nlayer ℓ within a block, we compute the average (absolute)\\nweight assigned to connections with layer s. Figure\\n5 shows\\na heat-map for all three dense blocks. The average absolute\\nweight serves as a surrogate for the dependency of a convo-\\nlutional layer on its preceding layers. A red dot in position\\n(ℓ, s) indicates that the layer ℓ makes, on average, strong use\\nof feature-maps produced s-layers before. Several observa-\\ntions can be made from the plot:\\n1. All layers spread their weights over many inputs within\\nthe same block. This indicates that features extracted\\nby very early layers are, indeed, directly used by deep\\nlayers throughout the same dense block.\\n2. The weights of the transition layers also spread their\\nweight across all layers within the preceding dense\\nblock, indicating information ﬂow from the ﬁrst to the\\nlast layers of the DenseNet through few indirections.\\n3. The layers within the second and third dense block\\nconsistently assign the least weight to the outputs of\\nthe transition layer (the top row of the triangles), in-\\ndicating that the transition layer outputs many redun-\\ndant features (with low weight on average). This is in\\nkeeping with the strong results of DenseNet-BC where\\nexactly these outputs are compressed.\\n4. Although the ﬁnal classiﬁcation layer, shown on the\\nvery right, also uses weights across the entire dense\\nblock, there seems to be a concentration towards ﬁnal\\nfeature-maps, suggesting that there may be some more\\nhigh-level features produced late in the network.\\n6. Conclusion\\nW e proposed a new convolutional network architec-\\nture, which we refer to as Dense Convolutional Network\\n(DenseNet). It introduces direct connections between any\\nDense Block 1\\nSource layer (s)\\nDense Block 2\\n9\\n1\\nDense Block 3\\nTarget layer (\\uf06c)\\n  0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n  1\\nTransition layer 1 \\n Transition layer 2 \\n Classification layer\\n1\\n3\\n5\\n7\\n2 4 6 8 10 12\\nTarget layer (\\uf06c)Target layer (\\uf06c)\\n1\\n3\\n5\\n7\\n9\\n11\\n1\\n3\\n5\\n7\\n9\\n11\\n2 4 6 8 10 12 2 4 6 8 10 12\\nFigure 5: The average absolute ﬁlter weights of convolutional lay-\\ners in a trained DenseNet. The color of pixel (s, ℓ) encodes the av-\\nerage L1 norm (normalized by number of input feature-maps) of\\nthe weights connecting convolutional layer s to ℓ within a dense\\nblock. Three columns highlighted by black rectangles correspond\\nto two transition layers and the classiﬁcation layer. The ﬁrst row\\nencodes weights connected to the input layer of the dense block.\\ntwo layers with the same feature-map size. W e showed that\\nDenseNets scale naturally to hundreds of layers, while ex-\\nhibiting no optimization difﬁculties. In our experiments,\\nDenseNets tend to yield consistent improvement in accu-\\nracy with growing number of parameters, without any signs\\nof performance degradation or overﬁtting. Under multi-\\nple settings, it achieved state-of-the-art results across sev-\\neral highly competitive datasets. Moreover, DenseNets\\nrequire substantially fewer parameters and less computa-\\ntion to achieve state-of-the-art performances. Because we\\nadopted hyperparameter settings optimized for residual net-\\nworks in our study , we believe that further gains in accuracy\\nof DenseNets may be obtained by more detailed tuning of\\nhyperparameters and learning rate schedules.\\nWhilst following a simple connectivity rule, DenseNets\\nnaturally integrate the properties of identity mappings, deep\\nsupervision, and diversiﬁed depth. They allow feature reuse\\nthroughout the networks and can consequently learn more\\ncompact and, according to our experiments, more accurate\\nmodels. Because of their compact internal representations\\nand reduced feature redundancy , DenseNets may be good\\nfeature extractors for various computer vision tasks that\\nbuild on convolutional features,e.g., [\\n4, 5]. W e plan to\\nstudy such feature transfer with DenseNets in future work.\\nAcknowledgements.The authors are supported in part\\nby the III-1618134, III-1526012, IIS-1149882 grants from\\nthe National Science Foundation, and the Bill and Melinda\\nGates foundation. Gao Huang is supported by the Interna-\\ntional Postdoctoral Exchange Fellowship Program of China\\nPostdoctoral Council (No.20150015). Zhuang Liu is sup-\\nported by the National Basic Research Program of China\\nGrants 2011CBA00300, 2011CBA00301, the National Nat-\\nural Science Foundation of China Grant 61361136003. W e\\nalso thank Daniel Sedra, Geoff Pleiss and Y u Sun for many\\ninsightful discussions.\\n4707\\n\\nReferences\\n[1] C. Cortes, X. Gonzalvo, V . Kuznetsov , M. Mohri, and\\nS. Y ang. Adanet: Adaptive structural learning of artiﬁcial\\nneural networks.arXiv preprint arXiv:1607.01097 , 2016.\\n2\\n[2] J. Deng, W . Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. Imagenet: A large-scale hierarchical image database. In\\nCVPR, 2009.\\n5\\n[3] S. E. Fahlman and C. Lebiere. The cascade-correlation learn-\\ning architecture. In NIPS, 1989. 2\\n[4] J. R. Gardner, M. J. Kusner, Y . Li, P . Upchurch, K. Q.\\nW einberger, and J. E. Hopcroft. Deep manifold traversal:\\nChanging labels with convolutional features.arXiv preprint\\narXiv:1511.06421, 2015.\\n8\\n[5] L. Gatys, A. Ecker, and M. Bethge. A neural algorithm of\\nartistic style. Nature Communications , 2015. 8\\n[6] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse rectiﬁer\\nneural networks. In AISTATS, 2011. 3\\n[7] I. Goodfellow , D. W arde-Farley , M. Mirza, A. Courville, and\\nY . Bengio. Maxout networks. In ICML, 2013. 5\\n[8] S. Gross and M. Wilber. Training and investigating residual\\nnets, 2016. 5, 6\\n[9] B. Hariharan, P . Arbeláez, R. Girshick, and J. Malik. Hyper-\\ncolumns for object segmentation and ﬁne-grained localiza-\\ntion. InCVPR, 2015.\\n2\\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\\nrectiﬁers: Surpassing human-level performance on imagenet\\nclassiﬁcation. InICCV, 2015.\\n5\\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\\nfor image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6\\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\\ndeep residual networks. In ECCV, 2016. 2, 3, 5, 7\\n[13] G. Huang, Y . Sun, Z. Liu, D. Sedra, and K. Q. W einberger.\\nDeep networks with stochastic depth. In ECCV, 2016. 1, 2,\\n5, 8\\n[14] S. Ioffe and C. Szegedy . Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift. In\\nICML, 2015.\\n3\\n[15] A. Krizhevsky and G. Hinton. Learning multiple layers of\\nfeatures from tiny images. T ech Report, 2009. 4\\n[16] A. Krizhevsky , I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nNIPS, 2012.\\n3, 7\\n[17] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:\\nUltra-deep neural networks without residuals. arXiv preprint\\narXiv:1605.07648, 2016. 1, 3, 5, 6\\n[18] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W . Hubbard, and L. D. Jackel. Backpropagation\\napplied to handwritten zip code recognition.Neural compu-\\ntation, 1(4):541–551, 1989.\\n1\\n[19] Y . LeCun, L. Bottou, Y . Bengio, and P . Haffner. Gradient-\\nbased learning applied to document recognition. Proceed-\\nings of the IEEE , 86(11):2278–2324, 1998. 1, 3\\n[20] C.-Y . Lee, S. Xie, P . Gallagher, Z. Zhang, and Z. Tu. Deeply-\\nsupervised nets. In AISTATS, 2015. 2, 3, 5, 7\\n[21] Q. Liao and T . Poggio. Bridging the gaps between residual\\nlearning, recurrent neural networks and visual cortex. arXiv\\npreprint arXiv:1604.03640 , 2016. 2\\n[22] M. Lin, Q. Chen, and S. Y an. Network in network. In ICLR,\\n2014. 3, 5\\n[23] J. Long, E. Shelhamer, and T . Darrell. Fully convolutional\\nnetworks for semantic segmentation. In CVPR, 2015. 2\\n[24] Y . Netzer, T . W ang, A. Coates, A. Bissacco, B. Wu, and A. Y .\\nNg. Reading digits in natural images with unsupervised fea-\\nture learning, 2011. InNIPS W orkshop, 2011.\\n5\\n[25] M. Pezeshki, L. Fan, P . Brakel, A. Courville, and Y . Bengio.\\nDeconstructing the ladder network architecture. In ICML,\\n2016. 3\\n[26] A. Rasmus, M. Berglund, M. Honkala, H. V alpola, and\\nT . Raiko. Semi-supervised learning with ladder networks.\\nInNIPS, 2015.\\n3\\n[27] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,\\nand Y . Bengio. Fitnets: Hints for thin deep nets. In ICLR,\\n2015. 5\\n[28] O. Russakovsky , J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy , A. Khosla, M. Bernstein,\\net al. Imagenet large scale visual recognition challenge.\\nIJCV.\\n1, 7\\n[29] P . Sermanet, S. Chintala, and Y . LeCun. Convolutional neu-\\nral networks applied to house numbers digit classiﬁcation. In\\nICPR, pages 3288–3291. IEEE, 2012.\\n5\\n[30] P . Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun.\\nPedestrian detection with unsupervised multi-stage feature\\nlearning. InCVPR, 2013.\\n2\\n[31] J. T . Springenberg, A. Dosovitskiy , T . Brox, and M. Ried-\\nmiller. Striving for simplicity: The all convolutional net.\\narXiv preprint arXiv:1412.6806, 2014.\\n5\\n[32] N. Srivastava, G. E. Hinton, A. Krizhevsky , I. Sutskever, and\\nR. Salakhutdinov . Dropout: a simple way to prevent neural\\nnetworks from overﬁtting.JMLR, 2014.\\n5\\n[33] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training\\nvery deep networks. In NIPS, 2015. 1, 2, 5\\n[34] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\\nimportance of initialization and momentum in deep learning.\\nInICML, 2013.\\n5\\n[35] C. Szegedy , W . Liu, Y . Jia, P . Sermanet, S. Reed,\\nD. Anguelov , D. Erhan, V . V anhoucke, and A. Rabinovich.\\nGoing deeper with convolutions. InCVPR, 2015.\\n2, 3\\n[36] C. Szegedy , V . V anhoucke, S. Ioffe, J. Shlens, and Z. W ojna.\\nRethinking the inception architecture for computer vision. In\\nCVPR, 2016.\\n2, 3, 4\\n[37] S. T arg, D. Almeida, and K. Lyman. Resnet in\\nresnet: Generalizing residual architectures. arXiv preprint\\narXiv:1603.08029, 2016. 2\\n[38] J. W ang, Z. W ei, T . Zhang, and W . Zeng. Deeply-fused nets.\\narXiv preprint arXiv:1605.07716 , 2016. 3\\n[39] B. M. Wilamowski and H. Y u. Neural network learning\\nwithout backpropagation. IEEE Transactions on Neural Net-\\nworks, 21(11):1793–1803, 2010. 2\\n[40] S. Y ang and D. Ramanan. Multi-scale recognition with dag-\\ncnns. In ICCV, 2015. 2\\n[41] S. Zagoruyko and N. Komodakis. Wide residual networks.\\narXiv preprint arXiv:1605.07146 , 2016. 3, 5, 6\\n[42] Y . Zhang, K. Lee, and H. Lee. Augmenting supervised neural\\nnetworks with unsupervised objectives for large-scale image\\nclassiﬁcation. InICML, 2016.\\n3\\n4708\\n\\n'}, {'file_name': 'ecg', 'text': 'FOCUS | Letters\\nhttps:/ / doi.org/10.1038/s41591-018-0268-3\\n1Department of Computer Science, Stanford University, Stanford, CA, USA. 2iRhythm T echnologies Inc., San Francisco, CA, USA. 3Division of Cardiology, \\nDepartment of Medicine, University of California San Francisco, San Francisco, CA, USA. 4Department of Medicine and Center for Digital Health, Stanford \\nUniversity School of Medicine, Stanford, CA, USA. 5Veterans Affairs Palo Alto Health Care System, Palo Alto, CA, USA. 6These authors contributed equally: \\nAwni Y . Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoffrey H. Tison. *e-mail: awni@cs.stanford.edu\\nComputerized electrocardiogram (ECG) interpretation plays \\na critical role in the clinical ECG workflow 1. Widely available \\ndigital ECG data and the algorithmic paradigm of deep learn-\\ning2 present an opportunity to substantially improve the accu-\\nracy and scalability of automated ECG analysis. However, a \\ncomprehensive evaluation of an end-to-end deep learning \\napproach for ECG analysis across a wide variety of diagnostic \\nclasses has not been previously reported. Here, we develop \\na deep neural network (DNN) to classify 12 rhythm classes \\nusing 91,232 single-lead ECGs from 53,549 patients who \\nused a single-lead ambulatory ECG monitoring device. When \\nvalidated against an independent test dataset annotated by \\na consensus committee of board-certified practicing cardiolo-\\ngists, the DNN achieved an average area under the receiver \\noperating characteristic curve (ROC) of 0.97. The average F1 \\nscore, which is the harmonic mean of the positive predictive \\nvalue and sensitivity, for the DNN (0.837) exceeded that of \\naverage cardiologists (0.780). With specificity fixed at the \\naverage specificity achieved by cardiologists, the sensitivity \\nof the DNN exceeded the average cardiologist sensitivity for \\nall rhythm classes. These findings demonstrate that an end-\\nto-end deep learning approach can classify a broad range of \\ndistinct arrhythmias from single-lead ECGs with high diagnos-\\ntic performance similar to that of cardiologists. If confirmed in \\nclinical settings, this approach could reduce the rate of misdi-\\nagnosed computerized ECG interpretations and improve the \\nefficiency of expert human ECG interpretation by accurately \\ntriaging or prioritizing the most urgent conditions.\\nThe electrocardiogram is a fundamental tool in the everyday \\npractice of clinical medicine, with more than 300 million ECGs \\nobtained annually worldwide 3. The ECG is pivotal for diagnos -\\ning a wide spectrum of abnormalities from arrhythmias to acute \\ncoronary syndrome4. Computer-aided interpretation has become \\nincreasingly important in the clinical ECG workflow since its intro-\\nduction over 50 years ago, serving as a crucial adjunct to physician \\ninterpretation in many clinical settings 1. However, existing com -\\nmercial ECG interpretation algorithms still show substantial rates \\nof misdiagnosis1,5–7. The combination of widespread digitization of \\nECG data and the development of algorithmic paradigms that can \\nbenefit from large-scale processing of raw data presents an opportu-\\nnity to reexamine the standard approach to algorithmic ECG analy-\\nsis and may provide substantial improvements to automated ECG \\ninterpretation.\\nSubstantial algorithmic advances in the past five years have been \\ndriven largely by a specific class of models known as deep neural \\nnetworks2. DNNs are computational models consisting of multiple \\nprocessing layers, with each layer being able to learn increasingly \\nabstract, higher-level representations of the input data relevant to \\nperform specific tasks. They have dramatically improved the state \\nof the art in speech recognition8, image recognition9, strategy games \\nsuch as Go 10, and in medical applications 11,12. The ability of DNNs \\nto recognize patterns and learn useful features from raw input data \\nwithout requiring extensive data preprocessing, feature engineer-\\ning or handcrafted rules 2 makes them particularly well suited to \\ninterpret ECG data. Furthermore, since DNN performance tends \\nto increase as the amount of training data increases2, this approach \\nis well positioned to take advantage of the widespread digitization \\nof ECG data.\\nA comprehensive evaluation of whether an end-to-end deep \\nlearning approach can be used to analyze raw ECG data to classify \\na broad range of diagnoses remains lacking. Much of the previous \\nwork to employ DNNs toward ECG interpretation has focused on \\nsingle aspects of the ECG processing pipeline, such as noise reduc -\\ntion13,14 or feature extraction 15,16, or has approached limited diag -\\nnostic tasks, detecting only a handful of heartbeat types (normal, \\nventricular or supraventricular ectopic, fusion, and so on) 17–20 or \\nrhythm diagnoses (most commonly atrial fibrillation or ventric -\\nular tachycardia) 21–25. Lack of appropriate data has limited many \\nefforts beyond these applications. Most prior efforts used data \\nfrom the MIT-BIH Arrhythmia database (PhysioNet)26, which \\nis limited by the small number of patients and rhythm episodes  \\npresent in the dataset.\\nIn this study, we constructed a large, novel ECG dataset that \\nunderwent expert annotation for a broad range of ECG rhythm \\nclasses. We developed a DNN to detect 12 rhythm classes from \\nraw single-lead ECG inputs using a training dataset consisting of \\n91,232 ECG records from 53,549 patients. The DNN was designed \\nto classify 10 arrhythmias as well as sinus rhythm and noise for \\na total of 12 output rhythm classes (Extended Data Fig. 1). ECG \\ndata were recorded by the Zio monitor, which is a Food and Drug \\nAdministration (FDA)-cleared, single-lead, patch-based ambula-\\ntory ECG monitor 27 that continuously records data from a single \\nvector (modified Lead II) at 200 Hz. The mean and median wear \\ntime of the Zio monitor in our dataset was 10.6 and 13.0 days, \\nrespectively. Mean age was 69 ±  16 years and 43% were women. \\nWe validated the DNN on a test dataset that consisted of 328 ECG \\nrecords collected from 328 unique patients, which was annotated by \\na consensus committee of expert cardiologists (see Methods). Mean \\nage on the test dataset was 70 ±  17 years and 38% were women. The \\nmean inter-annotator agreement on the test dataset was 72.8%. \\nCardiologist-level arrhythmia detection and \\nclassification in ambulatory electrocardiograms \\nusing a deep neural network\\nAwni\\xa0Y .\\xa0Hannun\\u200a \\u200a1,6*, Pranav\\xa0Rajpurkar\\u200a \\u200a1,6, Masoumeh\\xa0Haghpanahi2,6, Geoffrey\\xa0H.\\xa0Tison\\u200a \\u200a3,6, \\nCodie\\xa0Bourn2, Mintu\\xa0P.\\xa0T urakhia4,5 and Andrew\\xa0Y .\\xa0Ng1\\nFOCUS | Letters\\nhttps:/ / doi.org/10.1038/s41591-018-0268-3\\nNATuRe MediCiNe | VOL 25 | JANUARY 2019 | 65–69 | www.nature.com/ naturemedicine\\nCorrected: Publisher Correction\\n65\\nLetters | FOCUS NaTuRE MEDICINE\\nSupplementary Table 1 shows the number of unique patients  \\nexhibiting each rhythm class.\\nWe first compared the performance of the DNN against the gold \\nstandard cardiologist consensus committee diagnoses by calculat-\\ning the AUC (Table 1a). Since the DNN algorithm was designed \\nto make a rhythm class prediction approximately once per second \\n(see Methods), we report performance both as assessed once every \\nsecond—which we call “sequence-level” and consists of one rhythm \\nclass per interval—and once per record, which we call “set-level” \\nand consists of the group of unique diagnoses present in the record. \\nSequence-level metrics help capture the duration of an arrhythmia, \\nsuch as its onset and offset within a record, whereas set-level metrics \\nfocus only on the existence of a rhythm class within a record. The \\nDNN achieved an AUC of greater than 0.91 for all rhythm classes; \\nat the sequence-level all but one AUC was above 0.97. The class-\\nweighted average AUC was 0.978 at the sequence-level and 0.977 at \\nthe set-level. The model demonstrated high AUCs for arrhythmias \\nof greater clinical significance such as AF , atrio-ventricular block, \\nand ventricular tachycardia. The sequence and set-level results were \\nsimilar, though sequence-level AUC was higher in the majority of \\ncases. In sensitivity analyses, we calculated multi-class AUC using \\nthe method described by Hand and Till28 and results were materi-\\nally unchanged. Supplementary Table 2 shows the maximum sensi-\\ntivity achieved by the DNN with specificity > 90%, and vice versa. \\nWith one exception, all sensitivity and specificity pairs were > 90%.\\nIn addition to a cardiologist consensus committee annotation, \\neach ECG record in the test dataset received annotations from six \\nseparate individual cardiologists who were not part of the commit-\\ntee (see Methods). Using the committee labels as the gold standard, \\nwe compared the DNN algorithm F1  score to the average individual \\ncardiologist F1  score, which is the harmonic mean of the positive \\npredictive value (PPV; precision) and sensitivity (recall) (Table 1). \\nCardiologist F1  scores were averaged over six individual cardiolo-\\ngists. The trend of DNN F1  scores tended to follow that of the aver-\\naged cardiologist F1  scores: both had lower F 1  on similar classes, \\nsuch as ventricular tachycardia and ectopic atrial rhythm (EAR). \\nThe set-level average F1  scores weighted by the frequency of each \\nclass for the DNN (0.837) exceeded those for the averaged cardi-\\nologist (0.780). We performed multiple sensitivity analyses, all of \\nwhich were consistent with our main results: both AUC and F1  \\nscores on the 10% development dataset (n =  8,761) were materially \\nunchanged from the test dataset results, although they were slightly \\nhigher (Supplementary Tables 3 and 4). In addition, we retrained \\nthe DNN holding out an additional 10% of the training dataset as \\na second held-out test dataset (n =  8,768); the AUC and F1  scores \\nfor all rhythms were materially unchanged (Supplementary Tables 5 \\nand 6). We note that unlike the primary test dataset, which has gold-\\nstandard annotations from a committee of cardiologists, both sensi-\\ntivity analysis datasets are annotated by certified ECG technicians.\\nWe plotted receiver operating characteristic curves (ROCs) and \\nprecision-recall curves for the sequence-level analyses of three \\nexample classes: atrial fibrillation; trigeminy; and AVB (Fig. 1a,b). \\nIndividual cardiologist performance and averaged cardiologist per-\\nformance are plotted on the same figure. Extended Data Fig. 2 pres-\\nents ROCs for all classes, showing that the model met or exceeded \\nthe averaged cardiologist performance for all rhythm classes. Fixing \\nthe specificity at the average specificity level achieved by cardiolo-\\ngists, the sensitivity of the DNN exceeded the average cardiolo -\\ngist sensitivity for all rhythm classes (Table 2). We used confusion \\nmatrices to illustrate the discordance between the DNN’s predic-\\ntions (Fig. 2a) or averaged cardiologist predictions (Fig. 2b) and the \\ncommittee consensus. The two confusion matrices exhibit a similar \\npattern, highlighting those rhythm classes that were generally more \\nproblematic to classify (that is, supraventricular tachycardia (SVT) \\nversus atrial fibrillation, junctional versus sinus rhythm, and EAR \\nversus sinus rhythm).\\nFinally, to demonstrate the generalizability of our DNN \\narchitecture to external data, we applied our DNN to the \\n2017 PhysioNet Challenge data (https://physionet.org/chal-\\nlenge/2017/), which contained four rhythm classes: sinus \\nrhythm; atrial fibrillation; noise; and other. Keeping our DNN \\narchitecture fixed and without any other hyper-parameter tun-\\ning, we trained our DNN on the publicly available training \\ndataset (n =   8,528), holding out a 10% development dataset for \\nearly stopping. DNN performance on the hidden test dataset \\n(n =   3,658) demonstrated overall F1  scores that were among those \\nof the best performers from the competition (Supplementary \\nTable 7)24, with a class average F1  of 0.83. This demonstrates the \\nability of our end-to-end DNN-based approach to generalize to \\na new set of rhythm labels on a different dataset.\\nT able 1 | diagnostic performance of the dNN and averaged individual cardiologists compared to the cardiologist committee \\nconsensus (n\\u2009=\\u2009328)\\nAlgorithm AuC (95% Ci)a Algorithm F1\\nb Average cardiologist F1\\nSequencea Setb Sequence Set Sequence Set\\nAtrial fibrillation and flutter 0.973 (0.966–0.980) 0.965 (0.932–0.998) 0.801 0.831 0.677 0.686\\nAVB 0.988 (0.983–0.993) 0.981 (0.953–1.000) 0.828 0.808 0.772 0.761\\nBigeminy 0.997 (0.991–1.000) 0.996 (0.976–1.000) 0.847 0.870 0.842 0.853\\nEAR 0.913 (0.889–0.937) 0.940 (0.870–1.000) 0.541 0.596 0.482 0.536\\nIVR 0.995 (0.989–1.000) 0.987 (0.959–1.000) 0.761 0.818 0.632 0.720\\nJunctional rhythm 0.987 (0.980–0.993) 0.979 (0.946–1.000) 0.664 0.789 0.692 0.679\\nNoise 0.981 (0.973–0.989) 0.947 (0.898–0.996) 0.844 0.761 0.768 0.685\\nSinus rhythm 0.975 (0.971–0.979) 0.987 (0.976–0.998) 0.887 0.933 0.852 0.910\\nSVT 0.973 (0.960–0.985) 0.953 (0.903–1.000) 0.488 0.693 0.451 0.564\\nT rigeminy 0.998 (0.995–1.000) 0.997 (0.979–1.000) 0.907 0.864 0.842 0.812\\nVentricular tachycardia 0.995 (0.980–1.000) 0.980 (0.934–1.000) 0.541 0.681 0.566 0.769\\nWenckebach 0.978 (0.967–0.989) 0.977 (0.938–1.000) 0.702 0.780 0.591 0.738\\nFrequency-weighted average 0.978 0.977 0.807 0.837 0.753 0.780\\naDNN algorithm area under the ROC compared to the cardiologist committee consensus. bDNN algorithm and averaged individual cardiologist F1 scores compared to the cardiologist committee consensus. \\nSequence-level describes the algorithm predictions that are made once every 256 input samples (approximately every 1.3\\u2009s) and are compared against the gold-standard committee consensus at the same \\nintervals. Set-level describes the unique set of algorithm predictions that are present in the 30-s record. Sequence AUC prediction, n\\u2009= \\u20097,544; set AUC prediction, n\\u2009= \\u2009328.\\nLetters | FOCUS\\nLetters | FOCUS NaTuRE MEDICINE\\nNATuRe MediCiNe | VOL 25 | JANUARY 2019 | 65–69 | www.nature.com/ naturemedicine66\\nFOCUS | LettersNaTuRE MEDICINE\\nOur study is the first comprehensive demonstration of a deep \\nlearning approach to perform classification across a broad range \\nof the most common and important ECG rhythm diagnoses. Our \\nDNN had an average class-weighted AUC of 0.97, with higher aver-\\nage F1  scores and sensitivities than cardiologists. These findings \\ndemonstrate that an end-to-end DNN approach has the potential \\nto be used to improve the accuracy of algorithmic ECG interpreta-\\ntion. Recent algorithmic and computational advances compel us to \\nrevisit the standard approaches to automated ECG interpretation. \\nFurthermore, algorithmic approaches whose performance improves \\nas more data become available, such as deep learning2, can leverage \\nthe widespread digitization of ECG data and provide clear oppor -\\ntunities to bring us closer to the ideal of a learning health care sys-\\ntem29. We emphasize our use in this study of a dataset large enough \\nto evaluate an end-to-end deep learning approach to predict mul-\\ntiple diagnostic ECG classes, and our validation against the high \\nstandard of a cardiologist consensus committee. (Most cardiologists \\nwere subspecialized in rhythm abnormalities.) We believe this is the \\nmost clinically relevant gold standard, since cardiologists perform \\nthe final ECG diagnosis in nearly all clinical settings.\\nOur study demonstrates that the paradigm shift represented by \\nend-to-end deep learning may enable a new approach to automated \\nECG analysis. The standard approach to automated ECG interpreta-\\ntion employs various techniques across a series of steps that include \\nsignal preprocessing, feature extraction, feature selection/reduction, \\nand classification30. At each step, hand-engineered heuristics and deri-\\nvations of the raw ECG data are developed with the ultimate aim to \\nimprove classification for a given rhythm, such as atrial fibrillation31,32. \\nIn contrast, DNNs enable an approach that is fundamentally different \\nsince a single algorithm can accomplish all of these steps ‘end-to-end’ \\nwithout requiring class-specific feature extraction; in other words, the \\nDNN can accept the raw ECG data as input and output diagnostic \\nT able 2 | dNN algorithm and cardiologist sensitivity compared \\nto the cardiologist committee consensus, with specificity fixed \\nat the average specificity level achieved by cardiologists\\nSpecificity Average \\ncardiologist \\nsensitivity\\ndNN \\nalgorithm \\nsensitivity\\nAtrial fibrillation and \\nflutter\\n0.941 0.710 0.861\\nAVB 0.981 0.731 0.858\\nBigeminy 0.996 0.829 0.921\\nEAR 0.993 0.380 0.445\\nIVR 0.991 0.611 0.867\\nJunctional rhythm 0.984 0.634 0.729\\nNoise 0.983 0.749 0.803\\nSinus rhythm 0.859 0.901 0.950\\nSVT 0.983 0.408 0.487\\nVentricular tachycardia 0.996 0.652 0.702\\nWenckebach 0.986 0.541 0.651\\n0.0 0.1 0.2 0.3\\n1 – Specificity 1 – Specificity1  – Specificity\\n0.4 0.5\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSensitivity\\nClass Atrial fibrillation\\nModel\\nIndividual cardiologist\\nAverage cardiologist\\n0.0 0.1 0.2 0.3 0.4 0.5\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSensitivity\\nClass Trigeminy\\nModel\\nIndividual cardiologist\\nAverage cardiologist\\n0.0 0.1 0.2 0.3 0.4 0.5\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSensitivity\\nClass AVB\\nModel\\nIndividual cardiologist\\nAverage cardiologist\\nb\\n0.0 0.2 0.4 0.6 0.8 1.0\\nSensitivity (Recall)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nPPV (precision)\\nClass Atrial fibrillation\\nModel\\nIndividual cardiologist\\nAverage cardiologist\\n0.0 0.2 0.4 0.6 0.8 1.0\\nSensitivity (Recall)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nPPV (precision)\\nClass Trigeminy\\nModel\\nIndividual cardiologist\\nAverage cardiologist\\n0.0 0.2 0.4 0.6 0.8 1.0\\nSensitivity (Recall)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0PPV (precision)\\nClass AVB\\nModel\\nIndividual cardiologist\\nAverage cardiologist\\na\\nFig. 1 | ROC and precision-recall curves. a, Examples of ROC curves calculated at the sequence level for atrial fibrillation (AF), trigeminy, and AVB.  \\nb, Examples of precision-recall curves calculated at the sequence level for atrial fibrillation, trigeminy, and AVB. Individual cardiologist performance is \\nindicated by the red crosses and averaged cardiologist performance is indicated by the green dot. The line represents the ROC (a) or precision-recall curve \\n(b) achieved by the DNN model. n\\u2009= \\u20097,544 where each of the 328 30-s ECGs received 23 sequence-level predictions. \\nFOCUS | Letters\\nhttps:/ / doi.org/10.1038/s41591-018-0268-3\\nFOCUS | LettersNaTuRE MEDICINE\\nNATuRe MediCiNe | VOL 25 | JANUARY 2019 | 65–69 | www.nature.com/ naturemedicine 67\\nLetters | FOCUS NaTuRE MEDICINE\\nprobabilities. With sufficient training data, using a DNN in this man-\\nner has the potential to learn all of the important previously manually \\nderived features, along with as-yet-unrecognized features, in a data-\\ndriven way2, and may learn shared features useful in predicting multi-\\nple classes. These properties of DNNs can serve to improve prediction \\nperformance, particularly since there is ample evidence to suggest that \\nthe currently recognized, manually derived ECG features represent \\nonly a fraction of the informative features for any diagnosis33,34.\\nWhile artificial neural networks were first applied toward the \\ninterpretation of ECGs as early as two decades ago3,35, until recently \\nthey only contained several layers and were constrained by algo-\\nrithmic and computational limitations. More recent studies have \\nemployed deeper networks, although some only use DNNs to per-\\nform certain steps in the ECG processing pipeline, such as feature \\nextraction33 or classification25. End-to-end DNN approaches have \\nbeen used more recently showing good performance for a limited set \\nof ECG rhythms, such as atrial fibrillation22,23,36, ventricular arrhyth-\\nmias21, or individual heartbeat classes 20,21,37,38. While these prior \\nefforts demonstrated promising performance for specific rhythms, \\nthey do not provide a comprehensive evaluation of whether an end-\\nto-end approach can perform well across a wide range of rhythm \\nclasses, in a manner similar to that encountered clinically. Our \\napproach is unique in using a 34-layer network in an end-to-end \\nmanner to simultaneously output probabilities for a wide range of \\ndistinct rhythm diagnoses, all of which is enabled by our dataset, \\nwhich is orders of magnitude larger than most other datasets of \\nits kind26. Distinct from some other recent DNN approaches39, no \\nsubstantial preprocessing of ECG data, such as Fourier or wavelet \\ntransforms40, is needed to achieve strong classification performance.\\nSince arrhythmia detection is one of the most problematic tasks \\nfor existing ECG algorithms 1 ,5,6, if validated in clinical settings \\nthrough clinical trials, our approach has the potential for substantial \\nclinical impact. Paired with properly annotated digital ECG data, our \\napproach has the potential to increase the overall accuracy of prelim-\\ninary computerized ECG interpretations and can also be used to cus-\\ntomize predictions to institution- or population-specific applications \\nby additional training on institution-specific data. While expert pro-\\nvider confirmation will probably be appropriate in many clinical set-\\ntings, the DNN could expand the capability of an expert over-reader \\nin the clinical workflow, for example, by triaging urgent conditions \\nor those for which the DNN has the least ‘confidence’ . Since ECG data \\ncollected from different clinical applications range in duration from \\n10 s (standard 12-lead ECGs) to multiple days (single-lead ambula-\\ntory ECGs), the application of any algorithm, including ours, must \\nultimately be tailored to the target clinical application. For example, \\neven at the performance characteristics we report, applying our algo-\\nrithm sequentially across an ECG record of long duration would \\nresult in nontrivial false-positive diagnoses. Faced with a similar \\nproblem, cardiologists probably incorporate additional mechanisms \\nto improve their diagnostic performance, such as taking advantage of \\nthe increased context or knowledge about arrhythmia epidemiology. \\nSimilarly, additional algorithmic steps or post-processing heuristics \\nmay be important before clinical application.\\nAn important finding from our study is that the DNN appears to \\nrecapitulate the misclassifications made by individual cardiologists, \\nas demonstrated by the similarity in the confusion matrices for the \\nmodel and cardiologists. Manual review of the discordances revealed \\nthat the DNN misclassifications overall appear very reasonable. In \\nmany cases, the lack of context, limited signal duration, or having a \\nsingle lead limited the conclusions that could reasonably be drawn \\nfrom the data, making it difficult to definitively ascertain whether the \\ncommittee and/or the algorithm was correct. Similar factors, as well \\nas human error, may explain the inter-annotator agreement of 72.8%.\\nOf the rhythm classes we examined, ventricular tachycardia is a \\nclinically important rhythm for which the model had a lower F1 score \\nthan cardiologists, but interestingly had higher sensitivity (94.1%) \\nthan the averaged cardiologist (78.4%). Manual review of the 16 \\nrecords misclassified by the DNN as ventricular tachycardia showed \\nthat ‘mistakes’ made by the algorithm were very reasonable. For \\nexample, ventricular tachycardia and idioventricular rhythm (IVR) \\ndiffer only in the heart rate being above or below 100 beats per min-\\nute (b.p.m.), respectively. In 7 of the committee-labeled IVR cases, the \\nrecord contained periods of heart rate ≥   100 b.p.m., making ventricu-\\nlar tachycardia a reasonable classification by the DNN; the remaining \\n3 committee-labeled IVR records had rates close to 100 b.p.m.. Of the \\n5 cases where the committee label was atrial fibrillation (4) or SVT (1), \\nall but one displayed aberrant conduction, resulting in wide QRS com-\\nplexes (the ECG waveform corresponding to ventricular activation) \\nwith a similar appearance to ventricular tachycardia. If we recategorize \\nthe 7 IVR records with a rate ≥   100 b.p.m. as ventricular tachycardia, \\noverall DNN performance on ventricular tachycardia exceeds that of \\ncardiologists by F1 score, with a set-level F1 score of 0.82 (versus 0.77).\\nAverage cardiologist label\\nWenckebach\\nVentricular tachycardia\\nTrigeminy\\nSVT\\nSinus rhythm\\nNoise\\nJunctional rhythm\\nIVR\\nEAR\\nBigeminy\\nAVB\\nAtrial fibrillation\\nCommittee consensus label\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0b\\nAtrial fibrillation\\nAVB\\nBigeminy\\nEARIVR\\nJunctional rhythm\\nNoise\\nSinus rhythm\\nSVT\\nTrigeminy\\nVentricular tachycardia\\nWenckebach Atrial fibrillation\\nAVB\\nBigeminy\\nEARIVR\\nJunctional rhythm\\nNoise\\nSinus rhythm\\nSVT\\nTrigeminy\\nVentricular tachycardia\\nWenckebach\\nDNN predicted label\\nWenckebach\\nVentricular tachycardia\\nTrigeminy\\nSVT\\nSinus rhythm\\nNoise\\nJunctional rhythm\\nIVR\\nEAR\\nBigeminy\\nAVB\\nAtrial fibrillation\\nCommittee consensus label\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0a\\nFig. 2 | Confusion matrices. a, Confusion matrix for the predictions of the DNN versus the cardiology committee consensus. b, Confusion matrix for predictions of \\nindividual cardiologists versus the cardiology committee consensus. The percentage of all possible records in each category is displayed on a color gradient scale. \\nLetters | FOCUS\\nLetters | FOCUS NaTuRE MEDICINE\\nNATuRe MediCiNe | VOL 25 | JANUARY 2019 | 65–69 | www.nature.com/ naturemedicine68\\nFOCUS | LettersNaTuRE MEDICINE\\nThis study has several important limitations. Our input dataset \\nis limited to single-lead ECG records obtained from an ambula -\\ntory monitor, which provides limited signal compared to a standard \\n12-lead ECG; it remains to be determined if our algorithm perfor -\\nmance would be similar in 12-lead ECGs. However, it may be in \\napplications such as this, which have lower signal-to-noise ratio and \\nwhere the current standard of care leaves more room for improve-\\nment, that approaches such as deep learning may provide the greatest \\nimpact. As discussed earlier, a limitation facing this, or any algorithm, \\nbefore clinical application would be tailoring it to the target applica-\\ntion, which may require additional training or post-processing steps. \\nAdditionally, systematic differences in the way technicians versus \\ncardiologists labeled records in our dataset could have decreased \\nDNN performance, although we took precautions to limit this by \\nestablishing standard operating protocols for annotation. In addi -\\ntion, as revealed in our manual review of discordant predictions, in \\nsome cases there remains uncertainty in the correct label. Given the \\nresource-intensive nature of cardiologist committee ECG annotation, \\nour test dataset was limited to records from 328 patients; confidence \\nintervals (CIs) with our test dataset size were acceptably narrow, as we \\nreport in Table 1, although our ability to perform subgroup analysis \\n(such as by age/sex) is limited. Finally, we also note that to obtain a \\nsufficient quantity of rare rhythms in our training and test datasets, \\nwe targeted patients exhibiting these rhythms during data extraction. \\nThis implies that prevalence-dependent metrics such as the F1  score \\nwould not be expected to generalize to the broader population.\\nIn summary, we demonstrate that an end-to-end deep learning \\napproach can classify a broad range of distinct arrhythmias from \\nsingle-lead ECGs with high diagnostic performance similar to that \\nof cardiologists. If confirmed in clinical settings, this approach has \\nthe potential to improve the accuracy, efficiency, and scalability of \\nECG interpretation.\\nOnline content\\nAny methods, additional references, Nature Research reporting \\nsummaries, source data, statements of data availability and asso-\\nciated accession codes are available at https://doi.org/10.1038/\\ns41591-018-0268-3.\\nReceived: 12 March 2018; Accepted: 26 October 2018;  \\nPublished online: 7 January 2019\\nReferences\\n 1. Schläpfer, J. & Wellens, H. J. Computer-interpreted electrocardiograms: \\nbenefits and limitations. J. Am. Coll. Cardiol. 70, 1183–1192 (2017).\\n 2. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521,  \\n436–444 (2015).\\n 3. Holst, H., Ohlsson, M., Peterson, C. & Edenbrandt, L. A confident decision \\nsupport system for interpreting electrocardiograms. Clin. Physiol. 19,  \\n410–418 (1999).\\n 4. Schlant, R. C. et al. Guidelines for electrocardiography. A report of the \\nAmerican College of Cardiology/American Heart Association Task Force on \\nassessment of diagnostic and therapeutic cardiovascular procedures \\n(Committee on Electrocardiography). J. Am. Coll. Cardiol. 19, 473–481 (1992).\\n 5. Shah, A. P . & Rubin, S. A. Errors in the computerized electrocardiogram \\ninterpretation of cardiac rhythm. J. Electrocardiol. 40, 385–390 (2007).\\n 6. Guglin, M. E. & Thatai, D. Common errors in computer electrocardiogram \\ninterpretation. Int. J. Cardiol. 106, 232–237 (2006).\\n 7. Poon, K., Okin, P . M. & Kligfield, P . Diagnostic performance of a computer-\\nbased ECG rhythm algorithm. J. Electrocardiol. 38, 235–238 (2005).\\n 8. Amodei, D. et al. Deep Speech 2: end-to-end Speech recognition in English \\nand Mandarin. In Proc. 33rd International Conference on Machine Learning, \\n173–182 (2016).\\n 9. He, K., Zhang, X., Ren, S. & Sun, J. Delving deep into rectifiers: surpassing \\nhuman-level performance on ImageNet classification. In Proc. International \\nConference on Computer Vision, 1026–1034 (IEEE, 2015).\\n 10. Silver, D. et al. Mastering the game of Go with deep neural networks and tree \\nsearch. Nature 529, 484–489 (2016).\\n 11. Gulshan, V . et al. Development and validation of a deep learning algorithm \\nfor detection of diabetic retinopathy in retinal fundus photographs. JAMA \\n316, 2402–2410 (2016).\\n 12. Esteva, A. Dermatologist-level classification of skin cancer with deep neural \\nnetworks. Nature 542, 115–118 (2017).\\n 13. Poungponsri, S. & Yu, X. An adaptive filtering approach for \\nelectrocardiogram (ECG) signal noise reduction using neural networks. \\nNeurocomputing 117, 206–213 (2013).\\n 14. Ochoa, A., Mena, L. J. & Felix, V . G. Noise-tolerant neural network approach \\nfor electrocardiogram signal classification. In Proc. 3rd International \\nConference on Compute and Data Analysis, 277–282 (Association for \\nComputing Machinery, 2017).\\n 15. Mateo, J. & Rieta, J. J. Application of artificial neural networks for versatile \\npreprocessing of electrocardiogram recordings. J. Med. Eng. Technol. 36, \\n90–101 (2012).\\n 16. Pourbabaee, B., Roshtkhari, M. J. & Khorasani, K. Deep convolutional neural \\nnetworks and learning ECG features for screening paroxysmal atrial \\nfibrillation patients. IEEE Trans. Syst. Man Cybern. Syst. 99, 1–10 (2017).\\n 17. Javadi, M., Arani, S. A., Sajedin, A. & Ebrahimpour, R. Classification of ECG \\narrhythmia by a modular neural network based on mixture of experts and \\nnegatively correlated learning. Biomed. Signal Process. Control 8, 289–296 (2013).\\n 18. Acharya, U. R. et al. A deep convolutional neural network model to classify \\nheartbeats. Comput. Biol. Med. 89, 389–396 (2017).\\n 19. Banupriya, C. V . & Karpagavalli, S. Electrocardiogram beat classification \\nusing probabilistic neural network. In Proc. Machine Learning: Challenges and \\nOpportunities Ahead 31–37 (2014).\\n 20. Al Rahhal, M. M. et al. Deep learning approach for active classification of \\nelectrocardiogram signals. Inf. Sci. (NY) 345, 340–354 (2016).\\n 21. Acharya, U. R. et al. Automated detection of arrhythmias using different \\nintervals of tachycardia ECG segments with convolutional neural network. \\nInf. Sci. (NY) 405, 81–90 (2017).\\n 22. Zihlmann, M., Perekrestenko, D. & Tschannen, M. Convolutional recurrent \\nneural networks for electrocardiogram classification. Comput. Cardiol.  \\nhttps://doi.org/10.22489/CinC.2017.070-060 (2017).\\n 23. Xiong, Z., Zhao, J. & Stiles, M. K. Robust ECG signal classification for \\ndetection of atrial fibrillation using a novel neural network. Comput. Cardiol. \\nhttps://doi.org/10.22489/CinC.2017.066-138 (2017).\\n 24. Clifford, G. et al. AF classification from a short single lead ECG recording: \\nthe PhysioNet/Computing in Cardiology Challenge 2017. Comput. Cardiol. \\nhttps://doi.org/10.22489/CinC.2017.065-469 (2017).\\n 25. Teijeiro, T., Garcia, C. A., Castro, D. & Felix, P . Arrhythmia classification \\nfrom the abductive interpretation of short single-lead ECG records.  \\nComput. Cardiol. https://doi.org/10.22489/CinC.2017.166-054 (2017).\\n 26. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and PhysioNet: \\ncomponents of a new research resource for complex physiologic signals. \\nCirculation 101, E215–E220 (2000).\\n 27. Turakhia, M. P . et al. Diagnostic utility of a novel leadless arrhythmia \\nmonitoring device. Am. J. Cardiol. 112, 520–524 (2013).\\nAcknowledgements\\niRhythm Technologies, Inc. provided financial support for the data annotation in this \\nwork. M.H. and C.B. are employees of iRhythm Technologies, Inc. A.Y .H. was funded by \\nan NVIDIA fellowship. G.H.T. received support from the National Institutes of Health \\n(K23 HL135274). The only financial support provided by iRhythm Technologies, Inc. for \\nthis study was for the data annotation. Data analysis and interpretation was performed \\nindependently from the sponsor. The corresponding author had full access to all the data \\nin the study and had final responsibility for the decision to submit for publication.\\nAuthor contributions\\nM.H., A.Y .N., A.Y .H., and G.H.T. contributed to the study design. M.H. and C.B. were \\nresponsible for data collection. P .R. and A.Y .H. ran the experiments and created the \\nfigures. G.H.T., P .R., and A.Y .H. contributed to the analysis. G.H.T., A.Y .H., and M.P .T. \\ncontributed to the data interpretation and to the writing. G.H.T., M.P .T., and A.Y .N. \\nadvised and A.Y .N. was the senior supervisor of the project. All authors read and \\napproved the submitted manuscript.\\nCompeting interests\\nM.H. and C.B. are employees of iRhythm Technologies, Inc. G.H.T. is an advisor to \\nCardiogram, Inc. M.P .T. is a consultant to iRhythm Technologies, Inc. None of the other \\nauthors have potential conflicts of interest.\\nAdditional information\\nExtended data is available for this paper at https://doi.org/10.1038/s41591-018-0268-3.\\nSupplementary information is available for this paper at https://doi.org/10.1038/\\ns41591-018-0268-3.\\nReprints and permissions information is available at www.nature.com/reprints.\\nCorrespondence and requests for materials should be addressed to A.Y .H.\\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\n© The Author(s), under exclusive licence to Springer Nature America, Inc. 2019\\nFOCUS | Letters\\nhttps:/ / doi.org/10.1038/s41591-018-0268-3\\nFOCUS | LettersNaTuRE MEDICINE\\nNATuRe MediCiNe | VOL 25 | JANUARY 2019 | 65–69 | www.nature.com/ naturemedicine 69\\nLetters | FOCUS NaTuRE MEDICINE\\nMethods\\nStudy participants and sampling procedures. Our dataset contained \\nretrospective, de-identified data from adult patients > 18 years old who used  \\nthe Zio monitor (iRhythm Technologies, Inc) from January 2013 to March 2017. \\nAll extracted data were de-identified according to the Health Insurance Portability \\nand Accountability Act Safe Harbor. According to the iRhythm Technologies \\nprivacy policy, fully de-identified patient data may be shared externally for research \\npurposes; patients may opt out of this sharing. Accordingly, written informed \\nconsent was not necessary for this study given that the 30-s ECG samples of both \\nthe training and test datasets were appropriately de-identified before use. The \\nstudy was reviewed and exempted from full review by the Stanford University \\nInstitutional Review Board.\\nWe extracted a median of one 30-s record per patient to construct the training \\ndataset. ECG records were extracted based on the report summaries produced by \\niRhythm Technologies clinical workflow, which includes a full review by a certified \\nECG technician of initial annotations from an algorithm which is FDA 510(k) \\napproved for clinical use. We randomly sampled patients exhibiting each rhythm; \\nfrom these patients, we selected 30-s records where the rhythm class was present. \\nAlthough the targeted rhythm class was typically present within the record, most \\nrecords contained a mix of multiple rhythms. To further improve the balance \\nof classes in the training dataset, rare rhythms such as AVB, were intentionally \\noversampled, with a median of two 30-s records per patient. For the test dataset, \\n30-s records of each rhythm were sampled in a similar manner to achieve a greater \\nrepresentation of rare rhythms; however, the test dataset included only a single \\nrecord per patient. The training, development, and test datasets had completely \\ndisjointed sets of patients.\\nAnnotation procedures. All ECG records in the training and test datasets \\nunderwent additional annotation procedures. We used separate procedures to \\nannotate the training and test datasets, reserving the resource-intensive cardiologist \\nannotation for use as the gold standard in the test dataset. To annotate the training \\ndataset, a group of senior certified ECG technicians reviewed all records and noted \\nthe onset and offset of all rhythms on the record. Every record was randomly \\nassigned to be reviewed by a single technician specifically for this task, not for any \\nother purpose. All annotators received specific instructions and training regarding \\nhow to annotate transitions between rhythms to improve labeling consistency. We \\nheld out records from a random 10% of the training dataset patients for use as a \\ndevelopment dataset to perform DNN hyper-parameter tuning.\\nEight board-certified practicing cardiac electrophysiologists and one board-\\ncertified practicing cardiologist (all referred to as cardiologists) annotated records \\nin the test dataset. All iRhythm Technologies clinical annotations were removed \\nfrom the test dataset. Cardiologists were divided into three committees of three \\nmembers each; each committee annotated a separate one-third of the test dataset \\n(112 records). Cardiologist committees discussed records as a group and annotated \\nby consensus, providing the gold standard for model evaluation. Each of the \\nremaining six cardiologists that were not part of the committee for that record \\nalso provided individual annotations for that record. These annotations were \\nused to compare the model’s performance to that of the individual cardiologists. \\nIn summary, every record in the test dataset received one committee consensus \\nannotation from a group of three cardiologists and six individual cardiologist \\nannotations.\\nMany ECG records contained multiple rhythm class diagnoses since the onset \\nand offset of all unique classes were labeled within each 30-s record. The atrial \\nfibrillation class combined atrial fibrillation and atrial flutter. The AVB class \\ncombined both type 2 second-degree AVB (Mobitz II/Hay) and third-degree \\nAVB. We combined these classes because they have similar clinical consequences. \\nThe noise label was selected whenever artifact in the signal precluded accurate \\ninterpretation of the underlying rhythm.\\nAlgorithm development. We developed a convolutional DNN to detect \\narrhythmias (Extended Data Fig. 1), which takes as input the raw ECG data \\n(sampled at 200 Hz, or 200 samples per second) and outputs one prediction every \\n256 samples (or every 1.28 s), which we call the output interval. The network \\ntakes as input only the raw ECG samples and no other patient- or ECG-related \\nfeatures. The network architecture has 34 layers; to make the optimization of such \\na network tractable, we employed shortcut connections in a manner similar to \\nthe residual network architecture41. The network consists of 16 residual blocks \\nwith two convolutional layers per block. The convolutional layers have a filter \\nwidth of 16 and 32*2k filters, where k is a hyper-parameter which starts at 0 and \\nis incremented by 1 every fourth residual block. Every alternate residual block \\nsubsamples its inputs by a factor of 2. Before each convolutional layer, we applied \\nbatch normalization42 and a rectified linear activation, adopting the pre-activation  \\nblock design43. The first and last layers of the network are special-cased due \\nto this pre-activation block structure. We also applied Dropout44 between the \\nconvolutional layers and after the nonlinearity with a probability of 0.2. The final \\nfully connected softmax layer produces a distribution over the 12 output classes.\\nThe network was trained de novo with random initialization of the weights as \\ndescribed by He et al.9. We used the Adam optimizer45 with the default parameters \\nβ1 =  0.9 and β2 =  0.999, and a mini batch size of 128. We initialized the learning \\nrate to 1 ×  10−3 and reduced it by a factor of 10 when the developmentally set loss \\nstopped improving for two consecutive epochs. We chose the model that achieved \\nthe lowest error on the development dataset.\\nIn general, the hyper-parameters of the network architecture and optimization \\nalgorithm were chosen via a combination of grid search and manual tuning. For the \\narchitecture, we searched primarily over the number of convolutional layers, the size \\nand number of the convolutional filters, as well as the use of residual connections. \\nWe found the residual connections useful once the depth of the model exceeded \\neight layers. We also experimented with recurrent layers including long short-term \\nmemory cells46 and bidirectional recurrence, but found no improvement in accuracy \\nand a substantial increase in runtime; thus, we abandoned this class of models. We \\nmanually tuned the learning rate to achieve fastest convergence.\\nAlgorithm evaluation. Since the DNN outputs one class prediction every output \\ninterval, it makes a series of 23 rhythm predictions for every 30-s record. The \\ncardiologists annotated the start and end point of each rhythm class in the record. \\nWe used this to construct a cardiologist label at every output interval by rounding \\nthe annotation to the nearest interval boundary. Therefore, model accuracy can \\nbe assessed at the level of every output interval, which we call ‘sequence-level’ , or \\nat the record level, which we call ‘set-level’ . To compare model predictions at the \\nsequence level, the model predictions at each output interval were compared with \\nthe corresponding committee consensus labels for that same output interval. At \\nthe set level, the set of unique rhythm classes across a given ECG record that was \\npredicted by the DNN was compared with the set of rhythm classes annotated \\nacross the record by the committee consensus. The set-level evaluation, unlike  \\nthe sequence-level, does not penalize for time misalignment of a rhythm \\nclassification within a record.\\nAlgorithm evaluation at the sequence level allows comparison against the \\ngold standard at every output interval, providing the most comprehensive metric \\nof algorithm performance, which we therefore employ for most metrics. The \\nsequence-level evaluation is also similar to clinical applications for telemetry or \\nHolter monitor analysis, whereby it is critical to identify the onset and the offset \\nof rhythms. Evaluation at the set level is a useful abstraction, approximating how \\nthe DNN algorithm might be applied to a single ECG record to identify which \\ndiagnoses are present in a given record.\\nTo train and evaluate our model on the Physionet Challenge data, which contains \\nvariable length recordings, we made minor modifications to the DNN. Without any \\nchange, the DNN can accept as input any record with a length that is a multiple of 256 \\nsamples. To handle examples that are not a multiple of 256, records were truncated \\nto the nearest multiple. We used the given record label as the label for approximately \\nevery 1.3-s output prediction. To produce a single prediction for the variable length \\nrecord we used a majority vote of the sequence-level predictions.\\nStatistical analysis. We calculated the ROC analysis and AUC to assess model \\ndiscrimination for each rhythm class with a one versus other strategy28,47. AUCs \\nfor sequence-level and set-level analyses are presented separately. We give a two-\\nsided CI for the AUC scores48. Sensitivity and specificity were calculated at binary \\ndecision thresholds for every rhythm class. We computed the precision-recall \\ncurve, which shows the relationship between PPV (precision) and sensitivity \\n(recall)49. It provides complementary information to the ROC curve, especially \\nwith class-imbalanced datasets. To compare the relative performance of the \\nDNN to the cardiologist committee labels, we calculated the F1 score, which is \\nthe harmonic mean of the PPV and sensitivity. It ranges from 0 to 1 and rewards \\nalgorithms that maximize both PPV and sensitivity simultaneously, rather than \\nfavoring one over the other. The F1 score is complementary to the AUC, which \\nis particularly helpful in the setting of multi-class prediction and less sensitive \\nthan the AUC in settings of class imbalance49. For an aggregate measure of model \\nperformance, we computed the class frequency-weighted arithmetic mean for both \\nthe F1 score and the AUC. To obtain estimates of how the DNN compares to an \\naverage cardiologist, the characteristics of cardiologist performance were averaged \\nacross the six cardiologists who individually annotated each record. We used \\nconfusion matrices to illustrate the specific examples of rhythm classes where the \\nDNN prediction or the individual cardiologist’s prediction were discordant with \\nthe committee consensus at the sequence level. Among the individual cardiologist \\nannotations in the test dataset, we calculated inter-annotator agreement as the ratio \\nof the number of times two annotators agreed that a rhythm was present at each \\noutput interval and the total number of pairwise comparisons.\\nReporting Summary. Further information on research design is available in the \\nNature Research Reporting Summary linked to this article.\\nCode availability. Code for the algorithm development, evaluation, and statistical \\nanalysis is open source with no restrictions and is available from https://github.\\ncom/awni/ecg.\\ndata availability\\nThe test dataset used to support the findings of this study is publicly available at \\nhttps://irhythm.github.io/cardiol_test_set without restriction. Restrictions apply to \\nthe availability of the training dataset, which was used under license from iRhythm \\nLetters | FOCUS\\nLetters | FOCUS NaTuRE MEDICINE\\nNATuRe MediCiNe | www.nature.com/ naturemedicine\\nFOCUS | LettersNaTuRE MEDICINE\\nTechnologies, Inc. for the current study. iRhythm Technologies, Inc. will consider \\nrequests to access the training data on an individual basis. Any data use will be \\nrestricted to noncommercial research purposes, and the data will only be made \\navailable on execution of appropriate data use agreements.\\nReferences\\n 28. Hand, D. J. & Till, R. J. A simple generalisation of the area under  \\nthe ROC curve for multiple class classification problems. Mach. Learn. 45, \\n171–186 (2001).\\n 29. Smith, M. D. et al.in Best Care at Lower Cost: the Path to Continuously Learning \\nHealth Care in America (National Academies Press,: Washington, 2012).\\n 30. Lyon, A., Mincholé, A., Martínez, J. P ., Laguna, P . & Rodriguez, B. \\nComputational techniques for ECG analysis and interpretation in light of their \\ncontribution to medical advances. J. R. Soc. Interface 15, pii: 20170821 (2018).\\n 31. Carrara, M. et al. Heart rate dynamics distinguish among atrial fibrillation, \\nnormal sinus rhythm and sinus rhythm with frequent ectopy. Physiol. Meas. \\n36, 1873–1888 (2015).\\n 32. Zhou, X., Ding, H., Ung, B., Pickwell-MacPherson, E. & Zhang, Y . Automatic \\nonline detection of atrial fibrillation based on symbolic dynamics and \\nShannon entropy. Biomed. Eng. Online 13, 18 (2014).\\n 33. Hong, S. et al. ENCASE: an ENsemble ClASsifiEr for ECG Classification \\nusing expert features and deep neural networks. Comput. Cardiol. https://doi.\\norg/10.22489/CinC.2017.178-245 (2017).\\n 34. Nahar, J., Imam, T., Tickle, K. S. & Chen, Y . P . Computational intelligence  \\nfor heart disease diagnosis: a medical knowledge driven approach.  \\nExpert Syst. Appl. 40, 96–104 (2013).\\n 35. Cubanski, D., Cyganski, D., Antman, E. M. & Feldman, C. L. A neural \\nnetwork system for detection of atrial fibrillation in ambulatory \\nelectrocardiograms. J. Cardiovasc. Electrophysiol. 5, 602–608 (1994).\\n 36. Andreotti, F ., Carr, O., Pimentel, M. A. F ., Mahdi, A. & De Vos, M. \\nComparing feature-based classifiers and convolutional neural networks  \\nto detect arrhythmia from short segments of ECG. Comput. Cardiol.  \\nhttps://doi.org/10.22489/CinC.2017.360-239 (2017).\\n 37. Xu, S. S., Mak, M. & Cheung, C. Towards end-to-end ECG classification with \\nraw signal extraction and deep neural networks. IEEE J. Biomed. Health \\nInformatics 14, 1 (2018).\\n 38. Ong, S. L., Ng, E. Y . K., Tan, R. S. & Acharya, U. R. Automated diagnosis of \\narrhythmia using combination of CNN and LSTM techniques with variable \\nlength heart beats. Comput. Biol. Med. 102, 278–287 (2018).\\n 39. Shashikumar, S. P ., Shah, A. J., Clifford, G. D. & Nemati, S. Detection of \\nparoxysmal atrial fibrillation using attention-based bidirectional recurrent \\nneural networks. In Proc. 24th ACM SIGKDD International Conference on \\nKnowledge Discovery & Data Mining, 715–723 (Association for Computing \\nMachinery, 2018).\\n 40. Xia, Y ., Wulan, N., Wang, K. & Zhang, H. Detecting atrial fibrillation  \\nby deep convolutional neural networks. Comput. Biol. Med. 93,  \\n84–92 (2018).\\n 41. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep  \\nresidual networks. In Proc. European Conference on Computer Vision,  \\n630–645 (Springer, 2016).\\n 42. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep network \\ntraining by reducing internal covariate shift. In Proc. International Conference \\non Machine Learning, 448–456 (2015).\\n 43. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image \\nrecognition. In Proc. IEEE Conference on Computer Vision and Pattern \\nRecognition, 770–778 (IEEE, 2016).\\n 44. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. \\nDropout: a simple way to prevent neural networks from overfitting. J. Mach. \\nLearn. Res. 15, 1929–1958 (2014).\\n 45. Kingma, D. P . & Ba, J. L. Adam: a method for stochastic optimization.  \\nIn Proc. International Conference on Learning Representations  \\n1–15 (2015).\\n 46. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. \\n9, 1735–1780 (1997).\\n 47. Fawcett, T. An introduction to ROC analysis. Pattern Recognit. Lett. 27, \\n861–874 (2006).\\n 48. Hanley, J. A. & McNeil, B. J. A method of comparing the areas under receiver \\noperating characteristic curves derived from the same cases. Radiology 148, \\n839–843 (1983).\\n 49. Saito, T. & Rehmsmeier, M. The precision-recall plot is more informative than \\nthe ROC plot when evaluating binary classifiers on imbalanced datasets. PLoS \\nONE 10, e0118432 (2015).\\nFOCUS | Letters\\nhttps:/ / doi.org/10.1038/s41591-018-0268-3\\nFOCUS | LettersNaTuRE MEDICINE\\nNATuRe MediCiNe | www.nature.com/ naturemedicine\\nLetters | FOCUS NaTuRE MEDICINE\\nExtended Data Fig. 1 | deep Neural Network architecture. Our deep neural network consisted of 33 convolutional layers followed by a linear output layer \\ninto a softmax. The network accepts raw ECG data as input (sampled at 200\\u2009Hz, or 200 samples per second), and outputs a prediction of one out of 12 \\npossible rhythm classes every 256 input samples.\\nLetters | FOCUS\\nLetters | FOCUS NaTuRE MEDICINE\\nNATuRe MediCiNe | www.nature.com/ naturemedicine\\nFOCUS | LettersNaTuRE MEDICINE\\nExtended Data Fig. 2 | Receiver operating characteristic curves for deep neural network predictions on 12 rhythm classes. Individual cardiologist \\nperformance is indicated by the red crosses and averaged cardiologist performance is indicated by the green dot. The line represents the ROC curve \\nof model performance. AF-atrial fibrillation/ atrial flutter; AVB- atrioventricular block; EAR-ectopic atrial rhythm; IVR-idioventricular rhythm; SVT-\\nsupraventricular tachycardia; VT-ventricular tachycardia. n\\u2009= \\u20097,544 where each of the 328 30-second ECGs received 23 sequence-level predictions.\\nFOCUS | Letters\\nhttps:/ / doi.org/10.1038/s41591-018-0268-3\\nFOCUS | LettersNaTuRE MEDICINE\\nNATuRe MediCiNe | www.nature.com/ naturemedicine\\n1 nature research  |  reporting summaryMarch 2018\\nCorresponding author(s): Awni Hannun\\nReporting Summary\\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \\nin reporting. For further information on Nature Research policies, see Authors & Referees and the Editorial Policy Checklist.\\nStatistical parameters\\nWhen statistical analyses are reported, confirm that the following items are present in the relevant location (e.g. figure legend, table legend, main \\ntext, or Methods section).\\nn/a Confirmed\\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\\nAn indication of whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\\nThe statistical test(s) used AND whether they are one- or two-sided \\nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\\nA description of all covariates tested\\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\\nA full description of the statistics including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND \\nvariation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \\nGive P values as exact values whenever suitable.\\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\\nEstimates of effect sizes (e.g. Cohen\\'s d, Pearson\\'s r), indicating how they were calculated\\nClearly defined error bars \\nState explicitly what error bars represent (e.g. SD, SE, CI)\\nOur web collection on statistics for biologists may be useful.\\nSoftware and code\\nPolicy information about availability of computer code\\nData collection We used a small amount of custom software to build the data extraction and annotation tool. This software is not a major part of this \\nwork. The bulk of the custom and existing software we used is in the Data analysis section. The custom software we used for data \\ncollection was written in Python. It targeted records containing different rhythm classes, as described in the manuscript. Upon extraction, \\nthe data was saved in a PostgreSQL database.  The extracted data was then pulled in a web-based tool to be reviewed. An html-based \\ntool was designed for the purpose of accurately reviewing each record. Reviewers were able to see and scroll through 30-second ECG \\nrecords and color code existing rhythms in the record from the onset to their offset. The full segmentation of the input record was then \\npushed to a different table in the same database. The reviewed records were finally de-identified and saved in json format to be used for \\nmodel training purposes. \\nData analysis Code for the algorithm development, evaluation and statistical analysis is open source with no restrictions and available at https://\\ngithub.com/awni/ecg.\\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers \\nupon request. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\\nData\\n2 nature research  |  reporting summaryMarch 2018\\nPolicy information about availability of data\\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \\n- Accession codes, unique identifiers, or web links for publicly available datasets \\n- A list of figures that have associated raw data \\n- A description of any restrictions on data availability\\nThe test dataset used to support the findings of this study is publicly available at https://irhythm.github.io/cardiol_test_set without restriction. Restrictions apply to \\nthe availability of the training dataset, which was used under license from iRhythm Technologies Inc. for the current study. iRhythm will consider requests to access \\nthe training data on an individual basis. Any data use will be restricted to non-commercial research purposes, and the data will only be made available upon \\nexecution of appropriate data use agreements.\\nField-specific reporting\\nPlease select the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\nLife sciences Behavioural & social sciences\\nFor a reference copy of the document with all sections, see nature.com/authors/policies/ReportingSummary-flat.pdf\\nBehavioural & social sciences\\nStudy design\\nAll studies must disclose on these points even when the disclosure is negative.\\nStudy description We developed and validated the performance of a Deep Neural Network on ambulatory single-lead ECG. The data in the study is \\nquantitative consisting of ECG records and their corresponding annotation.\\nResearch sample The dataset was a deidentified, retrospective dataset of adult patients >18 years of age who have used the iRhythm Zio monitor for \\nclinical indications.\\nSampling strategy  Records were chosen randomly from within the study period, though abnormal rhythms were intentionally over-sampled to provide \\nmore training examples for these rhythms. For the training set the sample size was chosen such that our model matched the \\nperformance of certified ECG technicians on a validation dataset. For the test set our sample size was chosen so that we would obtain \\nroughly 20 (or more) examples for each rhythm class. We justified this sample size post-hoc with confidence intervals in our AUC \\ncomputations.\\nData collection We extracted a median of 1 (and a maximum of 3) 30-second records per patient to construct the training dataset. To improve the \\nbalance of classes in the training dataset, records that exhibited less prevalent rhythms were intentionally oversampled, to a maximum of \\nthree 30-second records per patient. For the test dataset, 30-second records of each rhythm were randomly sampled from patients \\nduring the study period to achieve an equal number of records per class. \\nTiming The data was collected retrospectively from a cohort who used the Zio monitor between January 2013 and March 2017\\nData exclusions We excluded patients under the age of 18 from the study. We preestablished this exclusion criteria in order to simplify any potential data \\nuse approval processes regarding the use of data from minors.\\nNon-participation We did not require informed consent for this study given that the data belongs to iRhythm Technologies and that the data was fully de-\\nidentified.\\nRandomization Our data was selected from the study period according to the selection strategy mentioned in the \"Data Collection\" section. Other than \\noversampling for certain arrhythmias all patients were randomly selected from pool of patients available in the study period. \\n'}, {'file_name': 'gpt-2', 'text': 'Language Models are Unsupervised Multitask Learners\\nAlec Radford* 1 Jeffrey Wu* 1 Rewon Child1 David Luan1 Dario Amodei** 1 Ilya Sutskever** 1\\nAbstract\\nNatural language processing tasks, such as ques-\\ntion answering, machine translation, reading com-\\nprehension, and summarization, are typically\\napproached with supervised learning on task-\\nspeciﬁc datasets. We demonstrate that language\\nmodels begin to learn these tasks without any ex-\\nplicit supervision when trained on a new dataset\\nof millions of webpages called WebText. When\\nconditioned on a document plus questions, the an-\\nswers generated by the language model reach 55\\nF1 on the CoQA dataset - matching or exceeding\\nthe performance of 3 out of 4 baseline systems\\nwithout using the 127,000+ training examples.\\nThe capacity of the language model is essential\\nto the success of zero-shot task transfer and in-\\ncreasing it improves performance in a log-linear\\nfashion across tasks. Our largest model, GPT-2,\\nis a 1.5B parameter Transformer that achieves\\nstate of the art results on 7 out of 8 tested lan-\\nguage modeling datasets in a zero-shot setting\\nbut still underﬁts WebText. Samples from the\\nmodel reﬂect these improvements and contain co-\\nherent paragraphs of text. These ﬁndings suggest\\na promising path towards building language pro-\\ncessing systems which learn to perform tasks from\\ntheir naturally occurring demonstrations.\\n1. Introduction\\nMachine learning systems now excel (in expectation) at\\ntasks they are trained for by using a combination of large\\ndatasets, high-capacity models, and supervised learning\\n(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei\\net al., 2016). Yet these systems are brittle and sensitive to\\nslight changes in the data distribution (Recht et al., 2018)\\nand task speciﬁcation (Kirkpatrick et al., 2017). Current sys-\\ntems are better characterized as narrow experts rather than\\n*, **Equal contribution 1OpenAI, San Francisco, Califor-\\nnia, United States. Correspondence to: Alec Radford\\n<alec@openai.com>.\\ncompetent generalists. We would like to move towards more\\ngeneral systems which can perform many tasks – eventually\\nwithout the need to manually create and label a training\\ndataset for each one.\\nThe dominant approach to creating ML systems is to col-\\nlect a dataset of training examples demonstrating correct\\nbehavior for a desired task, train a system to imitate these\\nbehaviors, and then test its performance on independent\\nand identically distributed (IID) held-out examples. This\\nhas served well to make progress on narrow experts. But\\nthe often erratic behavior of captioning models (Lake et al.,\\n2017), reading comprehension systems (Jia & Liang, 2017),\\nand image classiﬁers (Alcorn et al., 2018) on the diversity\\nand variety of possible inputs highlights some of the short-\\ncomings of this approach.\\nOur suspicion is that the prevalence of single task training\\non single domain datasets is a major contributor to the lack\\nof generalization observed in current systems. Progress\\ntowards robust systems with current architectures is likely\\nto require training and measuring performance on a wide\\nrange of domains and tasks. Recently, several benchmarks\\nhave been proposed such as GLUE (Wang et al., 2018) and\\ndecaNLP (McCann et al., 2018) to begin studying this.\\nMultitask learning (Caruana, 1997) is a promising frame-\\nwork for improving general performance. However, mul-\\ntitask training in NLP is still nascent. Recent work re-\\nports modest performance improvements (Yogatama et al.,\\n2019) and the two most ambitious efforts to date have\\ntrained on a total of 10 and 17(dataset, objective)\\npairs respectively (McCann et al., 2018) (Bowman et al.,\\n2018). From a meta-learning perspective, each (dataset,\\nobjective) pair is a single training example sampled\\nfrom the distribution of datasets and objectives. Current\\nML systems need hundreds to thousands of examples to\\ninduce functions which generalize well. This suggests that\\nmultitask training many need just as many effective training\\npairs to realize its promise with current approaches. It will\\nbe very difﬁcult to continue to scale the creation of datasets\\nand the design of objectives to the degree that may be re-\\nquired to brute force our way there with current techniques.\\nThis motivates exploring additional setups for performing\\nmultitask learning.\\nThe current best performing systems on language tasks\\nLanguage Models are Unsupervised Multitask Learners\\nFigure 1.Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. Reading Comprehension results\\nare on CoQA (Reddy et al., 2018), translation on WMT-14 Fr-En (Artetxe et al., 2017), summarization on CNN and Daily Mail (See et al.,\\n2017), and Question Answering on Natural Questions (Kwiatkowski et al., 2019). Section 3 contains detailed descriptions of each result.\\nutilize a combination of pre-training and supervised ﬁne-\\ntuning. This approach has a long history with a trend to-\\nwards more ﬂexible forms of transfer. First, word vectors\\nwere learned and used as inputs to task-speciﬁc architec-\\ntures (Mikolov et al., 2013) (Collobert et al., 2011), then\\nthe contextual representations of recurrent networks were\\ntransferred (Dai & Le, 2015) (Peters et al., 2018), and re-\\ncent work suggests that task-speciﬁc architectures are no\\nlonger necessary and transferring many self-attention blocks\\nis sufﬁcient (Radford et al., 2018) (Devlin et al., 2018).\\nThese methods still require supervised training in order\\nto perform a task. When only minimal or no supervised\\ndata is available, another line of work has demonstrated\\nthe promise of language models to perform speciﬁc tasks,\\nsuch as commonsense reasoning (Schwartz et al., 2017) and\\nsentiment analysis (Radford et al., 2017).\\nIn this paper, we connect these two lines of work and con-\\ntinue the trend of more general methods of transfer. We\\ndemonstrate language models can perform down-stream\\ntasks in a zero-shot setting – without any parameter or archi-\\ntecture modiﬁcation. We demonstrate this approach shows\\npotential by highlighting the ability of language models to\\nperform a wide range of tasks in a zero-shot setting. We\\nachieve promising, competitive, and state of the art results\\ndepending on the task.\\n2. Approach\\nAt the core of our approach is language modeling. Lan-\\nguage modeling is usually framed as unsupervised distri-\\nbution estimation from a set of examples (x1, x2, ..., xn)\\neach composed of variable length sequences of symbols\\n(s1, s2, ..., sn). Since language has a natural sequential or-\\ndering, it is common to factorize the joint probabilities over\\nsymbols as the product of conditional probabilities (Jelinek\\n& Mercer, 1980) (Bengio et al., 2003):\\np(x) =\\nn∏\\ni=1\\np(sn|s1, ..., sn−1) (1)\\nThis approach allows for tractable sampling from and es-\\ntimation of p(x) as well as any conditionals of the form\\np(sn−k, ..., sn|s1, ..., sn−k−1). In recent years, there have\\nbeen signiﬁcant improvements in the expressiveness of mod-\\nels that can compute these conditional probabilities, such as\\nself-attention architectures like the Transformer (Vaswani\\net al., 2017).\\nLearning to perform a single task can be expressed in a\\nprobabilistic framework as estimating a conditional distri-\\nbution p(output|input). Since a general system should be\\nable to perform many different tasks, even for the same\\ninput, it should condition not only on the input but also\\non the task to be performed. That is, it should model\\np(output|input, task). This has been variously formalized\\nin multitask and meta-learning settings. Task conditioning\\nis often implemented at an architectural level, such as the\\ntask speciﬁc encoders and decoders in (Kaiser et al., 2017)\\nor at an algorithmic level such as the inner and outer loop\\noptimization framework of MAML (Finn et al., 2017). But\\nas exempliﬁed in McCann et al. (2018), language provides\\na ﬂexible way to specify tasks, inputs, and outputs all as a\\nsequence of symbols. For example, a translation training\\nexample can be written as the sequence (translate to\\nfrench, english text, french text). Like-\\nwise, a reading comprehension training example can\\nbe written as (answer the question, document,\\nquestion, answer). McCann et al. (2018) demon-\\nstrated it was possible to train a single model, the MQAN,\\nLanguage Models are Unsupervised Multitask Learners\\nto infer and perform many different tasks on examples with\\nthis type of format.\\nLanguage modeling is also able to, in principle, learn the\\ntasks of McCann et al. (2018) without the need for explicit\\nsupervision of which symbols are the outputs to be pre-\\ndicted. Since the supervised objective is the the same as the\\nunsupervised objective but only evaluated on a subset of the\\nsequence, the global minimum of the unsupervised objective\\nis also the global minimum of the supervised objective. In\\nthis slightly toy setting, the concerns with density estimation\\nas a principled training objective discussed in (Sutskever\\net al., 2015) are side stepped. The problem instead becomes\\nwhether we are able to, in practice, optimize the unsuper-\\nvised objective to convergence. Preliminary experiments\\nconﬁrmed that sufﬁciently large language models are able to\\nperform multitask learning in this toy-ish setup but learning\\nis much slower than in explicitly supervised approaches.\\nWhile it is a large step from the well-posed setup described\\nabove to the messiness of “language in the wild”, Weston\\n(2016) argues, in the context of dialog, for the need to\\ndevelop systems capable of learning from natural language\\ndirectly and demonstrated a proof of concept – learning a\\nQA task without a reward signal by using forward prediction\\nof a teacher’s outputs. While dialog is an attractive approach,\\nwe worry it is overly restrictive. The internet contains a vast\\namount of information that is passively available without\\nthe need for interactive communication. Our speculation is\\nthat a language model with sufﬁcient capacity will begin\\nto learn to infer and perform the tasks demonstrated in\\nnatural language sequences in order to better predict them,\\nregardless of their method of procurement. If a language\\nmodel is able to do this it will be, in effect, performing\\nunsupervised multitask learning. We test whether this is the\\ncase by analyzing the performance of language models in a\\nzero-shot setting on a wide variety of tasks.\\n2.1. Training Dataset\\nMost prior work trained language models on a single do-\\nmain of text, such as news articles (Jozefowicz et al., 2016),\\nWikipedia (Merity et al., 2016), or ﬁction books (Kiros\\net al., 2015). Our approach motivates building as large and\\ndiverse a dataset as possible in order to collect natural lan-\\nguage demonstrations of tasks in as varied of domains and\\ncontexts as possible.\\nA promising source of diverse and nearly unlimited text is\\nweb scrapes such as Common Crawl. While these archives\\nare many orders of magnitude larger than current language\\nmodeling datasets, they have signiﬁcant data quality issues.\\nTrinh & Le (2018) used Common Crawl in their work on\\ncommonsense reasoning but noted a large amount of doc-\\numents “whose content are mostly unintelligible”. We ob-\\nserved similar data issues in our initial experiments with\\n”I’m not the cleverest man in the world, but like they say in\\nFrench: Je ne suis pas un imbecile [I’m not a fool].\\nIn a now-deleted post from Aug. 16, Soheil Eid, Tory candidate\\nin the riding of Joliette, wrote in French: ” Mentez mentez,\\nil en restera toujours quelque chose,” which translates as,\\n”Lie lie and something will always remain.”\\n“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better\\nin French: ‘parfum.’\\nIf listened carefully at 29:55, a conversation can be heard\\nbetween two guys in French: “-Comment on fait pour aller\\nde l’autre cot´e? -Quel autre cot´e?”, which means “- How\\ndo you get to the other side? - What side?”.\\nIf this sounds like a bit of a stretch, consider this ques-\\ntion in French: As-tu aller au cin´ema?, or Did you go to\\nthe movies?, which literally translates as Have-you to go to\\nmovies/theater?\\n“Brevet Sans Garantie Du Gouvernement”, translated to\\nEnglish: “Patented without government warranty”.\\nTable 1.Examples of naturally occurring demonstrations of En-\\nglish to French and French to English translation found throughout\\nthe WebText training set.\\nCommon Crawl. Trinh & Le (2018)’s best results were\\nachieved using a small subsample of Common Crawl which\\nincluded only documents most similar to their target dataset,\\nthe Winograd Schema Challenge. While this is a pragmatic\\napproach to improve performance on a speciﬁc task, we\\nwant to avoid making assumptions about the tasks to be\\nperformed ahead of time.\\nInstead, we created a new web scrape which emphasizes\\ndocument quality. To do this we only scraped web pages\\nwhich have been curated/ﬁltered by humans. Manually\\nﬁltering a full web scrape would be exceptionally expensive\\nso as a starting point, we scraped all outbound links from\\nReddit, a social media platform, which received at least 3\\nkarma. This can be thought of as a heuristic indicator for\\nwhether other users found the link interesting, educational,\\nor just funny.\\nThe resulting dataset, WebText, contains the text subset\\nof these 45 million links. To extract the text from HTML\\nresponses we use a combination of the Dragnet (Peters &\\nLecocq, 2013) and Newspaper1 content extractors. All re-\\nsults presented in this paper use a preliminary version of\\nWebText which does not include links created after Dec\\n2017 and which after de-duplication and some heuristic\\nbased cleaning contains slightly over 8 million documents\\nfor a total of 40 GB of text. We removed all Wikipedia\\ndocuments from WebText since it is a common data source\\nfor other datasets and could complicate analysis due to over-\\n1https://github.com/codelucas/newspaper\\nLanguage Models are Unsupervised Multitask Learners\\nlapping training data with test evaluation tasks.\\n2.2. Input Representation\\nA general language model (LM) should be able to compute\\nthe probability of (and also generate) any string. Current\\nlarge scale LMs include pre-processing steps such as lower-\\ncasing, tokenization, and out-of-vocabulary tokens which\\nrestrict the space of model-able strings. While processing\\nUnicode strings as a sequence of UTF-8 bytes elegantly ful-\\nﬁlls this requirement as exempliﬁed in work such as Gillick\\net al. (2015), current byte-level LMs are not competitive\\nwith word-level LMs on large scale datasets such as the\\nOne Billion Word Benchmark (Al-Rfou et al., 2018). We\\nobserved a similar performance gap in our own attempts to\\ntrain standard byte-level LMs on WebText.\\nByte Pair Encoding (BPE) (Sennrich et al., 2015) is a\\npractical middle ground between character and word level\\nlanguage modeling which effectively interpolates between\\nword level inputs for frequent symbol sequences and char-\\nacter level inputs for infrequent symbol sequences. Despite\\nits name, reference BPE implementations often operate on\\nUnicode code points and not byte sequences. These imple-\\nmentations would require including the full space of Uni-\\ncode symbols in order to model all Unicode strings. This\\nwould result in a base vocabulary of over 130,000 before\\nany multi-symbol tokens are added. This is prohibitively\\nlarge compared to the 32,000 to 64,000 token vocabularies\\noften used with BPE. In contrast, a byte-level version of\\nBPE only requires a base vocabulary of size 256. However,\\ndirectly applying BPE to the byte sequence results in sub-\\noptimal merges due to BPE using a greedy frequency based\\nheuristic for building the token vocabulary. We observed\\nBPE including many versions of common words like dog\\nsince they occur in many variations such as dog. dog!\\ndog? . This results in a sub-optimal allocation of limited\\nvocabulary slots and model capacity. To avoid this, we pre-\\nvent BPE from merging across character categories for any\\nbyte sequence. We add an exception for spaces which sig-\\nniﬁcantly improves the compression efﬁciency while adding\\nonly minimal fragmentation of words across multiple vocab\\ntokens.\\nThis input representation allows us to combine the empirical\\nbeneﬁts of word-level LMs with the generality of byte-level\\napproaches. Since our approach can assign a probability to\\nany Unicode string, this allows us to evaluate our LMs on\\nany dataset regardless of pre-processing, tokenization, or\\nvocab size.\\n2.3. Model\\nWe use a Transformer (Vaswani et al., 2017) based archi-\\ntecture for our LMs. The model largely follows the details\\nof the OpenAI GPT model (Radford et al., 2018) with a\\nParameters Layers dmodel\\n117M 12 768\\n345M 24 1024\\n762M 36 1280\\n1542M 48 1600\\nTable 2.Architecture hyperparameters for the 4 model sizes.\\nfew modiﬁcations. Layer normalization (Ba et al., 2016)\\nwas moved to the input of each sub-block, similar to a\\npre-activation residual network (He et al., 2016) and an\\nadditional layer normalization was added after the ﬁnal self-\\nattention block. A modiﬁed initialization which accounts\\nfor the accumulation on the residual path with model depth\\nis used. We scale the weights of residual layers at initial-\\nization by a factor of 1/\\n√\\nN where N is the number of\\nresidual layers. The vocabulary is expanded to 50,257. We\\nalso increase the context size from 512 to 1024 tokens and\\na larger batchsize of 512 is used.\\n3. Experiments\\nWe trained and benchmarked four LMs with approximately\\nlog-uniformly spaced sizes. The architectures are summa-\\nrized in Table 2. The smallest model is equivalent to the\\noriginal GPT, and the second smallest equivalent to the\\nlargest model from BERT (Devlin et al., 2018). Our largest\\nmodel, which we call GPT-2, has over an order of magni-\\ntude more parameters than GPT. The learning rate of each\\nmodel was manually tuned for the best perplexity on a 5%\\nheld-out sample of WebText. All models still underﬁt Web-\\nText and held-out perplexity has as of yet improved given\\nmore training time.\\n3.1. Language Modeling\\nAs an initial step towards zero-shot task transfer, we are\\ninterested in understanding how WebText LM’s perform\\nat zero-shot domain transfer on the primary task they are\\ntrained for – language modeling. Since our model operates\\non a byte level and does not require lossy pre-processing\\nor tokenization, we can evaluate it on any language model\\nbenchmark. Results on language modeling datasets are\\ncommonly reported in a quantity which is a scaled or ex-\\nponentiated version of the average negative log probability\\nper canonical prediction unit - usually a character, a byte, or\\na word. We evaluate the same quantity by computing the\\nlog-probability of a dataset according to a WebText LM and\\ndividing by the number of canonical units. For many of these\\ndatasets, WebText LMs would be tested signiﬁcantly out-\\nof-distribution, having to predict aggressively standardized\\ntext, tokenization artifacts such as disconnected punctuation\\nand contractions, shufﬂed sentences, and even the string\\nLanguage Models are Unsupervised Multitask Learners\\nLAMBADA LAMBADA CBT-CN CBT-NE WikiText2 PTB enwik8 text8 WikiText103 1BW\\n(PPL) (ACC) (ACC) (ACC) (PPL) (PPL) (BPB) (BPC) (PPL) (PPL)\\nSOTA 99.8 59.23 85.7 82.3 39.14 46.54 0.99 1.08 18.3 21.8\\n117M 35.13 45.99 87.65 83.4 29.41 65.85 1.16 1.17 37.50 75.20\\n345M 15.60 55.48 92.35 87.1 22.76 47.33 1.01 1.06 26.37 55.72\\n762M 10.87 60.12 93.45 88.0 19.93 40.31 0.97 1.02 22.05 44.575\\n1542M 8.63 63.24 93.30 89.05 18.34 35.76 0.93 0.98 17.48 42.16\\nTable 3.Zero-shot results on many datasets. No training or ﬁne-tuning was performed for any of these results. PTB and WikiText-2\\nresults are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018)\\nand LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019).\\n<UNK> which is extremely rare in WebText - occurring\\nonly 26 times in 40 billion bytes. We report our main re-\\nsults in Table 3 using invertible de-tokenizers which remove\\nas many of these tokenization / pre-processing artifacts as\\npossible. Since these de-tokenizers are invertible, we can\\nstill calculate the log probability of a dataset and they can\\nbe thought of as a simple form of domain adaptation. We\\nobserve gains of 2.5 to 5 perplexity for GPT-2 with these\\nde-tokenizers.\\nWebText LMs transfer well across domains and datasets,\\nimproving the state of the art on 7 out of the 8 datasets in a\\nzero-shot setting. Large improvements are noticed on small\\ndatasets such as Penn Treebank and WikiText-2 which have\\nonly 1 to 2 million training tokens. Large improvements\\nare also noticed on datasets created to measure long-term\\ndependencies like LAMBADA (Paperno et al., 2016) and\\nthe Children’s Book Test (Hill et al., 2015). Our model is\\nstill signiﬁcantly worse than prior work on the One Billion\\nWord Benchmark (Chelba et al., 2013). This is likely due\\nto a combination of it being both the largest dataset and\\nhaving some of the most destructive pre-processing - 1BW’s\\nsentence level shufﬂing removes all long-range structure.\\n3.2. Children’s Book Test\\nFigure 2.Performance on the Children’s Book Test as a function of\\nmodel capacity. Human performance are from Bajgar et al. (2016),\\ninstead of the much lower estimates from the original paper.\\nThe Children’s Book Test (CBT) (Hill et al., 2015) was\\ncreated to examine the performance of LMs on different cat-\\negories of words: named entities, nouns, verbs, and preposi-\\ntions. Rather than reporting perplexity as an evaluation met-\\nric, CBT reports accuracy on an automatically constructed\\ncloze test where the task is to predict which of 10 possible\\nchoices for an omitted word is correct. Following the LM\\napproach introduced in the original paper, we compute the\\nprobability of each choice and the rest of the sentence con-\\nditioned on this choice according to the LM, and predict\\nthe one with the highest probability. As seen in Figure 2\\nperformance steadily improves as model size is increased\\nand closes the majority of the gap to human performance\\non this test. Data overlap analysis showed one of the CBT\\ntest set books, The Jungle Book by Rudyard Kipling, is in\\nWebText, so we report results on the validation set which\\nhas no signiﬁcant overlap. GPT-2 achieves new state of the\\nart results of 93.3% on common nouns and 89.1% on named\\nentities. A de-tokenizer was applied to remove PTB style\\ntokenization artifacts from CBT.\\n3.3. LAMBADA\\nThe LAMBADA dataset (Paperno et al., 2016) tests the\\nability of systems to model long-range dependencies in\\ntext. The task is to predict the ﬁnal word of sentences\\nwhich require at least 50 tokens of context for a human to\\nsuccessfully predict. GPT-2 improves the state of the art\\nfrom 99.8 (Grave et al., 2016) to 8.6 perplexity and increases\\nthe accuracy of LMs on this test from 19% (Dehghani et al.,\\n2018) to 52.66%. Investigating GPT-2’s errors showed most\\npredictions are valid continuations of the sentence, but are\\nnot valid ﬁnal words. This suggests that the LM is not\\nusing the additional useful constraint that the word must be\\nthe ﬁnal of the sentence. Adding a stop-word ﬁlter as an\\napproximation to this further increases accuracy to 63.24%,\\nimproving the overall state of the art on this task by 4%. The\\nprevious state of the art (Hoang et al., 2018) used a different\\nrestricted prediction setting where the outputs of the model\\nwere constrained to only words that appeared in the context.\\nFor GPT-2, this restriction is harmful rather than helpful\\nLanguage Models are Unsupervised Multitask Learners\\nsince 19% of answers are not in context. We use a version\\nof the dataset without preprocessing.\\n3.4. Winograd Schema Challenge\\nFigure 3.Performance on the Winograd Schema Challenge as a\\nfunction of model capacity.\\nThe Winograd Schema challenge (Levesque et al., 2012)\\nwas constructed to measure the capability of a system to\\nperform commonsense reasoning by measuring its ability\\nto resolve ambiguities in text. Recently Trinh & Le (2018)\\ndemonstrated signiﬁcant progress on this challenge using\\nLMs, by predicting the resolution of the ambiguity with\\nhigher probability. We follow their problem formulation and\\nvisualize the performance of our models with both full and\\npartial scoring techniques in Figure 3. GPT-2 improves state\\nof the art accuracy by 7%, achieving 70.70%. The dataset\\nis quite small with only 273 examples so we recommend\\nreading Trichelair et al. (2018) to help contextualize this\\nresult.\\n3.5. Reading Comprehension\\nThe Conversation Question Answering dataset (CoQA)\\nReddy et al. (2018) consists of documents from 7 different\\ndomains paired with natural language dialogues between a\\nquestion asker and a question answerer about the document.\\nCoQA tests reading comprehension capabilities and also\\nthe ability of models to answer questions that depend on\\nconversation history (such as “Why?”).\\nGreedy decoding from GPT-2 when conditioned on a doc-\\nument, the history of the associated conversation, and a\\nﬁnal token A: achieves 55 F1 on the development set. This\\nmatches or exceeds the performance of 3 out of 4 base-\\nline systems without using the 127,000+ manually collected\\nquestion answer pairs those baselines were trained on. The\\nsupervised SOTA, a BERT based system (Devlin et al.,\\nR-1 R-2 R-L R-A VG\\nBottom-Up Sum 41.22 18.68 38.34 32.75\\nLede-3 40.38 17.66 36.62 31.55\\nSeq2Seq + Attn 31.33 11.81 28.83 23.99\\nGPT-2 TL;DR: 29.34 8.27 26.58 21.40\\nRandom-3 28.78 8.63 25.52 20.98\\nGPT-2 no hint 21.58 4.03 19.47 15.03\\nTable 4.Summarization performance as measured by ROUGE F1\\nmetrics on the CNN and Daily Mail dataset. Bottom-Up Sum is\\nthe SOTA model from (Gehrmann et al., 2018)\\n2018), is nearing the 89 F1 performance of humans. While\\nGPT-2’s performance is exciting for a system without any su-\\npervised training, some inspection of its answers and errors\\nsuggests GPT-2 often uses simple retrieval based heuristics\\nsuch as answer with a name from the document in response\\nto a who question.\\n3.6. Summarization\\nWe test GPT-2’s ability to perform summarization on the\\nCNN and Daily Mail dataset (Nallapati et al., 2016). To in-\\nduce summarization behavior we add the textTL;DR: after\\nthe article and generate 100 tokens with Top-k random sam-\\npling (Fan et al., 2018) with k = 2which reduces repetition\\nand encourages more abstractive summaries than greedy de-\\ncoding. We use the ﬁrst 3 generated sentences in these 100\\ntokens as the summary. While qualitatively the generations\\nresemble summaries, as shown in Table 14, they often focus\\non recent content from the article or confuse speciﬁc details\\nsuch as how many cars were involved in a crash or whether\\na logo was on a hat or shirt. On the commonly reported\\nROUGE 1,2,L metrics the generated summaries only begin\\nto approach the performance of classic neural baselines and\\njust barely outperforms selecting 3 random sentences from\\nthe article. GPT-2’s performance drops by 6.4 points on\\nthe aggregate metric when the task hint is removed which\\ndemonstrates the ability to invoke task speciﬁc behavior in\\na language model with natural language.\\n3.7. Translation\\nWe test whether GPT-2 has begun to learn how to translate\\nfrom one language to another. In order to help it infer that\\nthis is the desired task, we condition the language model\\non a context of example pairs of the format english\\nsentence = french sentence and then after a ﬁ-\\nnal prompt of english sentence = we sample from\\nthe model with greedy decoding and use the ﬁrst generated\\nsentence as the translation. On the WMT-14 English-French\\ntest set, GPT-2 gets 5 BLEU, which is slightly worse than\\na word-by-word substitution with a bilingual lexicon in-\\nferred in previous work on unsupervised word translation\\nLanguage Models are Unsupervised Multitask Learners\\nQuestion Generated Answer Correct Probability\\nWho wrote the book the origin of species? Charles Darwin \\x13 83.4%\\nWho is the founder of the ubuntu project? Mark Shuttleworth \\x13 82.0%\\nWho is the quarterback for the green bay packers? Aaron Rodgers \\x13 81.1%\\nPanda is a national animal of which country? China \\x13 76.8%\\nWho came up with the theory of relativity? Albert Einstein \\x13 76.4%\\nWhen was the ﬁrst star wars ﬁlm released? 1977 \\x13 71.4%\\nWhat is the most common blood type in sweden? A \\x17 70.6%\\nWho is regarded as the founder of psychoanalysis? Sigmund Freud \\x13 69.3%\\nWho took the ﬁrst steps on the moon in 1969? Neil Armstrong \\x13 66.8%\\nWho is the largest supermarket chain in the uk? Tesco \\x13 65.3%\\nWhat is the meaning of shalom in english? peace \\x13 64.0%\\nWho was the author of the art of war? Sun Tzu \\x13 59.6%\\nLargest state in the us by land mass? California \\x17 59.2%\\nGreen algae is an example of which type of reproduction? parthenogenesis \\x17 56.5%\\nVikram samvat calender is ofﬁcial in which country? India \\x13 55.6%\\nWho is mostly responsible for writing the declaration of independence? Thomas Jefferson \\x13 53.3%\\nWhat us state forms the western boundary of montana? Montana \\x17 52.3%\\nWho plays ser davos in game of thrones? Peter Dinklage \\x17 52.1%\\nWho appoints the chair of the federal reserve system? Janet Yellen \\x17 51.5%\\nState the process that divides one nucleus into two genetically identical nuclei? mitosis \\x13 50.7%\\nWho won the most mvp awards in the nba? Michael Jordan \\x17 50.2%\\nWhat river is associated with the city of rome? the Tiber \\x13 48.6%\\nWho is the ﬁrst president to be impeached? Andrew Johnson \\x13 48.3%\\nWho is the head of the department of homeland security 2017? John Kelly \\x13 47.0%\\nWhat is the name given to the common currency to the european union? Euro \\x13 46.8%\\nWhat was the emperor name in star wars? Palpatine \\x13 46.5%\\nDo you have to have a gun permit to shoot at a range? No \\x13 46.4%\\nWho proposed evolution in 1859 as the basis of biological development? Charles Darwin \\x13 45.7%\\nNuclear power plant that blew up in russia? Chernobyl \\x13 45.7%\\nWho played john connor in the original terminator? Arnold Schwarzenegger \\x17 45.2%\\nTable 5.The 30 most conﬁdent answers generated by GPT-2 on the development set of Natural Questions sorted by their probability\\naccording to GPT-2. None of these questions appear in WebText according to the procedure described in Section 4.\\n(Conneau et al., 2017b). On the WMT-14 French-English\\ntest set, GPT-2 is able to leverage its very strong English\\nlanguage model to perform signiﬁcantly better, achieving\\n11.5 BLEU. This outperforms several unsupervised machine\\ntranslation baselines from (Artetxe et al., 2017) and (Lample\\net al., 2017) but is still much worse than the 33.5 BLEU of\\nthe current best unsupervised machine translation approach\\n(Artetxe et al., 2019). Performance on this task was sur-\\nprising to us, since we deliberately removed non-English\\nwebpages from WebText as a ﬁltering step. In order to con-\\nﬁrm this, we ran a byte-level language detector2 on WebText\\nwhich detected only 10MB of data in the French language\\nwhich is approximately 500x smaller than the monolingual\\nFrench corpus common in prior unsupervised machine trans-\\nlation research.\\n3.8. Question Answering\\nA potential way to test what information is contained within\\na language model is to evaluate how often it generates the\\ncorrect answer to factoid-style questions. Previous showcas-\\ning of this behavior in neural systems where all information\\nis stored in parameters such as A Neural Conversational\\nModel (Vinyals & Le, 2015) reported qualitative results due\\nto the lack of high-quality evaluation datasets. The recently\\nintroduced Natural Questions dataset (Kwiatkowski et al.,\\n2https://github.com/CLD2Owners/cld2\\n2019) is a promising resource to test this more quantita-\\ntively. Similar to translation, the context of the language\\nmodel is seeded with example question answer pairs which\\nhelps the model infer the short answer style of the dataset.\\nGPT-2 answers 4.1% of questions correctly when evalu-\\nated by the exact match metric commonly used on reading\\ncomprehension datasets like SQUAD. 3 As a comparison\\npoint, the smallest model does not exceed the 1.0% accu-\\nracy of an incredibly simple baseline which returns the most\\ncommon answer for each question type (who, what, where,\\netc...). GPT-2 answers 5.3 times more questions correctly,\\nsuggesting that model capacity has been a major factor in\\nthe poor performance of neural systems on this kind of task\\nas of yet. The probability GPT-2 assigns to its generated\\nanswers is well calibrated and GPT-2 has an accuracy of\\n63.1% on the 1% of questions it is most conﬁdent in. The\\n30 most conﬁdent answers generated by GPT-2 on develop-\\nment set questions are shown in Table 5. The performance\\nof GPT-2 is still much, much, worse than the 30 to 50%\\nrange of open domain question answering systems which\\nhybridize information retrieval with extractive document\\nquestion answering (Alberti et al., 2019).\\n3Alec, who previously thought of himself as good at random\\ntrivia, answered 17 of 100 randomly sampled examples correctly\\nwhen tested in the same setting as GPT-2. He actually only got 14 right but he\\nshould have gotten those other 3\\nLanguage Models are Unsupervised Multitask Learners\\nPTB WikiText-2 enwik8 text8 Wikitext-103 1BW\\nDataset train 2.67% 0.66% 7.50% 2.34% 9.09% 13.19%\\nWebText train 0.88% 1.63% 6.31% 3.94% 2.42% 3.75%\\nTable 6.Percentage of test set 8 grams overlapping with training sets.\\n4. Generalization vs Memorization\\nRecent work in computer vision has shown that common im-\\nage datasets contain a non-trivial amount of near-duplicate\\nimages. For instance CIFAR-10 has 3.3% overlap between\\ntrain and test images (Barz & Denzler, 2019). This results in\\nan over-reporting of the generalization performance of ma-\\nchine learning systems. As the size of datasets increases this\\nissue becomes increasingly likely which suggests a similar\\nphenomena could be happening with WebText. Therefore it\\nis important to analyze how much test data also shows up in\\nthe training data.\\nTo study this we created Bloom ﬁlters containing 8-grams\\nof WebText training set tokens. To improve recall, strings\\nwere normalized to contain only lower-cased alphanumeric\\nwords with a single space as a delimiter. The Bloom ﬁlters\\nwere constructed such that the false positive rate is upper\\nbounded by 1\\n108 . We further veriﬁed the low false positive\\nrate by generating 1M strings, of which zero were found by\\nthe ﬁlter.\\nThese Bloom ﬁlters let us calculate, given a dataset, the\\npercentage of 8-grams from that dataset that are also found\\nin the WebText training set. Table 6 shows this overlap anal-\\nysis for the test sets of common LM benchmarks. Common\\nLM datasets’ test sets have between 1-6% overlap with Web-\\nText train, with an average of overlap of 3.2%. Somewhat\\nsurprisingly, many datasets have larger overlaps with their\\nown training splits, with an average of 5.9% overlap.\\nOur approach optimizes for recall, and while manual inspec-\\ntion of the overlaps shows many common phrases, there are\\nmany longer matches that are due to duplicated data. This is\\nnot unique to WebText. For instance, we discovered that the\\ntest set of WikiText-103 has an article which is also in the\\ntraining dataset. Since there are only 60 articles in the test\\nset there is at least an overlap of 1.6%. 4 Potentially more\\nworryingly, 1BW has an overlap of nearly 13.2% with its\\nown training set according to our procedure.\\nFor the Winograd Schema Challenge, we found only 10\\nschemata which had any 8-gram overlaps with the WebText\\ntraining set. Of these, 2 were spurious matches. Of the\\nremaining 8, only 1 schema appeared in any contexts that\\n4A signiﬁcant portion of additional overlap is due to editors\\nreusing some paragraphs across multiple articles with a shared\\ntheme such as various battles in the Korean War.\\ngave away the answer.\\nFor CoQA, about 15% of documents in the news domain\\nare already in WebText and the model performs about 3\\nF1 better on these. CoQA’s development set metric reports\\nthe average performance over 5 different domains and we\\nmeasure a gain of about 0.5-1.0 F1 due to overlap across the\\nvarious domains. However, no actual training questions or\\nanswers are in WebText since CoQA was released after the\\ncutoff date for links in WebText.\\nOn LAMBADA, the average overlap is 1.2%. GPT-2 per-\\nforms about 2 perplexity better on examples with greater\\nthan 15% overlap. Recalculating metrics when excluding\\nall examples with any overlap shifts results from 8.6 to 8.7\\nperplexity and reduces accuracy from 63.2% to 62.9%. This\\nvery small change in overall results is likely due to only 1\\nin 200 examples having signiﬁcant overlap.\\nOverall, our analysis suggests that data overlap between\\nWebText training data and speciﬁc evaluation datasets pro-\\nvides a small but consistent beneﬁt to reported results. How-\\never, for most datasets we do not notice signiﬁcantly larger\\noverlaps than those already existing between standard train-\\ning and test sets, as Table 6 highlights.\\nUnderstanding and quantifying how highly similar text im-\\npacts performance is an important research question. Better\\nde-duplication techniques such as scalable fuzzy matching\\ncould also help better answer these questions. For now, we\\nrecommend the use of n-gram overlap based de-duplication\\nas an important veriﬁcation step and sanity check during the\\ncreation of training and test splits for new NLP datasets.\\nAnother potential way of determining whether the perfor-\\nmance of WebText LMs is attributable to memorization is\\ninspecting their performance on their own held-out set. As\\nshown in Figure 4, performance on both the training and\\ntest sets of WebText are similar and improve together as\\nmodel size is increased. This suggests even GPT-2 is still\\nunderﬁtting on WebText in many ways.\\nGPT-2 is also able to write news articles about the discovery\\nof talking unicorns. An example is provided in Table 13.\\n5. Related Work\\nA signiﬁcant portion of this work measured the performance\\nof larger language models trained on larger datasets. This\\nLanguage Models are Unsupervised Multitask Learners\\nFigure 4.The performance of LMs trained on WebText as a func-\\ntion of model size.\\nis similar to the work of Jozefowicz et al. (2016) which\\nscaled RNN based language models on the 1 Billion Word\\nBenchmark. Bajgar et al. (2016) also previously improved\\nresults on the Children’s Book Test by creating a much larger\\ntraining dataset out of Project Gutenberg to supplement the\\nstandard training dataset. Hestness et al. (2017) conducted\\na thorough analysis of how the performance of various deep\\nlearning models changes as a function of both model capac-\\nity and dataset size. Our experiments, while much noisier\\nacross tasks, suggest similar trends hold for sub-tasks of an\\nobjective and continue into the 1B+ parameter regime.\\nInteresting learned functionality in generative models\\nhas been documented before such as the cells in an\\nRNN language model performing line-width tracking and\\nquote/comment detection Karpathy et al. (2015). More in-\\nspirational to our work was the observation of Liu et al.\\n(2018) that a model trained to generate Wikipedia articles\\nalso learned to translate names between languages.\\nPrevious work has explored alternative approaches to ﬁlter-\\ning and constructing a large text corpus of web pages, such\\nas the iWeb Corpus (Davies, 2018).\\nThere has been extensive work on pre-training methods\\nfor language tasks. In addition to those mentioned in the\\nintroduction, GloVe (Pennington et al., 2014) scaled word\\nvector representation learning to all of Common Crawl. An\\ninﬂuential early work on deep representation learning for\\ntext was Skip-thought Vectors(Kiros et al., 2015). McCann\\net al. (2017) explored the use of representations derived from\\nmachine translation models and Howard & Ruder (2018)\\nimproved the RNN based ﬁne-tuning approaches of (Dai\\n& Le, 2015). (Conneau et al., 2017a) studied the transfer\\nperformance of representations learned by natural language\\ninference models and (Subramanian et al., 2018) explored\\nlarge-scale multitask training.\\n(Ramachandran et al., 2016) demonstrated that seq2seq mod-\\nels beneﬁt from being initialized with pre-trained language\\nmodels as encoders and decoders. More recent work has\\nshown that LM pre-training is helpful when ﬁne-tuned for\\ndifﬁcult generation tasks like chit-chat dialog and dialog\\nbased question answering systems as well (Wolf et al., 2019)\\n(Dinan et al., 2018).\\n6. Discussion\\nMuch research has been dedicated to learning (Hill et al.,\\n2016), understanding (Levy & Goldberg, 2014), and criti-\\ncally evaluating (Wieting & Kiela, 2019) the representations\\nof both supervised and unsupervised pre-training methods.\\nOur results suggest that unsupervised task learning is an\\nadditional promising area of research to explore. These\\nﬁndings potentially help explain the widespread success of\\npre-training techniques for down-stream NLP tasks as we\\nshow that, in the limit, one of these pre-training techniques\\nbegins to learn to perform tasks directly without the need\\nfor supervised adaption or modiﬁcation.\\nOn reading comprehension the performance of GPT-2 is\\ncompetitive with supervised baselines in a zero-shot setting.\\nHowever, on other tasks such as summarization, while it\\nis qualitatively performing the task, its performance is still\\nonly rudimentary according to quantitative metrics. While\\nsuggestive as a research result, in terms of practical applica-\\ntions, the zero-shot performance of GPT-2 is still far from\\nuse-able.\\nWe have studied the zero-shot performance of WebText\\nLMs on many canonical NLP tasks, but there are many addi-\\ntional tasks that could be evaluated. There are undoubtedly\\nmany practical tasks where the performance of GPT-2 is\\nstill no better than random. Even on common tasks that we\\nevaluated on, such as question answering and translation,\\nlanguage models only begin to outperform trivial baselines\\nwhen they have sufﬁcient capacity.\\nWhile zero-shot performance establishes a baseline of the\\npotential performance of GPT-2 on many tasks, it is not\\nclear where the ceiling is with ﬁnetuning. On some tasks,\\nGPT-2’s fully abstractive output is a signiﬁcant departure\\nfrom the extractive pointer network (Vinyals et al., 2015)\\nbased outputs which are currently state of the art on many\\nquestion answering and reading comprehension datasets.\\nGiven the prior success of ﬁne-tuning GPT, we plan to in-\\nvestigate ﬁne-tuning on benchmarks such as decaNLP and\\nGLUE, especially since it is unclear whether the additional\\nLanguage Models are Unsupervised Multitask Learners\\ntraining data and capacity of GPT-2 is sufﬁcient to over-\\ncome the inefﬁciencies of uni-directional representations\\ndemonstrated by BERT (Devlin et al., 2018).\\n7. Conclusion\\nWhen a large language model is trained on a sufﬁciently\\nlarge and diverse dataset it is able to perform well across\\nmany domains and datasets. GPT-2 zero-shots to state of\\nthe art performance on 7 out of 8 tested language model-\\ning datasets. The diversity of tasks the model is able to\\nperform in a zero-shot setting suggests that high-capacity\\nmodels trained to maximize the likelihood of a sufﬁciently\\nvaried text corpus begin to learn how to perform a surprising\\namount of tasks without the need for explicit supervision.5\\nAcknowledgements\\nThanks to everyone who wrote the text, shared the links,\\nand upvoted the content in WebText. Many millions of\\npeople were involved in creating the data that GPT-2 was\\ntrained on. Also thanks to all the Googlers who helped us\\nwith training infrastructure, including Zak Stone, JS Riehl,\\nJonathan Hseu, Russell Power, Youlong Cheng, Noam\\nShazeer, Solomon Boulos, Michael Banﬁeld, Aman Gupta,\\nDaniel Sohn, and many more. Finally thanks to the people\\nwho gave feedback on drafts of the paper: Jacob Steinhardt,\\nSam Bowman, Geoffrey Irving, and Madison May.\\nReferences\\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L.\\nCharacter-level language modeling with deeper self-attention.\\narXiv preprint arXiv:1808.04444, 2018.\\nAlberti, C., Lee, K., and Collins, M. A bert baseline for the natural\\nquestions. arXiv preprint arXiv:1901.08634, 2019.\\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and\\nNguyen, A. Strike (with) a pose: Neural networks are easily\\nfooled by strange poses of familiar objects. arXiv preprint\\narXiv:1811.11553, 2018.\\nAmodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Batten-\\nberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen,\\nG., et al. Deep speech 2: End-to-end speech recognition in\\nenglish and mandarin. In International Conference on Machine\\nLearning, pp. 173–182, 2016.\\nArtetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsupervised\\nneural machine translation. arXiv preprint arXiv:1710.11041,\\n2017.\\nArtetxe, M., Labaka, G., and Agirre, E. An effective ap-\\nproach to unsupervised machine translation. arXiv preprint\\narXiv:1902.01313, 2019.\\n5Preliminary code for downloading and using the small model\\nis available at https://github.com/openai/gpt-2\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\\narXiv preprint arXiv:1607.06450, 2016.\\nBajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abun-\\ndance: Booktest dataset for reading comprehension. arXiv\\npreprint arXiv:1610.00956, 2016.\\nBarz, B. and Denzler, J. Do we train on test data? purging cifar of\\nnear-duplicates. arXiv preprint arXiv:1902.00423, 2019.\\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A neural\\nprobabilistic language model. Journal of machine learning\\nresearch, 3(Feb):1137–1155, 2003.\\nBowman, S. R., Pavlick, E., Grave, E., Van Durme, B., Wang, A.,\\nHula, J., Xia, P., Pappagari, R., McCoy, R. T., Patel, R., et al.\\nLooking for elmo’s friends: Sentence-level pretraining beyond\\nlanguage modeling. arXiv preprint arXiv:1812.10860, 2018.\\nCaruana, R. Multitask learning. Machine learning, 28(1):41–75,\\n1997.\\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn,\\nP., and Robinson, T. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv preprint\\narXiv:1312.3005, 2013.\\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu,\\nK., and Kuksa, P. Natural language processing (almost) from\\nscratch. Journal of Machine Learning Research, 12(Aug):2493–\\n2537, 2011.\\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bor-\\ndes, A. Supervised learning of universal sentence represen-\\ntations from natural language inference data. arXiv preprint\\narXiv:1705.02364, 2017a.\\nConneau, A., Lample, G., Ranzato, M., Denoyer, L., and J ´egou,\\nH. Word translation without parallel data. arXiv preprint\\narXiv:1710.04087, 2017b.\\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning. In\\nAdvances in neural information processing systems, pp. 3079–\\n3087, 2015.\\nDai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le,\\nQ. V ., and Salakhutdinov, R. Transformer-xl: Attentive lan-\\nguage models beyond a ﬁxed-length context. arXiv preprint\\narXiv:1901.02860, 2019.\\nDavies, M. The 14 billion word iweb corpus.\\nhttps://corpus.byu.edu/iWeb/, 2018.\\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser,\\nŁ. Universal transformers. arXiv preprint arXiv:1807.03819,\\n2018.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-\\ntraining of deep bidirectional transformers for language under-\\nstanding. arXiv preprint arXiv:1810.04805, 2018.\\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston,\\nJ. Wizard of wikipedia: Knowledge-powered conversational\\nagents. arXiv preprint arXiv:1811.01241, 2018.\\nFan, A., Lewis, M., and Dauphin, Y . Hierarchical neural story\\ngeneration. arXiv preprint arXiv:1805.04833, 2018.\\nLanguage Models are Unsupervised Multitask Learners\\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\\nlearning for fast adaptation of deep networks. arXiv preprint\\narXiv:1703.03400, 2017.\\nGehrmann, S., Deng, Y ., and Rush, A. M. Bottom-up abstractive\\nsummarization. arXiv preprint arXiv:1808.10792, 2018.\\nGillick, D., Brunk, C., Vinyals, O., and Subramanya, A. Mul-\\ntilingual language processing from bytes. arXiv preprint\\narXiv:1512.00103, 2015.\\nGong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y . Frage:\\nfrequency-agnostic word representation. In Advances in Neural\\nInformation Processing Systems, pp. 1341–1352, 2018.\\nGrave, E., Joulin, A., and Usunier, N. Improving neural\\nlanguage models with a continuous cache. arXiv preprint\\narXiv:1612.04426, 2016.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep\\nresidual networks. In European conference on computer vision,\\npp. 630–645. Springer, 2016.\\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kian-\\ninejad, H., Patwary, M., Ali, M., Yang, Y ., and Zhou, Y . Deep\\nlearning scaling is predictable, empirically. arXiv preprint\\narXiv:1712.00409, 2017.\\nHill, F., Bordes, A., Chopra, S., and Weston, J. The goldilocks\\nprinciple: Reading children’s books with explicit memory rep-\\nresentations. arXiv preprint arXiv:1511.02301, 2015.\\nHill, F., Cho, K., and Korhonen, A. Learning distributed repre-\\nsentations of sentences from unlabelled data. arXiv preprint\\narXiv:1602.03483, 2016.\\nHoang, L., Wiseman, S., and Rush, A. M. Entity tracking im-\\nproves cloze-style reading comprehension. arXiv preprint\\narXiv:1810.02891, 2018.\\nHoward, J. and Ruder, S. Universal language model ﬁne-tuning for\\ntext classiﬁcation. In Proceedings of the 56th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1:\\nLong Papers), volume 1, pp. 328–339, 2018.\\nJelinek, F. and Mercer, R. L. Interpolated estimation of markov\\nsource parameters from sparse data. In Proceedings of the\\nWorkshop on Pattern Recognition in Practice, Amsterdam, The\\nNetherlands: North-Holland, May., 1980.\\nJia, R. and Liang, P. Adversarial examples for evaluating read-\\ning comprehension systems. arXiv preprint arXiv:1707.07328,\\n2017.\\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu,\\nY . Exploring the limits of language modeling. arXiv preprint\\narXiv:1602.02410, 2016.\\nKaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N.,\\nJones, L., and Uszkoreit, J. One model to learn them all. arXiv\\npreprint arXiv:1706.05137, 2017.\\nKarpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and under-\\nstanding recurrent networks. arXiv preprint arXiv:1506.02078,\\n2015.\\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins,\\nG., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-\\nBarwinska, A., et al. Overcoming catastrophic forgetting in\\nneural networks. Proceedings of the national academy of sci-\\nences, pp. 201611835, 2017.\\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun, R.,\\nTorralba, A., and Fidler, S. Skip-thought vectors. In Advances\\nin neural information processing systems, pp. 3294–3302, 2015.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁ-\\ncation with deep convolutional neural networks. In Advances in\\nneural information processing systems, pp. 1097–1105, 2012.\\nKwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M., Parikh,\\nA., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin,\\nJ., et al. Natural questions: a benchmark for question answering\\nresearch. 2019.\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J.\\nBuilding machines that learn and think like people. Behavioral\\nand Brain Sciences, 40, 2017.\\nLample, G., Conneau, A., Denoyer, L., and Ranzato, M. Unsu-\\npervised machine translation using monolingual corpora only.\\narXiv preprint arXiv:1711.00043, 2017.\\nLevesque, H., Davis, E., and Morgenstern, L. The winograd\\nschema challenge. In Thirteenth International Conference on\\nthe Principles of Knowledge Representation and Reasoning,\\n2012.\\nLevy, O. and Goldberg, Y . Neural word embedding as implicit ma-\\ntrix factorization. In Advances in neural information processing\\nsystems, pp. 2177–2185, 2014.\\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L.,\\nand Shazeer, N. Generating wikipedia by summarizing long\\nsequences. arXiv preprint arXiv:1801.10198, 2018.\\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned\\nin translation: Contextualized word vectors. In Advances in\\nNeural Information Processing Systems, pp. 6294–6305, 2017.\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural\\nlanguage decathlon: Multitask learning as question answering.\\narXiv preprint arXiv:1806.08730, 2018.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel\\nmixture models. arXiv preprint arXiv:1609.07843, 2016.\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean,\\nJ. Distributed representations of words and phrases and their\\ncompositionality. In Advances in neural information processing\\nsystems, pp. 3111–3119, 2013.\\nNallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstrac-\\ntive text summarization using sequence-to-sequence rnns and\\nbeyond. arXiv preprint arXiv:1602.06023, 2016.\\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi,\\nR., Pezzelle, S., Baroni, M., Boleda, G., and Fern´andez, R. The\\nlambada dataset: Word prediction requiring a broad discourse\\ncontext. arXiv preprint arXiv:1606.06031, 2016.\\nPennington, J., Socher, R., and Manning, C. Glove: Global vectors\\nfor word representation. In Proceedings of the 2014 conference\\non empirical methods in natural language processing (EMNLP),\\npp. 1532–1543, 2014.\\nLanguage Models are Unsupervised Multitask Learners\\nPeters, M. E. and Lecocq, D. Content extraction using diverse fea-\\nture sets. In Proceedings of the 22nd International Conference\\non World Wide Web, pp. 89–90. ACM, 2013.\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\\nLee, K., and Zettlemoyer, L. Deep contextualized word repre-\\nsentations. arXiv preprint arXiv:1802.05365, 2018.\\nRadford, A., Jozefowicz, R., and Sutskever, I. Learning to\\ngenerate reviews and discovering sentiment. arXiv preprint\\narXiv:1704.01444, 2017.\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I.\\nImproving language understanding by generative pre-training.\\n2018.\\nRamachandran, P., Liu, P. J., and Le, Q. V . Unsupervised pre-\\ntraining for sequence to sequence learning. arXiv preprint\\narXiv:1611.02683, 2016.\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do\\ncifar-10 classiﬁers generalize to cifar-10? arXiv preprint\\narXiv:1806.00451, 2018.\\nReddy, S., Chen, D., and Manning, C. D. Coqa: A conversational\\nquestion answering challenge. arXiv preprint arXiv:1808.07042,\\n2018.\\nSchwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y ., and Smith,\\nN. A. Story cloze task: Uw nlp system. In Proceedings of the\\n2nd Workshop on Linking Models of Lexical, Sentential and\\nDiscourse-level Semantics, pp. 52–55, 2017.\\nSee, A., Liu, P. J., and Manning, C. D. Get to the point: Sum-\\nmarization with pointer-generator networks. arXiv preprint\\narXiv:1704.04368, 2017.\\nSennrich, R., Haddow, B., and Birch, A. Neural machine trans-\\nlation of rare words with subword units. arXiv preprint\\narXiv:1508.07909, 2015.\\nSubramanian, S., Trischler, A., Bengio, Y ., and Pal, C. J. Learning\\ngeneral purpose distributed sentence representations via large\\nscale multi-task learning. arXiv preprint arXiv:1804.00079,\\n2018.\\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to sequence\\nlearning with neural networks. In Advances in neural informa-\\ntion processing systems, pp. 3104–3112, 2014.\\nSutskever, I., Jozefowicz, R., Gregor, K., Rezende, D., Lillicrap,\\nT., and Vinyals, O. Towards principled unsupervised learning.\\narXiv preprint arXiv:1511.06440, 2015.\\nTrichelair, P., Emami, A., Cheung, J. C. K., Trischler, A., Sule-\\nman, K., and Diaz, F. On the evaluation of common-sense\\nreasoning in natural language understanding. arXiv preprint\\narXiv:1811.01778, 2018.\\nTrinh, T. H. and Le, Q. V . A simple method for commonsense\\nreasoning. arXiv preprint arXiv:1806.02847, 2018.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\\nGomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is\\nall you need. In Advances in Neural Information Processing\\nSystems, pp. 5998–6008, 2017.\\nVinyals, O. and Le, Q. A neural conversational model. arXiv\\npreprint arXiv:1506.05869, 2015.\\nVinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In\\nAdvances in Neural Information Processing Systems, pp. 2692–\\n2700, 2015.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-\\nman, S. R. Glue: A multi-task benchmark and analysis\\nplatform for natural language understanding. arXiv preprint\\narXiv:1804.07461, 2018.\\nWeston, J. E. Dialog-based language learning. In Advances in\\nNeural Information Processing Systems, pp. 829–837, 2016.\\nWieting, J. and Kiela, D. No training required: Exploring\\nrandom encoders for sentence classiﬁcation. arXiv preprint\\narXiv:1901.10444, 2019.\\nWolf, T., Sanh, V ., Chaumond, J., and Delangue, C. Transfer-\\ntransfo: A transfer learning approach for neural network based\\nconversational agents. arXiv preprint arXiv:1901.08149, 2019.\\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky, T.,\\nChrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L.,\\nDyer, C., et al. Learning and evaluating general linguistic intel-\\nligence. arXiv preprint arXiv:1901.11373, 2019.\\nLanguage Models are Unsupervised Multitask Learners\\n8. Appendix A: Samples\\n8.1. Model capacity\\nTo complement the reported perplexity gains of bigger LMs on\\nWebText show in Figure 4, Tables 7 through 11 show side-by-side\\ncompletions of the smallest WebText LM and GPT-2 on random\\nunseen WebText test set articles.\\n8.2. Text Memorization\\nWe observe some memorizing behavior in GPT-2 on longer strings\\nthat are repeated many times in the dataset such as famous quotes\\nor speeches. For example, when conditioned on the ﬁrst sentence\\nand a half of the Gettysburg Address (which occurs approximately\\n40 times throughout WebText), an argmax decode from GPT-2\\nrecovers the speech. Even when sampling without truncation, we\\nﬁnd that the model copies the speech for awhile before drifting,\\nalbeit in a similar style. It typically drifts within 100-200 tokens,\\nand displays widening diversity once it drifts.\\nTo quantify how often exact memorization shows up in samples,\\nwe generated samples from GPT-2 conditioned on WebText test\\nset articles and compared the overlap rates of GPT-2’s generations\\nto the overlap rates of the ground-truth completions. The results of\\nthis analysis are shown below and suggest that GPT-2 repeats text\\nfrom the training set less often then the baseline rate of held-out\\narticles.\\nFigure 5.CDF of percentage 8-gram overlap with WebText train-\\ning set, for both WebText test set and samples (conditioned on\\nWebText test set, with top- k truncated random sampling with\\nk = 40). Most samples have less than 1% overlap, including over\\n30% of samples with no overlap, whereas the median for test set is\\n2.6% overlap.\\n8.3. Diversity\\nTable 12 shows multiple completions of the same random WebText\\ntest set context, showing the diversity of completions with standard\\nsampling settings.\\n8.4. Robustness\\nTable 13 shows the previously mentioned talking unicorns news\\narticle. We ﬁnd the model to be capable of handling out of dis-\\ntribution contexts, but the quality of these samples is generally\\nlower.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (WebText test)\\nOutﬁt 1: Typical\\nThis pairing was the ﬁrst outﬁt I thought of when I bought the shoes. It’s like a summer version of this\\nJake Grantham outﬁt; in fact, my shoes are close to the colors of his Nike Racers! Instead of a heavy Harris Tweed\\njacket and denim shirt, I’m wearing a cotton DB jacket and and a linen shirt. Both fabrics (in these colors) are an\\nabsolute must for summer, as they go with both dark and and light pants! As you can see, they pair wonderfully\\nwith the dark jeans and shoes. It’s a pseudo menswear/prep outﬁt.\\nOverall, this is a very casual outﬁt which is why I paired my sneakers with it. I’m not about wearing a\\nfull wool suit with sneakers (as GQ shows a lot) but I’m deﬁnitely open to keeping things casual, like this cotton\\nDB. Casual fabrics are key to pulling off your sneakers in a dressed down menswear outﬁt. I’d even suggest to\\nwear these sneakers with a khaki chino suit or a white linen suit. Just be sure to ditch the tie or wear a tee or polo;\\nwearing a tie with sneakers is a bit too much\\nSmall model completion\\n.\\nOutﬁt 2: High End\\nThis outﬁt was just in the eye of the storm. It’s a\\nhybrid of what ASICS does with streetwear (like J. Crew)\\nand what J. Crew does with casual pieces that are a bit\\nmore traditional. Again, as I mentioned before, it’s a low\\nbudget item and they can be very expensive. These will\\ndeﬁnitely be an issue if you’re trying to work as a casual\\nsneakermaker, and that’s OK but don’t hesitate if you\\nwant to try the lower price versions of their outﬁts!\\nThis version features a very low cut-cut neckline\\n(which is not a bad thing), slightly longer legs, and a pair\\nof long legs to keep things nice. My preferred color of\\ncolor these items are blue-gray (which is my ﬁrst colors,\\nas they are a light white), or black (which is my favorite\\nof my designs). As always, my pick of colors from this\\nstore have nothing to do with that other than the de-\\nsign. For a high end, I would recommend this to everyone!\\nOutﬁt 3: High Light\\nThis was a ”high light” look while I had a chance\\nto get some color. I think the color is based on\\nGPT-2 completion\\nfor me!\\nOutﬁt 2: Casual Jacket\\nI know that some people might be opposed to wearing\\nsneakers in a turtleneck, but I wanted to be true to myself,\\nso I went with a slim ﬁtting turtleneck in a color more\\nsimilar to my favorite color of the day. Overall, it’s a\\nclassic turtleneck, with nothing too ﬂashy. I did swap my\\nshoes for these ASICS Tiger Killshots and I wish I hadn’t\\ndid that. The shoe itself is very comfortable, however,\\nI found that wearing them in these sneakers made my\\nfeet look a bit larger. I’d wear them with denim, chinos,\\nwhatever, but would never recommend wearing them\\nalone. There are just too many variables involved.\\nOutﬁt 3: Dress\\nThis is the ﬁnal outﬁt I wore the ASICS Tiger\\nKillshots. This version of the ASICS is a much more\\nmodern and streamlined version. The shoe has the most\\nmodern and minimal design that I can think of! With a lot\\nof black, these sneakers look very cool. They’ll deﬁnitely\\nbe very comfortable for summer.\\nTo me, wearing them with a suit is the easiest way\\nto have a good pair of sneaker shoes. That’s pretty much\\nit! If\\nTable 7.Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\\nrandom sampling with k = 40was used for generation.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (WebText test)\\nSome of the most glorious historical attractions in Spain date from the period of Muslim rule, including The\\nMezquita, built as the Great Mosque of C´ordoba and the Medina Azahara, also in C´ordoba and now in ruins but\\nstill visitable as such and built as the Madinat al-Zahra, the Palace of al-Andalus; and the Alhambra in Granada, a\\nsplendid, intact palace. There are also two synagogues still standing that were built during the era of Muslim Spain:\\nSanta Mar´ıa la Blanca in Toledo and the Synagogue of C´ordoba, in the Old City.\\nReconquista and Imperial era [ edit ]\\nSmall model completion\\nThe era of Muslim Spain is very different from that of\\nother parts of the world, with a rather extensive medieval\\nhistory. While the Spanish Empire existed during the\\nsecond half of the 9th century BC it was quite slowly\\ntransformed to a small, secularized empire by 1150 A.D.,\\nin a much more prosperous and diverse state. In 1050 CE\\nthe emperor of Naples, at least partially in characterised by\\na series of wars and invasions from the north-western coast\\nto the western coast of the region (e.g., on the Danube\\nRiver which extended all the way from Gibraltar into the\\nsouth of Spain, but was quickly destroyed on the same\\nday by the Roman army), assumed the title of Emperor\\nof Naples. He made a large contribution over the last few\\ndecades to restore and maintain the empire, the ﬁrst of\\nwhich to the west. His successors were Ferdinand, who\\ntook power from the Papal Throne in 1148, while the ﬁrst\\nimperial successor was King Alfred II of Sweden. The\\nempire was again made one of the few independent states\\nin the European Continent and although no more than 200\\nmillion people lived on its territory during this period the\\npeople had much to say to the Emperor. From 1720 on the\\nMuslims made up half the population of Spain.\\nGPT-2 completion\\nThe Reconquista (literally, ”The return to the country”)\\nof 1092 and 1095 saw the establishment in the Iberian\\npeninsula of a united kingdom of Castile and Aragon in\\nwhich a Muslim dynasty would exist. This was the last\\ntime in Europe where Muslim rule would be considered\\n”native” and the last occasion where Christian-inﬂamed\\nand organized conﬂicts would make themselves known in\\nEurope.\\nThe reign of Muslim Moors in Spain is often re-\\nferred to, especially among non-Muslims, as the\\n”Conquest of Granada” and Spain is sometimes referred\\nto as Spain in ”Granada”. The last years of Muslim rule\\nwere followed by the Spanish Reconquista where the\\nMuslims were driven or compelled out of the Iberian\\nPeninsula with the victory of Ferdinand IX. in the Battle\\nof Poitiers in March 1189 and the conquest of Jerusalem\\nin 1187 by Saladin.\\nThe Reconquista period in Spain came to an end\\nwhen the Moors retreated north. However the Moorish\\nrule would not be fully replaced by the Christian rule until\\nthe Reconquista era resumed when the Reconquista lasted\\nfrom 1229 to 1492.\\nThe last Muslim state in Spain, Granada, was con-\\nquered by the\\nTable 8.Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\\nrandom sampling with k = 40was used for generation.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (WebText test)\\nPinky\\nThe pink ghost’s AI is designed to ”feel” opposite of the red ghost’s behavior. Pinky actually attempts to\\nget out in front of Pac-Man. This is accomplished by setting the target 4 tiles ahead of Pac-Man’s current location in\\nthe direction that Pac-Man is travelling. One exception to this is when Pac-Man is traveling up. Due to an overﬂow\\nbug in the code, the calculation includes a left offset equal to the expected up offset.\\nThe use of Pac-Man’s expected position allows the player to exploit the navigation code for Pinky. This was most\\noften seen in situations where Pac-Man and Pinky are in close proximity to each other. For example, if they were\\nheaded toward one another, once they are less than 4 tiles apart, the target tile is now behind Pinky. If he encounters\\nan intersection while in this conﬁguration, he will attempt to double-back (by turning) in order to reach the target tile.\\nPlayer’s would often perform what was referred to as a ”head fake” by changing directions rapidly just\\nas Pinky was approaching an intersection. By displacing the target tile just as Pinky\\nSmall model completion\\nwas coming to the left, they would get the ”correct”\\ndirection.\\nIf the location is actually farther from what Pac-\\nMan was at, the player’s will not respond to the ghost’s\\nmovement, and the ghost will instead return to the player’s\\ncurrent location. This is because Pac-Man does not move\\nby turning.\\nThe ghost will have to go to another location of\\nsome kind to return where he began, and this is achieved\\nin many ways. It will often simply leave the location, as\\nfor example if Pac-Man did jump off a cliff, he would\\nreturn to the spot where he was before vanishing into\\nspace. There is a good number of possibilities these ghost\\npaths may be possible, but the most obvious ones are if\\nthe player wants to avoid getting caught up in the game,\\nwith a goal as easy as keeping Pac-Man in the game for\\n1/3 as long as he was.\\nChasing\\nIn Pac-Man’s case, this would be similar to how\\nPac-Man can ”catch up” in a ﬁght.\\nA ghost could chase through enemies. This is use-\\nful for many reasons, both for Pac-Man and his AI.\\nUnlike most Ghost games, this ghost simply travels in the\\ndirection from\\nGPT-2 completion\\nwas about to cross the intersection, Pac-Man would be\\nable to dodge the ghosts’ projectiles and return to the safe\\nlocation of the safe house.\\nBuster\\nThe yellow ghost’s AI is speciﬁcally designed to\\ntry to avoid Pac-Man from leaving the safe house. The\\nplayer must press the SPACE bar to reveal the safe\\nhouse’s doors, then go into them to ﬁnd the yellow ghost\\nhiding in the next room.\\nThe player must also note that the ghost will not\\nattempt to exit the safe house if it sees Pac-Man already\\nthere. In fact, the ghost will not attempt an exit in the ﬁrst\\nplace unless they’re already in the safe house.\\nThese two behaviors are based on the fact that the\\nplayer does not always attempt to reach the safe house\\nbefore Pac-Man (or Blinky).\\nThe player must also beware of this glitch if a\\nghost (or another player) is already in the safe house\\nand attempting to get out as soon as Pac-Man or Blinky\\nis gone. At this point, the ghost will attempt to exit\\nthe safe house in a similar manner as it is in ”Chase” mode.\\nPac-Man\\nThere are some ghosts that follow Pac-Man. These ghosts\\nare\\nTable 9.Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\\nrandom sampling with k = 40was used for generation.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (WebText test)\\nCorporal Michael P. Goeldin was an unskilled laborer from Ireland when he enlisted in Company A in November\\n1860. Goldein survived the war. Corporal Patrick O’Neal, also from Ireland, ﬁrst enlisted in 1854 and served with\\nCompany L, 3d U.S. Artillery, in Oregon. He returned to the East Coast and enlisted in the company in 1860.\\nO’Neal served until 1874, when he was named superintendent of the National Cemetery at Willets Point, New York.\\nCorporal Benjamin Browne was a shoemaker from Orange County, New York. In August 1862, he enlisted in the\\nnewly formed 124th New York V olunteers, and was one of sixty-one men who transferred into Company A that\\nOctober. Browne reenlisted in the company in February 1864 while it was camped at Brandy Station. He returned\\nto civilian life after completing his enlistment in 1867.\\nOn 10 June, Artiﬁcer William Collins was promoted to corporal, probably to ﬁll a combat leadership\\nvoid for the crossing of the James River. Collins’s service record does not reﬂect the qualities he demonstrated to\\nearn this promotion, but he had obviously overcome some serious problems. Born in Sacketts Harbor, New York,\\nCollins enlisted in the company in December 1853 at the age of twenty-two, and reenlisted in December 1858. Just\\na month before the war began in April 1861, Collins went ”over the hill” and was not caught until three years later.\\nReturned to the company on 22 March 1864, he was tried\\nSmall model completion\\non military charges and was discharged from the U.S.\\nMilitary. He returned to the company, and was assigned\\nto Division of Internal Affairs during the Civil War.\\nAs an engineer, Collins also had training in engi-\\nneering from his native New York. After graduate studies\\nin the Union College of Engineering, he was employed\\nin the United States Army for two years at the same\\ntime. There he worked under Captain Daniel Pfeiffer\\nand was promoted to colonel in May 1870, three years\\nafter Captain Pfeiffer’s enlistment. Collins returned\\nhome to England after completing his ﬁrst year in the\\nFirst Division to serve the next two years in the Army\\nCorps of Engineers, and by his second year, he began to\\nexperience his service duty.¡—endoftext—¿MOSCOW,\\nJuly 5 (Reuters) - U.K.-based Russian state media and\\ngovernment sources and experts say Moscow has sought\\nto downplay the ”military buildup” of the Syrian oppo-\\nsition while maintaining support for President Donald\\nTrump’s plan to build a wall along the U.S.-Mexico\\nborder, calling it a ”military coup”.\\nU.S.-led Syria war veteran and Syria envoy Michael\\nFlynn (R), right, gestures with U.S.-based Syrian political\\nanalyst Ahmet Koryat, with\\nGPT-2 completion\\nin June of that year. Sentenced to be shot for desertion, he\\nremained on active duty.\\nCaptain James J. Dall was born in Virginia in 1829 during\\nthe Revolution and joined his father in the Army in 1836.\\nHe served with the 2d Virginia Infantry until 1845, when\\nhe went to the Western Reserve Regiment; and, on his\\ndischarge, he reenlisted with Company I, 25th New York\\nV olunteers (which in 1845 numbered about 4,500 men).\\nOne of his brothers, James, served with his company\\nin the same capacity in the same brigade. While the\\nregiment remained in Connecticut through 1846, Captain\\nDall served with Company I at various times before\\nhis discharge. A veteran of the Mexican War, he had\\nserved two years on the staff of Brigadier General John J.\\nSullivan at San Antonio. During the Peninsula Campaign\\nof 1863, he commanded a company of twenty-three and\\nwas in charge of the battery of ﬁfty-four heavy mortars\\nand the gunboat Pinta of the Battery, a gunboat and\\nseveral other boats. Captain Dall was on active duty with\\nCompany I in the fall of 1865. Two years later, he was\\npromoted to second lieutenant.\\nCaptain John A. O’\\nTable 10.Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\\nrandom sampling with k = 40was used for generation.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (WebText test)\\nXmonad, Xresources, and Fonts\\nXresources can be an absolute pain (they were for me). So, this a bit of help:\\nFirst of all, the way that fonts are generally handled in Xmonad is through XFT . Xft is the X FreeType\\nInterface library and is a library that handles the actual rendering of fonts in Xmonad/urxvt (Xresources speciﬁes\\na bunch of resources to things that launch under the X window manager. Settings for individual programs are\\nspeciﬁed in Xresources like so: Urxvt.background:background settins here ).\\nAnyway, fonts are set using this syntax in Xresources :\\nurxvt*font:xft:your font name goes here:size=the size of the font goes here,xft:fall-back font 1 goes here\\nSo, thats the most basic part of fonts in urxvt speciﬁed via Xresources.\\nFixing /.ssh/ Permissions in Cygwin\\nIn Cygwin on Windows I found that I could not set the permissions of my /.ssh/ folder to be 0600, as is\\nrequired for ssh to allow you to use keys. The symptom I had was that I no matter what I did, it always modﬁed the\\nowner and the group\\nSmall model completion\\nwas set to /.ssh/ , which was the way things were. This\\ncould not be ﬁxed completely. I found I could make that\\nmuch more workable with just typing the username with\\nyour mouse then trying to set permissions to be ”0600”\\nfor the group, which is what my system is now conﬁgured\\nto do.\\nHowever, as of Cygwin 7 on Linux I have found\\nthat doing so is now ”unnecessary” for my system! I also\\ninstalled a package called chroot. It does that by allowing\\nyou to install/upgrade to your system directory.\\nSo, as of Windows 8 and up, installing that can re-\\nquire chroot, so I tried and tried and found that it wasn’t\\nso bad when installing the package on Linux and then\\nuninstalling it. As of my last write-up on Cygwin, the\\nscript just does its job (the sudo commands do the rest), if\\nyou try it you will have to log the change.\\nIf that wasn’t confusing enough, this is what I tried and\\njust went back to my /.ssh/ directory and the chroot script.\\nI found I was ﬁnally running out of permissions (because\\ni forgot to create a symlink to ssh from the ﬁle system and\\nGPT-2 completion\\n, making these ﬁles read-only. I tried just starting ssh\\nand using a program like passwd which modiﬁes the\\nowner/group to 0644 permissions, but it didn’t work. It\\nwas annoying but not nearly as bad as it could be, just\\nannoying to have to ﬁddle with permissions.\\nSo I decided to do this as the reverse:\\nFor each .ssh folder in your Cygwin directory, set\\nthe permission of each ﬁle and directory to 2644. Then\\nstart ssh with: ssh-add /.ssh/\\nThe ﬁrst time you connect, your permissions of\\nthe /.ssh folder that you just had in Cygwin will be\\nset to 0600. Then every time you connect again, your\\npermissions will change to 0644.\\nMakeﬁle for Xresources as used in /.xsessionrc\\nThese are made in my .xsessionrc :\\n.xproﬁle: .xsession-errors: color=””\\nFor example:\\n.xproﬁle: color=blue\\nAs the colors are stored in a separate ﬁle, the color in your\\n/.xsession-errors will look like cyan instead of blue.\\nThis is mostly because, as of now, only the color\\nis assigned to your\\nTable 11.Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\\nrandom sampling with k = 40was used for generation.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (WebText test)\\nAaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some of\\nthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of\\nthose in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate\\nchippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the ﬁnished\\nproduct, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And\\ndon’t be afraid to use a heavy hand. Texture = good.\\nIf you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this\\nGPT-2 Completion 1\\nrecipe. I made the longer version,\\nthen used the pan for a layer of\\nbuttercream frosting and stuck 1/3\\nof it into the fridge for a frosting\\ntopper. Just make sure you have all\\nof this frosting prepared earlier than\\nyou make this cake and you won’t\\nhave to wait on it as your egg whites\\nsit on the bottom of your mixing\\nbowl.\\nYou want the egg whites whipped\\nto stiff peaks? Then put your cake\\ndish over a pot of simmering water\\nand heat it up until it’s just barely\\nwarm. It doesn’t take more than 3-5\\nminutes; just enough to get it started\\nGPT-2 Completion 2\\nrecipe. If you do, and you know\\nhow to make that cake, then your\\nlife will never be the same.\\nI have two very simple rules\\nwhen it comes to cookie crumbs\\nor chocolate chips. 1) When I buy\\nraw sugar, I do not use it until it’s\\ncooled down and has at least half a\\nteaspoon of sugar on it. That’s my\\nrule. And 2) If I have a recipe that\\ncalls for something and I want to\\nuse it in a recipe, I do not substitute\\nit with something else on-line. Like\\nthis.\\nI found this really cute card\\nwith cute little kittens on\\nGPT-2 Completion 3\\nrecipe, which will likely yield about\\nthe same amount of cake.\\nChocolate Buttermilk Cake\\nPrint Chunks of chocolate cake,\\ncoated in buttermilk and topped\\nwith crunchy chopped chocolate,\\ndrizzled with chocolate sauce.\\nSimple but a nice change from\\nchocolate cake! Author: Momma\\nMade Ingredients For the chocolate\\nbuttermilk cake 1 1/2 cup unsalted\\nbutter , room temperature\\n1 1/2 cup granulated sugar\\n3 large eggs plus 1 egg yolk For\\nthe chocolate glaze 1/3 cup cocoa\\npowder\\n1 3/4 cups powdered sugar\\n6 ounces semis\\nGPT-2 Completion 4\\nrecipe.\\nMeringue Cookies\\n1 3/4 cup butter, softened\\n1 cup sugar\\n1 egg yolk\\n3 T. heavy cream\\n3 1/2-4 cups ﬂour\\npinch of salt\\nPeppermint-Jojo Topping\\n1 1/4 cups powdered sugar\\n1 cup chopped pecans\\n1/2 cup ﬁnely chopped mint leaves\\n1/2 cup chopped fresh mint (about\\n1/2 sheet)\\n1 tsp. peppermint extract\\n1/4 tsp. sea salt\\npinch of sea\\nGPT-2 Completion 5\\nrecipe.\\nMy 2 failures at Christmas morning\\nwere not due to poor frosting skills.\\nThat was another success story.\\nThe cake was beautifully moist,\\nand well, just a beautiful cake. It’s\\ngot very little air in it or moisture,\\nbut all the chocolate and mint was\\nperfectly blended together.\\nOne thing that made me cringe a\\nbit was that all those ingredients-\\negg yolks, sugar, melted butter,\\nchocolate chips, melted chocolate,\\nand mint- had to be mixed right\\ninto the cake batter, which is\\nsuper time-consuming. A friend\\nsuggested that if I put the batter in a\\nseparate bowl, I\\nGPT-2 Completion 6\\nrecipe, which will give you about\\n6-9 servings of delicious chocolate\\ncake. To prevent your layers\\nfrom sticking together, I used my\\n8-inch round tart pan, lined it with\\nparchment paper, then cut up 4\\n7-inch round cakes from it and\\nsandwiched the layers between\\nthem (this also worked well for\\nthe 1-layer chocolate cake as well).\\nWhen you’re ready to bake this\\ncake, remove the parchment paper,\\npreheat oven to 350 degrees and\\nbake for about 30 – 40 minutes,\\nuntil the top is golden and a tester\\ncomes out clean.\\nChocolate Buttermilk Layer\\nCake with Peppermint Jo\\nTable 12.Non-cherry-picked completions from GPT-2 generated from the same context (from WebText test). Context is 384 tokens\\n(shown truncated), and generations are 128 tokens. Top-k random sampling with k = 40was used for generation.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (human-written): In a shocking ﬁnding, scientist discovered a herd of unicorns living\\nin a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the\\nresearchers was the fact that the unicorns spoke perfect English.\\nGPT-2: The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These\\nfour-horned, silver-white unicorns were previously unknown to science.\\nNow, after almost two centuries, the mystery of what sparked this odd phenomenon is ﬁ-\\nnally solved.\\nDr. Jorge P ´erez, an evolutionary biologist from the University of La Paz, and several com-\\npanions, were exploring the Andes Mountains when they found a small valley, with no other animals\\nor humans. P´erez noticed that the valley had what appeared to be a natural fountain, surrounded by\\ntwo peaks of rock and silver snow.\\nP´erez and the others then ventured further into the valley. “By the time we reached the top\\nof one peak, the water looked blue, with some crystals on top,” said P´erez.\\nP´erez and his friends were astonished to see the unicorn herd. These creatures could be\\nseen from the air without having to move too much to see them – they were so close they could touch\\ntheir horns.\\nWhile examining these bizarre creatures the scientists discovered that the creatures also\\nspoke some fairly regular English. P´erez stated, “We can see, for example, that they have a common\\n‘language,’ something like a dialect or dialectic.”\\nDr. P ´erez believes that the unicorns may have originated in Argentina, where the animals\\nwere believed to be descendants of a lost race of people who lived there before the arrival of humans\\nin those parts of South America.\\nWhile their origins are still unclear, some believe that perhaps the creatures were created\\nwhen a human and a unicorn met each other in a time before human civilization. According to P ´erez,\\n“In South America, such incidents seem to be quite common.”\\nHowever, P ´erez also pointed out that it is likely that the only way of knowing for sure if\\nunicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to\\ncommunicate in English quite well, which I believe is a sign of evolution, or at least a change in\\nsocial organization,” said the scientist.\\nTable 13.Conditional generation on an out-of-distribution context by GPT-2. Cherry pick of 10 samples generated with k = 40.\\nLanguage Models are Unsupervised Multitask Learners\\nArticle: Prehistoric man sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave in\\nmodern day France 36,000 years ago.\\nNow, with the help of cutting-edge technology, those works of art in the Chauvet-Pont-d’Arc Cave have been\\nreproduced to create the biggest replica cave in the world.\\nThe manmade cavern named the Caverne du Pont-d’Arc has been built a few miles from the original site in\\nVallon-Pont-D’arc in Southern France and contains 1,000 painstakingly-reproduced drawings as well as around 450\\nbones and other features...\\nCavemen and women sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave\\n36,000 years ago and now a replica has been created (pictured)\\n...\\nGPT-2: The original site in Vallon-Pont-D’arc in South-\\nern France is a Unesco World Heritage site and is the\\noldest known and the best preserved cave decorated by\\nman. The replica cave was built a few miles from the orig-\\ninal site in Vallon-Pont-D’Arc in Southern France. The\\ncave contains images of 14 different species of animals\\nincluding woolly rhinoceros, mammoths, and big cats.\\nReference: Cave mimics famous Caverne du Pont-d’Arc\\nin France, the oldest cave decorated by man and the\\nbest preserved. The replica contains all 1,000 paint-\\nings which include 425 such as a woolly rhinoceros and\\nmammoths. Minute details were copied using 3D mod-\\nelling and anamorphic techniques, often used to shoot\\nwidescreen images. The modern cave also includes replica\\npaw prints of bears, bones and details preserved in the\\noriginal cave.\\nArticle: ‘If you can ﬁnd my submarine, it’s yours,’ Russian oil billionaire Roman Abramovich once said.\\nAnd it seems the oligarch and Chelsea FC owner, whose submarine is just one of the extras that came with his\\n£300million superyacht Eclipse (perfect for getting to shore undetected), is not the only wealthy businessman\\nsplashing out on underwater exploration.\\nDubbed Earth’s real ‘ﬁnal frontier’, the oceans are still so little-explored that billionaires are queuing up to buy\\nvessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world’s oceans.\\nSo if you have a spare few million in the bank and want some holiday snaps and Instagram posts that will really\\ntrump everyone else, you may want to snap up one of these...\\nWhale of a time: The OrcaSub takes you 2000 feet beneath the surface in two pressurised Perspex viewing domes\\nfor optimum exploration\\n...\\nGPT-2: The U-Boat Worx submarine can dive to 984 feet\\n(300 metres) and is capable of diving to the bottom of\\nthe ocean. It can be used for diving, snorkelling and even\\nsnorkel diving. The submarine is also equipped with a\\nsnorkelling platform.\\nReference: Feel like a James Bond villain with the most\\nadvanced submerged ocean vehicles. Some convert from\\na yacht into a submarine, and others detached off for a\\nspeedy exploration. Oliver’s Travels offers a Mile Low\\npackage on its submarine, complete with chef and butler\\nfor £175,000 a night.\\nArticle: Amina Ali Qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf.\\nOnly a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment\\nstarted. She lay awake, she says, in a village mosque on the Yemeni island of Birim, counting explosions as the\\nbaby cried.\\nIt could have been worse though. They could have still been in their house when the ﬁrst missile landed.\\n”Our neighbor shouted to my husband ’you have to leave, they’re coming.’ And we just ran. As soon as we left the\\nhouse, the ﬁrst missile fell right by it and then a second on it. It burned everything to the ground,” Qassim tells us\\n...\\nGPT-2: Yemen is in the middle of a civil war. Saudi\\nArabia is leading the coalition bombing campaign. It’s\\nbeen bombing Yemen for more than two months now.\\nReference: Amina Ali Qassim’s family sought shelter in\\na mosque before ﬂeeing Yemen. Thousands like them are\\nboarding boats to sail to Djibouti. Saudi Arabia has been\\npounding Yemen in a bid to defeat Houthi rebels.\\nTable 14.Summaries generated by GPT-2 and reference summaries on the test set of the CNN and Daily Mail dataset.\\nLanguage Models are Unsupervised Multitask Learners\\nEnglish reference GPT-2 French translation\\nThis re-release, titled The Next Day Extra, was presented\\nin the form of three disks: the original album, unpublished\\nstudio sessions and remixes, plus a DVD containing the\\nfour clips that have already been unveiled.\\nLes nouvelles re-releases, tout en premier disc, n´ecessaire\\nde l’album, un studio session et remixes, plus une DVD\\nde l’´ecran de quelques clips qui ont ´et´e d´ej`a ´echapp´es.\\nFrench reference GPT-2 English translation\\nUn homme expliquait que le fonctionnement de la hernia\\nfonctionnelle qu’il avait reconnaˆıt avant de faire, le fonc-\\ntionnement de la hernia fonctionnelle que j’ai r´eussi, j’ai\\nr´eussi.\\nThis release, originally released on three discs: the origi-\\nnal album, sessions studio in´edites and remixes, a DVD\\ncontaining the four clips that were released on the previous\\nday.\\nEnglish reference GPT-2 French translation\\nOne man explained that the free hernia surgery he’d re-\\nceived will allow him to work again.\\nUn homme expliquait que le fonctionnement de la hernia\\nfonctionnelle qu’il avait reconnaˆıt avant de faire, le fonc-\\ntionnement de la hernia fonctionnelle que j’ai r´eussi, j’ai\\nr´eussi.\\nFrench reference GPT-2 English translation\\nUn homme a expliqu´e que l’op´eration gratuite qu’il avait\\nsubie pour soigner une hernie lui permettrait de travailler\\n`a nouveau.\\nA man told me that the operation gratuity he had been\\npromised would not allow him to travel.\\nEnglish reference GPT-2 French translation\\nSpeaking to an open government conference in London\\nvia video link, Mr Kerry said: ”There is no question that\\nthe President and I and others in government have actually\\nlearned of some things that had been happening on an\\nautomatic pilot because the ability has been there, going\\nback to World War Two and to the very difﬁcult years of\\nthe Cold War, and then, of course, 9/11.”\\nLe premier conference de l’´edition internationale en ligne\\nsur la vide, le ministre de l’ ´Edition internationale a ´et´e\\nvide, lui dit : ”Il n’y a qu’une question que le pr´esident\\net moi et tous les autres personnes d’une administration\\nqui a ´et´e ´evident par la fois, par la part de la guerre, par la\\npart de la guer\\nFrench reference GPT-2 English translation\\nS’exprimant lors d’une conf´erence intergouvernementale\\n`a Londres par liaison vid ´eo, M. Kerry a d ´eclar´e: ”Il est\\nind´eniable que le Pr´esident, moi-mˆeme et d’autres mem-\\nbres du gouvernement avons pris connaissance de cer-\\ntaines choses en mode pilote automatique parce que nous\\nen avions la possibilit´e, d`es la Seconde guerre mondiale et\\njusqu’aux ann´ees difﬁciles de la Guerre froide, puis bien\\nsˆur le 11 septembre.”\\nIn a conf ´erence between the United States and London,\\nSecretary of State John Kerry said: ”It is indeniable that\\nthe President, myself and others of the government have\\nbeen aware of certain certain choices that have been made\\nin the past in order to be able to do certain things in a more\\nautomated way.”\\nTable 15.English to French and French to English translations generated by GPT-2.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (passage and previous question/answer pairs)\\nThe 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer\\nOlympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in\\nBeijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried\\nthe torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started\\nahead of the 1936 Summer Olympics.\\nAfter being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch trav-\\neled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was\\nfollowing a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing\\nancient links between China and the rest of the world. The relay also included an ascent with the ﬂame to the top of\\nMount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the\\nevent.\\nQ: What was the theme\\nA: “one world, one dream”.\\nQ: What was the length of the race?\\nA: 137,000 km\\nQ: Was it larger than previous ones?\\nA: No\\nQ: Where did the race begin?\\nA: Olympia, Greece\\nQ: Is there anything notable about that place?\\nA: birthplace of Olympic Games\\nQ: Where did they go after?\\nA: Athens\\nQ: How many days was the race?\\nA: seven\\nQ: Did they visit any notable landmarks?\\nA: Panathinaiko Stadium\\nQ: And did they climb any mountains?\\nA:\\nModel answer: Everest\\nTurker answers: unknown, yes, Yes, yes\\nTable 16. Selected CoQA completion.\\nLanguage Models are Unsupervised Multitask Learners\\nContext (passage and previous question/answer pairs)\\nTom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her ofﬁce at work and goes\\nshopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my\\nfourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food,\\npaying his health bills and his taxes, but in fact Tom is a dog.\\nCatherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life accord-\\ning to rules laid down by the government, which also provides a high level of care for its people. This level of care\\ncosts money.\\nPeople in Sweden pay taxes on everything, so aren’t surprised to ﬁnd that owning a dog means more\\ntaxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which\\nis spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most\\nsuch treatment is expensive, so owners often decide to offer health and even life for their dog.\\nIn Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club ofﬁcial ex-\\nplains what this means: if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay\\nfor any damage done to the car, even if your dog has been killed in the accident.\\nQ: How old is Catherine?\\nA: 54\\nQ: where does she live?\\nA:\\nModel answer: Stockholm\\nTurker answers: Sweden, Sweden, in Sweden, Sweden\\nTable 17. Selected CoQA completion.\\n'}, {'file_name': 'chexpert', 'text': 'The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)\\nCheXpert: A Large Chest Radiograph Dataset\\nwith Uncertainty Labels and Expert Comparison\\nJeremy Irvin,1,* Pranav Rajpurkar,1,* Michael Ko,1 Yifan Yu,1\\nSilviana Ciurea-Ilcus,1 Chris Chute,1 Henrik Marklund,1 Behzad Haghgoo,1\\nRobyn Ball,2 Katie Shpanskaya,3 Jayne Seekins,3 David A. Mong,3\\nSafwan S. Halabi,3 Jesse K. Sandberg,3 Ricky Jones,3 David B. Larson,3\\nCurtis P. Langlotz,3 Bhavik N. Patel,3 Matthew P. Lungren,3,† Andrew Y. Ng1,†\\n1Department of Computer Science, Stanford University\\n2Department of Medicine, Stanford University\\n3Department of Radiology, Stanford University\\n*Equal contribution\\n†Equal contribution\\n{jirvin16, pranavsr}@cs.stanford.edu\\nAbstract\\nLarge, labeled datasets have driven deep learning methods\\nto achieve expert-level performance on a variety of medical\\nimaging tasks. We present CheXpert, a large dataset that con-\\ntains 224,316 chest radiographs of 65,240 patients. We de-\\nsign a labeler to automatically detect the presence of 14 ob-\\nservations in radiology reports, capturing uncertainties inher-\\nent in radiograph interpretation. We investigate different ap-\\nproaches to using the uncertainty labels for training convolu-\\ntional neural networks that output the probability of these ob-\\nservations given the available frontal and lateral radiographs.\\nOn a validation set of 200 chest radiographic studies which\\nwere manually annotated by 3 board-certified radiologists, we\\nfind that different uncertainty approaches are useful for differ-\\nent pathologies. We then evaluate our best model on a test set\\ncomposed of 500 chest radiographic studies annotated by a\\nconsensus of 5 board-certified radiologists, and compare the\\nperformance of our model to that of 3 additional radiologists\\nin the detection of 5 selected pathologies. On Cardiomegaly,\\nEdema, and Pleural Effusion, the model ROC and PR curves\\nlie above all 3 radiologist operating points. We release the\\ndataset to the public as a standard benchmark to evaluate per-\\nformance of chest radiograph interpretation models.1\\nIntroduction\\nChest radiography is the most common imaging examina-\\ntion globally, critical for screening, diagnosis, and manage-\\nment of many life threatening diseases. Automated chest ra-\\ndiograph interpretation at the level of practicing radiologists\\ncould provide substantial benefit in many medical settings,\\nfrom improved workflow prioritization and clinical decision\\nsupport to large-scale screening and global population health\\ninitiatives. For progress, there is a need for labeled datasets\\nthat (1) are large, (2) have strong reference standards, and (3)\\nprovide expert human performance metrics for comparison.\\nCopyright c⃝ 2019, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\n1https://stanfordmlgroup.github.io/competitions/chexpert\\nLung Opacity\\nPneumonia\\nAtelectasis\\nEnlarged Cardiom. Cardiomegaly\\nConsolidation\\nSupport Devices\\nNo Finding\\nEdema\\nPneumothorax\\nPleural Other\\nPleural Effusion\\nLesion\\nModel\\n0.03 0.01\\n0.05\\n0.49\\n0.05\\n0.10\\n0.06\\n0.04\\n0.03\\n0.00\\n0.27\\n0.11\\n0.11\\nFracture0.05\\nFigure 1: The CheXpert task is to predict the probability of\\ndifferent observations from multi-view chest radiographs.\\nIn this work, we present CheXpert (Chest eXpert), a large\\ndataset for chest radiograph interpretation. The dataset con-\\nsists of 224,316 chest radiographs of 65,240 patients labeled\\nfor the presence of 14 common chest radiographic observa-\\ntions. We design a labeler that can extract observations from\\nfree-text radiology reports and capture uncertainties present\\nin the reports by using an uncertainty label.\\nThe CheXpert task is to predict the probability of 14 dif-\\nferent observations from multi-view chest radiographs (see\\nFigure 1). We pay particular attention to uncertainty labels\\nin the dataset, and investigate different approaches towards\\n590\\nPathology Positive (%) Uncertain (%) Negative (%)\\nNo Finding 16627 (8.86) 0 (0.0) 171014 (91.14)\\nEnlarged Cardiom. 9020 (4.81) 10148 (5.41) 168473 (89.78)\\nCardiomegaly 23002 (12.26) 6597 (3.52) 158042 (84.23)\\nLung Lesion 6856 (3.65) 1071 (0.57) 179714 (95.78)\\nLung Opacity 92669 (49.39) 4341 (2.31) 90631 (48.3)\\nEdema 48905 (26.06) 11571 (6.17) 127165 (67.77)\\nConsolidation 12730 (6.78) 23976 (12.78) 150935 (80.44)\\nPneumonia 4576 (2.44) 15658 (8.34) 167407 (89.22)\\nAtelectasis 29333 (15.63) 29377 (15.66) 128931 (68.71)\\nPneumothorax 17313 (9.23) 2663 (1.42) 167665 (89.35)\\nPleural Effusion 75696 (40.34) 9419 (5.02) 102526 (54.64)\\nPleural Other 2441 (1.3) 1771 (0.94) 183429 (97.76)\\nFracture 7270 (3.87) 484 (0.26) 179887 (95.87)\\nSupport Devices 105831 (56.4) 898 (0.48) 80912 (43.12)\\nTable 1: The CheXpert dataset consists of 14 labeled obser-\\nvations. We report the number of studies which contain these\\nobservations in the training set.\\nincorporating those labels into the training process. We as-\\nsess the performance of these uncertainty approaches on a\\nvalidation set of 200 labeled studies, where ground truth is\\nset by a consensus of 3 radiologists who annotated the set\\nusing the radiographs. We evaluate the approaches on 5 ob-\\nservations selected based on their clinical significance and\\nprevalence in the dataset, and find that different uncertainty\\napproaches are useful for different observations.\\nWe compare the performance of our final model to 3 addi-\\ntional board certified radiologists on a test set of 500 studies\\non which the consensus of 5 separate board-certified radi-\\nologists serves as ground truth. We find that on 4 out of 5\\npathologies, the model ROC and PR curves lie above at least\\n2 of 3 radiologist operating points. We make our dataset pub-\\nlicly available to encourage further development of models.\\nDataset\\nCheXpert is a large public dataset for chest radiograph inter-\\npretation, consisting of 224,316 chest radiographs of 65,240\\npatients labeled for the presence of 14 observations as posi-\\ntive, negative, or uncertain. We report the prevalences of the\\nlabels for the different obsevations in Table 1.\\nData Collection and Label Selection\\nWe retrospectively collected chest radiographic studies from\\nStanford Hospital, performed between October 2002 and\\nJuly 2017 in both inpatient and outpatient centers, along\\nwith their associated radiology reports. From these, we sam-\\npled a set of 1000 reports for manual review by a board-\\ncertified radiologist to determine feasibility for extraction of\\nobservations. We decided on 14 observations based on the\\nprevalence in the reports and clinical relevance, conforming\\nto the Fleischner Society’s recommended glossary (Hansell\\net al. 2008) whenever applicable. “Pneumonia”, despite be-\\ning a clinical diagnosis, was included as a label in order to\\nrepresent the images that suggested primary infection as the\\ndiagnosis. The “No Finding” observation was intended to\\ncapture the absence of all pathologies.\\n1. unremarkable cardiomediastinal silhouette\\n \\n2. diffuse reticular pattern, which can be \\nseen with an atypical infection or chronic \\nfibrotic change.  no focal consolidation.\\n \\n3. no pleural effusion or pneumothorax\\n \\n4. mild degenerative changes in the lumbar \\nspine and old right rib  fractures. \\nObservation Labeler\\nOutput\\nNo Finding \\nEnlarged Cardiom. 0\\nCardiomegaly \\nLung Opacity 1\\nLung Lesion \\nEdema \\nConsolidation 0\\nPneumonia u\\nAtelectasis \\nPneumothorax 0\\nPleural Effusion 0\\nPleural Other \\nFracture 1\\nSupport Devices \\nFigure 2: Output of the labeler when run on a report sampled\\nfrom our dataset. In this case, the labeler correctly extracts\\nall of the mentions in the report (underline) and classifies the\\nuncertainties (bolded) and negations (italicized).\\nLabel Extraction from Radiology Reports\\nWe developed an automated rule-based labeler to extract ob-\\nservations from the free text radiology reports to be used\\nas structured labels for the images. Our labeler is set up in\\nthree distinct stages: mention extraction, mention classifica-\\ntion, and mention aggregation.\\nMention Extraction The labeler extracts mentions from\\na list of observations from the Impression section of radiol-\\nogy reports, which summarizes the key findings in the ra-\\ndiographic study. A large list of phrases was manually cu-\\nrated by multiple board-certified radiologists to match vari-\\nous ways observations are mentioned in the reports.\\nMention Classification After extracting mentions of ob-\\nservations, we aim to classify them as negative (“no evi-\\ndence of pulmonary edema, pleural effusions or pneumoth-\\norax”), uncertain (“diffuse reticular pattern may represent\\nmild interstitial pulmonary edema”), or positive (“moder-\\nate bilateral effusions and bibasilar opacities”). The ‘uncer-\\ntain’ label can capture both the uncertainty of a radiologist\\nin the diagnosis as well as ambiguity inherent in the report\\n(“heart size is stable”). The mention classification stage is\\na 3-phase pipeline consisting of pre-negation uncertainty,\\nnegation, and post-negation uncertainty. Each phase consists\\nof rules which are matched against the mention; if a match is\\nfound, then the mention is classified accordingly (as uncer-\\ntain in the first or third phase, and as negative in the second\\nphase). If a mention is not matched in any of the phases, it\\nis classified as positive.\\nRules for mention classification are designed on the uni-\\nversal dependency parse of the report. To obtain the uni-\\nversal dependency parse, we follow a procedure similar to\\nPeng et al. (2018): first, the report is split and tokenized\\ninto sentences using NLTK (Bird, Klein, and Loper 2009);\\nthen, each sentence is parsed using the Bllip parser trained\\nusing David McClosky’s biomedical model (Charniak and\\nJohnson 2005; McClosky 2010); finally, the universal de-\\npendency graph of each sentence is computed using Stan-\\nford CoreNLP (De Marneffe et al. 2014).\\n591\\nMention F1 Negation F1 Uncertain F1\\nCategory NIH Ours NIH Ours NIH Ours\\nAtelectasis 0.976 0.998 0.526 0.833 0.661 0.936\\nCardiomegaly 0.647 0.973 0.000 0.909 0.211 0.727\\nConsolidation 0.996 0.999 0.879 0.981 0.438 0.924\\nEdema 0.978 0.993 0.873 0.962 0.535 0.796\\nPleural Effusion 0.985 0.996 0.951 0.971 0.553 0.707\\nPneumonia 0.660 0.992 0.703 0.750 0.250 0.817\\nPneumothorax 0.993 1.000 0.971 0.977 0.167 0.762\\nEnlarged Cardiom. N/A 0.935 N/A 0.959 N/A 0.854\\nLung Lesion N/A 0.896 N/A 0.900 N/A 0.857\\nLung Opacity N/A 0.966 N/A 0.914 N/A 0.286\\nPleural Other N/A 0.850 N/A 1.000 N/A 0.769\\nFracture N/A 0.975 N/A 0.807 N/A 0.800\\nSupport Devices N/A 0.933 N/A 0.720 N/A N/A\\nNo Finding N/A 0.769 N/A N/A N/A N/A\\nMacro-average N/A 0.948 N/A 0.899 N/A 0.770\\nMicro-average N/A 0.969 N/A 0.952 N/A 0.848\\nTable 2: Performance of the labeler of NIH and our la-\\nbeler on the report evaluation set on tasks of mention ex-\\ntraction, uncertainty detection, and negation detection, as\\nmeasured by the F1 score. The Macro-average and Micro-\\naverage rows are computed over all 14 observations.\\nMention Aggregation We use the classification for each\\nmention of observations to arrive at a final label for 14 ob-\\nservations that consist of 12 pathologies as well as the “Sup-\\nport Devices” and “No Finding” observations. Observations\\nwith at least one mention that is positively classified in the\\nreport is assigned a positive (1) label. An observation is as-\\nsigned an uncertain (u) label if it has no positively classified\\nmentions and at least one uncertain mention, and a negative\\nlabel if there is at least one negatively classified mention. We\\nassign (blank) if there is no mention of an observation. The\\n“No Finding” observation is assigned a positive label ( 1) if\\nthere is no pathology classified as positive or uncertain. An\\nexample of the labeling system run on a report is shown in\\nFigure 2.\\nLabeler Results\\nWe evaluate the performance of the labeler and compare it\\nto the performance of another automated radiology report\\nlabeler on a report evaluation set.\\nReport Evaluation Set\\nThe report evaluation set consists of 1000 radiology reports\\nfrom 1000 distinct randomly sampled patients that do not\\noverlap with the patients whose studies were used to develop\\nthe labeler. Two board-certified radiologists without access\\nto additional patient information annotated the reports to la-\\nbel whether each observation was mentioned as confidently\\npresent (1), confidently absent (0), uncertainly present (u),\\nor not mentioned (blank), after curating a list of labeling\\nconventions to adhere to. After both radiologists indepen-\\ndently labeled each of the 1000 reports, disagreements were\\nresolved by consensus discussion. The resulting annotations\\nserve as ground truth on the report evaluation set.\\nComparison to NIH labeler\\nOn the radiology report evaluation set, we compare our la-\\nbeler against the method employed in Peng et al. (2018)\\nwhich was used to annotate another large dataset of chest\\nradiographs using radiology reports (Wang et al. 2017). We\\nevaluate labeler performance on three tasks: mention extrac-\\ntion, negation detection, and uncertainty detection. For the\\nmention extraction task, we consider any assigned label ( 1,\\n0, or u) as positive and blank as negative. On the negation\\ndetection task, we consider 0 labels as positive and all other\\nlabels as negative. On the uncertainty detection task, we con-\\nsider u labels as positive and all other labels as negative. We\\nreport the F1 scores of the labeling algorithms for each of\\nthese tasks.\\nTable 2 shows the performance of the labeling methods.\\nAcross all observations and on all tasks, our labeling algo-\\nrithm achieves a higher F1 score. On negation detection, our\\nlabeling algorithm significantly outperforms the NIH labeler\\non Atelectasis and Cardiomegaly, and achieves notably bet-\\nter performance on Consolidation and Pneumonia. On un-\\ncertainty detection, our labeler shows large gains over the\\nNIH labeler, particularly on Cardiomegaly, Pneumonia, and\\nPneumothorax.\\nWe note three key differences between our method and\\nthe method of Wang et al. (2017). First, we do not the\\nuse automatic mention extractors like MetaMap or DNorm,\\nwhich we found produced weak extractions when applied\\nto our collection of reports. Second, we incorporate sev-\\neral additional rules in order to capture the large variation in\\nthe ways negation and uncertainty are conveyed. Third, we\\nsplit uncertainty classification of mentions into pre-negation\\nand post-negation, which allowed us to resolve cases of\\nuncertainty rules double matching with negation rules in\\nthe reports. For example, the following phrase “cannot ex-\\nclude pneumothorax.” conveys uncertainty in the presence\\nof pneumothorax. Without the pre-negation stage, the ‘pneu-\\nmothorax’ match is classified as negative due to the ‘exclude\\nXXX’ rule. However, by applying the ‘cannot exclude’ rule\\nin the pre-negation stage, this observation can be correctly\\nclassified as uncertain.\\nModel\\nWe train models that take as input a single-view chest ra-\\ndiograph and output the probability of each of the 14 obser-\\nvations. When more than one view is available, the models\\noutput the maximum probability of the observations across\\nthe views.\\nUncertainty Approaches\\nThe training labels in the dataset for each observation are\\neither 0 (negative), 1 (positive), or u (uncertain). We explore\\ndifferent approaches to using the uncertainty labels during\\nthe model training.\\nIgnoring A simple approach to handling uncertainty is to\\nignore the u labels during training, which serves as a base-\\nline to compare approaches which explicitly incorporate the\\nuncertainty labels. In this approach (called U-Ignore), we\\noptimize the sum of the masked binary cross-entropy losses\\n592\\nAtelectasis Cardiomegaly Consolidation Edema Pleural Effusion\\nU-Ignore 0.818 (0.759,0.877) 0.828 (0.769,0.888) 0.938 (0.905,0.970) 0.934 (0.893,0.975) 0.928 (0.894,0.962)\\nU-Zeros 0.811 (0.751,0.872) 0.840 (0.783,0.897) 0.932 (0.898,0.966) 0.929 (0.888,0.970) 0.931 (0.897,0.965)\\nU-Ones 0.858 (0.806,0.910) 0.832 (0.773,0.890) 0.899 (0.854,0.944) 0.941 (0.903,0.980) 0.934 (0.901,0.967)\\nU-SelfTrained 0.833 (0.776,0.890) 0.831 (0.770,0.891) 0.939 (0.908,0.971) 0.935 (0.896,0.974) 0.932 (0.899,0.966)\\nU-MultiClass 0.821 (0.763,0.879) 0.854 (0.800,0.909) 0.937 (0.905,0.969) 0.928 (0.887,0.968) 0.936 (0.904,0.967)\\nTable 3: AUROC scores on the validation set of the models trained using different approaches to using uncertainty labels. For\\neach of the uncertainty approaches, we choose the best 10 checkpoints per run using the average ROC across the competition\\ntasks. We run each model three times, and take the ensemble of the 30 generated checkpoints on the validation set.\\nover the observations, masking the loss for the observations\\nwhich are marked as uncertain for the study. Formally, the\\nloss for an example X is given by\\nL(X, y) =−\\n∑\\no\\n1 {yo ̸= u}[yo log p(Yo = 1|X)\\n+ (1− yo) logp(Yo = 0|X)],\\nwhere X is the input image,y is the vector of labels of length\\n14 for the study, and the sum is taken over all 14 observa-\\ntions. Ignoring the uncertainty label is analogous to the list-\\nwise (complete case) deletion method for imputation (Gra-\\nham 2009), which is when all cases with a missing value\\nare deleted. Such methods can produce biased models if the\\ncases are not missing completely at random. In this dataset,\\nuncertainty labels are quite prevalent for some observations:\\nfor Consolidation, the uncertainty label is almost twice as as\\nprevalent (12.78%) as the positive label (6.78%), and thus\\nthis approach ignores a large proportion of labels, reducing\\nthe effective size of the dataset.\\nBinary Mapping We investigate whether the uncertain la-\\nbels for any of the observations can be replaced by the 0 la-\\nbel or the 1 label. In this approach, we map all instances of\\nu to 0 (U-Zeroes model), or all to 1 (U-Ones model).\\nThese approaches are similar to zero imputation strategies\\nin statistics, and mimic approaches in multi-label classifica-\\ntion methods where missing examples are used as negative\\nlabels (Kolesov et al. 2014). If the uncertainty label does\\nconvey semantically useful information to the classifier, then\\nwe expect that this approach can distort the decision making\\nof classifiers and degrade their performance.\\nSelf-Training One framework for approaching uncer-\\ntainty labels is to consider them as unlabeled examples,\\nlending its way to semi-supervised learning (Zhu 2006).\\nMost closely tied to our setting is multi-label learning with\\nmissing labels (MLML) (Wu et al. 2015), which aims to\\nhandle multi-label classification given training instances that\\nhave a partial annotation of their labels.\\nWe investigate a self-training approach ( U-SelfTrained)\\nfor using the uncertainty label. In this approach, we first train\\na model using the U-Ignore approach (that ignores the u la-\\nbels during training) to convergence, and then use the model\\nto make predictions that re-label each of the uncertainty la-\\nbels with the probability prediction outputted by the model.\\nWe do not replace any instances of 1 or 0s. On these rela-\\nbeled examples, we set up loss as the mean of the binary\\ncross-entropy losses over the observations.\\nOur work follows the approach of (Yarowsky 1995), who\\ntrain a classifier on labeled examples and then predict on\\nunlabeled examples labeling them when the prediction is\\nabove a certain threshold, and repeating until convergence.\\n(Radosavovic et al. 2017) build upon the self-training tech-\\nnique and remove the need for iteratively training models,\\npredicting on transformed versions of the inputs instead of\\ntraining multiple models, and output a target label for each\\nunlabeled example; soft labels, which are continuous prob-\\nability outputs rather than binary, have also been used (Hin-\\nton, Vinyals, and Dean 2015; Li et al. 2017a).\\n3-Class Classification We finally investigate treating the\\nu label as its own class, rather than mapping it to a binary\\nlabel, for each of the 14 observations. We hypothesize that\\nwith this approach, we can better incorporate information\\nfrom the image by supervising uncertainty, allowing the net-\\nwork to find its own representation of uncertainty on differ-\\nent pathologies. In this approach (U-MultiClass model), for\\neach observation, we output the probability of each of the 3\\npossible classes {p0, p1, pu} ∈[0, 1], p0 + p1 + pu = 1. We\\nset up the loss as the mean of the multi-class cross-entropy\\nlosses over the observations. At test time, for the probabil-\\nity of a particular observation, we output the probability of\\nthe positive label after applying a softmax restricted to the\\npositive and negative classes.\\nTraining Procedure\\nWe follow the same architecture and training process for\\neach of the uncertainty approaches. We experimented with\\nseveral convolutional neural network architectures, specif-\\nically ResNet152, DenseNet121, Inception-v4, and SE-\\nResNeXt101, and found that the DenseNet121 architecture\\nproduced the best results. Thus we used DenseNet121 for\\nall our experiments. Images are fed into the network with\\nsize 320 × 320 pixels. We use the Adam optimizer with de-\\nfault β-parameters of β1 = 0.9, β2 = 0.999 and learning\\nrate 1 × 10−4 which is fixed for the duration of the training.\\nBatches are sampled using a fixed batch size of 16 images.\\nWe train for 3 epochs, saving checkpoints every 4800 itera-\\ntions.\\nValidation Results\\nWe compare the performance of the different uncertainty ap-\\nproaches on a validation set on which the consensus of radi-\\n593\\n0.00 0.25 0.50 0.75 1.00\\nFalse Positive Rate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0True Positive Rate\\nAtelectasis (>0 rads)\\nLabelL  (0.20,0.22)\\nLabelU  (0.10,0.51)\\nModel (AUC = 0.85)\\nRad1  (0.21,0.80)\\nRad2  (0.18,0.71)\\nRad3  (0.31,0.92)\\nRadMaj  (0.22,0.89)\\n0.00 0.25 0.50 0.75 1.00\\nFalse Positive Rate\\nCardiomegaly (>3 rads)\\nLabelL  (0.16,0.24)\\nLabelU  (0.04,0.42)\\nModel (AUC = 0.90)\\nRad1  (0.05,0.48)\\nRad2  (0.23,0.85)\\nRad3  (0.11,0.70)\\nRadMaj  (0.08,0.75)\\n0.00 0.25 0.50 0.75 1.00\\nFalse Positive Rate\\nConsolidation (>2 rads)\\nLabelL  (0.18,0.31)\\nLabelU  (0.05,0.41)\\nModel (AUC = 0.90)\\nRad1  (0.11,0.66)\\nRad2  (0.09,0.48)\\nRad3  (0.03,0.45)\\nRadMaj  (0.05,0.52)\\n0.00 0.25 0.50 0.75 1.00\\nFalse Positive Rate\\nEdema (>3 rads)\\nLabelL  (0.15,0.49)\\nLabelU  (0.12,0.65)\\nModel (AUC = 0.92)\\nRad1  (0.09,0.63)\\nRad2  (0.19,0.79)\\nRad3  (0.07,0.58)\\nRadMaj  (0.08,0.68)\\n0.00 0.25 0.50 0.75 1.00\\nFalse Positive Rate\\nPleural Effusion (>3 rads)\\nLabelL  (0.21,0.78)\\nLabelU  (0.16,0.88)\\nModel (AUC = 0.97)\\nRad1  (0.05,0.82)\\nRad2  (0.17,0.83)\\nRad3  (0.14,0.89)\\nRadMaj  (0.10,0.89)\\n0.00 0.25 0.50 0.75 1.00\\nSensitivity\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Precision\\nAtelectasis (>0 rads)\\nLabelL  (0.22,0.32)\\nLabelU  (0.51,0.70)\\nModel (AUC = 0.69)\\nRad1  (0.80,0.62)\\nRad2  (0.71,0.64)\\nRad3  (0.92,0.56)\\nRadMaj  (0.89,0.64)\\n0.00 0.25 0.50 0.75 1.00\\nSensitivity\\nCardiomegaly (>3 rads)\\nLabelL  (0.24,0.39)\\nLabelU  (0.42,0.82)\\nModel (AUC = 0.81)\\nRad1  (0.48,0.82)\\nRad2  (0.85,0.61)\\nRad3  (0.70,0.74)\\nRadMaj  (0.75,0.80)\\n0.00 0.25 0.50 0.75 1.00\\nSensitivity\\nConsolidation (>2 rads)\\nLabelL  (0.31,0.10)\\nLabelU  (0.41,0.34)\\nModel (AUC = 0.44)\\nRad1  (0.66,0.27)\\nRad2  (0.48,0.25)\\nRad3  (0.45,0.45)\\nRadMaj  (0.52,0.38)\\n0.00 0.25 0.50 0.75 1.00\\nSensitivity\\nEdema (>3 rads)\\nLabelL  (0.49,0.38)\\nLabelU  (0.65,0.50)\\nModel (AUC = 0.66)\\nRad1  (0.63,0.58)\\nRad2  (0.79,0.44)\\nRad3  (0.58,0.59)\\nRadMaj  (0.68,0.62)\\n0.00 0.25 0.50 0.75 1.00\\nSensitivity\\nPleural Effusion (>3 rads)\\nLabelL  (0.78,0.49)\\nLabelU  (0.88,0.59)\\nModel (AUC = 0.91)\\nRad1  (0.82,0.80)\\nRad2  (0.83,0.55)\\nRad3  (0.89,0.63)\\nRadMaj  (0.89,0.71)\\nFigure 3: We compare the performance of 3 radiologists to the model against the test set ground truth in both the ROC and the\\nPR space. We examine whether the radiologist operating points lie below the curves to determine if the model is superior to\\nthe radiologists. We also compute the lower (LabelL) and upper bounds (LabelU) of the performance of the labels extracted\\nautomatically from the radiology report using our labeling system against the test set ground truth.\\nologist annotations serves as ground truth.\\nValidation Set\\nThe validation set contains 200 studies from 200 patients\\nrandomly sampled from the full dataset with no patient over-\\nlap with the report evaluation set. Three board-certified radi-\\nologists individually annotated each of the studies in the val-\\nidation set, classifying each observation into one of present,\\nuncertain likely, uncertain unlikely, and absent. Their anno-\\ntations were binarized such that all present and uncertain\\nlikely cases are treated as positive and all absent and un-\\ncertain unlikely cases are treated as negative. The majority\\nvote of these binarized annotations is used to define a strong\\nground truth (Gulshan et al. 2016).\\nComparison of Uncertainty Approaches\\nProcedure We evaluate the approaches using the area un-\\nder the receiver operating characteristic curve (AUC) met-\\nric. We focus on the evaluation of 5 observations which we\\ncall the competition tasks, selected based of clinical im-\\nportance and prevalence in the validation set: (a) Atelec-\\ntasis, (b) Cardiomegaly, (c) Consolidation, (d) Edema, and\\n(e) Pleural Effusion. We report the 95% two-sided confi-\\ndence intervals of the AUC using the non-parametric method\\nby DeLong (DeLong, DeLong, and Clarke-Pearson 1988;\\nSun and Xu 2014). For each pathology, we also test whether\\nthe AUC of the best-performing approach is significantly\\ngreater than the AUC of the worst-performing approach us-\\ning the one-sided DeLong’s test for two correlated ROC\\ncurves (DeLong, DeLong, and Clarke-Pearson 1988). We\\ncontrol for multiple hypothesis testing using the Benjamini-\\nHochberg procedure (Benjamini and Hochberg 1995); an\\nadjusted p-value < 0.05 indicates statistical significance.\\nModel Selection For each of the uncertainty approaches,\\nwe choose the best 10 checkpoints per run using the average\\nAUC across the competition tasks. We run each model three\\ntimes, and take the ensemble of the 30 generated check-\\npoints on the validation set by computing the mean of the\\noutput probabilities over the 30 models.\\nResults The validation AUCs achieved by the different ap-\\nproaches to using the uncertainty labels are shown in Ta-\\nble 3. There are a few significant differences between the\\nperformance of the uncertainty approaches. On Atelecta-\\nsis, the U-Ones model (AUC=0.858) significantly outper-\\nforms ( p = 0.03) the U-Zeros model (AUC=0.811). On\\nCardiomegaly, we observe that the U-MultiClass model\\n(AUC=0.854) performs significantly better ( p < 0.01)\\nthan the U-Ignore model (AUC=0.828). On Consolidation,\\nEdema and Pleural Effusion, we do not find the best models\\nto be significantly better than the worst.\\nAnalysis We find that ignoring the uncertainty label is not\\nan effective approach to handling uncertainty in the dataset,\\nand is particularly ineffective on Cardiomegaly. Most of the\\nuncertain Cardiomegaly cases are borderline cases such as\\n“minimal cardiac enlargement”, which if ignored, would\\nlikely cause the model to perform poorly on cases which are\\ndifficult to distinguish. However, explicitly supervising the\\nmodel to distinguish between borderline and non-borderline\\ncases (as in the U-MultiClass approach) could enable the\\nmodel to better disambiguate the borderline cases. More-\\nover, assignment of the Cardiomegaly label when the heart\\nis mentioned in the impression are difficult to categorize in\\nmany cases, particularly for common mentions such as “un-\\n594\\n(a) Frontal and lateral radiographs of the chest in a patient\\nwith bilateral pleural effusions; the model localizes the ef-\\nfusions on both the frontal (top) and lateral (bottom) views,\\nwith predicted probabilities p = 0.936 and p = 0.939 on\\nthe frontal and lateral views respectively.\\n(b) Single frontal radiograph of the chest demonstrates bilateral\\nmid and lower lung interstitial predominant opacities and car-\\ndiomegaly most consistent with cardiogenic pulmonary edema.\\nThe model accurately classifies the edema by assigning a prob-\\nability of p = 0 .824 and correctly localizes the pulmonary\\nedema. Two independent radiologist readers misclassified this\\nexamination as negative or uncertain unlikely for edema.\\nFigure 4: The final model localizes findings in radiographs using Gradient-weighted Class Activation Mappings. The interpre-\\ntation of the radiographs in the subcaptions is provided by a board-certified radiologist.\\nchanged appearance of the heart” or “stable cardiac con-\\ntours” either of which could be used in both enlarged and\\nnon-enlarged cases. These cases were classified as uncer-\\ntain by the labeler, and therefore the binary assignment of 0s\\nand 1s in this setting fails to achieve optimal performance as\\nthere is insufficient information conveyed by these modifi-\\ncations.\\nIn the detection of Atelectasis, the U-Ones approach per-\\nforms the best, hinting that the uncertainty label for this ob-\\nservation is effectively utilized when treated as positive. We\\nexpect that phrases such as “possible atelectasis” or “may\\nbe atelectasis,” were meant to describe the most likely find-\\nings in the image, rather than convey uncertainty, which sup-\\nports the good performance of U-Ones on this pathology.\\nWe suspect a similar explanation for the high performance\\nof U-Ones on Edema, where uncertain phrases like “possi-\\nble mild pulmonary edema” in fact convey likely findings. In\\ncontrast, the U-Ones approach performs worst on the Con-\\nsolidation label, whereas theU-Zeros approach performs the\\nbest. We also note that Atelectasis and Consolidation are of-\\nten mentioned together in radiology reports. For example,\\nthe phrase “findings may represent atelectasis versus con-\\nsolidation” is very common. In these cases, our labeler as-\\nsigns uncertain for both observations, but we find that in the\\nground truth panel review that many of these sorts of uncer-\\ntainty cases are often instead resolved as Atelectasis-positive\\nand Consolidation-negative.\\nTest Results\\nWe compare the performance of our final model to radiol-\\nogists on a test set. We selected the final model based on\\nthe best performing ensemble on each competition task on\\nthe validation set: U-Ones for Atelectasis and Edema, U-\\nMultiClass for Cardiomegaly and Pleural Effusion, and U-\\nSelfTrained for Consolidation.\\nTest Set\\nThe test set consists of 500 studies from 500 patients ran-\\ndomly sampled from the 1000 studies in the report test\\nset. Eight board-certified radiologists individually annotated\\neach of the studies in the test set following the same proce-\\ndure and post-processing as described for the validation set.\\nThe majority vote of 5 radiologist annotations serves as a\\nstrong ground truth: 3 of these radiologists were the same as\\nthose who annotated the validation set and the other 2 were\\nrandomly sampled. The remaining 3 radiologist annotations\\nwere used to benchmark radiologist performance.\\nComparison to Radiologists\\nProcedure For each of the 3 individual radiologists and\\nfor their majority vote, we compute sensitivity (recall),\\nspecificity, and precision against the test set ground truth.\\nTo compare the model to radiologists, we plot the radiolo-\\ngist operating points with the model on both the ROC and\\nPrecision-Recall (PR) space. We examine whether the radi-\\nologist operating points lie below the curves to determine\\nif the model is superior to the radiologists. We also com-\\npute the performance of the labels extracted automatically\\n595\\nfrom the radiology report using our labeling system against\\nthe test set ground truth. We convert the uncertainty labels\\nto binary labels by computing the upper bound of the la-\\nbels performance (by assigning the uncertain labels to the\\nground truth values) and the lower bound of the labels (by\\nassigning the uncertain labels to the opposite of the ground\\ntruth values), and plot the two operating points on the curves,\\ndenoted LabelU and LabelL respectively. We also measure\\ncalibration of the model before and after applying post-\\nprocessing calibration techniques, namely isotonic regres-\\nsion (Zadrozny and Elkan 2002) and Platt scaling (Platt and\\nothers 1999), using the scaled Brier score (Steyerberg 2008).\\nResults Figure 3 illustrates these plots on all competition\\ntasks. The model achieves the best AUC on Pleural Effusion\\n(0.97), and the worst on Atelectasis (0.85). The AUC of all\\nother observations are at least 0.9. The model achieves the\\nbest AUPRC on Pleural Effusion (0.91) and the worst on\\nConsolidation (0.44). On Cardiomegaly, Edema, and Pleu-\\nral Effusion, the model achieves higher performance than\\nall 3 radiologists but not their majority vote. On Consoli-\\ndation, model performance exceeds 2 of the 3 radiologists,\\nand on Atelectasis, all 3 radiologists perform better than the\\nmodel. On all competition tasks, the lower bound of the re-\\nport labels lies below the model curves. On all tasks besides\\nAtelectasis, the upper bound of the report label lies on or\\nbelow the model operating curves. On most of the tasks, the\\nupper bound of the labeler performs comparably to the ra-\\ndiologists. The average scaled Brier score of the model be-\\nfore post-processing calibration is 0.110, after isotonic re-\\ngression is 0.107, and after platt scaling is 0.101.\\nLimitations We acknowledge two limitations to perform-\\ning this comparison. First, neither the radiologists nor the\\nmodel had access to patient history or previous examina-\\ntions, which has been shown to decrease diagnostic per-\\nformance in chest radiograph interpretation (Potchen et al.\\n1979; Berbaum, Franken, and Smith 1985). Second, no sta-\\ntistical test was performed to assess whether the difference\\nbetween the performance of the model and the radiologists\\nis statistically significant.\\nVisualization\\nWe visualize the areas of the radiograph which the model\\npredicts to be most indicative of each observation us-\\ning Gradient-weighted Class Activation Mappings (Grad-\\nCAMs) (Selvaraju et al. 2016). Grad-CAMs use the gradi-\\nent of an output class into the final convolutional layer to\\nproduce a low resolution map which highlights portions of\\nthe image which are important in the detection of the output\\nclass. Specifically, we construct the map by using the gradi-\\nent of the final linear layer as the weights and performing a\\nweighted sum of the final feature maps using those weights.\\nWe upscale the resulting map to the dimensions of the origi-\\nnal image and overlay the map on the image. Some examples\\nof the Grad-CAMs are illustrated in Figure 4.\\nExisting Chest Radiograph Datasets\\nOne of the main obstacles in the development of chest ra-\\ndiograph interpretation models has been the lack of datasets\\nwith strong radiologist-annotated groundtruth and expert\\nscores against which researchers can compare their mod-\\nels. There are few chest radiographic imaging datasets that\\nare publicly available, but none of them have test sets with\\nstrong ground truth or radiologist performances. The Indiana\\nNetwork for Patient Care hosts the OpenI dataset (Demner-\\nFushman et al. 2015) consisting of 7,470 frontal-view radio-\\ngraphs and radiology reports which have been labeled with\\nkey findings by human annotators . The National Cancer In-\\nstitute hosts the PLCO Lung dataset (Gohagan et al. 2000)\\nof chest radiographs obtained during a study on lung cancer\\nscreening . The dataset contains 185,421 full resolution im-\\nages, but due to the nature of the collection process, it is has\\na low prevalence of clinically important pathologies such as\\nPneumothorax, Consolidation, Effusion, and Cardiomegaly.\\nThe MIMIC-CXR dataset (Rubin et al. 2018) has been re-\\ncently announced but is not yet publicly available.\\nThe most commonly used benchmark for developing\\nchest radiograph interpretation models has been the ChestX-\\nray14 dataset (Wang et al. 2017). Due to the introduc-\\ntion of this large dataset, substantial progress has been\\nmade towards developing automated chest radiograph in-\\nterpretation models (Yao et al. 2017; Rajpurkar et al. 2017;\\nLi et al. 2017b; Kumar, Grewal, and Srivastava 2018; Wang\\net al. 2018; Guan et al. 2018; Yao et al. 2018). However, us-\\ning the NIH dataset as a benchmark on which to compare\\nmodels is problematic as the labels in the test set are ex-\\ntracted from reports using an automatic labeler. The CheX-\\npert dataset that we introduce features radiologist-labeled\\nvalidation and test sets which serve as strong reference stan-\\ndards, as well as expert scores to allow for robust evaluation\\nof different algorithms.\\nConclusion\\nWe present a large dataset of chest radiographs called\\nCheXpert, which features uncertainty labels and radiologist-\\nlabeled reference standard evaluation sets. We investigate a\\nfew different approaches to handling uncertainty and vali-\\ndate them on the evaluation sets. On a test set with a strong\\nground truth, we find that our best model outperforms at\\nleast 2 of the 3 radiologists in the detection of 4 clinically\\nrelevant pathologies. We hope that the dataset will help de-\\nvelopment and validation of chest radiograph interpretation\\nmodels towards improving healthcare access and delivery\\nworldwide.\\nAcknowledgements\\nWe would like to thank Luke Oakden-Rayner, Yifan Peng,\\nand Susan C. Weber for their help in this work.\\nReferences\\nBenjamini, Y ., and Hochberg, Y . 1995. Controlling the false dis-\\ncovery rate: a practical and powerful approach to multiple testing.\\nJournal of the royal statistical society. Series B (Methodological)\\n289–300.\\nBerbaum, K.; Franken, J. E.; and Smith, W. 1985. The effect of\\ncomparison films upon resident interpretation of pediatric chest ra-\\ndiographs. Investigative radiology 20:124–128.\\n596\\nBird, S.; Klein, E.; and Loper, E. 2009. Natural language process-\\ning with Python: analyzing text with the natural language toolkit .\\n” O’Reilly Media, Inc.”.\\nCharniak, E., and Johnson, M. 2005. Coarse-to-fine n-best parsing\\nand maxent discriminative reranking. In Proceedings of the 43rd\\nannual meeting on association for computational linguistics, 173–\\n180. Association for Computational Linguistics.\\nDe Marneffe, M.-C.; Dozat, T.; Silveira, N.; Haverinen, K.; Gin-\\nter, F.; Nivre, J.; and Manning, C. D. 2014. Universal stanford\\ndependencies: A cross-linguistic typology. In LREC, volume 14,\\n4585–4592.\\nDeLong, E. R.; DeLong, D. M.; and Clarke-Pearson, D. L. 1988.\\nComparing the areas under two or more correlated receiver oper-\\nating characteristic curves: a nonparametric approach. Biometrics\\n837–845.\\nDemner-Fushman, D.; Kohli, M. D.; Rosenman, M. B.; Shooshan,\\nS. E.; Rodriguez, L.; Antani, S.; Thoma, G. R.; and McDonald,\\nC. J. 2015. Preparing a collection of radiology examinations for\\ndistribution and retrieval. Journal of the American Medical Infor-\\nmatics Association 23(2):304–310.\\nGohagan, J. K.; Prorok, P. C.; Hayes, R. B.; and Kramer, B.-S.\\n2000. The prostate, lung, colorectal and ovarian (plco) cancer\\nscreening trial of the national cancer institute: history, organiza-\\ntion, and status. Controlled clinical trials 21(6):251S–272S.\\nGraham, J. W. 2009. Missing data analysis: Making it work in the\\nreal world. Annual review of psychology 60:549–576.\\nGuan, Q.; Huang, Y .; Zhong, Z.; Zheng, Z.; Zheng, L.; and Yang, Y .\\n2018. Diagnose like a radiologist: Attention guided convolutional\\nneural network for thorax disease classification. arXiv preprint\\narXiv:1801.09927.\\nGulshan, V .; Peng, L.; Coram, M.; Stumpe, M. C.; Wu, D.;\\nNarayanaswamy, A.; Venugopalan, S.; Widner, K.; Madams, T.;\\nCuadros, J.; et al. 2016. Development and validation of a deep\\nlearning algorithm for detection of diabetic retinopathy in retinal\\nfundus photographs. Jama 316(22):2402–2410.\\nHansell, D. M.; Bankier, A. A.; MacMahon, H.; McLoud, T. C.;\\nMuller, N. L.; and Remy, J. 2008. Fleischner society: glossary of\\nterms for thoracic imaging. Radiology 246(3):697–722.\\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the knowl-\\nedge in a neural network. arXiv preprint arXiv:1503.02531.\\nKolesov, A.; Kamyshenkov, D.; Litovchenko, M.; Smekalova, E.;\\nGolovizin, A.; and Zhavoronkov, A. 2014. On multilabel clas-\\nsification methods of incompletely labeled biomedical text data.\\nComputational and mathematical methods in medicine 2014.\\nKumar, P.; Grewal, M.; and Srivastava, M. M. 2018. Boosted cas-\\ncaded convnets for multilabel classification of thoracic diseases in\\nchest radiographs. In International Conference Image Analysis and\\nRecognition, 546–552. Springer.\\nLi, Y .; Yang, J.; Song, Y .; Cao, L.; Luo, J.; and Li, L.-J. 2017a.\\nLearning from noisy labels with distillation. In ICCV, 1928–1936.\\nLi, Z.; Wang, C.; Han, M.; Xue, Y .; Wei, W.; Li, L.-J.; and Li, F.-F.\\n2017b. Thoracic disease identification and localization with limited\\nsupervision. arXiv preprint arXiv:1711.06373.\\nMcClosky, D. 2010. Any domain parsing: automatic domain adap-\\ntation for natural language parsing.\\nPeng, Y .; Wang, X.; Lu, L.; Bagheri, M.; Summers, R.; and Lu,\\nZ. 2018. Negbio: a high-performance tool for negation and uncer-\\ntainty detection in radiology reports. AMIA Summits on Transla-\\ntional Science Proceedings 2017:188.\\nPlatt, J., et al. 1999. Probabilistic outputs for support vector ma-\\nchines and comparisons to regularized likelihood methods. Ad-\\nvances in large margin classifiers 10(3):61–74.\\nPotchen, E.; Gard, J.; Lazar, P.; Lahaie, P.; and Andary, M. 1979.\\nEffect of clinical history data on chest film interpretation-direction\\nor distraction. In Investigative Radiology, volume 14, 404–404.\\nLIPPINCOTT-RA VEN PUBL 227 EAST W ASHINGTON SQ,\\nPHILADELPHIA, PA 19106.\\nRadosavovic, I.; Doll´ar, P.; Girshick, R.; Gkioxari, G.; and He, K.\\n2017. Data distillation: Towards omni-supervised learning. arXiv\\npreprint arXiv:1712.04440.\\nRajpurkar, P.; Irvin, J.; Zhu, K.; Yang, B.; Mehta, H.; Duan, T.;\\nDing, D.; Bagul, A.; Langlotz, C.; Shpanskaya, K.; Lungren, M. P.;\\nand Ng, A. Y . 2017. CheXNet: Radiologist-Level Pneumonia De-\\ntection on Chest X-Rays with Deep Learning. arXiv:1711.05225\\n[cs, stat]. arXiv: 1711.05225.\\nRubin, J.; Sanghavi, D.; Zhao, C.; Lee, K.; Qadir, A.; and Xu-\\nWilson, M. 2018. Large scale automated reading of frontal and\\nlateral chest x-rays using dual convolutional neural networks.arXiv\\npreprint arXiv:1804.07839.\\nSelvaraju, R. R.; Das, A.; Vedantam, R.; Cogswell, M.; Parikh, D.;\\nand Batra, D. 2016. Grad-cam: Why did you say that? visual\\nexplanations from deep networks via gradient-based localization.\\nCoRR, abs/1610.02391 7.\\nSteyerberg, E. W. 2008.Clinical prediction models: a practical ap-\\nproach to development, validation, and updating. Springer Science\\n& Business Media.\\nSun, X., and Xu, W. 2014. Fast implementation of delong’s\\nalgorithm for comparing the areas under correlated receiver op-\\nerating characteristic curves. IEEE Signal Processing Letters\\n21(11):1389–1393.\\nWang, X.; Peng, Y .; Lu, L.; Lu, Z.; Bagheri, M.; and Summers,\\nR. M. 2017. ChestX-Ray8: Hospital-Scale Chest X-Ray Database\\nand Benchmarks on Weakly-Supervised Classification and Local-\\nization of Common Thorax Diseases. In 2017 IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR), 3462–3471.\\nHonolulu, HI: IEEE.\\nWang, X.; Peng, Y .; Lu, L.; Lu, Z.; and Summers, R. M. 2018.\\nTienet: Text-image embedding network for common thorax dis-\\nease classification and reporting in chest x-rays. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 9049–9058.\\nWu, B.; Lyu, S.; Hu, B.-G.; and Ji, Q. 2015. Multi-label learn-\\ning with missing labels for image annotation and facial action unit\\nrecognition. Pattern Recognition 48(7):2279–2289.\\nYao, L.; Poblenz, E.; Dagunts, D.; Covington, B.; Bernard, D.; and\\nLyman, K. 2017. Learning to diagnose from scratch by exploiting\\ndependencies among labels. arXiv preprint arXiv:1710.10501.\\nYao, L.; Prosky, J.; Poblenz, E.; Covington, B.; and Lyman, K.\\n2018. Weakly supervised medical diagnosis and localization from\\nmultiple resolutions. arXiv preprint arXiv:1803.07703.\\nYarowsky, D. 1995. Unsupervised word sense disambiguation\\nrivaling supervised methods. In Proceedings of the 33rd annual\\nmeeting on Association for Computational Linguistics , 189–196.\\nAssociation for Computational Linguistics.\\nZadrozny, B., and Elkan, C. 2002. Transforming classifier scores\\ninto accurate multiclass probability estimates. In Proceedings of\\nthe eighth ACM SIGKDD international conference on Knowledge\\ndiscovery and data mining, 694–699. ACM.\\nZhu, X. 2006. Semi-supervised learning literature survey. Com-\\nputer Science, University of Wisconsin-Madison 2(3):4.\\n597\\n'}, {'file_name': 'efficientnet', 'text': 'EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nMingxing Tan1 Quoc V . Le1\\nAbstract\\nConvolutional Neural Networks (ConvNets) are\\ncommonly developed at a ﬁxed resource budget,\\nand then scaled up for better accuracy if more\\nresources are available. In this paper, we sys-\\ntematically study model scaling and identify that\\ncarefully balancing network depth, width, and res-\\nolution can lead to better performance. Based\\non this observation, we propose a new scaling\\nmethod that uniformly scales all dimensions of\\ndepth/width/resolution using a simple yet highly\\neffective compound coefﬁcient. We demonstrate\\nthe effectiveness of this method on scaling up\\nMobileNets and ResNet.\\nTo go even further, we use neural architecture\\nsearch to design a new baseline network and scale\\nit up to obtain a family of models, called Efﬁcient-\\nNets, which achieve much better accuracy and efﬁ-\\nciency than previous ConvNets. In particular, our\\nEfﬁcientNet-B7 achieves state-of-the-art 84.4%\\ntop-1 / 97.1% top-5 accuracy on ImageNet, while\\nbeing 8.4x smaller and 6.1x faster on inference\\nthan the best existing ConvNet. Our EfﬁcientNets\\nalso transfer well and achieve state-of-the-art ac-\\ncuracy on CIFAR-100 (91.7%), Flowers (98.8%),\\nand 3 other transfer learning datasets, with an\\norder of magnitude fewer parameters.\\n1. Introduction\\nScaling up ConvNets is widely used to achieve better accu-\\nracy. For example, ResNet (He et al., 2016) can be scaled\\nup from ResNet-18 to ResNet-200 by using more layers;\\nRecently, GPipe (Huang et al., 2018) achieved 84.3% Ima-\\ngeNet top-1 accuracy by scaling up a baseline model four\\ntime larger. However, the process of scaling up ConvNets\\nhas never been well understood and there are currently many\\n1Google Research, Brain Team, Mountain View, CA. Corre-\\nspondence to: Mingxing Tan <tanmingxing@google.com>.\\nProceedings of the 36 th International Conference on Machine\\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\\n2019 by the author(s).\\n0 20 40 60 80 100 120 140 160 180\\nNumber of Parameters (Millions)\\n74\\n76\\n78\\n80\\n82\\n84Imagenet Top 1 Accuracy (%)\\nResNet-34\\nResNet-50\\nResNet-152\\nDenseNet-201\\nInception-v2\\nInception-ResNet-v2\\nNASNet-A\\nNASNet-A\\nResNeXt-101\\nXception\\nAmoebaNet-A AmoebaNet-C\\nSENet\\nB0\\nB3\\nB4\\nB5\\nB6\\nEfﬁcientNet-B7\\nTop1 Acc. #ParamsResNet-152 (He et al., 2016)77.8% 60MEfﬁcientNet-B1 78.8% 7.8MResNeXt-101 (Xie et al., 2017)80.9% 84MEfﬁcientNet-B3 81.1% 12MSENet (Hu et al., 2018)82.7% 146MNASNet-A (Zoph et al., 2018)82.7% 89MEfﬁcientNet-B4 82.6% 19MGPipe (Huang et al., 2018)† 84.3% 556MEfﬁcientNet-B7 84.4% 66M†Not plotted\\nFigure 1.Model Size vs. ImageNet Accuracy. All numbers are\\nfor single-crop, single-model. Our EfﬁcientNets signiﬁcantly out-\\nperform other ConvNets. In particular, EfﬁcientNet-B7 achieves\\nnew state-of-the-art 84.4% top-1 accuracy but being 8.4x smaller\\nand 6.1x faster than GPipe. EfﬁcientNet-B1 is 7.6x smaller and\\n5.7x faster than ResNet-152. Details are in Table 2 and 4.\\nways to do it. The most common way is to scale up Con-\\nvNets by their depth (He et al., 2016) or width (Zagoruyko &\\nKomodakis, 2016). Another less common, but increasingly\\npopular, method is to scale up models by image resolution\\n(Huang et al., 2018). In previous work, it is common to scale\\nonly one of the three dimensions – depth, width, and image\\nsize. Though it is possible to scale two or three dimensions\\narbitrarily, arbitrary scaling requires tedious manual tuning\\nand still often yields sub-optimal accuracy and efﬁciency.\\nIn this paper, we want to study and rethink the process\\nof scaling up ConvNets. In particular, we investigate the\\ncentral question: is there a principled method to scale up\\nConvNets that can achieve better accuracy and efﬁciency?\\nOur empirical study shows that it is critical to balance all\\ndimensions of network width/depth/resolution, and surpris-\\ningly such balance can be achieved by simply scaling each\\nof them with constant ratio. Based on this observation, we\\npropose a simple yet effective compound scaling method.\\nUnlike conventional practice that arbitrary scales these fac-\\ntors, our method uniformly scales network width, depth,\\nand resolution with a set of ﬁxed scaling coefﬁcients. For\\nexample, if we want to use 2N times more computational\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n(a) baseline (b) width scaling (c) depth scaling (d) resolution scaling (e) compound scaling\\n#channels\\nlayer_i\\nresolution HxW\\nwider\\ndeeper\\nhigher \\nresolution higher \\nresolution\\ndeeper\\nwider\\nFigure 2.Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network\\nwidth, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a ﬁxed ratio.\\nresources, then we can simply increase the network depth by\\nαN, width by βN, and image size by γN, where α,β,γ are\\nconstant coefﬁcients determined by a small grid search on\\nthe original small model. Figure 2 illustrates the difference\\nbetween our scaling method and conventional methods.\\nIntuitively, the compound scaling method makes sense be-\\ncause if the input image is bigger, then the network needs\\nmore layers to increase the receptive ﬁeld and more channels\\nto capture more ﬁne-grained patterns on the bigger image. In\\nfact, previous theoretical (Raghu et al., 2017; Lu et al., 2018)\\nand empirical results (Zagoruyko & Komodakis, 2016) both\\nshow that there exists certain relationship between network\\nwidth and depth, but to our best knowledge, we are the\\nﬁrst to empirically quantify the relationship among all three\\ndimensions of network width, depth, and resolution.\\nWe demonstrate that our scaling method work well on exist-\\ning MobileNets (Howard et al., 2017; Sandler et al., 2018)\\nand ResNet (He et al., 2016). Notably, the effectiveness of\\nmodel scaling heavily depends on the baseline network; to\\ngo even further, we use neural architecture search (Zoph &\\nLe, 2017; Tan et al., 2019) to develop a new baseline net-\\nwork, and scale it up to obtain a family of models, calledEfﬁ-\\ncientNets. Figure 1 summarizes the ImageNet performance,\\nwhere our EfﬁcientNets signiﬁcantly outperform other Con-\\nvNets. In particular, our EfﬁcientNet-B7 surpasses the best\\nexisting GPipe accuracy (Huang et al., 2018), but using\\n8.4x fewer parameters and running 6.1x faster on inference.\\nCompared to the widely used ResNet (He et al., 2016), our\\nEfﬁcientNet-B4 improves the top-1 accuracy from 76.3%\\nof ResNet-50 to 82.6% with similar FLOPS. Besides Ima-\\ngeNet, EfﬁcientNets also transfer well and achieve state-of-\\nthe-art accuracy on 5 out of 8 widely used datasets, while\\nreducing parameters by up to 21x than existing ConvNets.\\n2. Related Work\\nConvNet Accuracy: Since AlexNet (Krizhevsky et al.,\\n2012) won the 2012 ImageNet competition, ConvNets have\\nbecome increasingly more accurate by going bigger: while\\nthe 2014 ImageNet winner GoogleNet (Szegedy et al., 2015)\\nachieves 74.8% top-1 accuracy with about 6.8M parameters,\\nthe 2017 ImageNet winner SENet (Hu et al., 2018) achieves\\n82.7% top-1 accuracy with 145M parameters. Recently,\\nGPipe (Huang et al., 2018) further pushes the state-of-the-art\\nImageNet top-1 validation accuracy to 84.3% using 557M\\nparameters: it is so big that it can only be trained with a\\nspecialized pipeline parallelism library by partitioning the\\nnetwork and spreading each part to a different accelera-\\ntor. While these models are mainly designed for ImageNet,\\nrecent studies have shown better ImageNet models also per-\\nform better across a variety of transfer learning datasets\\n(Kornblith et al., 2019), and other computer vision tasks\\nsuch as object detection (He et al., 2016; Tan et al., 2019).\\nAlthough higher accuracy is critical for many applications,\\nwe have already hit the hardware memory limit, and thus\\nfurther accuracy gain needs better efﬁciency.\\nConvNet Efﬁciency: Deep ConvNets are often over-\\nparameterized. Model compression (Han et al., 2016; He\\net al., 2018; Yang et al., 2018) is a common way to re-\\nduce model size by trading accuracy for efﬁciency. As mo-\\nbile phones become ubiquitous, it is also common to hand-\\ncraft efﬁcient mobile-size ConvNets, such as SqueezeNets\\n(Iandola et al., 2016; Gholami et al., 2018), MobileNets\\n(Howard et al., 2017; Sandler et al., 2018), and ShufﬂeNets\\n(Zhang et al., 2018; Ma et al., 2018). Recently, neural archi-\\ntecture search becomes increasingly popular in designing\\nefﬁcient mobile-size ConvNets (Tan et al., 2019; Cai et al.,\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n2019), and achieves even better efﬁciency than hand-crafted\\nmobile ConvNets by extensively tuning the network width,\\ndepth, convolution kernel types and sizes. However, it is\\nunclear how to apply these techniques for larger models that\\nhave much larger design space and much more expensive\\ntuning cost. In this paper, we aim to study model efﬁciency\\nfor super large ConvNets that surpass state-of-the-art accu-\\nracy. To achieve this goal, we resort to model scaling.\\nModel Scaling: There are many ways to scale a Con-\\nvNet for different resource constraints: ResNet (He et al.,\\n2016) can be scaled down (e.g., ResNet-18) or up (e.g.,\\nResNet-200) by adjusting network depth (#layers), while\\nWideResNet (Zagoruyko & Komodakis, 2016) and Mo-\\nbileNets (Howard et al., 2017) can be scaled by network\\nwidth (#channels). It is also well-recognized that bigger\\ninput image size will help accuracy with the overhead of\\nmore FLOPS. Although prior studies (Raghu et al., 2017;\\nLin & Jegelka, 2018; Sharir & Shashua, 2018; Lu et al.,\\n2018) have shown that network deep and width are both\\nimportant for ConvNets’ expressive power, it still remains\\nan open question of how to effectively scale a ConvNet to\\nachieve better efﬁciency and accuracy. Our work systemati-\\ncally and empirically studies ConvNet scaling for all three\\ndimensions of network width, depth, and resolutions.\\n3. Compound Model Scaling\\nIn this section, we will formulate the scaling problem, study\\ndifferent approaches, and propose our new scaling method.\\n3.1. Problem Formulation\\nA ConvNet Layer i can be deﬁned as a function: Yi =\\nFi(Xi), where Fi is the operator, Yi is output tensor, Xi is\\ninput tensor, with tensor shape⟨Hi,Wi,Ci⟩1, where Hiand\\nWi are spatial dimension and Ci is the channel dimension.\\nA ConvNet Ncan be represented by a list of composed lay-\\ners: N= Fk⊙...⊙F1 ⊙F1(X1) =⨀\\nj=1...kFj(X1). In\\npractice, ConvNet layers are often partitioned into multiple\\nstages and all layers in each stage share the same architec-\\nture: for example, ResNet (He et al., 2016) has ﬁve stages,\\nand all layers in each stage has the same convolutional type\\nexcept the ﬁrst layer performs down-sampling. Therefore,\\nwe can deﬁne a ConvNet as:\\nN=\\n⨀\\ni=1...s\\nFLi\\ni\\n(\\nX⟨Hi,Wi,Ci⟩\\n)\\n(1)\\nwhere FLi\\ni denotes layer Fi is repeated Li times in stage i,\\n⟨Hi,Wi,Ci⟩denotes the shape of input tensor X of layer\\ni. Figure 2(a) illustrate a representative ConvNet, where\\nthe spatial dimension is gradually shrunk but the channel\\n1For the sake of simplicity, we omit batch dimension.\\ndimension is expanded over layers, for example, from initial\\ninput shape ⟨224,224,3⟩to ﬁnal output shape ⟨7,7,512⟩.\\nUnlike regular ConvNet designs that mostly focus on ﬁnd-\\ning the best layer architecture Fi, model scaling tries to ex-\\npand the network length (Li), width (Ci), and/or resolution\\n(Hi,Wi) without changing Fi predeﬁned in the baseline\\nnetwork. By ﬁxing Fi, model scaling simpliﬁes the design\\nproblem for new resource constraints, but it still remains\\na large design space to explore different Li,Ci,Hi,Wi for\\neach layer. In order to further reduce the design space, we\\nrestrict that all layers must be scaled uniformly with con-\\nstant ratio. Our target is to maximize the model accuracy\\nfor any given resource constraints, which can be formulated\\nas an optimization problem:\\nmax\\nd,w,r\\nAccuracy\\n(\\nN(d,w,r )\\n)\\ns.t. N(d,w,r ) =\\n⨀\\ni=1...s\\nˆFd·ˆLi\\ni\\n(\\nX⟨r·ˆHi,r·ˆWi,w·ˆCi⟩\\n)\\nMemory(N) ≤target memory\\nFLOPS(N) ≤target ﬂops\\n(2)\\nwhere w,d,r are coefﬁcients for scaling network width,\\ndepth, and resolution; ˆFi,ˆLi, ˆHi, ˆWi, ˆCi are predeﬁned pa-\\nrameters in baseline network (see Table 1 as an example).\\n3.2. Scaling Dimensions\\nThe main difﬁculty of problem 2 is that the optimal d,w,r\\ndepend on each other and the values change under different\\nresource constraints. Due to this difﬁculty, conventional\\nmethods mostly scale ConvNets in one of these dimensions:\\nDepth (ddd): Scaling network depth is the most common way\\nused by many ConvNets (He et al., 2016; Huang et al., 2017;\\nSzegedy et al., 2015; 2016). The intuition is that deeper\\nConvNet can capture richer and more complex features, and\\ngeneralize well on new tasks. However, deeper networks\\nare also more difﬁcult to train due to the vanishing gradient\\nproblem (Zagoruyko & Komodakis, 2016). Although sev-\\neral techniques, such as skip connections (He et al., 2016)\\nand batch normalization (Ioffe & Szegedy, 2015), alleviate\\nthe training problem, the accuracy gain of very deep network\\ndiminishes: for example, ResNet-1000 has similar accuracy\\nas ResNet-101 even though it has much more layers. Figure\\n3 (middle) shows our empirical study on scaling a baseline\\nmodel with different depth coefﬁcient d, further suggesting\\nthe diminishing accuracy return for very deep ConvNets.\\nWidth (www): Scaling network width is commonly used for\\nsmall size models (Howard et al., 2017; Sandler et al., 2018;\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n0 2 4 6 8\\nFLOPS (Billions)\\n75\\n76\\n77\\n78\\n79\\n80\\n81ImageNet Top-1 Accuracy(%)\\nw=1.0\\nw=1.4\\nw=1.8\\nw=2.6\\nw=3.8\\nw=5.0\\n0 1 2 3 4\\nFLOPS (Billions)\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\nd=1.0\\nd=2.0\\nd=3.0d=4.0\\nd=6.0 d=8.0\\n0 1 2 3\\nFLOPS (Billions)\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\nr=1.0\\nr=1.3\\nr=1.5\\nr=1.7\\nr=1.9\\nr=2.2 r=2.5\\nFigure 3.Scaling Up a Baseline Model with Different Network Width ( w), Depth ( d), and Resolution ( r) Coefﬁcients. Bigger\\nnetworks with larger width, depth, or resolution tend to achieve higher accuracy, but the accuracy gain quickly saturate after reaching\\n80%, demonstrating the limitation of single dimension scaling. Baseline network is described in Table 1.\\nTan et al., 2019) 2. As discussed in (Zagoruyko & Ko-\\nmodakis, 2016), wider networks tend to be able to capture\\nmore ﬁne-grained features and are easier to train. However,\\nextremely wide but shallow networks tend to have difﬁcul-\\nties in capturing higher level features. Our empirical results\\nin Figure 3 (left) show that the accuracy quickly saturates\\nwhen networks become much wider with larger w.\\nResolution (rrr): With higher resolution input images, Con-\\nvNets can potentially capture more ﬁne-grained patterns.\\nStarting from 224x224 in early ConvNets, modern Con-\\nvNets tend to use 299x299 (Szegedy et al., 2016) or 331x331\\n(Zoph et al., 2018) for better accuracy. Recently, GPipe\\n(Huang et al., 2018) achieves state-of-the-art ImageNet ac-\\ncuracy with 480x480 resolution. Higher resolutions, such as\\n600x600, are also widely used in object detection ConvNets\\n(He et al., 2017; Lin et al., 2017). Figure 3 (right) shows the\\nresults of scaling network resolutions, where indeed higher\\nresolutions improve accuracy, but the accuracy gain dimin-\\nishes for very high resolutions (r= 1.0 denotes resolution\\n224x224 and r= 2.5 denotes resolution 560x560).\\nThe above analyses lead us to the ﬁrst observation:\\nObservation 1 – Scaling up any dimension of network\\nwidth, depth, or resolution improves accuracy, but the accu-\\nracy gain diminishes for bigger models.\\n3.3. Compound Scaling\\nWe empirically observe that different scaling dimensions are\\nnot independent. Intuitively, for higher resolution images,\\nwe should increase network depth, such that the larger re-\\nceptive ﬁelds can help capture similar features that include\\nmore pixels in bigger images. Correspondingly, we should\\nalso increase network depth when resolution is higher, in\\n2In some literature, scaling number of channels is called “depth\\nmultiplier”, which means the same as our width coefﬁcient w.\\n0 5 10 15 20 25\\nFLOPS (billions)\\n76\\n77\\n78\\n79\\n80\\n81\\n82ImageNet Top1 Accuracy (%)\\nd=1.0, r=1.0\\nd=1.0, r=1.3\\nd=2.0, r=1.0\\nd=2.0, r=1.3\\nFigure 4.Scaling Network Width for Different Baseline Net-\\nworks. Each dot in a line denotes a model with different width\\ncoefﬁcient (w). All baseline networks are from Table 1. The ﬁrst\\nbaseline network (d=1.0, r=1.0) has 18 convolutional layers with\\nresolution 224x224, while the last baseline (d=2.0, r=1.3) has 36\\nlayers with resolution 299x299.\\norder to capture more ﬁne-grained patterns with more pixels\\nin high resolution images. These intuitions suggest that we\\nneed to coordinate and balance different scaling dimensions\\nrather than conventional single-dimension scaling.\\nTo validate our intuitions, we compare width scaling under\\ndifferent network depths and resolutions, as shown in Figure\\n4. If we only scale network width w without changing\\ndepth (d=1.0) and resolution (r=1.0), the accuracy saturates\\nquickly. With deeper (d=2.0) and higher resolution (r=2.0),\\nwidth scaling achieves much better accuracy under the same\\nFLOPS cost. These results lead us to the second observation:\\nObservation 2 – In order to pursue better accuracy and\\nefﬁciency, it is critical to balance all dimensions of network\\nwidth, depth, and resolution during ConvNet scaling.\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nIn fact, a few prior work (Zoph et al., 2018; Real et al., 2019)\\nhave already tried to arbitrarily balance network width and\\ndepth, but they all require tedious manual tuning.\\nIn this paper, we propose a newcompound scaling method,\\nwhich use a compound coefﬁcient φ to uniformly scales\\nnetwork width, depth, and resolution in a principled way:\\ndepth: d= αφ\\nwidth: w= βφ\\nresolution: r= γφ\\ns.t. α·β2 ·γ2 ≈2\\nα≥1,β ≥1,γ ≥1\\n(3)\\nwhere α,β,γ are constants that can be determined by a\\nsmall grid search. Intuitively, φis a user-speciﬁed coefﬁ-\\ncient that controls how many more resources are available\\nfor model scaling, while α,β,γ specify how to assign these\\nextra resources to network width, depth, and resolution re-\\nspectively. Notably, the FLOPS of a regular convolution op\\nis proportional to d, w2, r2, i.e., doubling network depth\\nwill double FLOPS, but doubling network width or resolu-\\ntion will increase FLOPS by four times. Since convolution\\nops usually dominate the computation cost in ConvNets,\\nscaling a ConvNet with equation 3 will approximately in-\\ncrease total FLOPS by\\n(\\nα·β2 ·γ2)φ\\n. In this paper, we\\nconstraint α·β2 ·γ2 ≈2 such that for any new φ, the total\\nFLOPS will approximately3 increase by 2φ.\\n4. EfﬁcientNet Architecture\\nSince model scaling does not change layer operators ˆFi\\nin baseline network, having a good baseline network is\\nalso critical. We will evaluate our scaling method using\\nexisting ConvNets, but in order to better demonstrate the\\neffectiveness of our scaling method, we have also developed\\na new mobile-size baseline, called EfﬁcientNet.\\nInspired by (Tan et al., 2019), we develop our baseline net-\\nwork by leveraging a multi-objective neural architecture\\nsearch that optimizes both accuracy and FLOPS. Speciﬁ-\\ncally, we use the same search space as (Tan et al., 2019),\\nand use ACC(m)×[FLOPS (m)/T]w as the optimization\\ngoal, where ACC(m) and FLOPS (m) denote the accu-\\nracy and FLOPS of model m, T is the target FLOPS and\\nw=-0.07 is a hyperparameter for controlling the trade-off\\nbetween accuracy and FLOPS. Unlike (Tan et al., 2019;\\nCai et al., 2019), here we optimize FLOPS rather than la-\\ntency since we are not targeting any speciﬁc hardware de-\\nvice. Our search produces an efﬁcient network, which we\\nname EfﬁcientNet-B0. Since we use the same search space\\nas (Tan et al., 2019), the architecture is similar to Mnas-\\n3FLOPS may differ from theocratic value due to rounding.\\nTable 1.EfﬁcientNet-B0 baseline network – Each row describes\\na stage iwith ˆLi layers, with input resolution ⟨ ˆHi, ˆWi⟩ and output\\nchannels ˆCi. Notations are adopted from equation 2.\\nStage Operator Resolution#Channels#Layers\\ni ˆFi ˆHi×ˆWi ˆCi ˆLi\\n1 Conv3x3 224×224 32 1\\n2 MBConv1, k3x3 112×112 16 1\\n3 MBConv6, k3x3 112×112 24 2\\n4 MBConv6, k5x5 56×56 40 2\\n5 MBConv6, k3x3 28×28 80 3\\n6 MBConv6, k5x5 28×28 112 3\\n7 MBConv6, k5x5 14×14 192 4\\n8 MBConv6, k3x3 7×7 320 1\\n9 Conv1x1 & Pooling & FC7×7 1280 1\\nNet, except our EfﬁcientNet-B0 is slightly bigger due to\\nthe larger FLOPS target (our FLOPS target is 400M). Ta-\\nble 1 shows the architecture of EfﬁcientNet-B0. Its main\\nbuilding block is mobile inverted bottleneck MBConv (San-\\ndler et al., 2018; Tan et al., 2019), to which we also add\\nsqueeze-and-excitation optimization (Hu et al., 2018).\\nStarting from the baseline EfﬁcientNet-B0, we apply our\\ncompound scaling method to scale it up with two steps:\\n• STEP 1: we ﬁrst ﬁx φ= 1, assuming twice more re-\\nsources available, and do a small grid search of α,β,γ\\nbased on Equation 2 and 3. In particular, we ﬁnd\\nthe best values for EfﬁcientNet-B0 are α= 1.2,β =\\n1.1,γ = 1.15, under constraint of α·β2 ·γ2 ≈2.\\n• STEP 2: we then ﬁx α,β,γ as constants and scale up\\nbaseline network with different φusing Equation 3, to\\nobtain EfﬁcientNet-B1 to B7 (Details in Table 2).\\nNotably, it is possible to achieve even better performance by\\nsearching for α,β,γ directly around a large model, but the\\nsearch cost becomes prohibitively more expensive on larger\\nmodels. Our method solves this issue by only doing search\\nonce on the small baseline network (step 1), and then use\\nthe same scaling coefﬁcients for all other models (step 2).\\n5. Experiments\\nIn this section, we will ﬁrst evaluate our scaling method on\\nexisting ConvNets and the new proposed EfﬁcientNets.\\n5.1. Scaling Up MobileNets and ResNets\\nAs a proof of concept, we ﬁrst apply our scaling method\\nto the widely-used MobileNets (Howard et al., 2017; San-\\ndler et al., 2018) and ResNet (He et al., 2016). Table 3\\nshows the ImageNet results of scaling them in different\\nways. Compared to other single-dimension scaling methods,\\nour compound scaling method improves the accuracy on all\\nthese models, suggesting the effectiveness of our proposed\\nscaling method for general existing ConvNets.\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nTable 2. EfﬁcientNet Performance Results on ImageNet (Russakovsky et al., 2015). All EfﬁcientNet models are scaled from our\\nbaseline EfﬁcientNet-B0 using different compound coefﬁcient φin Equation 3. ConvNets with similar top-1/top-5 accuracy are grouped\\ntogether for efﬁciency comparison. Our scaled EfﬁcientNet models consistently reduce parameters and FLOPS by an order of magnitude\\n(up to 8.4x parameter reduction and up to 16x FLOPS reduction) than existing ConvNets.\\nModel Top-1 Acc. Top-5 Acc. #Params Ratio-to-EfﬁcientNet#FLOPS Ratio-to-EfﬁcientNet\\nEfﬁcientNet-B0 76.3% 93.2% 5.3M 1x 0.39B 1x\\nResNet-50 (He et al., 2016) 76.0% 93.0% 26M 4.9x 4.1B 11x\\nDenseNet-169 (Huang et al., 2017) 76.2% 93.2% 14M 2.6x 3.5B 8.9x\\nEfﬁcientNet-B1 78.8% 94.4% 7.8M 1x 0.70B 1x\\nResNet-152 (He et al., 2016) 77.8% 93.8% 60M 7.6x 11B 16x\\nDenseNet-264 (Huang et al., 2017) 77.9% 93.9% 34M 4.3x 6.0B 8.6x\\nInception-v3 (Szegedy et al., 2016) 78.8% 94.4% 24M 3.0x 5.7B 8.1x\\nXception (Chollet, 2017) 79.0% 94.5% 23M 3.0x 8.4B 12x\\nEfﬁcientNet-B2 79.8% 94.9% 9.2M 1x 1.0B 1x\\nInception-v4 (Szegedy et al., 2017) 80.0% 95.0% 48M 5.2x 13B 13x\\nInception-resnet-v2 (Szegedy et al., 2017)80.1% 95.1% 56M 6.1x 13B 13x\\nEfﬁcientNet-B3 81.1% 95.5% 12M 1x 1.8B 1x\\nResNeXt-101 (Xie et al., 2017) 80.9% 95.6% 84M 7.0x 32B 18x\\nPolyNet (Zhang et al., 2017) 81.3% 95.8% 92M 7.7x 35B 19x\\nEfﬁcientNet-B4 82.6% 96.3% 19M 1x 4.2B 1x\\nSENet (Hu et al., 2018) 82.7% 96.2% 146M 7.7x 42B 10x\\nNASNet-A (Zoph et al., 2018) 82.7% 96.2% 89M 4.7x 24B 5.7x\\nAmoebaNet-A (Real et al., 2019) 82.8% 96.1% 87M 4.6x 23B 5.5x\\nPNASNet (Liu et al., 2018) 82.9% 96.2% 86M 4.5x 23B 6.0x\\nEfﬁcientNet-B5 83.3% 96.7% 30M 1x 9.9B 1x\\nAmoebaNet-C (Cubuk et al., 2019) 83.5% 96.5% 155M 5.2x 41B 4.1x\\nEfﬁcientNet-B6 84.0% 96.9% 43M 1x 19B 1x\\nEfﬁcientNet-B7 84.4% 97.1% 66M 1x 37B 1x\\nGPipe (Huang et al., 2018) 84.3% 97.0% 557M 8.4x - -\\nWe omit ensemble and multi-crop models (Hu et al., 2018), or models pretrained on 3.5B Instagram images (Mahajan et al., 2018).\\nTable 3. Scaling Up MobileNets and ResNet.\\nModel FLOPS Top-1 Acc.\\nBaseline MobileNetV1 (Howard et al., 2017)0.6B 70.6%\\nScale MobileNetV1 by width (w=2) 2.2B 74.2%\\nScale MobileNetV1 by resolution (r=2) 2.2B 72.7%\\ncompound scale (ddd=1.4,www=1.2,rrr=1.3) 2.3B 75.6%\\nBaseline MobileNetV2 (Sandler et al., 2018)0.3B 72.0%\\nScale MobileNetV2 by depth (d=4) 1.2B 76.8%\\nScale MobileNetV2 by width (w=2) 1.1B 76.4%\\nScale MobileNetV2 by resolution (r=2) 1.2B 74.8%\\nMobileNetV2 compound scale 1.3B 77.4%\\nBaseline ResNet-50 (He et al., 2016) 4.1B 76.0%\\nScale ResNet-50 by depth (d=4) 16.2B 78.1%\\nScale ResNet-50 by width (w=2) 14.7B 77.7%\\nScale ResNet-50 by resolution (r=2) 16.4B 77.5%\\nResNet-50 compound scale 16.7B 78.8%\\nTable 4. Inference Latency Comparison – Latency is measured\\nwith batch size 1 on a single core of Intel Xeon CPU E5-2690.\\nAcc. @ Latency Acc. @ Latency\\nResNet-152 77.8% @ 0.554s GPipe 84.3% @ 19.0s\\nEfﬁcientNet-B1 78.8% @ 0.098sEfﬁcientNet-B7 84.4% @ 3.1s\\nSpeedup 5.7x Speedup 6.1x\\n0 5 10 15 20 25 30 35 40 45\\nFLOPS (Billions)\\n74\\n76\\n78\\n80\\n82\\n84Imagenet Top 1 Accuracy (%)\\nResNet-34\\nResNet-50\\nResNet-152\\nDenseNet-201\\nInception-v2\\nInception-ResNet-v2\\nNASNet-A\\nNASNet-A\\nResNeXt-101\\nXception\\nAmeobaNet-A\\nAmoebaNet-C\\nSENet\\nB0\\nB3\\nB4\\nB5\\nEfﬁcientNet-B6\\nTop1 Acc. FLOPSResNet-152 (Xie et al., 2017)77.8% 11BEfﬁcientNet-B1 78.8% 0.7BResNeXt-101 (Xie et al., 2017)80.9% 32BEfﬁcientNet-B3 81.1% 1.8BSENet (Hu et al., 2018)82.7% 42BNASNet-A (Zoph et al., 2018)80.7% 24BEfﬁcientNet-B4 82.6% 4.2BAmeobaNet-C (Cubuk et al., 2019)83.5% 41BEfﬁcientNet-B5 83.3% 9.9B\\nFigure 5.FLOPS vs. ImageNet Accuracy.\\n5.2. ImageNet Results for EfﬁcientNet\\nWe train our EfﬁcientNet models on ImageNet using simi-\\nlar settings as (Tan et al., 2019): RMSProp optimizer with\\ndecay 0.9 and momentum 0.9; batch norm momentum 0.99;\\nweight decay 1e-5; initial learning rate 0.256 that decays\\nby 0.97 every 2.4 epochs. We also use swish activation\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nTable 5. EfﬁcientNet Performance Results on Transfer Learning Datasets. Our scaled EfﬁcientNet models achieve new state-of-the-\\nart accuracy for 5 out of 8 datasets, with 9.6x fewer parameters on average.\\nComparison to best public-available results Comparison to best reported results\\nModel Acc. #Param Our Model Acc. #Param(ratio) Model Acc. #Param Our Model Acc. #Param(ratio)\\nCIFAR-10 NASNet-A 98.0% 85M EfﬁcientNet-B0 98.1% 4M (21x)†Gpipe99.0% 556M EfﬁcientNet-B7 98.9% 64M (8.7x)\\nCIFAR-100 NASNet-A 87.5% 85M EfﬁcientNet-B0 88.1% 4M (21x)Gpipe 91.3% 556M EfﬁcientNet-B791.7% 64M (8.7x)\\nBirdsnap Inception-v4 81.8% 41M EfﬁcientNet-B5 82.0% 28M (1.5x)GPipe 83.6% 556M EfﬁcientNet-B784.3% 64M (8.7x)\\nStanford CarsInception-v4 93.4% 41M EfﬁcientNet-B3 93.6% 10M (4.1x)‡DAT 94.8% - EfﬁcientNet-B7 94.7% -\\nFlowers Inception-v4 98.5% 41M EfﬁcientNet-B5 98.5% 28M (1.5x)DAT 97.7% - EfﬁcientNet-B7 98.8% -\\nFGVC AircraftInception-v4 90.9% 41M EfﬁcientNet-B3 90.7% 10M (4.1x)DAT 92.9% - EfﬁcientNet-B7 92.9% -\\nOxford-IIIT PetsResNet-152 94.5% 58M EfﬁcientNet-B4 94.8% 17M (5.6x)GPipe 95.9% 556M EfﬁcientNet-B6 95.4% 41M (14x)\\nFood-101 Inception-v4 90.8% 41M EfﬁcientNet-B4 91.5% 17M (2.4x)GPipe 93.0% 556M EfﬁcientNet-B793.0% 64M (8.7x)\\nGeo-Mean (4.7x) (9.6x)\\n†GPipe (Huang et al., 2018) trains giant models with specialized pipeline parallelism library.\\n‡DAT denotes domain adaptive transfer learning (Ngiam et al., 2018). Here we only compare ImageNet-based transfer learning results.\\nTransfer accuracy and #params for NASNet (Zoph et al., 2018), Inception-v4 (Szegedy et al., 2017), ResNet-152 (He et al., 2016) are from (Kornblith et al., 2019).\\n0.0 0.2 0.4 0.6 0.8 1.0\\nNumber of Parameters (Millions, log-scale)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n101 102 103\\n97\\n98\\n99Accuracy(%)\\nCIFAR10\\n101 102 103\\n84\\n86\\n88\\n90\\n92\\nCIFAR100\\n101 102 103\\n70\\n75\\n80\\n85\\nBirdsnap\\n101 102 103\\n91\\n92\\n93\\n94\\nStanford Cars\\n101 102 103\\n97.0\\n97.5\\n98.0\\n98.5\\nAccuracy(%)\\nFlowers\\n101 102 103\\n82.5\\n85.0\\n87.5\\n90.0\\n92.5\\nFGVC Aircraft\\n101 102 103\\n92\\n94\\n96\\nOxford-IIIT Pets\\n101 102 103\\n86\\n88\\n90\\n92\\nFood-101\\nInception-v1\\nInception-v3\\nInception-v4\\nInception-ResNet-v2\\nResNet-50\\nResNet-101\\nResNet-152\\nDenseNet-169\\nDenseNet-201\\nNASNet-A\\nGPIPE\\nEfﬁcientNet\\nFigure 6.Model Parameters vs. Transfer Learning Accuracy – All models are pretrained on ImageNet and ﬁnetuned on new datasets.\\n(Ramachandran et al., 2018; Elfwing et al., 2018), ﬁxed Au-\\ntoAugment policy (Cubuk et al., 2019), and stochastic depth\\n(Huang et al., 2016) with drop connect ratio 0.3. As com-\\nmonly known that bigger models need more regularization,\\nwe linearly increase dropout (Srivastava et al., 2014) ratio\\nfrom 0.2 for EfﬁcientNet-B0 to 0.5 for EfﬁcientNet-B7.\\nTable 2 shows the performance of all EfﬁcientNet models\\nthat are scaled from the same baseline EfﬁcientNet-B0. Our\\nEfﬁcientNet models generally use an order of magnitude\\nfewer parameters and FLOPS than other ConvNets with\\nsimilar accuracy. In particular, our EfﬁcientNet-B7 achieves\\n84.4% top1 / 97.1% top-5 accuracy with 66M parameters\\nand 37B FLOPS, being more accurate but8.4x smaller than\\nthe previous best GPipe (Huang et al., 2018).\\nFigure 1 and Figure 5 illustrates the parameters-accuracy\\nand FLOPS-accuracy curve for representative ConvNets,\\nwhere our scaled EfﬁcientNet models achieve better accu-\\nracy with much fewer parameters and FLOPS than other\\nConvNets. Notably, our EfﬁcientNet models are not only\\nsmall, but also computational cheaper. For example, our\\nEfﬁcientNet-B3 achieves higher accuracy than ResNeXt-\\n101 (Xie et al., 2017) using 18x fewer FLOPS.\\nTo validate the computational cost, we have also mea-\\nsured the inference latency on a real CPU as shown in\\nTable 4, where we report average latency over 20 runs.\\nOur EfﬁcientNet-B1 runs 5.7x faster than the widely used\\nResNet-152 (He et al., 2016), while EfﬁcientNet-B7 runs\\nabout 6.1x faster than GPipe (Huang et al., 2018), suggest-\\ning our EfﬁcientNets are indeed fast on real hardware.\\n5.3. Transfer Learning Results for EfﬁcientNet\\nWe have also evaluated our EfﬁcientNet on a list of com-\\nmonly used transfer learning datasets, as shown in Table\\n6. We borrow the same training settings from (Kornblith\\net al., 2019) and (Huang et al., 2018), which take ImageNet\\npretrained checkpoints and ﬁnetune on new datasets.\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nbakeshop\\noriginal image\\n baseline model\\n deeper (d=4)\\n wider (w=2)\\n higher resolution (r=2)\\n compound scaling\\nmaze\\nFigure 7.Class Activation Map (CAM) (Zhou et al., 2016) for Models with different scaling methods – Our compound scaling\\nmethod allows the scaled model (last column) to focus on more relevant regions with more object details.\\nTable 6. Transfer Learning Datasets.\\nDataset Train Size Test Size #Classes\\nCIFAR-10 (Krizhevsky & Hinton, 2009)50,000 10,000 10\\nCIFAR-100 (Krizhevsky & Hinton, 2009)50,000 10,000 100\\nBirdsnap (Berg et al., 2014)47,386 2,443 500\\nStanford Cars (Krause et al., 2013)8,144 8,041 196\\nFlowers (Nilsback & Zisserman, 2008)2,040 6,149 102\\nFGVC Aircraft (Maji et al., 2013)6,667 3,333 100\\nOxford-IIIT Pets (Parkhi et al., 2012)3,680 3,369 37\\nFood-101 (Bossard et al., 2014)75,750 25,250 101\\nTable 5 shows the transfer learning performance: (1) Com-\\npared to public available models, such as NASNet-A (Zoph\\net al., 2018) and Inception-v4 (Szegedy et al., 2017), our Ef-\\nﬁcientNet models achieve better accuracy with 4.7x average\\n(up to 21x) parameter reduction. (2) Compared to state-\\nof-the-art models, including DAT (Ngiam et al., 2018) that\\ndynamically synthesizes training data and GPipe (Huang\\net al., 2018) that is trained with specialized pipeline paral-\\nlelism, our EfﬁcientNet models still surpass their accuracy\\nin 5 out of 8 datasets, but using 9.6x fewer parameters\\nFigure 6 compares the accuracy-parameters curve for a va-\\nriety of models. In general, our EfﬁcientNets consistently\\nachieve better accuracy with an order of magnitude fewer pa-\\nrameters than existing models, including ResNet (He et al.,\\n2016), DenseNet (Huang et al., 2017), Inception (Szegedy\\net al., 2017), and NASNet (Zoph et al., 2018).\\n6. Discussion\\nTo disentangle the contribution of our proposed scaling\\nmethod from the EfﬁcientNet architecture, Figure 8 com-\\npares the ImageNet performance of different scaling meth-\\nods for the same EfﬁcientNet-B0 baseline network. In gen-\\neral, all scaling methods improve accuracy with the cost\\nof more FLOPS, but our compound scaling method can\\nfurther improve accuracy, by up to 2.5%, than other single-\\ndimension scaling methods, suggesting the importance of\\nour proposed compound scaling.\\n0 1 2 3 4 5\\nFLOPS (Billions)\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83ImageNet Top-1 Accuracy(%)\\nscale by width\\nscale by depth\\nscale by resolution\\ncompound scaling\\nFigure 8.Scaling Up EfﬁcientNet-B0 with Different Methods.\\nIn order to further understand why our compound scaling\\nmethod is better than others, Figure 7 compares the class ac-\\ntivation map for a few representative models with different\\nscaling methods. All these models are scaled from the same\\nEfﬁcientNet-B0 baseline with about 4x more FLOPS than\\nthe baseline. Images are randomly picked from ImageNet\\nvalidation set. As shown in the ﬁgure, the model with com-\\npound scaling tends to focus on more relevant regions with\\nmore object details, while other models are either lack of\\nobject details or unable to capture all objects in the images.\\n7. Conclusion\\nIn this paper, we systematically study ConvNet scaling and\\nidentify that carefully balancing network width, depth, and\\nresolution is an important but missing piece, preventing us\\nfrom better accuracy and efﬁciency. To address this issue,\\nwe propose a simple and highly effective compound scaling\\nmethod, which enables us to easily scale up a baseline Con-\\nvNet to any target resource constraints in a more principled\\nway, while maintaining model efﬁciency. Powered by this\\ncompound scaling method, we demonstrate that a mobile-\\nsize EfﬁcientNet model can be scaled up very effectively,\\nsurpassing state-of-the-art accuracy with an order of magni-\\ntude fewer parameters and FLOPS, on both ImageNet and\\nﬁve commonly used transfer learning datasets.\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nAcknowledgements\\nWe thank Ruoming Pang, Vijay Vasudevan, Alok Aggarwal,\\nBarret Zoph, Hongkun Yu, Xiaodan Song, Samy Bengio,\\nJeff Dean, and Google Brain team for their help.\\nReferences\\nBerg, T., Liu, J., Woo Lee, S., Alexander, M. L., Jacobs,\\nD. W., and Belhumeur, P. N. Birdsnap: Large-scale\\nﬁne-grained visual categorization of birds. CVPR, pp.\\n2011–2018, 2014.\\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101–\\nmining discriminative components with random forests.\\nECCV, pp. 446–461, 2014.\\nCai, H., Zhu, L., and Han, S. Proxylessnas: Direct neural\\narchitecture search on target task and hardware. ICLR,\\n2019.\\nChollet, F. Xception: Deep learning with depthwise separa-\\nble convolutions. CVPR, pp. 1610–02357, 2017.\\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V ., and Le,\\nQ. V . Autoaugment: Learning augmentation policies\\nfrom data. CVPR, 2019.\\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\\nlinear units for neural network function approximation\\nin reinforcement learning. Neural Networks, 107:3–11,\\n2018.\\nGholami, A., Kwon, K., Wu, B., Tai, Z., Yue, X., Jin, P.,\\nZhao, S., and Keutzer, K. Squeezenext: Hardware-aware\\nneural network design. ECV Workshop at CVPR’18 ,\\n2018.\\nHan, S., Mao, H., and Dally, W. J. Deep compression:\\nCompressing deep neural networks with pruning, trained\\nquantization and huffman coding. ICLR, 2016.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\\nlearning for image recognition. CVPR, pp. 770–778,\\n2016.\\nHe, K., Gkioxari, G., Doll ´ar, P., and Girshick, R. Mask\\nr-cnn. ICCV, pp. 2980–2988, 2017.\\nHe, Y ., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.\\nAmc: Automl for model compression and acceleration\\non mobile devices. ECCV, 2018.\\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,\\nW., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:\\nEfﬁcient convolutional neural networks for mobile vision\\napplications. arXiv preprint arXiv:1704.04861, 2017.\\nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-\\nworks. CVPR, 2018.\\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,\\nK. Q. Deep networks with stochastic depth. ECCV, pp.\\n646–661, 2016.\\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\\nK. Q. Densely connected convolutional networks. CVPR,\\n2017.\\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of giant\\nneural networks using pipeline parallelism.arXiv preprint\\narXiv:1808.07233, 2018.\\nIandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K.,\\nDally, W. J., and Keutzer, K. Squeezenet: Alexnet-level\\naccuracy with 50x fewer parameters and <0.5 mb model\\nsize. arXiv preprint arXiv:1602.07360, 2016.\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\nICML, pp. 448–456, 2015.\\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\\nmodels transfer better? CVPR, 2019.\\nKrause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a\\nlarge-scale dataset of ﬁne-grained cars. Second Workshop\\non Fine-Grained Visual Categorizatio, 2013.\\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\\nfeatures from tiny images. Technical Report, 2009.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassiﬁcation with deep convolutional neural networks.\\nIn NIPS, pp. 1097–1105, 2012.\\nLin, H. and Jegelka, S. Resnet with one-neuron hidden\\nlayers is a universal approximator. NeurIPS, pp. 6172–\\n6181, 2018.\\nLin, T.-Y ., Doll´ar, P., Girshick, R., He, K., Hariharan, B.,\\nand Belongie, S. Feature pyramid networks for object\\ndetection. CVPR, 2017.\\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L.,\\nYuille, A., Huang, J., and Murphy, K. Progressive neural\\narchitecture search. ECCV, 2018.\\nLu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres-\\nsive power of neural networks: A view from the width.\\nNeurIPS, 2018.\\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufﬂenet v2:\\nPractical guidelines for efﬁcient cnn architecture design.\\nECCV, 2018.\\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,\\nM., Li, Y ., Bharambe, A., and van der Maaten, L. Explor-\\ning the limits of weakly supervised pretraining. arXiv\\npreprint arXiv:1805.00932, 2018.\\nMaji, S., Rahtu, E., Kannala, J., Blaschko, M., and Vedaldi,\\nA. Fine-grained visual classiﬁcation of aircraft. arXiv\\npreprint arXiv:1306.5151, 2013.\\nNgiam, J., Peng, D., Vasudevan, V ., Kornblith, S., Le, Q. V .,\\nand Pang, R. Domain adaptive transfer learning with spe-\\ncialist models. arXiv preprint arXiv:1811.07056, 2018.\\nNilsback, M.-E. and Zisserman, A. Automated ﬂower clas-\\nsiﬁcation over a large number of classes. ICVGIP, pp.\\n722–729, 2008.\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C.\\nCats and dogs. CVPR, pp. 3498–3505, 2012.\\nRaghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-\\nDickstein, J. On the expressive power of deep neural\\nnetworks. ICML, 2017.\\nRamachandran, P., Zoph, B., and Le, Q. V . Searching for\\nactivation functions. arXiv preprint arXiv:1710.05941,\\n2018.\\nReal, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-\\nlarized evolution for image classiﬁer architecture search.\\nAAAI, 2019.\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\\nM., et al. Imagenet large scale visual recognition chal-\\nlenge. International Journal of Computer Vision, 115(3):\\n211–252, 2015.\\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\\nbottlenecks. CVPR, 2018.\\nSharir, O. and Shashua, A. On the expressive power of\\noverlapping architectures of deep learning. ICLR, 2018.\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\\nand Salakhutdinov, R. Dropout: a simple way to prevent\\nneural networks from overﬁtting. The Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\\nA. Going deeper with convolutions. CVPR, pp. 1–9,\\n2015.\\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\\nZ. Rethinking the inception architecture for computer\\nvision. CVPR, pp. 2818–2826, 2016.\\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi, A. A.\\nInception-v4, inception-resnet and the impact of residual\\nconnections on learning. AAAI, 4:12, 2017.\\nTan, M., Chen, B., Pang, R., Vasudevan, V ., Sandler, M.,\\nHoward, A., and Le, Q. V . MnasNet: Platform-aware\\nneural architecture search for mobile. CVPR, 2019.\\nXie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre-\\ngated residual transformations for deep neural networks.\\nCVPR, pp. 5987–5995, 2017.\\nYang, T.-J., Howard, A., Chen, B., Zhang, X., Go, A., Sze,\\nV ., and Adam, H. Netadapt: Platform-aware neural net-\\nwork adaptation for mobile applications. ECCV, 2018.\\nZagoruyko, S. and Komodakis, N. Wide residual networks.\\nBMVC, 2016.\\nZhang, X., Li, Z., Loy, C. C., and Lin, D. Polynet: A pursuit\\nof structural diversity in very deep networks. CVPR, pp.\\n3900–3908, 2017.\\nZhang, X., Zhou, X., Lin, M., and Sun, J. Shufﬂenet: An ex-\\ntremely efﬁcient convolutional neural network for mobile\\ndevices. CVPR, 2018.\\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,\\nA. Learning deep features for discriminative localization.\\nCVPR, pp. 2921–2929, 2016.\\nZoph, B. and Le, Q. V . Neural architecture search with\\nreinforcement learning. ICLR, 2017.\\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\\ntransferable architectures for scalable image recognition.\\nCVPR, 2018.\\n'}, {'file_name': 'pubmedbert', 'text': '2Domain-Specific Language Model Pretraining for Biomedical Natural\\nLanguage Processing\\nYU GU, ROBERT TINN, HAO CHENG, MICHAEL LUCAS, NAOTO USUYAMA,\\nXIAODONG LIU, TRISTAN NAUMANN, JIANFENG GAO, and HOIFUNG POON,\\nMicrosoft Research\\nPretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing\\n(NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing\\nassumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this\\narticle, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pre-\\ntraining language models from scratch results in substantial gains over continual pretraining of general-domain language\\nmodels. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available\\ndatasets.Ourexperimentsshowthatdomain-specificpretrainingservesasasolidfoundationforawiderangeofbiomedical\\nNLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of mod-\\neling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary\\nwith BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in\\nbiomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created\\na leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at\\nhttps://aka.ms/BLURB.\\nCCS Concepts: • Computing methodologies → Natural language processing ;• Applied computing →\\nBioinformatics;\\nAdditional Key Words and Phrases: Biomedical, NLP, domain-specific pretraining\\nACM Reference format:\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,NaotoUsuyama,XiaodongLiu,TristanNaumann,JianfengGao,andHoifung\\nPoon.2021.Domain-SpecificLanguageModelPretrainingforBiomedicalNaturalLanguageProcessing. ACM Trans. Comput.\\nHealthcare 3, 1, Article 2 (October 2021), 23 pages.\\nhttps://doi.org/10.1145/3458754\\n1 INTRODUCTION\\nInnaturallanguageprocessing(NLP) ,pretraininglargeneurallanguagemodelsonunlabeledtexthasproven\\nto be a successful strategy for transfer learning. A prime example isBidirectional Encoder Representations\\nfrom Transformers (BERT)[16], which has become a standard building block for training task-specific NLP\\nmodels.ExistingpretrainingworktypicallyfocusesonthenewswireandWebdomains.Forexample,theoriginal\\nY. Gu, R. Tinn, and H. Cheng contributed equally to this research.\\nAuthors’address:Y.Gu,R.Tinn,H.Cheng,M.Lucas,N.Usuyama,X.Liu,T.Naumann,J.Gao,andH.Poon,MicrosoftResearch,OneMicrosoft\\nWay, Redmond, WA, 98052; emails: {Aiden.Gu, Robert.Tinn, chehao, Michael.Lucas, naotous, xiaodl, tristan, jfgao, hoifung}@microsoft.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\\nCopyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopy\\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions\\nfrompermissions@acm.org.\\n© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\n2637-8051/2021/10-ART2 $15.00\\nhttps://doi.org/10.1145/3458754\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n\\n2:2 • Y. Gu et al.\\nFig.1. Twoparadigmsforneurallanguagemodelpretraining.Top:Theprevailingmixed-domainparadigmassumesthatout-\\ndomain text is still helpful and typically initializes domain-specific pretraining with a general-domain language model and\\ninherits its vocabulary. Bottom: Domain-specific pretraining from scratch derives the vocabulary and conducts pretraining\\nusing solely in-domain text. In this article, we show that for domains with abundant text such as biomedicine, domain-\\nspecific pretraining from scratch can substantially outperform the conventional mixed-domain approach.\\nBERTmodelwastrainedonWikipedia 1 andBookCorpus[ 62],andsubsequenteffortshavefocusedoncrawling\\nadditional text from the Web to power even larger-scale pretraining [39,50].\\nInspecializeddomainslikebiomedicine,pastworkhasshownthatusingin-domaintextcanprovideadditional\\ngainsovergeneral-domainlanguagemodels[ 8,34,45].However,aprevailingassumptionisthatout-domaintext\\nisstillhelpfulandpreviousworktypicallyadoptsamixed-domainapproach,suchasbystartingdomain-specific\\npretraining from an existing general-domain language model (Figure1, top). In this article, we question this\\nassumption. We observe that mixed-domain pretraining such as continual pretraining can be viewed as a form\\nof transfer learning in itself, where the source domain is general text, such as newswire and the Web, and the\\ntarget domain is specialized text, such as biomedical papers. Based on the rich literature of multi-task learning\\nand transfer learning [4, 13, 38, 59], successful transfer learning occurs when the target data is scarce and the\\nsourcedomainishighlyrelevanttothetargetone.Fordomainswithabundantunlabeledtextsuchasbiomedicine,\\nit is unclear that domain-specific pretraining can benefit by transfer from general domains. In fact, the majority\\nof general domain text is substantively different from biomedical text, raising the prospect of negative transfer\\nthat actually hinders the target performance.\\n1http://wikipedia.org.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:3\\nWe thus set out to conduct a rigorous study on domain-specific pretraining and its impact on downstream\\napplications, using biomedicine as a running example.We show that domain-specific pretraining from scratch\\nsubstantiallyoutperformscontinualpretrainingofgenericlanguagemodels,thusdemonstratingthattheprevailing\\nassumption in support of mixed-domain pretraining is not always applicable(Figure1).\\nTo facilitate this study, we compile a comprehensive biomedical NLP benchmark from publicly-available\\ndatasets, and conduct in-depth comparisons of modeling choices for pretraining and task-specific fine-tuning\\nby their impact on domain-specific applications. Our experiments show that domain-specific pretraining from\\nscratchcanprovideasolidfoundationforbiomedicalNLP,leadingtonewstate-of-the-artperformanceacrossa\\nwiderange oftasks.Additionally,we discover thattheuseoftransformer-basedmodels, likeBERT,necessitates\\nrethinking several common practices. For example, BIO tags and more complex variants are the standard label\\nrepresentation forNamed Entity Recognition (NER). However, we find that simply using IO (in or out of\\nentity mentions) suffices with BERT models, leading to comparable or better performance.\\nTo help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-\\nspecific models for the community, and created a leaderboard featuring our comprehensive benchmark at\\nhttps://aka.ms/BLURB.\\n2M E T H O D S\\n2.1 Language Model Pretraining\\nInthissection,weprovideabriefoverviewofneurallanguagemodelpretraining,usingBERT[ 16]asarunning\\nexample.\\n2.1.1 Vocabulary. We assume that the input consists of text spans, such as sentences separated by special\\ntokens[SEP].Toaddresstheproblemofout-of-vocabularywords,neurallanguagemodelsgenerateavocabulary\\nfrom subword units, usingByte-Pair Encoding (BPE)[51] or variants such as WordPiece [32]. Essentially, the\\nBPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in the given\\ncorpus.Itdoesthisbyfirstshatteringallwordsinthecorpusandinitializingthevocabularywithcharactersand\\ndelimiters. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus\\nand can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size\\n(e.g.,30,000instandardBERTmodelsor50,000inRoBERTa[ 39]).Inthisarticle,weusetheWordPiecealgorithm,\\nwhich is a BPE variant that uses likelihood based on the unigram language model rather than frequency in\\nchoosing which subwords to concatenate. The text corpus and vocabulary may preserve the case (cased)o r\\nconvert all characters to lowercase (uncased).\\n2.1.2 Model Architecture. State-of-the-art neural language models are generally based on transformer archi-\\ntectures [55], following the recent success of BERT [16, 39]. The transformer model introduces a multi-layer,\\nmulti-headself-attentionmechanism,whichhasdemonstratedsuperiorityinleveragingGPU-basedparallelcom-\\nputationandmodelinglong-rangedependenciesintexts,comparedtorecurrentneuralnetworks,suchas Long\\nShort-Term Memory (LSTM)[22]. The input token sequence is first processed by a lexical encoder, which\\ncombines a token embedding, a (token) position embedding and a segment embedding (i.e., which text span\\nthe token belongs to) by element-wise summation. This embedding layer is then passed to multiple layers of\\ntransformer modules [55]. In each transformer layer, a contextual representation is generated for each token by\\nsumming a non-linear transformation of the representations of all tokens in the prior layer, weighted by the at-\\ntentionscomputedusingthegiventoken’srepresentationinthepriorlayerasthequery.Thefinallayeroutputs\\ncontextual representations for all tokens, which combine information from the whole text span.\\n2.1.3 Self-Supervision. A key innovation in BERT [16]i st h eu s eo faMasked Language Model (MLM)\\nfor self-supervised pretraining. Traditional language models are typically generative models that predict the\\nnext token based on the preceding tokens—for example, n-gram models represent the conditional probability\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:4 • Y. Gu et al.\\nof the next token by a multinomial of the preceding n-gram, with various smoothing strategies to handle rare\\noccurrences [43]. The MLM instead randomly replaces a subset of tokens by a special token (e.g., [MASK]) and\\nasks the language model to predict them. The training objective is the cross-entropy loss between the original\\ntokens and the predicted ones. In BERT and RoBERTa, 15% of the input tokens are chosen, among which a\\nrandom80%arereplacedby[ MASK],10%areleftunchanged,and10%arerandomlyreplacedbyatokenfromthe\\nvocabulary.Insteadofusingaconstantmaskingrateof15%,astandardapproachistograduallyincreaseitfrom\\n5% to 25% with 5% increment for every 20% of training epochs, which makes pretraining more stable [37]. The\\noriginalBERTalgorithmalsouses NextSentencePrediction(NSP) ,whichdeterminesforagivensentencepair\\nwhetheronesentencefollowstheotherintheoriginaltext.TheutilityofNSPhasbeencalledintoquestion[ 39],\\nbut we include it in our pretraining experiments to enable a head-to-head comparison with prior BERT models.\\n2.1.4 Advanced Pretraining Techniques. In the original formulation of BERT [16], the MLM simply selects\\nrandom subwords to mask. When a word is only partially masked, it is relatively easy to predict the masked\\nportion given the observed ones. In contrast,Whole-Word Masking (WWM)enforces that the whole word\\nmust be masked if one of its subwords is chosen. This has been adopted as the standard approach because it\\nforces the language model to capture more contextual semantic dependencies.\\nInthisarticle,wealsoexploreadversarialpretraininganditsimpactondownstreamapplications.Motivatedby\\nsuccessesincounteringadversarialattacksincomputervision,adversarialpretrainingintroducesperturbations\\nintheinputembeddinglayerthatmaximizetheadversarialloss,thusforcingthemodeltonotonlyoptimizethe\\nstandard training objective (MLM) but also minimize adversarial loss [37].\\n2.2 Biomedical Language Model Pretraining\\nInthisarticle,wewillusebiomedicineasarunningexampleinourstudyofdomain-specificpretraining.Inother\\nwords, biomedical text is considered in-domain, whereas others are regarded as out-domain. Intuitively, using\\nin-domaintextinpretrainingshouldhelpwithdomain-specificapplications.Indeed,priorworkhasshownthat\\npretrainingwithPubMedtextleadstobetterperformanceinbiomedicalNLPtasks[ 8,34,45].Themainquestion\\nis whether pretraining should include text from other domains. The prevailing assumption is that pretraining\\ncanalwaysbenefitfrommoretext,includingout-domaintext.Infact,noneofthepriorbiomedical-relatedBERT\\nmodels have been pretrained using purely biomedical text [8, 34, 45]. Here, we challenge this assumption and\\nshow thatdomain-specific pretraining from scratchcan be superior tomixed-domain pretrainingfor downstream\\napplications.\\n2.2.1 Mixed-Domain Pretraining. The standard approach to pretraining a biomedical BERT model conducts\\ncontinualpretraining ofageneral-domainpretrainedmodel,asexemplifiedbyBioBERT[ 34].Specifically,thisap-\\nproachwouldinitializewiththestandardBERTmodel[ 16],pretrainedusingWikipediaandBookCorpus.Itthen\\ncontinues the pretraining process with MLM and NSP using biomedical text. In the case of BioBERT, continual\\npretrainingisconductedusingPubMedabstractsand PubMedCentral(PMC) full-textarticles.BlueBERT[ 45]\\nuses both PubMed text and de-identified clinical notes from MIMIC-III [26].\\nNote that in the continual pretraining approach, the vocabulary is the same as the original BERT model, in\\nthiscasetheonegeneratedfromWikipediaandBookCorpus.Althoughconvenient,thisisamajordisadvantage\\nfor this approach, as the vocabulary is not representative of the target biomedical domain.\\nComparedtotheotherbiomedical-relatedpretrainingefforts,SciBERT[ 8]isanotableexceptionasitgenerates\\nthevocabularyandpretrainsfromscratch,usingbiomedicineandcomputerscienceasrepresentativesforscien-\\ntificliterature.However,fromtheperspectiveofbiomedicalapplications,SciBERTstilladoptsthemixed-domain\\npretraining approach, as computer science text is clearly out-domain.\\n2.2.2 Domain-Specific Pretraining from Scratch.The mixed-domain pretraining approach makes sense if the\\ntargetapplicationdomainhaslittletextofitsownandcantherebybenefitfrompretrainingusingrelateddomains.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:5\\nTable 1. Comparison of Common Biomedical Terms in Vocabularies Used by the Standard BERT,\\nSciBERT, and PubMedBERT (Ours)\\nBiomedical Term Category BERT SciBERT PubMedBERT (Ours)\\ndiabetes disease ✓✓ ✓\\nleukemia disease ✓✓ ✓\\nlithium drug ✓✓ ✓\\ninsulin drug ✓✓ ✓\\nDNA gene ✓✓ ✓\\npromoter gene ✓✓ ✓\\nhypertension disease hyper-tension ✓✓\\nnephropathy disease ne-ph-rop-athy ✓✓\\nlymphoma disease l-ym-ph-oma ✓✓\\nlidocaine drug lid-oca-ine] ✓✓\\noropharyngeal organ oro-pha-ryn-ge-al or-opharyngeal ✓\\ncardiomyocyte cell card-iom-yo-cy-te cardiomy-ocyte ✓\\nchloramphenicol drug ch-lor-amp-hen-ico-l chlor-amp-hen-icol ✓\\nRecA gene Rec-A Rec-A ✓\\nacetyltransferase gene ace-ty-lt-ran-sf-eras-e acetyl-transferase ✓\\nclonidine drug cl-oni-dine clon-idine ✓\\nnaloxone drug na-lo-xon-e nal-oxo-ne ✓\\nA ✓ indicates the biomedical term appears in the corresponding vocabulary; otherwise, the term will be broken into word\\npiecesseparatedbyahyphen.Thesewordpiecesoftenhavenobiomedicalrelevanceandmayhinderlearningindownstream\\ntasks.\\nHowever, this is not the case for biomedicine, which has more than 30 million abstracts in PubMed, and adds\\nmore than a million each year. We thus hypothesize that domain-specific pretraining from scratch is a better\\nstrategy for biomedical language model pretraining.\\nA major advantage of domain-specific pretraining from scratch stems from having an in-domain vocabulary.\\nTable 1 compares the vocabularies used in various pretraining strategies. BERT models using continual pre-\\ntrainingare stuckwith theoriginal vocabularyfrom thegeneral-domain corpora,whichdoesnot contain many\\ncommonbiomedicalterms.EvenforSciBERT,whichgeneratesitsvocabularypartiallyfrombiomedicaltext,the\\ndeficiency compared to a purely biomedical vocabulary is substantial. As a result, standard BERT models are\\nforcedtodivertparameterizationcapacityandtrainingbandwidthtomodelbiomedicaltermsusingfragmented\\nsubwords. For example,naloxone, a common medical term, is divided into four pieces ([na, ##lo, ##xon, ##e]) by\\nBERT, andacetyltransferase is shattered into seven pieces ([ace, ##ty, ##lt, ##ran, ##sf, ##eras, ##e]) by BERT.2\\nBoth terms appear in the vocabulary of PubMedBERT.\\nAnother advantage of domain-specific pretraining from scratch is that the language model is trained using\\npurely in-domain data. For example, SciBERT pretraining has to balance optimizing for biomedical text and\\ncomputer science text, the latter of which is unlikely to be beneficial for biomedical applications. Continual\\npretraining, however, may potentially recover from out-domain modeling, although not completely. Aside from\\nthe vocabulary issue mentioned earlier, neural network training uses non-convex optimization, which means\\nthatcontinualpretrainingmaynotbeabletocompletelyundosuboptimalinitializationfromthegeneral-domain\\nlanguage model.\\n2Prior work also observed similar shattering for clinical words [52].\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:6 • Y. Gu et al.\\nTable 2. Comparison of the Biomedical Datasets in Prior Language Model\\nPretraining Studies and BLURB\\nBioBERT [34] SciBERT [ 8]B L U E [45]B L U R B\\nBC5-chem [35] ✓✓ ✓ ✓\\nBC5-disease [35] ✓✓ ✓ ✓\\nNCBI-disease [18] ✓✓ — ✓\\nBC2GM [53] ✓ —— ✓\\nJNLPBA [27] ✓ —— ✓\\nEBM PICO [44]— ✓ — ✓\\nChemProt [31] ✓✓ ✓ ✓\\nDDI [21] ✓ — ✓✓\\nGAD [11] ✓ —— ✓\\nBIOSSES [54]—— ✓✓\\nHoC [20]— — ✓✓\\nPubMedQA [25]— — — ✓\\nBioASQ [42] ✓ —— ✓\\nIn our experiments, we show that domain-specific pretraining with in-domain vocabulary confers clear ad-\\nvantagesovermixed-domainpretraining,beitcontinualpretrainingofgeneral-domainlanguagemodelsorpre-\\ntraining on mixed-domain text.\\n2.3 BLURB: A Comprehensive Benchmark for Biomedical NLP\\nThe ultimate goal of language model pretraining is to improve performance on a wide range of downstream\\napplications.Ingeneral-domainNLP,thecreationofcomprehensivebenchmarks,suchasGLUE[ 56,57],greatly\\naccelerates advances in language model pretraining by enabling head-to-head comparisons among pretrained\\nlanguage models. In contrast, prior work on biomedical pretraining tends to use different tasks and datasets for\\ndownstream evaluation, as shown in Table2. This makes it hard to assess the impact of pretrained language\\nmodels on the downstream tasks we care about. To the best of our knowledge, BLUE [45] is the first attempt\\nto create an NLP benchmark in the biomedical domain. We aim to improve on its design by addressing some\\nof its limitations. First, BLUE has limited coverage of biomedical applications used in other recent work on\\nbiomedicallanguagemodels,asshowninTable 2.Forexample,itdoesnotincludeanyquestion-answeringtask.\\nMore importantly, BLUE mixes PubMed-based biomedical applications (six datasets such as BC5, ChemProt,\\nand HoC) with MIMIC-based clinical applications (four datasets such as i2b2 and MedNLI). Clinical notes differ\\nsubstantiallyfrombiomedicalliterature,totheextentthatweobserveBERTmodelspretrainedonclinicalnotes\\nperform poorly on biomedical tasks, similar to the standard BERT. Consequently, it is advantageous to create\\nseparate benchmarks for these two domains.\\nTo facilitate investigations of biomedical language model pretraining and help accelerate progress in biomed-\\nicalNLP,wecreateanewbenchmark, BLURB(BiomedicalLanguageUnderstanding&ReasoningBench-\\nmark).WefocusonPubMed-basedbiomedicalapplicationsandleavetheexplorationoftheclinicaldomain,and\\nother high-value verticals, to future work. To make our effort tractable and facilitate head-to-head comparison\\nwith prior work, we prioritize the selection of datasets used in recent work on biomedical language models and\\nwill explore the addition of other datasets in future work.\\nBLURBiscomprisedofacomprehensivesetofbiomedicalNLPtasksfrompubliclyavailabledatasets,including\\nNER, evidence-based medical information extraction (PICO), relation extraction, sentence similarity, document\\nclassification, and question answering. Table3 provides an overview of the BLURB datasets. For question an-\\nswering, prior work has considered both classification tasks (e.g., whether a reference text contains the answer\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:7\\nTable 3. Datasets Used in the BLURB Biomedical NLP Benchmark\\nDataset Task Train Dev Test Evaluation Metrics\\nBC5-chem NER 5,203 5,347 5,385 F1 entity-level\\nBC5-disease NER 4,182 4,244 4,424 F1 entity-level\\nNCBI-disease NER 5,134 787 960 F1 entity-level\\nBC2GM NER 15,197 3,061 6,325 F1 entity-level\\nJNLPBA NER 46,750 4,551 8,662 F1 entity-level\\nEBM PICO PICO 339,167 85,321 16,364 Macro F1 word-level\\nChemProt Relation Extraction 18,035 11,268 15,745 Micro F1\\nDDI Relation Extraction 25,296 2,496 5,716 Micro F1\\nGAD Relation Extraction 4,261 535 534 Micro F1\\nBIOSSES Sentence Similarity 64 16 20 Pearson\\nHoC Document Classification 1,295 186 371 Micro F1\\nPubMedQA Question Answering 450 50 500 Accuracy\\nBioASQ Question Answering 670 75 140 Accuracy\\nNote: We list the numbers of instances in train, dev, and test (e.g., entity mentions in NER and PICO elements in\\nevidence-based medical information extraction).\\nto a given question) and more complex tasks such as list and summary [42]. The latter types often require ad-\\nditional engineering efforts that are not relevant to evaluating neural language models. For simplicity, we focus\\nontheclassificationtaskssuchasyes/noquestionansweringinBLURBandleavetheinclusionofmorecomplex\\nquestion answering to future work.\\nTo compute a summary score for BLURB, the simplest way is to report the average score among all tasks.\\nHowever, this may place undue emphasis on simpler tasks such as NER for which there are many existing\\ndatasets.Therefore,wegroupthedatasetsbytheirtasktypes,computetheaveragescoreforeachtasktype,and\\nreport the macro average among the task types. To help accelerate research in biomedical NLP, we release the\\nBLURB benchmark as well as a leaderboard athttp://aka.ms/BLURB.\\nIn the following are detailed descriptions for each task and corresponding datasets.\\n2.3.1 Named Entity Recognition.\\nBC5-Chemical&BC5-Disease. TheBioCreativeVChemical-DiseaseRelationcorpus[ 35]wascreatedforeval-\\nuating relation extraction of drug-disease interactions, but it is frequently used as a NER corpus for detect-\\ning chemical (drug) and disease entities. The dataset consists of 1,500 PubMed abstracts broken into three\\neven splits for training, development, and test. We use a pre-processed version of this dataset generated by\\nCrichton et al. [14], discard the relation labels, and train NER models for chemical (BC5-Chemical) and disease\\n(BC5-Disease) separately.\\nNCBI-Disease. The Natural Center for Biotechnology Information Disease corpus [18] contains 793 PubMed\\nabstracts with 6,892 annotated disease mentions linked to 790 distinct disease entities. We use a pre-processed\\nset of train, development, and test splits generated by Crichton et al. [14].\\nBC2GM. TheBiocreativeIIGeneMentioncorpus[ 53]consistsofsentencesfromPubMedabstractswithman-\\nually labeled gene and alternative gene entities. Following prior work, we focus on the gene entity annotation.\\nIn its original form, BC2GM contains 15,000 train and 5,000 test sentences. We use a pre-processed version\\nof the dataset generated by Crichton et al. [14], which carves out 2,500 sentences from the training data for\\ndevelopment.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:8 • Y. Gu et al.\\nJNLPBA. The Joint Workshop on Natural Language Processing in Biomedicine and Its Applications shared\\ntask [27] is a NER corpus on PubMed abstracts. The entity types are chosen for molecular biology applications:\\nprotein, DNA, RNA, cell line, and cell type. Some of the entity type distinctions are not very meaningful. For\\nexample,agenementionoftenreferstoboththeDNAandgeneproductssuchastheRNAandprotein.Following\\nprior work that evaluates on this dataset [34], we ignore the type distinction and focus on detecting the entity\\nmentions. We use the same train, development, and test splits as in Crichton et al. [14].\\n2.3.2 Evidence-Based Medical Information Extraction (PICO).\\nEBM PICO.The Evidence-Based Medicine corpus [44] contains PubMed abstracts on clinical trials, where\\neach abstract is annotated with P, I, and O in PICO: Participants (e.g.,diabetic patients), Intervention (e.g.,\\ninsulin), Comparator (e.g.,placebo), and Outcome (e.g.,blood glucose levels). Comparator (C) labels are\\nomitted as they are standard in clinical trials: placebo for passive control and standard of care for active con-\\ntrol. There are 4,300, 500, and 200 abstracts in training, development, and test, respectively. The training and\\ndevelopment sets were labeled by Amazon Mechanical Turkers, whereas the test set was labeled by Upwork\\ncontributors with prior medical training. EBM PICO provides labels at the word level for each PIO element. For\\neach of the PIO elements in an abstract, we tally the F1 score at the word level, then compute the final score as\\ntheaverageamongPIOelementsinthedataset.Occasionally,twoPICOelementsmightoverlapwitheachother\\n(e.g., a participant span might contain within it an intervention span). In EBM PICO, about 3% of the PIO words\\nare in the overlap. Note that the dataset released along with SciBERT appears to remove the overlapping words\\nfrom the larger span (e.g., the participant span as mentioned earlier). We instead use the original dataset [44]\\nand their scripts for preprocessing and evaluation.\\n2.3.3 Relation Extraction.\\nChemProt. The Chemical Protein Interaction corpus [31] consists of PubMed abstracts annotated with\\nchemical-protein interactions between chemical and protein entities. There are 23 interactions organized in\\na hierarchy, with 10 high-level interactions (includingNONE). Most relation instances in ChemProt are within\\nsingle sentences. Following prior work [8, 34], we only consider sentence-level instances. We follow the\\nChemProt authors’ suggestions and focus on classifying five high-level interactions—UPREGULATOR (CPR : 3),\\nDOWNREGULATOR (CPR : 4), AGONIST (CPR : 5), ANTAGONIST (CPR : 6), SUBSTRATE (CPR : 9)—as well as every-\\nthingelse( false).TheChemProtannotationisnotexhaustiveforallchemical-proteinpairs.Followingprevious\\nwork [34, 45], we expand the training and development sets by assigning afalse label for all chemical-protein\\npairs that occur in a training or development sentence, but we do not have an explicit label in the ChemProt\\ncorpus. Note that prior work uses slightly different label expansion of the test data. To facilitate head-to-head\\ncomparison, we will provide instructions for reproducing the test set in BLURB from the original dataset.\\nDDI. TheDrug-DrugInteractioncorpus[ 21]wascreatedtofacilitateresearchonpharmaceuticalinformation\\nextraction, with a particular focus on pharmacovigilance. It contains sentence-level annotation of drug-drug\\ninteractionsonPubMedabstracts.Notethatsomepriorwork[ 45,61]discarded90trainingfilesthattheauthors\\nconsidered not conducive to learning drug-drug interactions. We instead use the original dataset and produce\\nour train/dev/test split of 624/90/191 files.\\nGAD. The Genetic Association Database corpus [11] was created semi-automatically using the Genetic As-\\nsociation Archive.3 Specifically, the archive contains a list of gene-disease associations, with the corresponding\\nsentences in the PubMed abstracts reporting the association studies. Bravo et al. [11]u s e dab i o m e d i c a lN E R\\ntooltoidentifygeneanddiseasementions,andcreatethepositiveexamplesfromtheannotatedsentencesinthe\\n3http://geneticassociationdb.nih.gov/.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:9\\narchive, and negative examples from gene-disease co-occurrences that were not annotated in the archive. We\\nuseanexistingpreprocessedversionofGAD anditscorrespondingtrain/dev/testsplitcreatedbyLeeetal.[ 34].\\n2.3.4 Sentence Similarity.\\nBIOSSES. The Sentence Similarity Estimation System for the Biomedical Domain [54] contains 100 pairs of\\nPubMedsentenceseachofwhichisannotatedbyfiveexpert-levelannotatorswithanestimatedsimilarityscore\\nin the range from 0 (no relation) to 4 (equivalent meanings). It is a regression task, with the average score as\\nthe final annotation. We use the same train/dev/test split as that in the work of Peng et al. [45] and use Pearson\\ncorrelation for evaluation.\\n2.3.5 Document Classification.\\nHoC. The Hallmarks of Cancer corpus was motivated by the pioneering work on cancer hallmarks [20]. It\\ncontains annotation on PubMed abstracts with binary labels, each of which signifies the discussion of a specific\\ncancer hallmark. The authors use 37 fine-grained hallmarks that are grouped into 10 top-level ones. We focus\\non predicting the top-level labels. The dataset was released with 1,499 PubMed abstracts [6] and has since been\\nexpanded to 1,852 abstracts [5]. Note that Peng et al. [45] discarded a control subset of 272 abstracts that do not\\ndiscuss any cancer hallmark (i.e., all binary labels are false). We instead adopt the original dataset and report\\nmicro F1 across the 10 cancer hallmarks. Although the original dataset provided sentence-level annotation, we\\nfollowthecommonpracticeandevaluateontheabstractlevel[ 19,60].Wecreatethetrain/dev/testsplit,asthey\\nwere not available previously.4\\n2.3.6 Question Answering (QA).\\nPubMedQA. ThePubMedQAdataset[ 25]containsasetofresearchquestions,eachwithareferencetextfrom\\naPubMedabstractaswellasanannotatedlabelofwhetherthetextcontainstheanswertotheresearchquestion\\n(yes/maybe/no). We use the original train/dev/test split with 450, 50, and 500 questions, respectively.\\nBioASQ. TheBioASQcorpus[ 42]containsmultiplequestionansweringtasksannotatedbybiomedicalexperts,\\nincludingyes/no,factoid,list,andsummaryquestions.Pertainingtoourobjectiveofcomparingneurallanguage\\nmodels, we focus on the yes/no questions (Task 7b) and leave the inclusion of other tasks to future work. Each\\nquestion is paired with a reference text containing multiple sentences from a PubMed abstract and a yes/no\\nanswer. We use the official train/dev/test split of 670/75/140 questions.\\n2.4 Task-Specific Fine-Tuning\\nPretrained neural language models provide a unifying foundation for learning task-specific models. Given an\\ninput token sequence, the language model produces a sequence of vectors in the contextual representation. A\\ntask-specific prediction model is then layered on top to generate the final output for a task-specific application.\\nGiven task-specific training data, we can learn the task-specific model parameters and refine the BERT model\\nparameters by gradient descent using backpropagation.\\nPrior work on biomedical NLP often adopts different task-specific models and fine-tuning methods, which\\nmakes it difficult to understand the impact of an underlying pretrained language model on task performance.\\nIn this section, we review standard methods and common variants used for each task. In our primary investi-\\ngation comparing pretraining strategies, we fix the task-specific model architecture using the standard method\\nidentifed here, to facilitate a head-to-head comparison among the pretrained neural language models. Subse-\\nquently,westartwiththesamepretrainedBERTmodelandconductadditionalinvestigationontheimpactforthe\\n4The original authors used cross validation for their evaluation.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:10 • Y. Gu et al.\\nFig. 2. A general architecture for task-specific fine-tuning of neural language models, with a relation-extraction example.\\nNote that the input goes through additional processing such as word-piece tokenization in the neural language model\\nmodule.\\nvarious choices in the task-specific models. For prior biomedical BERT models, our standard task-specific meth-\\nods generally lead to comparable or better performance when compared to their published results.\\n2.4.1 A General Architecture for Fine-Tuning Neural Language Models.Figure 2 shows a general architecture\\nof fine-tuning neural language models for downstream applications. An input instance is first processed by a\\nTransformInput modulethatperformstask-specifictransformationssuchasappendingspecialinstancemarker\\n(e.g., [CLS]) or dummifying entity mentions for relation extraction. The transformed input is then tokenized\\nusing the neural language model’s vocabulary and fed into the neural language model. Next, the contextual\\nrepresentation at the top layer is processed by aFeaturizer module and then fed into thePredict module to\\ngenerate the final output for a given task.\\nTo facilitate a head-to-head comparison, we apply the same fine-tuning procedure for all BERT models and\\ntasks. Specifically, we use cross-entropy loss for classification tasks and mean square error for regression tasks.\\nWeconducthyperparametersearchusingthedevelopmentsetbasedontask-specificmetrics.Similartoprevious\\nwork, we jointly fine-tune the parameters of the task-specific prediction layer as well as the underlying neural\\nlanguage model.\\n2.4.2 Task-SpecificProblemFormulationandModelingChoices. ManyNLPapplicationscanbeformulatedas\\na classification or regression task, wherein either individual tokens or sequences are the prediction target. Mod-\\neling choices usually vary in two aspects: the instance representation and the prediction layer. Table4 presents\\nan overview of the problem formulation and modeling choices for tasks we consider, and detailed descriptions\\nare provided in the following. For each task, we highlight the standard modeling choices with an asterisk (*).\\nNER. Given an input text span (usually a sentence), the NER task seeks to recognize mentions of entities of\\ninterest. It is typically formulated as a sequential labeling task, where each token is assigned a tag to signify\\nwhether it is in an entity mention or not. The modeling choices primarily vary on the tagging scheme and clas-\\nsificationmethod. BIO isthestandardtaggingschemethatclassifieseachtokenasthebeginningofanentity( B),\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:11\\nTable 4. Standard NLP Tasks and Their Problem Formulations and Modeling Choices\\nTask Problem Formulation Modeling Choices\\nNER Token Classification Tagging Scheme, Classification Layer\\nPICO Token Classification Tagging Scheme, Classification Layer\\nRelation Extraction Sequence Classification Entity/Relation Representation, Classification Layer\\nSentence Similarity Sequence Regression Sentence Representation, Regression Loss\\nDocument Classification Sequence Classification Document Representation, Classification Layer\\nQuestion Answering Sequence Classification Question/Text Representation, Classification Layer\\ninsideanentity( I),oroutside( O).TheNERtasksinBLURBareonlyconcernedaboutoneentitytype(inJNLPBA,\\nallthetypesaremergedintoone).Inthecasewhentherearemultipleentitytypes,the BI tagswouldbefurther\\ndivided into fine-grained tags for specific types. Prior work has also considered more complex tagging schemes\\nsuch asBIOUL,w h e r eU stands for the last word of an entity andL stands for a single-word entity. We also con-\\nsiderthesimpler IO schemethatonlydifferentiatesbetweeninandoutofanentity.Classificationisdoneusing\\na simple linear layer or more sophisticated sequential labeling methods such as LSTM orconditional random\\nfield (CRF)[33].\\n•TransformInput: Returns the input sequence as is.\\n•Featurizer: Returns the BERT encoding of a given token.\\n•Tagging scheme:BIO*; BIOUL; IO.\\n•Classification layer:L i n e a rl a y e r * ;L S T M ;C R F .\\nPICO. Conceptually,evidence-basedmedicalinformationextractionisakintoslotfilling,asittriestoidentify\\nthe PIO elements in an abstract describing a clinical trial. However, it can be formulated as a sequential tagging\\ntask like NER, by classifying tokens belonging to each element. A token may belong to more than one element\\n(e.g., participant (P) and intervention (I)).\\n•TransformInput: Returns the input sequence as is.\\n•Featurizer: Returns the BERT encoding of a given token.\\n•Tagging scheme:BIO*; BIOUL; IO.\\n•Classification layer:L i n e a rl a y e r * ;L S T M ;C R F .\\nRelation extraction.Existing work on relation extraction tends to focus on binary relations. Given a pair of\\nentity mentions in a text span (typically a sentence), the goal is to determine if the text indicates a relation for\\nthe mention pair. There are significant variations in the entity and relation representations. To prevent overfit-\\nting by memorizing the entity pairs, the entity tokens are often augmented with start/end markers or replaced\\nby a dummy token. For featurization, the relation instance is either represented by a special [CLS] token or by\\nconcatenating the mention representations. In the latter case, if an entity mention contains multiple tokens, its\\nrepresentationisusuallyproducedbypoolingthoseofindividualtokens(maxoraverage).Forcomputationalef-\\nficiency,weusepaddingortruncationtosettheinputlengthto128tokensforGADand256tokensforChemProt\\nand DDI that contain longer input sequences.\\n•TransformInput: Entity (dummification*; start/end marker; original); relation ([CLS]*; original).\\n•Featurizer: Entity (dummy token*; pooling); relation ([CLS] BERT encoding*; concatenation of the men-\\ntion BERT encoding).\\n•Classification layer: Linear layer*; more sophisticated classifiers (e.g., MLP).\\nSentence similarity.The similarity task can be formulated as a regression problem to generate a normalized\\nscoreforasentencepair.Bydefault,aspecial[ SEP]tokenisinsertedtoseparatethetwosentences,andaspecial\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:12 • Y. Gu et al.\\nTable 5. Summary of Pretraining Details for the Various BERT Models Used in Our Experiments\\nVocabulary Pretraining Corpus Text Size\\nBERT Wiki + Books — Wiki + Books 3.3B words/16 GB\\nRoBERTa Web crawl — Web crawl 160 GB\\nBioBERT Wiki + Books continual pretraining PubMed 4.5B words\\nSciBERT PMC + CS from scratch PMC + CS 3.2B words\\nClinicalBERT Wiki + Books continual pretraining MIMIC 0.5B words/3.7 GB\\nBlueBERT Wiki + Books continual pretraining PubMed + MIMIC 4.5B words\\nPubMedBERT PubMed from scratch PubMed 3.1B words/21 GB\\nStatistics for prior BERT models are taken from their publications when available. The size of a text corpus such as\\nPubMed may vary a bit, depending on downloading time and preprocessing (e.g., filtering out empty or very short\\nabstracts). Both BioBERT and PubMedBERT also have a version pretrained with additional PMC full text; here we list the\\nstandard version pretrained using PubMed only.\\n[CLS]tokenisprependedtothebeginningtorepresentthepair.TheBERTencodingof[ CLS]isusedtocompute\\nthe regression score.\\n•TransformInput:[ CLS]S1 [SEP]S2 [SEP], for sentence pairS1, S2.\\n•Featurizer:[ CLS] BERT encoding.\\n•Regression layer: Linear regression.\\nDocument classification.For each text span and category(an abstractand a cancer hallmark in HoC), the goal\\nis to classifywhetherthe text belongs to the category.By default, a [CLS] token is appendedto the beginning of\\nthe text, and its BERT encoding is passed on by theFeaturizer for the final classification, which typically uses\\na simple linear layer.\\n•TransformInput:[ CLS]D [SEP], for documentD.\\n•Featurizer: Returns [CLS] BERT encoding.\\n•Classification layer: Linear layer.\\nQuestion answering.For the two-way (yes/no) or three-way (yes/maybe/no) question-answering task, the en-\\ncoding is similar to the sentence similarity task. Namely, a [CLS] token is prepended to the beginning, followed\\nby the question and reference text, with a [SEP] token to separate the two text spans. The [CLS] BERT encoding\\nis then used for the final classification. For computational efficiency, we use padding or truncation to set the\\ninput length to 512 tokens.\\n•TransformInput:[ CLS]Q [SEP]T [SEP], for questionQ and reference textT .\\n•Featurizer: Returns [CLS] BERT encoding.\\n•Classification layer: Linear layer.\\n2.5 Experimental Settings\\nForbiomedicaldomain-specificpretraining,wegeneratethevocabularyandconductpretrainingusingthelatest\\ncollection of PubMed5 abstracts: 14 million abstracts, 3.2 billion words, 21 GB. (The original collection contains\\nmore than 4 billion words; we filter out any abstracts with less than 128 words to reduce noise.)\\nWefollowthestandardpretrainingprocedurebasedontheTensorFlowimplementationreleasedbyNVIDIA. 6\\nWe use Adam [30]fortheoptimizerusingastandardslantedtriangularlearningrateschedulewithwarm-upin\\n10%ofstepsandcool-downin90%ofsteps.Specifically,thelearningrateincreaseslinearlyfromzerotothepeak\\n5https://pubmed.ncbi.nlm.nih.gov/ (downloaded in February 2020).\\n6https://github.com/NVIDIA/DeepLearningExamples.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:13\\nrateof6 ×10−4 inthefirst10%ofsteps,thendecayslinearlytozerointheremaining90%ofsteps.Trainingisdone\\nfor 62,500 steps with batch size of 8,192, which is comparable to the computation used in previous biomedical\\npretraining.7 The training takes about 5 days on one DGX-2 machine with 16 V100 GPUs. We find that the\\ncased version has similar performance to the uncased version in preliminary experiments; thus, we focus on\\nuncased models in this study. We use WWM, with a masking rate of 15%. We denote the resulting BERT model\\nPubMedBERT.\\nFor comparison, we use the public releases of BERT [16], RoBERTa [39], BioBERT [34], SciBERT [8], Clini-\\ncalBERT [1], and BlueBERT [45]. Table5 presents an overview. BioBERT and BlueBERT conduct continual pre-\\ntraining from BERT, whereas ClinicalBERT conducts continual pretraining from BioBERT; thus, they all share\\nthe same vocabulary as BERT. BioBERT comes with two versions. We use BioBERT++ (v1.1), which was trained\\nfor a longer time and performed better. ClinicalBERT also comes with two versions. We use Bio+Clinical BERT.\\nPrior pretraining work has explored two settings: BERT-BASE with 12 transformer layers and 100 million\\nparameters; BERT-LARGE with 24 transformer layers and 300 million parameters. Prior work in biomedical\\npretraining uses BERT-BASE only. For head-to-head comparison, we also use BERT-BASE in pretraining Pub-\\nMedBERT.BERT-LARGEappearstoyieldimprovedperformanceinsomepreliminaryexperiments.Weleavean\\nin-depth exploration to future work.\\nFor task-specific fine-tuning, we use Adam [30] with the standard slanted triangular learning rate schedule\\n(warm-up in the first 10% of steps and cool-down in the remaining 90% of steps) and a dropout probability of\\n0.1.Duetorandominitializationofthetask-specificmodelanddropout,theperformancemayvaryfordifferent\\nrandomseeds,especiallyforsmalldatasetslikeBIOSSES,BioASQ,andPubMedQA.Wereporttheaveragescores\\nfrom 10 runs for BIOSSES, BioASQ, and PubMedQA, and 5 runs for the others.\\nFor all datasets, we use the development set for tuning the hyperparameters with the same range: learning\\nrate(1e-5,3e-5,5e-5),batchsize(16,32),andepochnumber(2–60).Ideally,wewouldconductseparatehyperpa-\\nrametertuningforeachmodeloneachdataset.However,thiswouldincuraprohibitiveamountofcomputation,\\nas we have to enumerate all combinations of models, datasets, and hyperparameters, each of which requires\\naveragingovermultiplerunswithdifferentrandomization.Inpractice,weobservethatthedevelopmentperfor-\\nmance is not very sensitive to hyperparameter selection, as long as they are in a ballpark range. Consequently,\\nwefocusonhyperparametertuningusingasubsetofrepresentativemodelssuchasBERTandBioBERT,anduse\\nacommonsetofhyperparametersforeachdatasetthatworkwellforbothout-domainandin-domainlanguage\\nmodels.\\n3R E S U L T S\\nInthissection,weconductathoroughevaluationtoassesstheimpactofdomain-specificpretraininginbiomed-\\nical NLP applications. First, we fix the standard task-specific model for each task in BLURB, and conduct a\\nhead-to-head comparison of domain-specific pretraining and mixed-domain pretraining. Next, we evaluate the\\nimpact of various pretraining options such as vocabulary, WWM, and adversarial pretraining. Finally, we fix a\\npretrained BERT model and compare various modeling choices for task-specific fine-tuning.\\n3.1 Domain-Specific Pretraining vs Mixed-Domain Pretraining\\nWe compare BERT models by applying them to the downstream NLP applications in BLURB. For each task, we\\nconduct the same fine-tuning process using the standard task-specific model as specified in Section2.4.T a b l e6\\nshows the results.\\nByconductingdomain-specificpretrainingfromscratch,PubMedBERTconsistentlyoutperformsalltheother\\nBERTmodelsinmostbiomedicalNLPtasks,oftenbyasignificantmargin.Thegainsaremostsubstantialagainst\\n7For example, BioBERT started with the standard BERT, which was pretrained using 1M steps with batch size of 256 and ran another 1M\\nsteps in continual pretraining.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:14 • Y. Gu et al.\\nTable 6. Comparison of Pretrained Language Models on the BLURB Biomedical NLP Benchmark\\nBERT RoBERTa BioBERT SciBERT ClinicalBERT BlueBERT PubMedBERT\\nUncased Cased Cased Cased Uncased Cased Cased Cased Uncased\\nBC5-chem 89.25 89.99 89.43 92.85 92.49 92.51 90.80 91.19 93.33\\nBC5-disease 81.44 79.92 80.65 84.70 84.54 84.70 83.04 83.69 85.62\\nNCBI-disease 85.67 85.87 86.62 89.13 88.10 88.25 86.32 88.04 87.82\\nBC2GM 80.90 81.23 80.90 83.82 83.36 83.36 81.71 81.87 84.52\\nJNLPBA 77.69 77.51 77.86 78.55 78.68 78.51 78.07 77.71 79.10\\nEBM PICO 72.34 71.70 73.02 73.18 73.12 73.06 72.06 72.54 73.38\\nChemProt 71.86 71.54 72.98 76.14 75.24 75.00 72.04 71.46 77.24\\nDDI 80.04 79.34 79.52 80.88 81.06 81.22 78.20 77.78 82.36\\nGAD 80.41 79.61 80.63 82.36 82.38 81.34 80.48 79.15 83.96\\nBIOSSES 82.68 81.40 81.25 89.52 86.25 87.15 91.23 85.38 92.30\\nHoC 80.20 80.12 79.66 81.54 80.66 81.16 80.74 80.48 82.32\\nPubMedQA 51.62 49.96 52.84 60.24 57.38 51.40 49.08 48.44 55.84\\nBioASQ 70.36 74.44 75.20 84.14 78.86 74.22 68.50 68.71 87.56\\nBLURB score 76.11 75.86 76.46 80.34 78.86 78.14 77.29 76.27 81.16\\nThe standard task-specific models are used in the same fine-tuning process for all BERT models. The BLURB score is the macro\\naverage of average test results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification,\\nquestion answering). See Table3for the evaluation metric used in each task.\\nBERTmodelstrainedusingout-domaintext.Notably,althoughthepretrainingcorpusisthelargestforRoBERTa,\\nits performance on biomedical NLP tasks is among the worst, similar to the original BERT model. Models using\\nbiomedical text in pretraining generally perform better. However, mixing out-domain data in pretraining gen-\\nerally leads to worse performance. In particular, even though clinical notes are more relevant to the biomedical\\ndomainthangeneral-domaintext,addingthemdoesnotconferanyadvantage,asisevidentbytheresultsofClin-\\nicalBERT and BlueBERT. Not surprisingly, BioBERT is the closest to PubMedBERT, as it also uses PubMed text\\nfor pretraining. However, by conducting domain-specific pretraining from scratch, including using the PubMed\\nvocabulary, PubMedBERT is able to obtain consistent gains over BioBERT in most tasks. A notable exception is\\nPubMedQA, but this dataset is small, and there are relatively high variances among runs with different random\\nseeds.\\nCompared to the published results for BioBERT, SciBERT, and BlueBERT in their original papers, our results\\nare generally comparable or better for the tasks they have been evaluated on. The ClinicalBERT paper does not\\nreport any results on these biomedical applications [1].\\n3.2 Ablation Study on Pretraining Techniques\\nTo assess the impact of pretraining options on downstream applications, we conduct several ablation studies\\nusing PubMedBERT as a running example. Table7 shows results assessing the effect of vocabulary and WWM.\\nUsing the original BERT vocabulary derived from Wikipedia & BookCorpus (by continual pretraining from the\\noriginal BERT), the results are significantly worse than using an in-domain vocabulary from PubMed. Addition-\\nally, WWM leads to consistent improvement across the board, regardless of the vocabulary in use. A significant\\nadvantageinusinganin-domainvocabularyisthattheinputwillbeshorterindownstreamtasks,asshowninTa-\\nble8,whichmakeslearningeasier.Figure 3showsexamplesofhowdomain-specificpretrainingwithin-domain\\nvocabulary helps correct errors from mixed-domain pretraining.\\nFurthermore, we found that pretraining on general-domain text provides no benefit even if we use the in-\\ndomainvocabulary(Table 9).ThefirstcolumninTable 9correspondstoBioBERT,whichconductedpretraining\\nfirst on the general domain and then on PubMed. The second column adopts the same continual pretraining\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:15\\nTable 7. Evaluation of the Impact of Vocabulary and WWM on the\\nPerformance of PubMedBERT on BLURB\\nWiki + Books PubMed\\nWord Piece Whole Word Word Piece Whole Word\\nBC5-chem 93.20 93.31 92.96 93.33\\nBC5-disease 85.00 85.28 84.72 85.62\\nNCBI-disease 88.39 88.53 87.26 87.82\\nBC2GM 83.65 83.93 83.19 84.52\\nJNLPBA 78.83 78.77 78.63 79.10\\nEBM PICO 73.30 73.52 73.44 73.38\\nChemProt 75.04 76.70 75.72 77.24\\nDDI 81.30 82.60 80.84 82.36\\nGAD 83.02 82.42 81.74 83.96\\nBIOSSES 91.36 91.79 92.45 92.30\\nHoC 81.76 81.74 80.38 82.32\\nPubMedQA 52.20 55.92 54.76 55.84\\nBioASQ 73.69 76.41 78.51 87.56\\nBLURB score 79.16 79.96 79.62 81.16\\nTable 8. Comparison of the Average Input\\nLength in Word Pieces Using General-Domain\\nvs In-Domain Vocabulary\\nVocab Wiki + Books PubMed\\nBC5-chem 35.9 28.0\\nBC5-disease 35.9 28.0\\nNCBI-disease 34.2 27.4\\nBC2GM 38.5 30.5\\nJNLPBA 33.7 26.0\\nEBM PICO 30.7 25.1\\nChemProt 75.4 55.5\\nDDI 106.0 75.9\\nGAD 47.0 35.7\\nBIOSSES 80.7 61.6\\nHoC 40.6 31.0\\nPubMedQA 343.1 293.0\\nBioASQ 702.4 541.4\\nstrategy, except that the in-domain vocabulary (from PubMed) was used, which actually led to slight degra-\\ndation in performance. However, by conducting pretraining from scratch on PubMed, we attained similar per-\\nformance even with half of the compute (third column) and attained significant gain with the same amount\\nof compute (fourth column; PubMedBERT). In sum, general-domain pretraining confers no advantage here in\\ndomain-specific pretraining.\\nInourstandardPubMedBERTpretraining,weusedPubMedabstractsonly.Wealsotriedaddingfull-textarti-\\nclesfromPMC, 8 withthetotalpretrainingtextincreasedsubstantiallyto16.8billionwords(107GB).Surprisingly,\\n8https://www.ncbi.nlm.nih.gov/pmc/.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:16 • Y. Gu et al.\\nFig. 3. Examples of how domain-specific pretraining helps correct errors from mixed-domain pretraining. Top: Attention\\nfor the leading word piece of the gene mention “epithelial-restricted with serine box” (abbreviation “ESX”) in the BC2GM\\ndataset. Bottom: Attention for the [CLS] token in an instance of AGONIST relation between a pair of dummified chemical\\nand protein. In both cases, we show the aggregate attention from the penultimate layer to the preceding layer, which tends\\nto be most informative about the final classification. Note how BioBERT tends to shatter the relevant words by inheriting\\nthegeneral-domainvocabulary.Thedomain-specificvocabularyenablesPubMedBERTtolearnbetterattentionpatternsand\\nmake correct predictions.\\nthis generally leads to a slight degradation in performance across the board. However, by extending pretraining\\nfor 60% longer (100K steps in total), the overall results improve and slightly outperform the standard PubMed-\\nBERTusingonlyabstracts.Theimprovementissomewhatmixedacrossthetasks,withsomegainingandothers\\nlosing.Wehypothesizethatthereasonforthisbehavioristwofold.First,PMCinclusionisinfluencedbyfunding\\npolicy and differs from general PubMed distribution, and full texts generally contain more noise than abstracts.\\nAs most existing biomedical NLP tasks are based on abstracts, full texts may be slightly out-domain compared\\nto abstracts. Moreover, even if full texts are potentially helpful, their inclusion requires additional pretraining\\ncycles to make use of the extra information.\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:17\\nTable 9. Evaluation of the Impact of Pretraining Corpora and Time on the\\nPerformance on BLURB\\nPretraining Wiki + Books →PubMed PubMed (half time) PubMed\\nVocab Wiki + Books PubMed PubMed PubMed\\nBC5-chem 92.85 93.41 93.05 93.33\\nBC5-disease 84.70 85.43 85.02 85.62\\nNCBI-disease 89.13 87.60 87.77 87.82\\nBC2GM 83.82 84.03 84.11 84.52\\nJNLPBA 78.55 79.01 78.98 79.10\\nEBM PICO 73.18 73.80 73.74 73.38\\nChemProt 76.14 77.05 76.69 77.24\\nDDI 80.88 81.21 81.96 82.36\\nGAD 82.36 82.47 82.80 83.96\\nBIOSSES 89.52 89.93 92.12 92.30\\nHoC 81.54 83.14 82.13 82.32\\nPubMedQA 60.24 54.84 55.28 55.84\\nBioASQ 84.14 79.00 79.43 87.56\\nBLURB score 80.34 80.03 80.23 81.16\\nIn the first two columns, pretraining was first conducted on Wiki & Books, then on PubMed\\nabstracts. All use the same amount of compute (twice as long as original BERT pretraining),\\nexcept for the third column, which only uses half (same as original BERT pretraining).\\nTable 10. Evaluation of the Impact of Pretraining Text on the Performance\\nof PubMedBERT on BLURB\\nPubMed PubMed + PMC PubMed + PMC (longer training)\\nBC5-chem 93.33 93.36 93.34\\nBC5-disease 85.62 85.62 85.76\\nNCBI-disease 87.82 88.34 88.04\\nBC2GM 84.52 84.39 84.37\\nJNLPBA 79.10 78.90 79.16\\nEBM PICO 73.38 73.64 73.72\\nChemProt 77.24 76.96 76.80\\nDDI 82.36 83.56 82.06\\nGAD 82.34 82.24 81.58\\nBIOSSES 92.30 90.39 92.31\\nHoC 82.32 82.16 82.62\\nPubMedQA 55.84 61.02 60.02\\nBioASQ 87.56 83.43 87.20\\nBLURB score 81.07 80.91 81.42\\nThe first result column corresponds to the standard PubMedBERT pretrained using PubMed\\nabstracts (“PubMed”). The second one corresponds to PubMedBERT trained using both PubMed\\nabstracts and PMC full text (“PubMed+PMC”). The last one corresponds to PubMedBERT trained\\nusing both PubMed abstracts and PMC full text, for 60% longer (“PubMed+PMC (longer training)”).\\nAdversarial pretraining has been shown to be highly effective in boosting performance in general-domain\\napplications [37]. We thus conducted adversarial pretraining in PubMedBERT and compared its performance\\nwithstandardpretraining(Table 11).Surprisingly,adversarialpretraininggenerallyleadstoaslightdegradation\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:18 • Y. Gu et al.\\nTable 11. Comparison of PubMedBERT Performance on\\nBLURB Using Standard and Adversarial Pretraining\\nPubMedBERT + Adversarial\\nBC5-chem 93.33 93.17\\nBC5-disease 85.62 85.48\\nNCBI-disease 87.82 87.99\\nBC2GM 84.52 84.07\\nJNLPBA 79.10 79.18\\nEBM PICO 73.38 72.92\\nChemProt 77.24 77.04\\nDDI 82.36 83.62\\nGAD 83.96 83.54\\nBIOSSES 92.30 94.11\\nHoC 82.32 82.20\\nPubMedQA 55.84 53.30\\nBioASQ 87.56 82.71\\nBLURB score 81.16 80.77\\nin performance, with some exceptions such as sentence similarity (BIOSSES). We hypothesize that the reason\\nmay be similar to what we observe in pretraining with full texts. Namely, adversarial training is most useful if\\nthe pretraining corpus is more diverse and relatively out-domain compared to the application tasks. We leave a\\nmore thorough evaluation of adversarial pretraining to future work.\\n3.3 Ablation Study on Fine-Tuning Methods\\nIn the preceding studies on pretraining methods, we fix the fine-tuning methods to the standard methods de-\\nscribed in Section2.4. Next, we will study the effect of modeling choices in task-specific fine-tuning, by fixing\\ntheunderlyingpretrainedlanguagemodeltoourstandardPubMedBERT(WWM,PubMedvocabulary,pretrained\\nusing PubMed abstracts).\\nPrior to the current success of pretraining neural language models, standard NLP approaches were often\\ndominated by sequential labeling methods, such as CRF and more recently recurrent neural networks such as\\nLSTM. Such methods were particularly popular for NER and relation extraction.\\nWiththeadventofBERTmodelsandtheself-attentionmechanism,theutilityofexplicitsequentialmodeling\\nbecomes questionable. The top layer in the BERT model already captures many non-linear dependencies across\\nthe entire text span. Therefore, it is conceivable that even a linear layer on top can perform competitively. We\\nfind that this is indeed the case for NER and relation extraction, as shown in Table12. The use of a bidirectional\\nLSTM (Bi-LSTM) does not lead to any substantial gain compared to linear layer.\\nWealsoinvestigatethetaggingschemeusedinNER.Thestandardtaggingschemedistinguisheswordsbytheir\\npositions within an entity. For sequential tagging methods such as CRF and LSTM, distinguishing the position\\nwithinanentityispotentiallyadvantageouscomparedtotheminimalIOschemethatonlydistinguishesbetween\\ntheinsideandoutsideofentities.ButforBERTmodels,onceagain,theutilityofmorecomplextaggingschemes\\nis diminished. We thus conducted a head-to-head comparison of the tagging schemes using three biomedical\\nNER tasks in BLURB. As we can see in Table13, the difference is minuscule, suggesting that with self-attention,\\nthe sequential nature of the tags is less essential in NER modeling.\\nThe use of neural methods also has subtle, but significant, implications for relation extraction. Previously,\\nrelationextractionwasgenerallyframedasaclassificationproblemwithmanuallycraftedfeaturetemplates.To\\nprevent overfitting and enhance generalization, the feature templates would typically avoid using the entities\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:19\\nTable 12. Comparison of Linear Layers vs Recurrent\\nNeural Networks for Task-Specific Fine Tuning in NER\\n(Entity-level F1) and Relation Extraction (Micro F1), All\\nUsing the Standard PubMedBERT\\nTask-Specific Model Linear Layer Bi-LSTM\\nBC5-chem 93.33 93.12\\nBC5-disease 85.62 85.64\\nJNLPBA 79.10 79.10\\nChemProt 77.24 75.40\\nDDI 82.36 81.70\\nGAD 83.96 83.42\\nTable 13. Comparison of Entity-Level F1 for\\nBiomedical NER Using Different Tagging\\nSchemes and the Standard PubMedBERT\\nTagging Scheme BIO BIOUL IO\\nBC5-chem 93.33 93.37 93.11\\nBC5-disease 85.62 85.59 85.63\\nJNLPBA 79.10 79.02 79.05\\nin question. Neural methods do not need handcrafted features but rather use the neural encoding of the given\\ntextspan,includingtheentitiesthemselves.Thisintroducesapotentialriskthattheneuralnetworkmaysimply\\nmemorize the entity combination. This problem is particularly pronounced in self-supervision settings, such as\\ndistant supervision, because the positive instances are derived from entity tuples with known relations. As a\\nresult,it isa common practiceto “dummify” entities(i.e.,replacean entitywitha generictagsuchas$DRUGor\\n$GENE) [24,58].\\nThis risk remains in the standard supervised setting, such as in the tasks that comprise BLURB. We thus\\nconducted a systematic evaluation of entity dummification and relation encoding, using two relation extraction\\ntasks in BLURB.\\nFor entity marking, we consider three variants: dummify the entities in question; use the original text; add\\nstartandendtagstoentitiesinquestion.Forrelationencoding,weconsiderthreeschemes.Inthe[ CLS]encoding\\nintroduced by the original BERT paper, the special token [CLS] is prepended to the beginning of the text span,\\nanditscontextualrepresentationatthetoplayerisusedastheinputinthefinalclassification.Anotherstandard\\napproachconcatenatestheBERTencodingofthegivenentitymentions,eachobtainedbyapplyingmaxpooling\\ntothecorrespondingtokenrepresentations.Finally,followingpriorwork,wealsoconsidersimplyconcatenating\\nthe top contextual representation of the entity start tag, if the entity markers are in use [7].\\nTable 14 shows the results. Simply using the original text indeed exposes the neural methods to significant\\noverfitting risk. Using [CLS] with the original text is the worst choice, as the relation encoding has a hard time\\ndistinguishing which entities in the text span are in question. Dummification remains the most reliable method,\\nwhich works for either relation encoding method. Interestingly, using entity markers leads to slightly better\\nresultsinbothdatasets,asitappearstopreventoverfittingwhilepreservingusefulentityinformation.Weleave\\nit to future work to study whether this would generalize to all relation extraction tasks.\\n4 DISCUSSION\\nStandard supervised learning requires labeled examples, which are expensive and time consuming to annotate.\\nSelf-supervision using unlabeled text is thus a long-standing direction for alleviating the annotation bottleneck\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:20 • Y. Gu et al.\\nTable 14. Evaluation of the Impact of Entity Dummification and Relation Encoding\\nin Relation Extraction, All Using PubMedBERT\\nInput Text Classification Encoding ChemProt DDI\\nENTITY DUMMIFICATION [CLS] 77.24 82.36\\nENTITY DUMMIFICATION MENTION 77.22 82.08\\nORIGINAL [CLS] 50.52 37.00\\nORIGINAL MENTION 75.48 79.42\\nENTITY MARKERS [CLS] 77.72 82.22\\nENTITY MARKERS MENTION 77.22 82.42\\nENTITY MARKERS ENTITY START 77.58 82.18\\nWith entity dummification, the entity mentions in question are anonymized using entity type\\ntags such as $DRUG or $GENE. With entity marker, special tags marking the start and end of an\\nentity are appended to the entity mentions in question. Relation encoding is derived from the\\nspecial [CLS] token appended to the beginning of the text or the special entity start token, or by\\nconcatenating the contextual representation of the entity mentions in question.\\nusing transfer learning. Early methods focused on clustering related words using distributed similarity, such as\\nBrownClusters[ 12,36].Withtherevivalofneuralapproaches,neuralembeddinghasbecomethenewstaplefor\\ntransferlearningfromunlabeledtext.Thisstartswithsimplestand-alonewordembeddings[ 41,46]andevolves\\nintomoresophisticatedpretrainedlanguagemodels,fromLSTMinULMFiT[ 23]andELMo[ 47]totransformer-\\nbasedmodelsinGPT[ 48,49]andBERT[ 16,39].Theirsuccessisfueledbyaccesstolargetextcorpora,advanced\\nhardwaresuchasGPUs,andaculminationofadvancesinoptimizationmethods,suchasAdam[ 30]andslanted\\ntriangular learning rate [23]. Here, transfer learning goes from the pretrained language models to fine-tuning\\ntask-specific models for downstream applications.\\nAs the community ventures beyond the standard newswire and Web domains, and begins to explore high-\\nvalue verticals such as biomedicine, a different kind of transfer learning is brought into play by combining text\\nfrom various domains in pretraining language models. The prevailing assumption is that such mixed-domain\\npretraining is advantageous. In this article, we show that this type of transfer learning may not be applicable\\nwhen there is a sufficient amount of in-domain text, as is the case in biomedicine. In fact, our experiments\\ncomparingclinicalBERTswithPubMedBERTonbiomedicalNLPtasksshowthatevenrelatedtextsuchasclinical\\nnotes may not be helpful, since we already have abundant biomedical text from PubMed. Our results show that\\nweshoulddistinguishdifferenttypesoftransferlearningandseparatelyassesstheirutilityinvarioussituations.\\nThere are a plethora of biomedical NLP datasets, especially from various shared tasks such as BioCre-\\native[3,29,40,53],BioNLP [15,28],SemEval[ 2,9,10,17] ,an dBi o A SQ[42].Thefocushase v olv edfr omsimple\\ntasks, such as NER, to more sophisticated tasks, such as relation extraction and question answering, and new\\ntaskshavebeenproposedforemergingapplicationscenariossuchasevidence-basedmedicalinformationextrac-\\ntion[44].However,althoughcomprehensivebenchmarksandleaderboardsareavailableforthegeneraldomains\\n(e.g., GLUE [57]a n dS u p e r G L U E[56]), they are still a rarity in biomedical NLP. In this article, inspired by prior\\neffort toward this direction [45], we create the first leaderboard for biomedical NLP, BLURB—a comprehensive\\nbenchmark containing 13 datasets for six tasks.\\n5 CONCLUSION\\nInthisarticle,wechallengeaprevailingassumptioninpretrainingneurallanguagemodelsandshowthatdomain-\\nspecific pretraining from scratch can significantly outperform mixed-domain pretraining such as continual pre-\\ntraining from a general-domain language model, leading to new state-of-the-art results for a wide range of\\nbiomedicalNLPapplications.Tofacilitatethisstudy,wecreateBLURB,acomprehensivebenchmarkforbiomed-\\nical NLP featuring a diverse set of tasks such as NER, relation extraction, document classification, and question\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:21\\nanswering. To accelerate research in biomedical NLP, we release our state-of-the-art biomedical BERT models\\nand set up a leaderboard based on BLURB.\\nFuture directions include further exploration of domain-specific pretraining strategies, incorporating more\\ntasks in biomedical NLP, and extension of the BLURB benchmark to clinical and other high-value domains.\\nREFERENCES\\n[1] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly\\navailableclinicalBERTembeddings.In Proceedings ofthe2ndClinical Natural Language Processing Workshop.72–78. https://doi.org/10.\\n18653/v1/W19-1909\\n[2] Marianna Apidianaki, Saif M. Mohammad, Jonathan May, Ekaterina Shutova, Steven Bethard, and Marine Carpuat (Eds.). 2018.Pro-\\nceedingsofthe12thInternationalWorkshoponSemanticEvaluation,SemEval@NAACL-HLT2018,NewOrleans,Louisiana,USA,June5-6,\\n2018. Association for Computational Linguistics.https://www.aclweb.org/anthology/volumes/S18-1/.\\n[3] Cecilia N. Arighi, Phoebe M. Roberts, Shashank Agarwal, Sanmitra Bhattacharya, Gianni Cesareni, Andrew Chatr-aryamontri, Simon\\nClematide, et al. 2011. BioCreative III interactive task: An overview.BMC Bioinformatics12, 8 (Oct. 2011), S4.https://doi.org/10.1186/\\n1471-2105-12-S8-S4\\n[4] Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. InProceedings of the\\n2011 Conference on Empirical Methods in Natural Language Processing. 355–362.https://www.aclweb.org/anthology/D11-1033.\\n[5] Simon Baker, Imran Ali, Ilona Silins, Sampo Pyysalo, Yufan Guo, Johan Högberg, Ulla Stenius, and Anna Korhonen. 2017. Cancer\\nhallmarks analytics tool (CHAT): A text mining approach to organize and evaluate scientific literature on cancer.Bioinformatics 33, 24\\n(2017), 3973–3981.\\n[6] Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Högberg, Ulla Stenius, and Anna Korhonen. 2015. Automatic semantic classifi-\\ncation of scientific literature according to the hallmarks of cancer.Bioinformatics 32, 3 (2015), 432–440.\\n[7] Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for\\nrelation learning. InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2895–2905.https://doi.org/\\n10.18653/v1/P19-1279\\n[8] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. InProceedings of the 2019 Con-\\nference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP’19). 3615–3620.https://doi.org/10.18653/v1/D19-1371\\n[9] StevenBethard,MarineCarpuat,MariannaApidianaki,SaifM.Mohammad,DanielM.Cer,andDavidJurgens(Eds.).2017. Proceedings\\nof the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017. Association for\\nComputational Linguistics.https://www.aclweb.org/anthology/volumes/S17-2/.\\n[10] Steven Bethard, Daniel M. Cer, Marine Carpuat, David Jurgens, Preslav Nakov, and Torsten Zesch (Eds.). 2016.Proceedings of the 10th\\nInternational Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, USA, June 16-17, 2016. Association for\\nComputational Linguistics.https://www.aclweb.org/anthology/volumes/S16-1/.\\n[11] ÀlexBravo,JanetPiñero,NúriaQueralt-Rosinach,MichaelRautschka,andLauraI.Furlong.2015.Extractionofrelationsbetweengenes\\nand diseases from text and large-scale data analysis: Implications for translational research.BMC Bioinformatics16, 1 (2015), 55.\\n[12] Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of\\nnatural language.Computational Linguistics18, 4 (1992), 467–480.\\n[13] Rich Caruana. 1997. Multitask learning.Machine Learning28, 1 (1997), 41–75.\\n[14] Gamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna Korhonen. 2017. A neural network multi-task learning approach to biomedical\\nnamed entity recognition.BMC Bioinformatics18, 1 (2017), 368.\\n[15] Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou, and Junichi Tsujii (Eds.). 2019.Proceedings of the 18th BioNLP\\nWorkshop and Shared Task, BioNLP@ACL 2019, Florence, Italy, August 1, 2019. Association for Computational Linguistics.https://www.\\naclweb.org/anthology/volumes/W19-50/.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for\\nlanguage understanding. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies (Volume 1: Long and Short Papers). 4171–4186.\\n[17] Mona T. Diab, Timothy Baldwin, and Marco Baroni (Eds.). 2013.Proceedings of the 7th International Workshop on Semantic Evaluation,\\nSemEval@NAACL-HLT 2013, Atlanta, Georgia, USA, June 14-15, 2013. Association for Computational Linguistics.https://www.aclweb.\\norg/anthology/volumes/S13-2/.\\n[18] Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong Lu. 2014. NCBI disease corpus: A resource for disease name recognition and\\nconcept normalization.Journal of Biomedical Informatics47 (2014), 1–10.\\n[19] Jingcheng Du, Qingyu Chen, Yifan Peng, Yang Xiang, Cui Tao, and Zhiyong Lu. 2019. ML-Net: Multi-label classification of biomedical\\ntexts with deep neural networks.Journal of the American Medical Informatics Association26, 11 (2019), 1279–1285.https://doi.org/10.\\n1093/jamia/ocz085\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n2:22 • Y. Gu et al.\\n[20] Douglas Hanahan and Robert A. Weinberg. 2000. The hallmarks of cancer.Cell 100, 1 (2000), 57–70.\\n[21] María Herrero-Zazo, Isabel Segura-Bedmar, Paloma Martínez, and Thierry Declerck. 2013. The DDI corpus: An annotated corpus with\\npharmacological substances and drug–drug interactions.Journal of Biomedical Informatics46, 5 (2013), 914–920.\\n[22] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory.Neural Computation9, 8 (1997), 1735–1780.\\n[23] JeremyHowardandSebastianRuder.2018.Universallanguagemodelfine-tuningfortextclassification.In Proceedingsofthe56thAnnual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers). 328–339.https://doi.org/10.18653/v1/P18-1031\\n[24] Robin Jia, Cliff Wong, and Hoifung Poon. 2019. Document-levelN-ary relation extraction with multiscale representation learning.\\nIn Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies (NAACL-HLT’19).\\n[25] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research\\nquestionanswering.In Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternational\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP’19). 2567–2577.https://doi.org/10.18653/v1/D19-1259\\n[26] Alistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-Wei H. Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter\\nSzolovits, Leo Anthony Celi, and Roger G. Mark. 2016. MIMIC-III, a freely accessible critical care database.Scientific Data3, 1 (May\\n2016), 160035.https://doi.org/10.1038/sdata.2016.35\\n[27] Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduction to the bio-entity recognition\\ntask at JNLPBA. InProceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications\\n(NLPBA/BioNLP’04). 73–78.https://www.aclweb.org/anthology/W04-1213.\\n[28] Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011. Overview of Genia Event Task in BioNLP Shared Task 2011.\\nInProceedings of the BioNLP Shared Task 2011 Workshop (BioNLP Shared Task’11). 7–15.\\n[29] Sun Kim, Rezarta Islamaj Dogan, Andrew Chatr-aryamontri, Mike Tyers, W. John Wilbur, and Donald C. Comeau. 2015. Overview of\\nBioCreative V BioC track. InProceedings of the Fifth BioCreative Challenge Evaluation Workshop, Sevilla, Spain.1 – 9 .\\n[30] DiederikP.KingmaandJimmyBa.2015.Adam:Amethodforstochasticoptimization.In Proceedingsofthe3rdInternationalConference\\non Learning Representations (ICLR’15). http://arxiv.org/abs/1412.6980.\\n[31] Martin Krallinger, Obdulia Rabal, Saber A. Akhondi, Martın Pérez Pérez, Jesús Santamaría, G. P. Rodríguez, G. Tsatsaronis, et al. 2017.\\nOverviewoftheBioCreativeVIChemical-ProteinInteractionTrack.In Proceedingsofthe6thBioCreativeChallengeEvaluationWorkshop ,\\nVol. 1. 141–146.\\n[32] TakuKudoandJohnRichardson.2018.SentencePiece:AsimpleandlanguageindependentsubwordtokenizeranddetokenizerforNeu-\\nral Text Processing. InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.\\n66–71.https://doi.org/10.18653/v1/D18-2012\\n[33] John Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting\\nand labeling sequence data. InProceedings of the 18th International Conference on Machine Learning. 282–289.\\n[34] JinhyukLee,WonjinYoon,SungdongKim,DonghyeonKim,SunkyuKim,ChanHoSo,andJaewooKang.2019.BioBERT:Apre-trained\\nbiomedical language representation model for biomedical text mining.Bioinformatics 36, 4 (2019), 1234–1240.https://doi.org/10.1093/\\nbioinformatics/btz682\\n[35] Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly,\\nThomasC.Wiegers,andZhiyongLu.2016.BioCreativeVCDRtaskcorpus:Aresourceforchemicaldiseaserelationextraction. Database.\\nOnline, May 8, 2016.\\n[36] Percy Liang. 2005.Semi-Supervised Learning for Natural Language. Ph.D. Dissertation. Massachusetts Institute of Technology, Cam-\\nbridge, MA.\\n[37] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. 2020. Adversarial training for\\nlarge neural language models. arXiv:2004.08994.\\n[38] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. 2015. Representation learning using multi-task deep\\nneuralnetworksforsemanticclassificationandinformationretrieval.In Proceedingsofthe2015ConferenceoftheNorthAmericanChapter\\nof the Association for Computational Linguistics: Human Language Technologies. 912–921.\\n[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\\nStoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692.\\n[40] Yuqing Mao, Kimberly Van Auken, Donghui Li, Cecilia N. Arighi, Peter McQuilton, G. Thomas Hayman, Susan Tweedie, et al. 2014.\\nOverview of the gene ontology task at BioCreative IV.Database.Online, August 25, 2014.\\n[41] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.2013.Efficientestimationofwordrepresentationsinvectorspace.arXiv:1301.\\n3781.\\n[42] Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, and Georgios Paliouras. 2019. Results of the seventh edition of the\\nBioASQchallenge.In ProceedingsoftheJointEuropeanConferenceonMachineLearningandKnowledgeDiscoveryinDatabases .553–568.\\n[43] HermannNey,UteEssen,andReinhardKneser.1994.Onstructuringprobabilisticdependencesinstochasticlanguagemodelling. Com-\\nputer Speech & Language8, 1 (1994), 1–38.https://doi.org/10.1006/csla.1994.1001\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing • 2:23\\n[44] BenjaminNye,JunyiJessyLi,RomaPatel,YinfeiYang,IainJ.Marshall,AniNenkova,andByronC.Wallace.2018.Acorpuswithmulti-\\nlevel annotations of patients, interventions and outcomes to support language processing for medical literature. InProceedings of the\\nConference of the Association for Computational Linguistics, Vol. 2018. 197.\\n[45] Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer learning in biomedical natural language processing: An evaluation of BERT\\nandELMoontenbenchmarkingdatasets.In Proceedings ofthe18thBioNLPWorkshopandSharedTask .58–65. https://doi.org/10.18653/\\nv1/W19-5006\\n[46] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. InProceedings\\nof the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP’14). 1532–1543.\\n[47] MatthewPeters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLukeZettlemoyer.2018.Deepcontex-\\ntualizedwordrepresentations.In Proceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputational\\nLinguistics: Human Language Technologies (Volume 1: Long Papers). 2227–2237.https://doi.org/10.18653/v1/N18-1202\\n[48] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-\\ntraining.https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf.\\n[49] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised\\nmultitask learners.OpenAI Blog1, 8 (2019), 9.\\n[50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.Journal of Machine Learning Research21, 140 (2020),\\n1–67. http://jmlr.org/papers/v21/20-074.html.\\n[51] RicoSennrich,BarryHaddow,andAlexandraBirch.2016.Neuralmachinetranslationofrarewordswithsubwordunits.In Proceedings\\nofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers) .1715–1725. https://doi.org/10.18653/\\nv1/P16-1162\\n[52] Yuqi Si, Jingqi Wang, Hua Xu, and Kirk Roberts. 2019. Enhancing clinical concept extraction with contextual embeddings.Journal of\\nthe American Medical Informatics Association26, 11 (2019), 1297–1304.\\n[53] LarrySmith,LorraineK.Tanabe,RieJohnsonneeAndo,Cheng-JuKuo,I.-FangChung,Chun-NanHsu,Yu-ShiLin,etal.2008.Overview\\nof BioCreative II gene mention recognition.Genome Biology9 (2008), S2.\\n[54] Gizem Soğancıoğlu, Hakime Öztürk, and Arzucan Özgür. 2017. BIOSSES: A semantic sentence similarity estimation system for the\\nbiomedical domain.Bioinformatics 33, 14 (2017), i49–i58.\\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\\nAttention is all you need. InAdvances in Neural Information Processing Systems. 5998–6008.\\n[56] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems. InAdvances in Neural Information Processing\\nSystems. 3266–3280.\\n[57] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelR.Bowman.2019.GLUE:Amulti-taskbenchmarkand\\nanalysis platform for natural language understanding. InProceedings of the 2019 International Conference on Learning Representations\\n(ICLR’19).\\n[58] Hai Wang and Hoifung Poon. 2018. Deep probabilistic logic: A unifying framework for indirect supervision. InProceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing (EMNLP’18).\\n[59] Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, and Jianfeng Gao. 2019. Multi-task learning with sample re-weighting for ma-\\nchinereadingcomprehension. In Proceedings of the2019 Conferenceof theNorthAmerican ChapteroftheAssociation for Computational\\nLinguistics: Human Language Technologies (Volume 1: Long and Short Papers). 2644–2655.https://doi.org/10.18653/v1/N19-1271\\n[60] M. Zhang and Z. Zhou. 2014. A review on multi-label learning algorithms.IEEE Transactions on Knowledge and Data Engineering26, 8\\n(2014), 1819–1837.https://doi.org/10.1109/TKDE.2013.39\\n[61] Yijia Zhang, Wei Zheng, Hongfei Lin, Jian Wang, Zhihao Yang, and Michel Dumontier. 2018. Drug–drug interaction extraction via\\nhierarchical RNNs on sequence and shortest dependency paths.Bioinformatics 34, 5 (2018), 828–835.\\n[62] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books\\nandmovies:Towardsstory-likevisualexplanationsbywatchingmoviesandreadingbooks.In Proceedingsofthe2015IEEEInternational\\nConference on Computer Vision (ICCV’15).\\nReceived July 2020; revised January 2021; accepted March 2021\\nACM Transactions on Computing for Healthcare, Vol. 3, No. 1, Article 2. Publication date: October 2021.\\n'}, {'file_name': 'clip', 'text': \"Learning Transferable Visual Models From Natural Language Supervision\\nAlec Radford * 1 Jong Wook Kim* 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\\nGirish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1\\nAbstract\\nSOTA computer vision systems are trained to pre-\\ndict a ﬁxed set of predetermined object categories.\\nThis restricted form of supervision limits their\\ngenerality and usability since additional labeled\\ndata is needed to specify any other visual con-\\ncept. Learning directly from raw text about im-\\nages is a promising alternative which leverages a\\nmuch broader source of supervision. We demon-\\nstrate that the simple pre-training task of predict-\\ning which caption goes with which image is an\\nefﬁcient and scalable way to learn SOTA image\\nrepresentations from scratch on a dataset of 400\\nmillion (image, text) pairs collected from the inter-\\nnet. After pre-training, natural language is used to\\nreference learned visual concepts (or describe new\\nones) enabling zero-shot transfer of the model to\\ndownstream tasks. We study performance on over\\n30 different computer vision datasets, spanning\\ntasks such as OCR, action recognition in videos,\\ngeo-localization, and many types of ﬁne-grained\\nobject classiﬁcation. The model transfers non-\\ntrivially to most tasks and is often competitive\\nwith a fully supervised baseline without the need\\nfor any dataset speciﬁc training. For instance, we\\nmatch the accuracy of the original ResNet50 on\\nImageNet zero-shot without needing to use any of\\nthe 1.28 million training examples it was trained\\non. We release our code and pre-trained model\\nweights at https://github.com/OpenAI/CLIP.\\n1. Introduction and Motivating Work\\nPre-training methods which learn directly from raw text\\nhave revolutionized NLP over the last few years (Dai & Le,\\n2015; Peters et al., 2018; Howard & Ruder, 2018; Radford\\net al., 2018; Devlin et al., 2018; Raffel et al., 2019). The\\ndevelopment of “text-to-text” as a standardized input-output\\n*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\\nCorrespondence to: <{alec, jongwook}@openai.com>.\\nProceedings of the 38 th International Conference on Machine\\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\\ninterface (McCann et al., 2018; Radford et al., 2019; Raffel\\net al., 2019) has enabled task-agnostic architectures to zero-\\nshot transfer to downstream datasets. Flagship systems like\\nGPT-3 (Brown et al., 2020) are now competitive across\\nmany tasks with bespoke models while requiring little to no\\ndataset speciﬁc training data.\\nThese results suggest that the aggregate supervision acces-\\nsible to modern pre-training methods within web-scale col-\\nlections of text surpasses that of high-quality crowd-labeled\\nNLP datasets. However, in other ﬁelds such as computer\\nvision it is still standard practice to pre-train models on\\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\\nCould scalable pre-training methods which learn directly\\nfrom web text result in a similar breakthrough in computer\\nvision? Prior work is encouraging.\\nJoulin et al. (2016) demonstrated that CNNs trained to pre-\\ndict words in image captions can learn representations com-\\npetitive with ImageNet training. Li et al. (2017) then ex-\\ntended this approach to predicting phrase n-grams in ad-\\ndition to individual words and demonstrated the ability of\\ntheir system to zero-shot transfer to other image classiﬁ-\\ncation datasets. Adopting more recent architectures and\\npre-training approaches, VirTex (Desai & Johnson, 2020),\\nICMLM (Bulent Sariyildiz et al., 2020), and ConVIRT\\n(Zhang et al., 2020) have recently demonstrated the po-\\ntential of transformer-based language modeling, masked\\nlanguage modeling, and contrastive objectives to learn im-\\nage representations from text.\\nHowever, the aforementioned models still under-perform\\ncurrent SOTA computer vision models such as Big Trans-\\nfer (Kolesnikov et al., 2019) and the weakly supervised\\nResNeXt (Mahajan et al., 2018). A crucial difference is\\nscale. While Mahajan et al. (2018) and Kolesnikov et al.\\n(2019) trained for accelerator years on millions to billions\\nof images, VirTex, ICMLM, and ConVIRT trained for ac-\\ncelerator days on one to two hundred thousand images. We\\nclose this gap and study the behaviors of image models\\ntrained from natural language supervision at large scale. We\\ndemonstrate that a simpliﬁed version of ConVIRT trained\\nfrom scratch, which we call CLIP, for Contrastive Language-\\nImage Pre-training, is an efﬁcient and scalable method of\\nlearning from natural language supervision. We ﬁnd that\\nLearning Transferable Visual Models From Natural Language Supervision\\nI 1 ·T 2 I 1 ·T 3 …\\nI 2 ·T 1 I 2 ·T 3 …\\nI 3 ·T 1 I 3 ·T 2 …\\n⋮ ⋮ ⋮ \\nI 1 ·T 1 \\nI 2 ·T 2 \\nI 3 ·T 3 \\n(1) Contrastive pre-training\\nImage \\nEncoder \\nT ext \\nEncoder Pepper\\tthe\\naussie\\tpup\\nPepper\\tthe\\naussie\\tpup\\nPepper\\tthe\\naussie\\tpup\\nPepper\\tthe\\naussie\\tpup\\nT 1 T 2 T 3 …\\nI 1 \\nI 2 \\nI 3 \\n⋮ \\n(2) Create dataset classiﬁer from label text\\nplane\\ncar\\ndog\\n⋮ \\nbird\\nA\\tphoto\\tof\\na\\t{object}.\\n⋮ \\nT ext \\nEncoder \\nT 1 T 2 T 3 T N \\n…\\n(3) Use for zero-shot prediction\\nImage \\nEncoder I 1 I 1 ·T 2 I 1 ·T N I 1 ·T 1 \\n…\\n…\\nA\\tphoto\\tof\\n\\ta\\tdog.\\nT N \\nI N ·T 1 I N ·T 2 I N ·T 3 \\nI 1 ·T N \\nI 2 ·T N \\nI 3 ·T N \\n⋮ \\n…I N \\n…\\n⋮ ⋱ \\nI N ·T N \\nI 1 ·T 3 \\nFigure 1.Summary of our approach. While standard image models jointly train an image feature extractor and a linear classiﬁer to predict\\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classiﬁer by embedding the names or descriptions of the\\ntarget dataset’s classes.\\nCLIP learns to perform a wide set of tasks during pre-\\ntraining including OCR, geo-localization, action recogni-\\ntion, and outperforms the best publicly available ImageNet\\nmodel while being more computationally efﬁcient. We also\\nﬁnd that zero-shot CLIP models are much more robust than\\nequivalent accuracy supervised ImageNet models.\\n2. Approach\\nAt the core of our work is the idea of learning perception\\nfrom the supervision contained in natural language paired\\nwith images. In the following subsections we detail our\\nspeciﬁc approach.\\n2.1. Creating a Sufﬁciently Large Dataset\\nExisting work has mainly used three datasets, MS-COCO\\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\\nYFCC100M (Thomee et al., 2016). While MS-COCO and\\nVisual Genome are high quality crowd-labeled datasets, they\\nare small by modern standards with approximately 100,000\\ntraining photos each. By comparison, other computer vision\\nsystems are trained on up to 3.5 billion Instagram photos\\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\\nis a possible alternative, but the metadata for each image is\\nsparse and of varying quality. Many images use automati-\\ncally generated ﬁlenames like 20160716 113957.JPG\\nas “titles” or contain “descriptions” of camera exposure\\nsettings. After ﬁltering to keep only images with natural\\nlanguage titles and/or descriptions in English, the dataset\\nshrunk by a factor of 6 to only 15 million photos. This is\\napproximately the same size as ImageNet.\\nA major motivation for natural language supervision is the\\nlarge quantities of data of this form available publicly on\\nthe internet. To test this we constructed a new dataset of\\n400 million (image, text) pairs collected form a variety of\\npublicly available sources on the Internet. To attempt to\\ncover as broad a set of visual concepts as possible, we\\nsearch for (image, text) pairs as part of the construction\\nprocess whose text includes one of a set of 500,000 queries.\\nWe approximately class balance the results by including\\nup to 20,000 (image, text) pairs per query. The resulting\\ndataset has a similar total word count as the WebText dataset\\nused to train GPT-2. We refer to this dataset as WIT for\\nWebImageText. 1\\n2.2. Selecting an Efﬁcient Pre-Training Method\\nOur initial approach, similar to VirTex, jointly trained an\\nimage CNN and text transformer from scratch to predict\\nthe caption of an image. However, we encountered difﬁcul-\\nties efﬁciently scaling this method. In Figure 2 we show\\nthat a 63 million parameter transformer language model,\\nwhich already uses twice the compute of its ResNet50 im-\\nage encoder, learns to recognize ImageNet classes three\\ntimes slower than an approach similar to Joulin et al. (2016)\\nthat predicts a bag-of-words encoding of the same text.\\nRecent work in contrastive representation learning has found\\nthat contrastive objectives can outperform the equivalent\\npredictive objective (Tian et al., 2019). Noting this ﬁnding,\\n1The base query list is all words occurring at least 100 times in\\nthe English version of Wikipedia. This is augmented with bi-grams\\nwith high pointwise mutual information for the pair (Church &\\nHanks, 1990) as well as the names of all Wikipedia articles above a\\ncertain search volume. Finally all WordNet (Miller, 1995) synsets\\nnot already in the query list are added.\\nLearning Transferable Visual Models From Natural Language Supervision\\n2M 33M 67M 134M 268M 400M\\n# of images processed\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40Zero-Shot ImageNet Accuracy\\n3X efficiency4X efficiency\\nBag of Words Contrastive (CLIP)\\nBag of Words Prediction\\nTransformer Language Model\\nFigure 2.CLIP is much more efﬁcient at zero-shot transfer\\nthan our image caption baseline. Although highly expressive,\\nwe found that transformer-based language models are relatively\\nweak at zero-shot ImageNet classiﬁcation. Here, we see that it\\nlearns 3x slower than a baseline which predicts a bag-of-words\\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\\nprediction objective for the contrastive objective of CLIP further\\nimproves efﬁciency another 4x.\\nwe explored training a system to solve the potentially eas-\\nier proxy task of predicting only which text as a whole is\\npaired with which image and not the exact words of that text.\\nStarting with the same bag-of-words encoding baseline, we\\nswapped the predictive objective for a contrastive objective\\nin Figure 2, observed a further 4x efﬁciency improvement\\nin the rate of zero-shot transfer to ImageNet.\\nGiven a batch of N (image, text) pairs, CLIP is trained to\\npredict which of the N ×N possible (image, text) pairings\\nacross a batch actually occurred. To do this, CLIP learns a\\nmulti-modal embedding space by jointly training an image\\nencoder and text encoder to maximize the cosine similar-\\nity of the image and text embeddings of the N real pairs\\nin the batch while minimizing the cosine similarity of the\\nembeddings of the N2 −N incorrect pairings. We optimize\\na symmetric cross entropy loss over these similarity scores.\\nIn Figure 3 we include pseudocode for the core of an imple-\\nmentation of CLIP. This batch construction technique and\\nobjective was ﬁrst introduced as the multi-class N-pair loss\\nSohn (2016) and was recently adapted for contrastive (text,\\nimage) representation learning in the domain of medical\\nimaging by Zhang et al. (2020).\\nSince over-ﬁtting is not a major concern, the details of train-\\ning CLIP are simpliﬁed compared to Zhang et al. (2020).\\nWe train CLIP from scratch instead of initializing with pre-\\ntrained weights. We remove the non-linear projection be-\\ntween the representation and the contrastive embedding\\nspace. We use only a linear projection to map from each en-\\ncoder’s representation to the multi-modal embedding space.\\n# image_encoder - ResNet or Vision Transformer\\n# text_encoder  - CBOW or Text Transformer\\n# I[n, h, w, c] - minibatch of aligned images\\n# T[n, l]       - minibatch of aligned texts\\n# W_i[d_i, d_e] - learned proj of image to embed\\n# W_t[d_t, d_e] - learned proj of text to embed\\n# t             - learned temperature parameter\\n# extract feature representations of each modality\\nI_f = image_encoder(I) #[n, d_i]\\nT_f = text_encoder(T)  #[n, d_t]\\n# joint multimodal embedding [n, d_e]\\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\\n# scaled pairwise cosine similarities [n, n]\\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\\n# symmetric loss function\\nlabels = np.arange(n)\\nloss_i = cross_entropy_loss(logits, labels, axis=0)\\nloss_t = cross_entropy_loss(logits, labels, axis=1)\\nloss   = (loss_i + loss_t)/2\\nFigure 3.Numpy-like pseudocode for the core of an implementa-\\ntion of CLIP.\\nWe also remove the text transformation functiontu which\\nsamples a single sentence at uniform from the text since\\nmany of the (image, text) pairs in CLIP’s pre-training dataset\\nare only a single sentence. We also simplify the image trans-\\nformation function tv. A random square crop from resized\\nimages is the only data augmentation used during training.\\nFinally, the temperature parameter which controls the range\\nof the logits in the softmax, τ, is directly optimized during\\ntraining as a log-parameterized multiplicative scalar to avoid\\nturning as a hyper-parameter.\\n2.3. Choosing and Scaling a Model\\nWe consider two different architectures for the image en-\\ncoder. For the ﬁrst, we use ResNet50 (He et al., 2016a)\\nas the base architecture for the image encoder due to its\\nwidespread adoption and proven performance. We make sev-\\neral modiﬁcations to the original version using the ResNetD\\nimprovements from He et al. (2019) and the antialiased\\nrect-2 blur pooling from Zhang (2019). We also replace\\nthe global average pooling layer with an attention pooling\\nmechanism. The attention pooling is implemented as a sin-\\ngle layer of “transformer-style” multi-head QKV attention\\nwhere the query is conditioned on the global average-pooled\\nrepresentation of the image. For the second architecture, we\\nexperiment with the recently introduced Vision Transformer\\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\\nimplementation with only the minor modiﬁcation of adding\\nan additional layer normalization to the combined patch\\nand position embeddings before the transformer and use a\\nslightly different initialization scheme.\\nLearning Transferable Visual Models From Natural Language Supervision\\nThe text encoder is a Transformer (Vaswani et al., 2017)\\nwith the architecture modiﬁcations described in Radford\\net al. (2019). As a base size we use a 12-layer 512-wide\\nmodel with 8 attention heads. The transformer operates on a\\nlower-cased byte pair encoding (BPE) representation of the\\ntext (Sennrich et al., 2015). The text sequence is bracketed\\nwith [SOS] and [EOS] tokens and the activations of the\\nhighest layer of the transformer at the [EOS] token are\\nused as the feature representation of the text which is layer\\nnormalized and then linearly projected into the multi-modal\\nembedding space. Masked self-attention was used in the text\\nencoder to preserve the ability to add language modeling as\\nan auxiliary objective, though exploration of this is left as\\nfuture work.\\nWhile previous computer vision research has often scaled\\nmodels by increasing the width (Mahajan et al., 2018) or\\ndepth (He et al., 2016a) in isolation, for the ResNet image\\nencoders we adapt the approach of Tan & Le (2019) which\\nfound that allocating additional compute across all of width,\\ndepth, and resolution outperforms allocating it to only one\\ndimension. We use a simple variant which allocates addi-\\ntional compute equally to increasing the width, depth, and\\nresolution of the model. For the text encoder, we only scale\\nthe width of the model to be proportional to the calculated\\nincrease in width of the ResNet and do not scale the depth\\nat all, as we found CLIP’s performance to be less sensitive\\nto the text encoder.\\n2.4. Pre-training\\nWe train a series of 5 ResNets and 3 Vision Transformers.\\nFor the ResNets we train a ResNet50, a ResNet101, and\\nthen 3 more which follow EfﬁcientNet-style model scaling\\nand use approximately 4x, 16x, and 64x the compute of a\\nResNet50. They are denoted as RN50x4, RN50x16, and\\nRN50x64 respectively. For the Vision Transformers we\\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. The largest\\nResNet model, RN50x64, took 18 days to train on 592 V100\\nGPUs while the largest Vision Transformer took 12 days on\\n256 V100 GPUs. For the ViT-L/14 we also pre-train at a\\nhigher 336 pixel resolution for one additional epoch to boost\\nperformance similar to FixRes (Touvron et al., 2019). We\\ndenote this model as ViT-L/14@336px. Unless otherwise\\nspeciﬁed, all results reported in this paper as “CLIP” use\\nthis model which we found to perform best. Full model\\nhyperparameters and details are in supplementary material.\\n2.5. Using CLIP\\nCLIP is pre-trained to predict if an image and a text snip-\\npet are paired together in WIT. To apply CLIP to down-\\nstream tasks, we reuse this capability and study the zero-\\nshot transfer performance of CLIP on standard computer\\nvision datasets. Similar to Radford et al. (2019) we motivate\\naYahoo ImageNet SUN\\nVisual N-Grams 72.4 11.5 23.0\\nCLIP 98.4 76.2 58.5\\nTable 1.Comparing CLIP to prior zero-shot transfer image classi-\\nﬁcation work. CLIP improves performance on all three datasets by\\na large amount. This improvement reﬂects many differences since\\nthe development of Visual N-Grams (Li et al., 2017).\\nthis as a way of measuring the task learning capability of a\\nsystem (as opposed to itsrepresentation learningcapability).\\nFor each dataset, we use the names of all the classes in the\\ndataset as the set of potential text pairings and predict the\\nmost probable (image, text) pair according to CLIP. We addi-\\ntionally experiment with providing CLIP with text prompts\\nto help specify the task as well as ensembling multiple of\\nthese templates in order to boost performance. However,\\nsince the vast majority of unsupervised and self-supervised\\ncomputer vision research focuses on representation learning,\\nwe also investigate this for CLIP using the common linear\\nprobe protocol.\\n3. Analysis\\n3.1. Initial Comparison to Visual N-Grams\\nTo our knowledge, Visual N-Grams (Li et al., 2017) ﬁrst\\nstudied zero-shot transfer to existing image classiﬁcation\\ndatasets in the manner described above. It is also the only\\nother work we are aware of that has studied zero-shot trans-\\nfer to standard image classiﬁcation datasets using a task\\nagnostic pre-trained model. In Table 1 we compare Visual\\nN-Grams to CLIP. The best CLIP model improves accuracy\\non ImageNet from a proof of concept 11.5% to 76.2% and\\nmatches the performance of the original ResNet50 despite\\nusing none of the 1.28 million crowd-labeled training exam-\\nples. Additionally, the top-5 accuracy of CLIP models are\\nnoticeably higher and this model has a 95% top-5 accuracy,\\nmatching Inception-V4 (Szegedy et al., 2016). The abil-\\nity to match the performance of a strong, fully supervised\\nbaseline in a zero-shot setting suggests CLIP is a signiﬁ-\\ncant step towards ﬂexible and practical zero-shot computer\\nvision classiﬁers. This comparison is not direct because\\nmany differences between CLIP and Visual N-Grams were\\nnot controlled for. As a closer comparison, we trained a\\nCLIP ResNet50 on the same YFCC100M dataset that Vi-\\nsual N-Grams was trained on and found it matched their\\nreported ImageNet performance within a V100 GPU day.\\nThis baseline was also trained from scratch instead of being\\ninitialized from pre-trained ImageNet weights as in Visual\\nN-Grams.\\nLearning Transferable Visual Models From Natural Language Supervision\\n−40 −30 −20 −10 0 10 20 30 40\\nΔ Score (%)\\nZero-Shot CLIP vs. Linear Probe on ResNet50\\nEuroSAT-37.1\\nKITTI Distance-34.0\\nPatchCamelyon-19.5\\nGTSRB-18.4\\nCLEVRCounts-18.2\\nDTD-16.6\\nFlowers102-12.5\\nRESISC45-11.9\\nFGVCAircraft-11.3\\nMNIST-10.0\\nBirdsnap-3.2\\n+0.5PascalVOC2007\\n+1.1OxfordPets\\n+1.9ImageNet\\n+2.0Caltech101\\n+2.8FER2013\\n+3.0STL10\\n+3.0CIFAR100\\n+3.9CIFAR10\\n+6.7HatefulMemes\\n+7.7UCF101\\n+7.8SUN397\\n+12.4SST2\\n+14.5Kinetics700\\n+22.5Food101\\n+23.2Country211\\n+28.9StanfordCars\\nFigure 4.Zero-shot CLIP is competitive with a fully super-\\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\\nclassiﬁer outperforms a fully supervised linear classiﬁer ﬁtted on\\nResNet50 features on 16 datasets, including ImageNet.\\n3.2. Zero-Shot Performance\\nIn computer vision, zero-shot learning usually refers to the\\nstudy of generalizing to unseen object categories in image\\nclassiﬁcation (Lampert et al., 2009). We instead use the\\nterm in a broader sense and study generalization to unseen\\ndatasets. We motivate this as a proxy for performing un-\\nseen tasks, as aspired to in the zero-data learning paper of\\nLarochelle et al. (2008). While much research in the ﬁeld of\\nunsupervised learning focuses on the representation learn-\\ning capabilities of machine learning systems, we motivate\\nstudying zero-shot transfer as a way of measuring the task-\\nlearning capabilities of machine learning systems. In this\\nview, a dataset evaluates performance on a task on a spe-\\nciﬁc distribution. However, many popular computer vision\\ndatasets were created by the research community primarily\\nas benchmarks to guide the development of generic image\\nclassiﬁcation methods rather than measuring performance\\non a speciﬁc task. To our knowledge, Visual N-Grams (Li\\net al., 2017) ﬁrst studied zero-shot transfer to existing image\\nclassiﬁcation datasets in the manner described above\\nTo conduct a more comprehensive analysis, we implement\\na much larger evaluation suite detailed in the supplementary\\nmaterial. In total we expand from the 3 datasets reported in\\nVisual N-Grams to include over 30 datasets and compare to\\nover 50 existing computer vision systems to contextualize\\nresults. To start, we look at how well CLIP’s zero-shot clas-\\nsiﬁers perform when compared to the a simple off-the-shelf\\nbaseline: ﬁtting a fully supervised, regularized, logistic re-\\ngression classiﬁer on the features of the canonical ResNet50.\\nIn Figure 4 we show this comparison across 27 datasets.\\nZero-shot CLIP outperforms this baseline slightly and wins\\non 16 of the 27 datasets. The dataset zero-shot CLIP im-\\nproves by the most is STL10, a dataset designed to en-\\ncourage unsupervised learning by containing only a limited\\nnumber of labeled examples. Zero-shot CLIP, without using\\nany training examples, achieves 99.3% on this dataset which\\nappears to be a new SOTA. On ﬁne-grained classiﬁcation\\ntasks, we observe a wide spread in performance. On two of\\nthese datasets, Stanford Cars and Food101, zero-shot CLIP\\noutperforms logistic regression on ResNet50 features by\\nover 20% while on Flowers102 and FGVCAircraft, zero-\\nshot CLIP underperforms by over 10%. We suspect these\\ndifferences are primarily due to varying amounts of per-task\\nsupervision between WIT and ImageNet. On “general” ob-\\nject classiﬁcation datasets such as ImageNet, CIFAR10, and\\nPascalVOC2007 performance is relatively similar with a\\nslight advantage for zero-shot CLIP. Zero-shot CLIP sig-\\nniﬁcantly outperforms a ResNet50 on two datasets measur-\\ning action recognition in videos. On Kinetics700, CLIP\\noutperforms a ResNet50 by 14.5%. Zero-shot CLIP also\\noutperforms a ResNet50’s features by 7.7% on UCF101.\\nWe speculate this is due to natural language providing wider\\nsupervision for visual concepts involving verbs, compared\\nto the noun-centric object supervision in ImageNet.\\nLooking at where zero-shot CLIP notably underperforms,\\nwe see that zero-shot CLIP is quite weak on several spe-\\ncialized, complex, or abstract tasks such as satellite image\\nclassiﬁcation (EuroSAT and RESISC45), lymph node tumor\\ndetection (PatchCamelyon), counting objects in synthetic\\nscenes (CLEVRCounts), self-driving related tasks such as\\nGerman trafﬁc sign recognition (GTSRB), recognizing dis-\\ntance to the nearest car (KITTI Distance). These results\\nhighlight the poor capability of zero-shot CLIP on more\\ncomplex tasks. By contrast, non-expert humans can robustly\\nperform several of these tasks, such as counting, satellite\\nimage classiﬁcation, and trafﬁc sign recognition, suggesting\\nsigniﬁcant room for improvement. However, we caution\\nthat it is unclear whether measuring zero-shot transfer, as\\nopposed to few-shot transfer, is a meaningful evaluation for\\ndifﬁcult tasks that a learner has no prior experience with,\\nsuch as lymph node tumor classiﬁcation for almost all hu-\\nmans (and possibly CLIP).\\nWhile comparing zero-shot performance to fully supervised\\nmodels contextualizes the task-learning capabilities of CLIP,\\ncomparing to few-shot methods is a more direct comparison,\\nsince zero-shot is its limit. In Figure 5, we visualize how\\nzero-shot CLIP compares to few-shot logistic regression on\\nLearning Transferable Visual Models From Natural Language Supervision\\n0 1 2 4 8 16\\n# of labeled training examples per class\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75Average Score (%)\\nZero-Shot\\nCLIP\\nLinear Probe CLIP\\nResNet50\\nSimCLRv2\\nBiT-M (ImageNet-21K)\\nFigure 5.Zero-shot CLIP outperforms few-shot linear probes.\\nZero-shot CLIP matches the average performance of a 4-shot linear\\nclassiﬁer trained on the same feature space and nearly matches the\\nbest results of a 16-shot linear classiﬁer across publicly available\\nmodels. For both BiT-M and SimCLRv2, the best performing\\nmodel is highlighted. Light gray lines are other models in the eval\\nsuite. The 20 datasets with at least 16 examples per class were\\nused in this analysis.\\nthe features of many image models including the best pub-\\nlicly available ImageNet models, self-supervised learning\\nmethods, and CLIP itself. While one might expect zero-shot\\nto underperform one-shot, we instead ﬁnd that zero-shot\\nCLIP matches the performance of 4-shot logistic regression\\non the same feature space. This is likely due to a key dif-\\nference between the zero-shot and few-shot approach. First,\\nCLIP’s zero-shot classiﬁer is generated via natural language\\nwhich allows for visual concepts to be directly speciﬁed\\n(“communicated”). By contrast, “normal” supervised learn-\\ning must infer concepts indirectly from training examples.\\nContext-less example-based learning has the drawback that\\nmany different hypotheses can be consistent with the data,\\nespecially in the one-shot case. A single image often con-\\ntains many different visual concepts. Although a capable\\nlearner is able to exploit visual cues and heuristics, such as\\nassuming that the concept being demonstrated is the primary\\nobject in an image, there is no guarantee.\\nWhen comparing zero-shot CLIP to few-shot logistic re-\\ngression on the features of other models, zero-shot CLIP\\nroughly matches the performance of the best performing\\n16-shot classiﬁer in our evaluation suite, which uses the fea-\\ntures of a BiT-M ResNet152x2 trained on ImageNet-21K.\\nWe are certain that a BiT-L model trained on JFT-300M\\nwould perform even better but these models have not been\\npublicly released. That a BiT-M ResNet152x2 performs\\nbest in a 16-shot setting is somewhat surprising since, as\\nanalyzed in Section 3.3, the Noisy Student EfﬁcientNet-L2\\noutperforms it in a fully supervised setting by almost 5% on\\naverage across 27 datasets.\\n3.3. Representation Learning\\nWhile we have focused on studying the task-learning capa-\\nbilities of CLIP through zero-shot transfer, it is more com-\\nmon to study the representation learning capabilities of a\\nmodel. We use a linear probe evaluation protocol because it\\nrequires minimal hyper-parameter tuning and has standard-\\nized evaluation procedures. Please see the supplementary\\nmaterial for further details on evaluation.\\nFigure 6 summarizes our ﬁndings. To minimize selection\\neffects that could raise concerns of conﬁrmation or reporting\\nbias, we ﬁrst study performance on the 12 dataset evalua-\\ntion suite from Kornblith et al. (2019). Models trained with\\nCLIP scale very well with compute and our largest model\\nslightly outperforms the best existing model (a Noisy Stu-\\ndent EfﬁcientNet-L2) on both overall score and compute\\nefﬁciency. We also ﬁnd that CLIP vision transformers are\\nabout 3x more compute efﬁcient than CLIP ResNets, which\\nallows higher overall performance within our compute bud-\\nget. These results replicate the ﬁndings of Dosovitskiy et al.\\n(2020) which reported that vision transformers are more\\ncompute efﬁcient than convnets when trained on sufﬁciently\\nlarge datasets. Our best overall model ViT-L/14@336px\\noutperforms the best existing model across this evaluation\\nsuite by an average of 2.6%.\\nCLIP models learn a wider set of tasks than has previously\\nbeen demonstrated in a single computer vision model trained\\nend-to-end from random initialization. These tasks include\\ngeo-localization, optical character recognition, facial emo-\\ntion recognition, and action recognition. None of these\\ntasks are measured in the evaluation suite of Kornblith et al.\\n(2019). This could be argued to be a form of selection bias\\nin Kornblith et al. (2019)’s study towards tasks that overlap\\nwith ImageNet. To address this, we also measure perfor-\\nmance on a broader 27 dataset evaluation suite. This eval-\\nuation suite, detailed in Appendix A includes datasets rep-\\nresenting the aforementioned tasks, German Trafﬁc Signs\\nRecognition Benchmark (Stallkamp et al., 2011), as well\\nas several other datasets adapted from VTAB (Zhai et al.,\\n2019). On this broader evaluation suite, the beneﬁts of CLIP\\nare more clear. All CLIP models, regardless of scale, outper-\\nform all evaluated systems in terms of compute efﬁciency.\\nThe improvement in average score of the best model over\\nprevious systems increases from 2.6% to 5%.\\n3.4. Robustness to Natural Distribution Shift\\nIn 2015, it was announced that a deep learning model ex-\\nceeded human performance on the ImageNet test set (He\\net al., 2015). However, research in the subsequent years has\\nrepeatedly found that these models still make many simple\\nmistakes (Dodge & Karam, 2017; Geirhos et al., 2018; Al-\\ncorn et al., 2019), and new benchmarks testing these systems\\nhas often found their performance to be much lower than\\nLearning Transferable Visual Models From Natural Language Supervision\\n100 101 102\\nForward-pass GFLOPs/image\\n75\\n80\\n85\\n90Average Score (%)\\nLinear probe average over Kornblith et al.'s 12 datasets\\n100 101 102\\nForward-pass GFLOPs/image\\n70\\n75\\n80\\n85Average Score (%)\\nLinear probe average over all 27 datasets\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\nFigure 6.Linear probe performance of CLIP models in comparison with SOTA computer vision models , including EfﬁcientNet\\n(Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020b), Instagram-pretrained ResNeXt models (Mahajan et al., 2018; Touvron\\net al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020a), BYOL (Grill et al., 2020), and\\nthe original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019). (Right)\\nScores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models ﬁne-tuned or evaluated on\\nimages at a higher-resolution than pre-training. Please see supplementary material for individual model scores for each dataset.\\nboth human accuracy and ImageNet performance (Recht\\net al., 2019; Barbu et al., 2019). Taori et al. (2020) is a re-\\ncent comprehensive study moving towards quantifying and\\nunderstanding this for ImageNet models. Taori et al. (2020)\\nstudy how the performance of ImageNet models change\\nwhen evaluated on natural distribution shifts. They measure\\nperformance on a set of 7 distribution shifts. Taori et al.\\n(2020) ﬁnd that accuracy under distribution shift increases\\npredictably with ImageNet accuracy and is well modeled\\nas a linear function of logit-transformed accuracy. Taori\\net al. (2020) use this ﬁnding to propose that robustness\\nanalysis should distinguish between effective and relative\\nrobustness. Effective robustness measures improvements\\nin accuracy under distribution shift above what is predicted\\nby the documented relationship between in-distribution and\\nout-of-distribution accuracy. Relative robustness captures\\nany improvement in out-of-distribution accuracy. Taori et al.\\n(2020) argue that robustness techniques should aim to im-\\nprove both effective robustness and relative robustness.\\nHowever, almost all models studied in Taori et al. (2020) are\\ntrained or ﬁne-tuned on the ImageNet dataset. Is training\\nor adapting to the ImageNet dataset distribution the cause\\nof the observed robustness gap? Intuitively, a zero-shot\\nmodel should not be able to exploit spurious correlations\\nor patterns that hold only on a speciﬁc distribution, since it\\nis not trained on that distribution. Thus it is possible that\\nzero-shot models exhibit higher effective robustness. In\\nFigure 7, we compare the performance of zero-shot CLIP\\nwith existing ImageNet models on natural distribution shifts.\\nAll zero-shot CLIP models improve effective robustness\\nby a large amount and reduce the gap between ImageNet\\naccuracy and accuracy under distribution shift by up to\\n75%. Zero-shot CLIP models trace a completely distinct\\nrobustness frontier from all 204 prior models studied in\\nTaori et al. (2020). These results suggest that the recent shift\\ntowards large-scale task and dataset agnostic pre-training\\ncombined with a reorientation towards zero-shot transfer\\nevaluation (as advocated by Yogatama et al. (2019) and\\nLinzen (2020)) promotes the development of more robust\\nsystems and provides a more accurate assessment of true\\nmodel performance.\\nLearning Transferable Visual Models From Natural Language Supervision\\n65 70 75 80 85 90 95 100\\nAverage on class subsampled ImageNet (top-1, %)\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n100Average on 7 natural distribution shift datasets (top-1, %)\\nIdeal robust model (y = x)\\nZero-Shot CLIP\\nStandard ImageNet training\\nExisiting robustness techniques\\nImageNet\\nImageNetV2\\nImageNet-A\\nImageNet-R\\nObjectNet\\nImageNet \\nSketch\\n76.2 76.2\\n64.3 70.1\\n2.7 77.1\\n37.7 88.9\\n32.6 72.3\\n25.2 60.2\\nImageNet\\nResNet101\\nZero-Shot\\nCLIP\\n0%\\n+5.8%\\n+74.4%\\n+51.2%\\n+39.7%\\n+35.0%\\nΔ Score\\nDataset Examples\\nFigure 7.Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\\nthis “robustness gap” by up to 75%. Linear ﬁts on logit transformed values are shown with bootstrap estimated 95% conﬁdence intervals.\\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\\nthe best zero-shot CLIP model is compared with a model that has the same performance on the ImageNet validation set, ResNet101.\\n4. Data Overlap Analysis\\nA concern with pre-training on a very large internet dataset\\nis unintentional overlap with downstream evals. We con-\\nducted de-duplication analysis to investigate this with full\\ndetails in the supplementary material. Out of 35 datasets\\nstudied, 9 datasets have no detected overlap at all. There is\\na median overlap of 2.2% and an average overlap of 3.2%.\\nDue to this small amount of overlap, overall accuracy is\\nrarely shifted by more than 0.1% with only 7 datasets above\\nthis threshold. Of these, only 2 are statistically signiﬁcant\\nafter Bonferroni correction. The max detected improve-\\nment is only 0.6% on Birdsnap. This echos the ﬁndings of\\nsimilar duplicate analysis in previous work on large scale\\npre-training. Mahajan et al. (2018) and Kolesnikov et al.\\n(2019) detected similar overlap rates for their models and\\nalso observed minimal changes in overall performance.\\n5. Broader Impacts\\nCLIP allows people to design their own classiﬁers and re-\\nmoves the need for task-speciﬁc training data. How these\\nclasses are designed heavily inﬂuences both model per-\\nformance and model biases. For example, we ﬁnd that\\nwhen given a set of labels including Fairface race labels\\n(K¨arkk¨ainen & Joo, 2019) and a handful of egregious terms\\nsuch as “criminal” and “animal” the model tends to classify\\nimages of people aged 0–20 in the egregious category at a\\nrate of 32.3%. However, when we add the class “child” to\\nthe list of possible classes, this behaviour drops to 8.7%.\\nWe also found discrepancies across gender and race for peo-\\nple categorized into the ‘crime’ and ‘non-human’ categories,\\nhighlighting the potential for disparate impact even when\\nextreme care is taken for thoughtful class design.\\nAdditionally, given that CLIP does not need task-speciﬁc\\ntraining data, it can unlock certain niche tasks with greater\\nease. Some of these tasks may raise privacy or surveillance\\nrelated risks, which we explore by testing CLIP’s perfor-\\nmance on celebrity identiﬁcation using the CelebA dataset\\n(Liu et al., 2018). CLIP has a top-1 accuracy of 59.2% for\\n“in the wild” celebrity image classiﬁcation when choosing\\nfrom 100 candidates and of 43.3% when choosing from\\n1000 possible choices. Although it’s noteworthy to achieve\\nthese results with task agnostic pre-training, this perfor-\\nmance is not competitive with widely available production\\nlevel models. We explore challenges that CLIP poses in our\\nsupplemental materials and hope that this work motivates\\nfuture research on the characterization of the capabilities,\\nshortcomings, and biases of such models.\\n6. Limitations\\nThe performance of zero-shot CLIP is often just compet-\\nitive with the supervised baseline of a linear classiﬁer on\\nResNet-50 features. This baseline is now well below the\\noverall SOTA. Signiﬁcant work is still needed to improve\\nthe task learning and transfer capabilities of CLIP. We es-\\ntimate around a 1000x increase in compute is required for\\nzero-shot CLIP to reach overall SOTA performance across\\nour evaluation suite. This is infeasible to train with cur-\\nrent hardware. Further research into improving upon the\\ncomputational and data efﬁciency of CLIP will be necessary.\\nDespite our emphasis on zero-shot transfer, we repeatedly\\nLearning Transferable Visual Models From Natural Language Supervision\\nqueried performance on validation sets to guide develop-\\nment. This is unrealistic for true zero-shot scenarios. Similar\\nconcerns have been raised in the ﬁeld of semi-supervised\\nlearning (Oliver et al., 2018). Another potential issue is our\\nselection of evaluation datasets. While we report results\\non Kornblith et al. (2019)’s 12 dataset evaluation suite as a\\nstandardized collection, our main analysis uses a somewhat\\nhaphazard collection of 27 datasets that is undeniably co-\\nadapted with the capabilities of CLIP. A new benchmark of\\ntasks designed to evaluate broad zero-shot transfer capabili-\\nties would help address this issue.\\nWe emphasize that specifying image classiﬁers through nat-\\nural language is a ﬂexible interface but this has its own\\nlimitations. Many complex tasks can be difﬁcult to specify\\njust through text. Actual training examples are undeniably\\nuseful but CLIP does not optimize for few-shot performance\\ndirectly. We fall back to ﬁtting linear classiﬁers on top of\\nCLIP’s features. This results in a counter-intuitive drop\\nin performance when transitioning from a zero-shot to a\\nfew-shot setting.\\n7. Related Work\\nThe idea of learning to perform computer vision tasks from\\nnatural language supervision is by no means new. Rather,\\nour main contribution is studying its behavior at large scale.\\nOver 20 years ago Mori et al. (1999) explored improving\\ncontent based image retrieval by training a model to predict\\nthe nouns and adjectives in text paired with images. Quat-\\ntoni et al. (2007) demonstrated it was possible to learn more\\ndata efﬁcient image representations via manifold learning in\\nthe weight space of classiﬁers trained to predict words in im-\\nage captions. Srivastava & Salakhutdinov (2012) explored\\ndeep representation learning by training multimodal Deep\\nBoltzmann Machines on top of low-level image and text tag\\nfeatures. More recent work inspiring CLIP is described in\\nthe Introduction.\\nLearning from collections of internet images is commonly\\ninvestigated in webly supervised learning with Fergus et al.\\n(2005) demonstrating the ability to train competitive com-\\nputer vision classiﬁers by treating image search engine re-\\nsults as supervision. Of this line of work, Learning Every-\\nthing about Anything: Webly-Supervised Visual Concept\\nLearning (Divvala et al., 2014) has a notably similar ambi-\\ntion and goal as CLIP.\\nDevelopments in zero-shot computer vision (Larochelle\\net al., 2008; Lampert et al., 2009) were essential for CLIP.\\nSocher et al. (2013a) demonstrated that connecting image\\nand language representations enabled zero-shot transfer to\\nunseen classes on CIFAR10 and Frome et al. (2013) im-\\nproved and scaled this ﬁnding to ImageNet. The idea of\\ngenerating a classiﬁer from natural language dates back to\\nat least Elhoseiny et al. (2013) and a form similar to CLIP’s\\nzero-shot classiﬁer was explored in Lei Ba et al. (2015).\\nNatural language supervision has also been explored for\\ntasks beyond image classiﬁcation including video under-\\nstanding (Ramanathan et al., 2013; Miech et al., 2019), Re-\\ninforcement Learning (Hermann et al., 2017), and a burst of\\nrecent work on learning joint models of vision and language\\n(Lu et al., 2019; Tan & Bansal, 2019; Chen et al., 2019; Li\\net al., 2020b; Yu et al., 2020) for complex joint tasks beyond\\nthose studied here including visual question answering.\\n8. Conclusion\\nWe have investigated whether it is possible to transfer the\\nsuccess of task-agnostic web-scale pre-training in NLP to\\nanother domain. We ﬁnd that adopting this formula re-\\nsults in similar behaviors emerging in the ﬁeld of computer\\nvision and discuss the social implications of this line of\\nresearch. In order to optimize their training objective, CLIP\\nmodels learn to perform a wide variety of tasks during pre-\\ntraining. This task learning can then be leveraged via natural\\nlanguage prompting to enable zero-shot transfer to many\\nexisting datasets. At sufﬁcient scale, the performance of this\\napproach can be competitive with task-speciﬁc supervised\\nmodels although there is still room for much improvement.\\nACKNOWLEDGMENTS\\nWe’d like to thank the millions of people involved in creating\\nthe data CLIP is trained on. We’d also like to thank Susan\\nZhang for her work on image conditional language models\\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\\nGillian Hadﬁeld for their thoughtful feedback on the broader\\nimpacts section of the paper. We are also grateful to the\\nAcceleration and Supercomputing teams at OpenAI for their\\ncritical work on software and hardware infrastructure this\\nproject used. Finally, we’d also like to thank the developers\\nof the many software packages used throughout this project\\nincluding, but not limited, to Numpy (Harris et al., 2020),\\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\\npandas (pandas development team, 2020), and scikit-learn\\n(Pedregosa et al., 2011).\\nReferences\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\\nTensorﬂow: A system for large-scale machine learning. In\\n12th {USENIX}symposium on operating systems design\\nand implementation ({OSDI}16), pp. 265–283, 2016.\\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi´c,\\nLearning Transferable Visual Models From Natural Language Supervision\\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\\nand Zisserman, A. Self-supervised multimodal versatile\\nnetworks. arXiv preprint arXiv:2006.16228, 2020.\\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\\nS., and Nguyen, A. Strike (with) a pose: Neural networks\\nare easily fooled by strange poses of familiar objects. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 4845–4854, 2019.\\nAssiri, Y . Stochastic optimization of plain convolutional\\nneural networks with simple methods. arXiv preprint\\narXiv:2001.08856, 2020.\\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\\nlarge-scale bias-controlled dataset for pushing the lim-\\nits of object recognition models. In Advances in Neural\\nInformation Processing Systems, pp. 9453–9463, 2019.\\nBechmann, A. and Bowker, G. C. Unsupervised by any\\nother name: Hidden layers of knowledge production in\\nartiﬁcial intelligence on social media. Big Data & Society,\\n6(1):205395171881956, January 2019. doi: 10.1177/\\n2053951718819569. URL https://doi.org/10.\\n1177/2053951718819569.\\nBlaise Aguera y Arcas, M. M. and Todorov,\\nA. Physiognomy’s new clothes. 2017.\\nURL https://medium.com/@blaisea/\\nphysiognomys-new-clothes-f2d4b59fdd6a .\\nBolukbasi, T., Chang, K.-W., Zou, J. Y ., Saligrama, V ., and\\nKalai, A. T. Man is to computer programmer as woman\\nis to homemaker? debiasing word embeddings. Advances\\nin neural information processing systems, 29:4349–4357,\\n2016.\\nBowker, G. C. and Star, S. L. Sorting things out: Classiﬁca-\\ntion and its consequences. MIT press, 2000.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\narXiv preprint arXiv:2005.14165, 2020.\\nBrowne, S. Dark Matters: Surveillance of Blackness. Duke\\nUniversity Press, 2015.\\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\\nvisual representations with caption annotations. arXiv\\ne-prints, pp. arXiv–2008, 2020.\\nBuolamwini, J. and Gebru, T. Gender shades: Intersec-\\ntional accuracy disparities in commercial gender classi-\\nﬁcation. In Conference on fairness, accountability and\\ntransparency, pp. 77–91, 2018.\\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\\nshort note on the kinetics-700 human action dataset.arXiv\\npreprint arXiv:1907.06987, 2019.\\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\\nHinton, G. Big self-supervised models are strong semi-\\nsupervised learners. arXiv preprint arXiv:2006.10029,\\n2020a.\\nChen, X., Fan, H., Girshick, R., and He, K. Improved\\nbaselines with momentum contrastive learning. arXiv\\npreprint arXiv:2003.04297, 2020b.\\nChen, Y .-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\\nCheng, Y ., and Liu, J. Uniter: Learning universal image-\\ntext representations. arXiv preprint arXiv:1909.11740,\\n2019.\\nCheng, G., Han, J., and Lu, X. Remote sensing image scene\\nclassiﬁcation: Benchmark and state of the art. Proceed-\\nings of the IEEE, 105(10):1865–1883, 2017.\\nChurch, K. W. and Hanks, P. Word association norms,\\nmutual information, and lexicography. Computational\\nLinguistics, 16(1):22–29, 1990. URL https://www.\\naclweb.org/anthology/J90-1003.\\nCoates, A., Ng, A., and Lee, H. An analysis of single-\\nlayer networks in unsupervised feature learning. In Pro-\\nceedings of the fourteenth international conference on\\nartiﬁcial intelligence and statistics, pp. 215–223, 2011.\\nCrawford, K. The trouble with bias. NIPS 2017\\nKeynote, 2017. URL https://www.youtube.com/\\nwatch?v=fMym_BKWQzk.\\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\\nIn Advances in neural information processing systems ,\\npp. 3079–3087, 2015.\\nD’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\\nJ., Hoffman, M. D., et al. Underspeciﬁcation presents\\nchallenges for credibility in modern machine learning.\\narXiv preprint arXiv:2011.03395, 2020.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\\nFei, L. ImageNet: A Large-Scale Hierarchical Image\\nDatabase. In CVPR09, 2009.\\nDeng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\\nimage-net.org/challenges/LSVRC/2012/.\\nDesai, K. and Johnson, J. Virtex: Learning visual rep-\\nresentations from textual annotations. arXiv preprint\\narXiv:2006.06666, 2020.\\nLearning Transferable Visual Models From Natural Language Supervision\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nDivvala, S. K., Farhadi, A., and Guestrin, C. Learning\\neverything about anything: Webly-supervised visual con-\\ncept learning. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 3270–\\n3277, 2014.\\nDodge, S. and Karam, L. A study and comparison of human\\nand deep learning recognition performance under visual\\ndistortions. In 2017 26th international conference on\\ncomputer communication and networks (ICCCN), pp. 1–\\n7. IEEE, 2017.\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale.arXiv\\npreprint arXiv:2010.11929, 2020.\\nElhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\\nﬁer: Zero-shot learning using purely textual descriptions.\\nIn Proceedings of the IEEE International Conference on\\nComputer Vision, pp. 2584–2591, 2013.\\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\\ning object categories from google’s image search. In\\nTenth IEEE International Conference on Computer Vision\\n(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\\n2005.\\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\\nRanzato, M., and Mikolov, T. Devise: A deep visual-\\nsemantic embedding model. In Advances in neural infor-\\nmation processing systems, pp. 2121–2129, 2013.\\nGan, Z., Chen, Y .-C., Li, L., Zhu, C., Cheng, Y ., and Liu, J.\\nLarge-scale adversarial training for vision-and-language\\nrepresentation learning. arXiv preprint arXiv:2006.06195,\\n2020.\\nGao, T., Fisch, A., and Chen, D. Making pre-trained lan-\\nguage models better few-shot learners. arXiv preprint\\narXiv:2012.15723, 2020.\\nGarvie, C., May 2019. URL https://www.\\nflawedfacedata.com/.\\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready for\\nautonomous driving? the kitti vision benchmark suite. In\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), 2012.\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\\nmann, F. A., and Brendel, W. Imagenet-trained cnns are\\nbiased towards texture; increasing shape bias improves ac-\\ncuracy and robustness. arXiv preprint arXiv:1811.12231,\\n2018.\\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\\nMirza, M., Hamner, B., Cukierski, W., Tang, Y ., Thaler,\\nD., Lee, D.-H., et al. Challenges in representation learn-\\ning: A report on three machine learning contests. Neural\\nNetworks, 64:59–63, 2015.\\nGoogle. Google cloud api: Celebrity recognition. URL\\nhttps://cloud.google.com/vision/docs/\\ncelebrity-recognition.\\nGrill, J.-B., Strub, F., Altch ´e, F., Tallec, C., Richemond,\\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020.\\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\\nKerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del\\nR´ıo, J., Wiebe, M., Peterson, P., G ´erard-Marchant, P.,\\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\\nGohlke, C., and Oliphant, T. E. Array programming\\nwith NumPy. Nature, 585:357–362, 2020. doi: 10.1038/\\ns41586-020-2649-2.\\nHays, J. and Efros, A. A. Im2gps: estimating geographic\\ninformation from a single image. In 2008 ieee confer-\\nence on computer vision and pattern recognition, pp. 1–8.\\nIEEE, 2008.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\\ninto rectiﬁers: Surpassing human-level performance on\\nimagenet classiﬁcation. In Proceedings of the IEEE inter-\\nnational conference on computer vision, pp. 1026–1034,\\n2015.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016a.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016b.\\nHe, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-\\nmentum contrast for unsupervised visual representation\\nlearning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pp. 9729–\\n9738, 2020.\\nLearning Transferable Visual Models From Natural Language Supervision\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classiﬁcation with convolutional\\nneural networks. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 558–\\n567, 2019.\\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\\nA novel dataset and deep learning benchmark for land\\nuse and land cover classiﬁcation. IEEE Journal of Se-\\nlected Topics in Applied Earth Observations and Remote\\nSensing, 12(7):2217–2226, 2019.\\nHenaff, O. Data-efﬁcient image recognition with contrastive\\npredictive coding. In International Conference on Ma-\\nchine Learning, pp. 4182–4192. PMLR, 2020.\\nHendrycks, D. and Gimpel, K. Gaussian error linear units\\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\\nR., and Song, D. Pretrained transformers improve out-of-\\ndistribution robustness. arXiv preprint arXiv:2004.06100,\\n2020.\\nHermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,\\nSoyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,\\nM., Teplyashin, D., et al. Grounded language learning in\\na simulated 3d world. arXiv preprint arXiv:1706.06551,\\n2017.\\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\\nKianinejad, H., Patwary, M., Ali, M., Yang, Y ., and Zhou,\\nY . Deep learning scaling is predictable, empirically.arXiv\\npreprint arXiv:1712.00409, 2017.\\nHongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\\nEnhancing image geolocalization by combinatorial parti-\\ntioning of maps. In Proceedings of the European Confer-\\nence on Computer Vision (ECCV), pp. 536–551, 2018.\\nHoward, J. and Ruder, S. Universal language model\\nﬁne-tuning for text classiﬁcation. arXiv preprint\\narXiv:1801.06146, 2018.\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\narXiv preprint arXiv:1502.03167, 2015.\\nJaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\\nA. Deep structured output learning for unconstrained text\\nrecognition. arXiv preprint arXiv:1412.5903, 2014.\\nJaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\\ntransformer networks. Advances in neural information\\nprocessing systems, 28:2017–2025, 2015.\\nJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\\nnostic dataset for compositional language and elementary\\nvisual reasoning. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition , pp.\\n2901–2910, 2017.\\nJoulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\\nLearning visual features from large weakly supervised\\ndata. In European Conference on Computer Vision, pp.\\n67–84. Springer, 2016.\\nKalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\\nmodeling in 3d cnn architectures with bert for action\\nrecognition. arXiv preprint arXiv:2008.01232, 2020.\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint arXiv:2001.08361, 2020.\\nKeyes, O. The misgendering machines: Trans/hci implica-\\ntions of automatic gender recognition. Proceedings of the\\nACM on Human-Computer Interaction, 2(CSCW):1–22,\\n2018.\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\narXiv preprint arXiv:2005.04790, 2020.\\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\\nJ., Gelly, S., and Houlsby, N. Large scale learning of\\ngeneral visual representations for transfer. arXiv preprint\\narXiv:1912.11370, 2019.\\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\\nmodels transfer better? In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 2661–2671, 2019.\\nKrishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K.,\\nKravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma,\\nD. A., et al. Visual genome: Connecting language and\\nvision using crowdsourced dense image annotations. In-\\nternational journal of computer vision , 123(1):32–73,\\n2017.\\nK¨arkk¨ainen, K. and Joo, J. Fairface: Face attribute dataset\\nfor balanced race, gender, and age, 2019.\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\\nman, S. J. Building machines that learn and think like\\npeople, 2016.\\nLampert, C. H., Nickisch, H., and Harmeling, S. Learning\\nto detect unseen object classes by between-class attribute\\ntransfer. In 2009 IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 951–958. IEEE, 2009.\\nLarochelle, H., Erhan, D., and Bengio, Y . Zero-data learning\\nof new tasks. 2008.\\nLearning Transferable Visual Models From Natural Language Supervision\\nLeCun, Y . The mnist database of handwritten digits.\\nhttp://yann. lecun. com/exdb/mnist/.\\nLei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\\nzero-shot convolutional neural networks using textual\\ndescriptions. In Proceedings of the IEEE International\\nConference on Computer Vision, pp. 4247–4255, 2015.\\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\\nvisual n-grams from web data. In Proceedings of the\\nIEEE International Conference on Computer Vision, pp.\\n4183–4192, 2017.\\nLi, G., Duan, N., Fang, Y ., Gong, M., and Jiang, D.\\nUnicoder-vl: A universal encoder for vision and language\\nby cross-modal pre-training. 2020a.\\nLi, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\\nL., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\\nsemantics aligned pre-training for vision-language tasks.\\narXiv preprint arXiv:2004.06165, 2020b.\\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:\\nCommon objects in context. In European conference on\\ncomputer vision, pp. 740–755. Springer, 2014.\\nLinzen, T. How can we accelerate progress towards\\nhuman-like linguistic generalization? arXiv preprint\\narXiv:2005.00955, 2020.\\nLippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\\ntimodal framework for the detection of hateful memes.\\narXiv preprint arXiv:2012.12871, 2020.\\nLiu, Z., Luo, P., Wang, X., and Tang, X. Large-scale celeb-\\nfaces attributes (celeba) dataset. Retrieved August, 15\\n(2018):11, 2018.\\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\\ntask-agnostic visiolinguistic representations for vision-\\nand-language tasks. In Advances in Neural Information\\nProcessing Systems, pp. 13–23, 2019.\\nLu, Z., Xiong, X., Li, Y ., Stroud, J., and Ross, D. Leveraging\\nweakly supervised data and pose representation for action\\nrecognition, 2020. URL https://www.youtube.\\ncom/watch?v=KOQFxbPPLOE&t=1390s.\\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,\\nM., Li, Y ., Bharambe, A., and van der Maaten, L. Ex-\\nploring the limits of weakly supervised pretraining. In\\nProceedings of the European Conference on Computer\\nVision (ECCV), pp. 181–196, 2018.\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\\nnatural language decathlon: Multitask learning as ques-\\ntion answering. arXiv preprint arXiv:1806.08730, 2018.\\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\\nI., and Sivic, J. Howto100m: Learning a text-video em-\\nbedding by watching hundred million narrated video clips.\\nIn Proceedings of the IEEE international conference on\\ncomputer vision, pp. 2630–2640, 2019.\\nMiech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\\nman, A. Rareact: A video dataset of unusual interactions.\\narXiv preprint arXiv:2008.01018, 2020a.\\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\\nand Zisserman, A. End-to-end learning of visual represen-\\ntations from uncurated instructional videos. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pp. 9879–9889, 2020b.\\nMiller, G. A. Wordnet: a lexical database for english. Com-\\nmunications of the ACM, 38(11):39–41, 1995.\\nMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\\nof natural distribution shift on question answering models.\\narXiv preprint arXiv:2004.14444, 2020.\\nMishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\\ntion using higher order language priors. 2012.\\nMori, Y ., Takahashi, H., and Oka, R. Image-to-word trans-\\nformation based on dividing and vector quantizing images\\nwith words. Citeseer, 1999.\\nMuller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\\ncation estimation of photos using a hierarchical model\\nand scene classiﬁcation. In Proceedings of the European\\nConference on Computer Vision (ECCV), pp. 563–579,\\n2018.\\nNetzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,\\nand Ng, A. Y . Reading digits in natural images with\\nunsupervised feature learning. 2011.\\nNoble, S. U. Algorithms of oppression: How search engines\\nreinforce racism. 2018.\\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\\ning implicit group attitudes and beliefs from a demonstra-\\ntion web site. Group Dynamics: Theory, Research, and\\nPractice, 6(1):101, 2002.\\nOh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\\nA large-scale benchmark dataset for event recognition in\\nsurveillance video. In CVPR 2011, pp. 3153–3160. IEEE,\\n2011.\\nOliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\\nfellow, I. Realistic evaluation of deep semi-supervised\\nlearning algorithms. Advances in neural information pro-\\ncessing systems, 31:3235–3246, 2018.\\nLearning Transferable Visual Models From Natural Language Supervision\\npandas development team, T. pandas-dev/pandas: Pan-\\ndas, February 2020. URL https://doi.org/10.\\n5281/zenodo.3509134.\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\\nC. V . Cats and dogs. InIEEE Conference on Computer\\nVision and Pattern Recognition, 2012.\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\\nBai, J., and Chintala, S. Pytorch: An imperative style,\\nhigh-performance deep learning library. In Wallach, H.,\\nLarochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E.,\\nand Garnett, R. (eds.), Advances in Neural Information\\nProcessing Systems 32, pp. 8024–8035, 2019.\\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,\\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\\nWeiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-\\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\\nScikit-learn: Machine learning in Python. Journal of\\nMachine Learning Research, 12:2825–2830, 2011.\\nPennington, J., Socher, R., and Manning, C. D. Glove:\\nGlobal vectors for word representation. In Proceedings\\nof the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP), pp. 1532–1543, 2014.\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. arXiv preprint arXiv:1802.05365,\\n2018.\\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\\nA. Imagebert: Cross-modal pre-training with large-\\nscale weak-supervised image-text data. arXiv preprint\\narXiv:2001.07966, 2020.\\nQuattoni, A., Collins, M., and Darrell, T. Learning visual\\nrepresentations using images with captions. In2007 IEEE\\nConference on Computer Vision and Pattern Recognition,\\npp. 1–8. IEEE, 2007.\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by generative pre-\\ntraining, 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\\nRaji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\\nJ., and Denton, E. Saving face: Investigating the ethical\\nconcerns of facial recognition auditing, 2020.\\nRamanathan, V ., Liang, P., and Fei-Fei, L. Video event\\nunderstanding using natural language descriptions. In\\nProceedings of the IEEE International Conference on\\nComputer Vision, pp. 905–912, 2013.\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do im-\\nagenet classiﬁers generalize to imagenet? arXiv preprint\\narXiv:1902.10811, 2019.\\nSalimans, T. and Kingma, D. P. Weight normalization: A\\nsimple reparameterization to accelerate training of deep\\nneural networks. In Advances in neural information pro-\\ncessing systems, pp. 901–909, 2016.\\nScheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\\ncomputers see gender: An evaluation of gender classiﬁca-\\ntion in commercial facial analysis services. Proceedings\\nof the ACM on Human-Computer Interaction, 3(CSCW):\\n1–33, 2019.\\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\\ngender bias in image recognition systems. Socius, 6:\\n2378023120967171, 2020.\\nSennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units. arXiv\\npreprint arXiv:1508.07909, 2015.\\nSingh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X.,\\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\\nmodels that can read. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition, pp.\\n8317–8326, 2019.\\nSocher, R., Ganjoo, M., Manning, C. D., and Ng, A. Zero-\\nshot learning through cross-modal transfer. In Advances\\nin neural information processing systems, pp. 935–943,\\n2013a.\\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\\nC. D., Ng, A. Y ., and Potts, C. Recursive deep models for\\nsemantic compositionality over a sentiment treebank. In\\nProceedings of the 2013 conference on empirical methods\\nin natural language processing, pp. 1631–1642, 2013b.\\nSohn, K. Improved deep metric learning with multi-class\\nn-pair loss objective. In Advances in neural information\\nprocessing systems, pp. 1857–1865, 2016.\\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\\nV oss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\\nMcGufﬁe, K., and Wang, J. Release strategies and the\\nsocial impacts of language models, 2019.\\nLearning Transferable Visual Models From Natural Language Supervision\\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\\nof 101 human actions classes from videos in the wild.\\narXiv preprint arXiv:1212.0402, 2012.\\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/\\n10.5281/zenodo.2591652. Version 5.5.\\nSrivastava, N. and Salakhutdinov, R. Multimodal learning\\nwith deep boltzmann machines. In NIPS, 2012.\\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\\nGerman Trafﬁc Sign Recognition Benchmark: A multi-\\nclass classiﬁcation competition. In IEEE International\\nJoint Conference on Neural Networks , pp. 1453–1460,\\n2011.\\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi,\\nA. Inception-v4, inception-resnet and the impact\\nof residual connections on learning. arXiv preprint\\narXiv:1602.07261, 2016.\\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\\nencoder representations from transformers.arXiv preprint\\narXiv:1908.07490, 2019.\\nTan, M. and Le, Q. V . Efﬁcientnet: Rethinking model\\nscaling for convolutional neural networks. arXiv preprint\\narXiv:1905.11946, 2019.\\nTaori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,\\nand Schmidt, L. Measuring robustness to natural dis-\\ntribution shifts in image classiﬁcation. arXiv preprint\\narXiv:2007.00644, 2020.\\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\\nnew data in multimedia research. Communications of the\\nACM, 59(2):64–73, 2016.\\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\\ncoding. arXiv preprint arXiv:1906.05849, 2019.\\nTian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and\\nIsola, P. Rethinking few-shot image classiﬁcation: a\\ngood embedding is all you need? arXiv preprint\\narXiv:2003.11539, 2020.\\nTouvron, H., Vedaldi, A., Douze, M., and J ´egou, H. Fix-\\ning the train-test resolution discrepancy. In Advances in\\nneural information processing systems, pp. 8252–8262,\\n2019.\\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\\nanalysis and abnormality detection. In 2009 IEEE 12th\\nInternational Conference on Computer Vision Workshops,\\nICCV Workshops, pp. 1338–1345. IEEE, 2009.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in neural information\\nprocessing systems, pp. 5998–6008, 2017.\\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\\nWelling, M. Rotation equivariant CNNs for digital pathol-\\nogy. June 2018.\\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,\\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\\n1.0: Fundamental Algorithms for Scientiﬁc Computing\\nin Python. Nature Methods, 17:261–272, 2020. doi:\\n10.1038/s41592-019-0686-2.\\nV o, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\\ndeep learning era. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pp. 2621–2630,\\n2017.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and anal-\\nysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461, 2018.\\nWang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y ., He,\\nM., Wang, Y ., and Liu, W. All you need is boundary: To-\\nward arbitrary-shaped text spotting. In Proceedings of the\\nAAAI Conference on Artiﬁcial Intelligence, volume 34,\\npp. 12160–12167, 2020.\\nWeyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\\ncation with convolutional neural networks. In European\\nConference on Computer Vision , pp. 37–55. Springer,\\n2016.\\nWu, Y ., Kirillov, A., Massa, F., Lo, W.-Y ., and Gir-\\nshick, R. Detectron2. https://github.com/\\nfacebookresearch/detectron2, 2019.\\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V . Self-training\\nwith noisy student improves imagenet classiﬁcation. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 10687–10698, 2020.\\nYang, Z., Lu, Y ., Wang, J., Yin, X., Florencio, D., Wang,\\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\\npre-training for text-vqa and text-caption. arXiv preprint\\narXiv:2012.04638, 2020.\\nLearning Transferable Visual Models From Natural Language Supervision\\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\\nYu, L., Dyer, C., et al. Learning and evaluating general\\nlinguistic intelligence. arXiv preprint arXiv:1901.11373,\\n2019.\\nYu, F., Tang, J., Yin, W., Sun, Y ., Tian, H., Wu, H.,\\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\\nlanguage representations through scene graph. arXiv\\npreprint arXiv:2006.16934, 2020.\\nZeiler, M. D. and Fergus, R. Visualizing and understand-\\ning convolutional networks. In European conference on\\ncomputer vision, pp. 818–833. Springer, 2014.\\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\\nmann, M., Dosovitskiy, A., et al. A large-scale study of\\nrepresentation learning with the visual task adaptation\\nbenchmark. arXiv preprint arXiv:1910.04867, 2019.\\nZhang, R. Making convolutional networks shift-invariant\\nagain. arXiv preprint arXiv:1904.11486, 2019.\\nZhang, Y ., Jiang, H., Miura, Y ., Manning, C. D., and Lan-\\nglotz, C. P. Contrastive learning of medical visual repre-\\nsentations from paired images and text. arXiv preprint\\narXiv:2010.00747, 2020.\\nZuboff, S. Big other: surveillance capitalism and the\\nprospects of an information civilization. Journal of Infor-\\nmation Technology, 30(1):75–89, 2015.\\n\"}, {'file_name': 'clam', 'text': 'Articles\\nhttps:/ / doi.org/10.1038/s41551-020-00682-w\\n1Department of Pathology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA. 2Cancer Program, Broad Institute of Harvard and \\nMIT, Cambridge, MA, USA. 3Cancer Data Science, Dana–Farber Cancer Institute, Boston, MA, USA. 4Department of Biomedical Informatics, Harvard \\nMedical School, Boston, MA, USA. 5These authors contributed equally: Drew F. K. Williamson, Tiffany Y . Chen. ✉e-mail: faisalmahmood@bwh.harvard.edu\\nA\\ndvances in digital pathology and artificial intelligence have \\npresented the potential to analyse gigapixel whole-slide \\nimages (WSIs) for objective diagnosis, prognosis and \\ntherapeutic-response prediction1,2. Apart from the immediate clini-\\ncal benefits3–6, computational pathology has demonstrated promise \\nin a variety of different tasks for quantifying the tissue microenviron-\\nment7–12, conducting integrative image-omic analysis13–19, identifying \\nmorphological features of prognostic relevance 20,21 and associating \\nmorphologies with response and resistance to treatment22.\\nAlthough deep learning 23,24 has revolutionized medical imag -\\ning by solving many image classification and prediction tasks 25–30, \\nwhole-slide imaging is a complex domain with several unique chal-\\nlenges. Deep-learning-based computational pathology approaches \\nrequire either manual annotation of gigapixel WSIs in fully super-\\nvised settings or large datasets with slide-level labels in a weakly \\nsupervised setting. Given that slide-level labels may only corre-\\nspond to tiny regions of each large gigapixel image, most approaches \\nhave relied on pixel, patch or region-of-interest (ROI)-level annota-\\ntions to saliently localize these ‘needles in a haystack’31–34. Although \\npromising results have been reported by assigning the same label \\nto every patch in a WSI35, this approach suffers from noisy train -\\ning labels and is not applicable to problems that may have limited \\ntumour content (for example, micro-metastasis). Furthermore, if \\nonly a subset of tissue regions in WSIs are sampled for training at the \\nROI or patch-level, the model may not generalize well at test time \\nor provide useful slide-level interpretability. Recent work has dem-\\nonstrated exceptional clinical-grade performance using slide-level \\nlabels for training binary classifiers for patient stratification in a \\nweakly supervised setting36 based on variants of multiple-instance \\nlearning (MIL). However, this methodology was reported to require \\nthousands of WSIs to achieve comparable performance to fully \\nsupervised and ROI-level classifiers. Such large datasets, although \\nimportant and beneficial for capturing the immense diversity and \\nheterogeneity present in histology, are difficult to curate for rare \\ndiagnoses where only a handful of examples may exist or for clinical \\ntrials where it may be useful to predict the outcome from a small \\ncohort of patients. Moreover, to produce a slide-level prediction \\nfrom ROI or patch-level predictions, weakly supervised whole-slide \\nclassification methods commonly require the selection of a fixed, \\npredefined aggregation function (for example, max-pooling or \\naveraging over ROIs) and may not be suitable for both binary \\ntumour versus normal classification and multi-class tissue subtyp-\\ning problems, where normal tissue slides are not available. In addi-\\ntion, the performance of deep-learning diagnostic models, when \\ntrained using patch-level supervision, has been shown to suffer \\nwhen tested on data from different sources and imaging devices35,36. \\nSuch methods also need to be interpretable, with the capability to \\nsaliently localize regions used to make predictive determinations. \\nIn summary, for the broader adaptation of computational pathology \\nin both clinical and research settings, there is a need for methods \\nthat do not require manual ROI extraction, pixel/patch-level label-\\nling or naive sampling, which are still data efficient, interpretable, \\nadaptable and generally applicable to both binary classification and \\nmulti-class subtyping problems.\\nIn this Article, we propose clustering-constrained-attention \\nmultiple-instance learning (CLAM) as a high-throughput \\ndeep-learning framework that aims to address the key challenges \\nwith the whole-slide-level computational pathology outlined \\nabove. In three separate analyses (renal-cell-carcinoma (RCC) and \\nnon-small-cell-lung-cancer (NSCLC) subtyping and the detection \\nof lymph node metastasis) using both publicly available datasets as \\nwell as independent test cohorts, we show that our approach is data \\nefficient and can achieve high performance across different tasks \\nwhile using a systematically decreasing number of training labels. \\nWe demonstrate the adaptability of CLAM by showing that models \\ntrained on tissue resection WSIs can be directly applied to biopsy \\nWSIs as well as photomicrographs taken with a consumer-grade \\nsmartphone using data from independent test cohorts. We also \\nData-efficient and weakly supervised \\ncomputational pathology on whole-slide images\\nMing Y . Lu\\u200a \\u200a1,2,3, Drew F. K. Williamson\\u200a \\u200a1,5, Tiffany Y . Chen\\u200a \\u200a1,5, Richard J. Chen\\u200a \\u200a1,4, Matteo Barbieri1,2 \\nand Faisal Mahmood\\u200a \\u200a1,2,3\\u2009✉\\nDeep-learning methods for computational pathology require either manual annotation of gigapixel whole-slide images (WSIs) \\nor large datasets of WSIs with slide-level labels and typically suffer from poor domain adaptation and interpretability. Here \\nwe report an interpretable weakly supervised deep-learning method for data-efficient WSI processing and learning that \\nonly requires slide-level labels. The method, which we named clustering-constrained-attention multiple-instance learning \\n(CLAM), uses attention-based learning to identify subregions of high diagnostic value to accurately classify whole slides and \\ninstance-level clustering over the identified representative regions to constrain and refine the feature space. By applying CLAM \\nto the subtyping of renal cell carcinoma and non-small-cell lung cancer as well as the detection of lymph node metastasis, we \\nshow that it can be used to localize well-known morphological features on WSIs without the need for spatial labels, that it \\noverperforms standard weakly supervised classification algorithms and that it is adaptable to independent test cohorts, smart-\\nphone microscopy and varying tissue content.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 555\\nArticles NATure BIoMeDICAL eNgINeerINg\\ndemonstrate that CLAM can generalize to multi-class classification \\nand subtyping problems in addition to the binary tumour versus \\nnormal classification tasks typically studied in weakly supervised \\nsettings. Our study presents a computational pathology framework \\nthat extends attention-based multiple-instance aggregation37 to \\ngeneral multi-class weakly supervised WSI classification without \\nrequiring any pixel-level annotation, ROI extraction or sampling. \\nWe make this possible by first using transfer learning and a convolu-\\ntional neural network (CNN) encoder with pre-trained parameters \\nfor dimensionality reduction, which also has the benefit of drasti-\\ncally increasing the speed of model training. Through the use of \\nattention-based learning, CLAM is able to produce interpretable \\nheatmaps that allow clinicians to visualize, for each slide, the relative \\ncontribution and importance of every tissue region to the predic-\\ntions of the model without using any pixel-level annotations during \\ntraining. These heatmaps show that our models are capable of iden-\\ntifying well-known morphological features used by pathologists \\nto make diagnostic determinations, and show that the models are \\ncapable of distinguishing between tumour and adjacent normal tis-\\nsue without any normal slides or ROIs used during training. CLAM \\nis publicly available as an easy-to-use Python package over GitHub \\n(https://github.com/mahmoodlab/CLAM), and whole-slide-level \\nattention maps can be viewed in our interactive demo (http://clam.\\nmahmoodlab.org).\\nCLAM is a deep-learning-based weakly supervised method that \\nuses attention-based learning to automatically identify subregions \\nof high diagnostic value to accurately classify the whole slide, while \\nalso enabling the use of instance-level clustering over the represen-\\ntative regions identified to constrain and refine the feature space. \\nUnder the standard MIL formulation and the weakly supervised \\nlearning paradigm in general, one major challenge in developing \\nhigh-performance machine-learning classifiers for computational \\npathology is the suboptimal usage of labelled WSI data. For example, \\nwhen only the slide-level labels are known, despite having access to \\nmany (up to hundreds of thousands) instances or patches per WSI, \\nthe standard MIL algorithm uses max-pooling and thus uses the \\ngradient signal from only a single instance in each slide to update \\nthe learning parameters of the neural network model. This draw-\\nback might partly explain why, empirically, a deep-learning model \\ntrained using MIL would require the observation of a large number \\nof example WSIs that are annotated at the slide level to achieve high \\nperformance for relatively simple binary classification tasks36. On \\nthe other hand, although assigning the slide-level label to each and \\nevery patch in the slide and treating them as independent training \\nexamples maximizes the number of labelled data points, it might \\nnot benefit model performance as a result of the use of noisy labels.\\nFor whole-slide-level learning without annotation, CLAM uses \\nan attention-based pooling function to aggregate patch-level fea-\\ntures into slide-level representations for classification. At a high \\nlevel, during both training and inference, the model examines and \\nranks all patches in the tissue regions of a WSI, assigning an atten-\\ntion score to each patch, which informs its contribution or impor-\\ntance to the collective slide-level representation for a specific class \\n(Fig. 1). This interpretation of the attention score is reflected in the \\nslide-level aggregation rule of attention-based pooling, which com-\\nputes the slide-level representation as the average of all patches in \\nthe slide weighted by their respective attention score. Unlike the \\nstandard MIL algorithm36–38, which was designed and widely used \\nfor weakly supervised positive/negative binary classification (for \\nexample, cancer versus normal), CLAM is designed to solve generic \\nmulti-class classification problems. A CLAM model has N paral-\\nlel attention branches that together calculate N unique slide-level \\nrepresentations, where each representation is determined from \\na different set of highly attended regions in the image viewed by \\nthe network as strong positive evidence for the one of N classes in  \\na multi-class diagnostic task (Fig. 1b,c). Each class-specific slide \\nrepresentation is then examined by a classification layer to obtain \\nthe final probability score predictions for the whole slide.\\nBeyond adopting the attention-based pooling37 aggregation rule \\nin favour of max-pooling, we explored additional means to address \\nthe data inefficiency in existing weakly supervised learning algo-\\nrithms for computational pathology. Namely, we make use of the \\nslide-level ground-truth label and the attention scores predicted by \\nthe network to generate pseudo labels for both highly and weakly \\nattended patches as a technique to increase the supervisory signals \\nfor learning a separable patch-level feature space. During training, \\nthe network learns from an additional supervised learning task of \\nseparating the most- and least-attended patches of each class into \\ndistinct clusters. In addition, it is possible to incorporate domain \\nknowledge into the instance-level clustering to add further supervi-\\nsion. For example, cancer subtypes are often mutually exclusive or \\nassumed to be mutually exclusive during classification. If the mutual \\nexclusivity assumption is adopted, in addition to supervising the \\nattention branch for which the ground-truth class is present, the \\nattention network branches corresponding to the remaining classes \\ncan be supervised by clustering their highly attended instances as \\n‘false positive’ (that is, negative) evidence for their respective classes. \\nIn practice, if one were to assume that only morphology correspond-\\ning to a single class is present in a given slide, one can also choose to \\nuse a simpler framework of having a single attention module instead \\nof multiple branches by always treating the high-attention patches \\nfrom the attention module as positive evidence for the ground-truth \\nclass and as false positive evidence for the remaining classes when \\ncomputing the clustering loss.\\nTo make CLAM a high-throughput pipeline that research -\\ners can readily adopt and utilize without requiring dedicated \\nhigh-performance compute clusters, we also propose and make \\navailable an open source easy-to-use WSI processing and learn-\\ning toolbox. Our pipeline first automatically segments the tissue \\nregion of each slide and divides it into many smaller patches (for \\nexample, 256 × 256 pixels) so they can serve as direct inputs to a \\nCNN (Fig. 1a). Next, using a CNN for feature extraction, we con-\\nvert all tissue patches into sets of low-dimensional feature embed-\\ndings (Fig. 1b). Following this feature extraction, both training and \\ninference can occur in the low-dimensional feature space instead of \\nthe high-dimensional pixel space. The volume of the data space is \\ndecreased nearly 200-fold and we can drastically reduce the subse-\\nquent computation required to train supervised deep-learning mod-\\nels. We found that working with a low-dimensional feature space \\nenables training models on thousands of gigapixel-sized resection \\nslides within hours on modern workstations with consumer-grade \\nGraphics Processing Units (GPUs).\\nIn the proceeding sections, we demonstrate the data efficiency, \\nadaptability and interpretability of CLAM on three different com-\\nputational pathology problems: (1) RCC subtyping, (2) NSCLC \\nsubtyping and (3) the detection of breast-cancer lymph node metas-\\ntasis. We also show that CLAM models trained on WSIs are adapt-\\nable to smartphone microscopy images and biopsy slides.\\nResults\\nDataset-size dependent, cross-validated model performance. We \\nevaluated the slide-level classification performance of CLAM for \\nthe three clinical diagnostic tasks mentioned above using 10-fold \\nMonte Carlo cross-validation. For each cross-validated fold, we \\nrandomly partitioned each public WSI dataset into a training set \\n(80% of cases), a validation set (10% of cases) and a test set (10% \\nof cases), stratified by each class. In the event that a single case has \\nmultiple slides, all of them are sampled together into the same set. \\nIn each fold, the performance of the model on the validation set is \\nmonitored during training and used for model selection while the \\ntest set is held out and referred to just once after training is com-\\nplete to evaluate the model. On the Cancer Genome Atlas (TCGA) \\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng556\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nRCC dataset (Fig. 2a), the model achieved a 10-fold macro-averaged \\none-versus-rest mean test area under the curve (AUC) ± s.d. \\nof 0.991 ± 0.004 for the three-class RCC subtyping of papillary \\n(PRCC), chromophobe (CRCC) and clear cell RCC (CCRCC) at \\n×20 magnification. For the per subtype one-versus-rest AUC, see \\nSupplementary Fig. 1. For the two-class NSCLC subtyping of lung \\nadenocarcinoma (LUAD) and squamous cell carcinoma (LUSC) \\non the combined TCGA and Clinical Proteomic Tumor Analysis \\nConsortium (CPTAC) NSCLC dataset, at ×20 magnification, the \\nmodel achieved an average test AUC of 0.956 ± 0.020 (Fig. 2b). \\nOn the combined CAMELYON16 and CAMELYON17 dataset for \\nbreast-cancer-metastasis detection in axillary lymph nodes, the \\nmodel achieved an average test AUC of 0.953 ± 0.029 at ×40 mag-\\nnification (Fig. 2c). Additional performance metrics are reported in \\nAttention\\nbackbone\\nMulti-class attention branches\\nPrediction\\nGround-truth label\\nPatch features\\nFeature extraction\\nClass 1\\nClass 2\\nClass N\\nb\\nTissue segmentation WSI patching\\n800 µm\\n3 mm\\nWSIs\\n89,000 × 79,000 pixels\\nInterpretability\\nCCRCC\\nNegative evidencePositive evidence\\nClustering\\nlayer\\nInstance-level clustering\\nClustering\\nloss\\nAttention pooling\\nAttention scoring\\na\\nc d\\n800 µm\\nSlide-level classifier\\nexp(xi)\\nΣj exp(xj)\\nFig. 1 | overview of the CLaM conceptual framework, architecture and interpretability. a, Following segmentation (left), image patches are extracted \\nfrom the tissue regions of the WSI (right). b, Patches are encoded once by a pre-trained Cnn into a descriptive feature representation. During training \\nand inference, the extracted patches in each WSI are passed to a CLAM model as feature vectors. An attention network is used to aggregate patch-level \\ninformation into slide-level representations, which are used to make the final diagnostic prediction. c, For each class, the attention network ranks each \\nregion in the slide and assigns an attention score based on its relative importance to the slide-level diagnosis (left). Attention pooling weighs patches \\nby their respective attention scores and summarizes patch-level features into slide-level representations (bottom right). During training, given the \\nground-truth label, the strongly attended (red) and weakly attended (blue) regions can additionally be used as representative samples to supervise \\nclustering layers that learn a rich patch-level feature space separable between the positive and negative instances of distinct classes (top right). d, The \\nattention scores can be visualized as a heatmap to identify ROIs and interpret the important morphology used for diagnosis.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 557\\nArticles NATure BIoMeDICAL eNgINeerINg\\n50% data (347) 25% data (172) \\n100% data (710) \\n3\\nPC21\\n–2\\n3\\nPC21\\n–1\\n–2 0\\nPC1\\n3 –2 0\\nPC1\\n3\\n75% data (523) \\n50% data (853) 25% data (419) \\n100% data (1,577) 75% data (1,244) \\n50% data (289) 25% data (189)\\n100% data (719) 75% data (619) i\\ng\\nh\\na\\nb\\nc\\ne\\nf\\nd\\n1.0\\n1.00\\n0.75\\n0.50\\n0.80\\n0.40\\n0\\n1.00\\n0.75\\n0.50\\n10\\n(70)\\nPercentage of training set used\\n25\\n(172)\\n50\\n(347)\\n75\\n(523)\\n100\\n(710)\\n0.8\\n0.98\\n0.92\\n0.86\\n0.80\\n0.74\\n0.02\\n100% train data: AUC = 0.991 ± 0.004\\n75% train data: AUC = 0.984 ± 0.010\\n50% train data: AUC = 0.971 ± 0.016\\n0.08 0.14 0.20 0.26\\nCLAM\\nCRCC\\nCCRCC\\nPRCC\\nCRCC\\nCCRCC\\nPRCC\\n3\\nPC20\\n–2\\n3\\nPC21\\n–2\\n–2 1\\nPC1\\n4 –2 0\\nPC1\\n3\\n1\\nPC20\\n–1\\n2\\nPC20\\n–1\\n–2 0\\nPC1\\n3 –2 0\\nPC1\\n3\\nCRCC\\nCCRCC\\nPRCC\\nLUAD\\nLUSC\\nLUAD\\nLUSC\\n1\\nPC20\\n–1\\n1\\nPC20\\n–1\\n–2 0\\nPC1\\n3 –3 0\\nPC1\\n3\\n3\\nPC20\\n–3\\n1\\nPC20\\n–1\\n–2 1\\nPC1\\n3 –2 1\\nPC1\\n4\\nLUAD\\nLUSC\\nNegative\\nPositive\\nNegative\\nPositive\\n2\\nPC20\\n–1\\n2\\nPC20\\n–1\\n–2 0\\nPC1\\n3 –3 1\\nPC1\\n4\\nNegative\\nPositive\\nNegative\\nPositive\\nLUAD\\nLUSC\\nCRCC\\nCCRCC\\nPRCC\\nMIL\\nSL\\nCLAM\\nMIL\\nSL\\nCorrect\\nIncorrect\\n0.6\\nSensitivity\\nTest AUCBalanced errorConfidence\\n1.00\\n0.75\\n0.50\\n0.80\\n0.40\\n0\\n1.00\\n0.75\\n0.50\\n10\\n(138)\\nPercentage of training set used \\n25\\n(419)\\n50\\n(853)\\n75\\n(1,244)\\n100\\n(1,577)\\nCLAM\\nMIL\\nSL\\nCLAM\\nMIL\\nSL\\nCorrect\\nIncorrect\\nTest AUCBalanced errorConfidence\\n1.00\\n0.65\\n0.30\\n0.80\\n0.40\\n0\\n1.00\\n0.75\\n0.50\\n10\\n(100)\\nPercentage of training set used\\n25\\n(189)\\n50\\n(289)\\n75\\n(619)\\n100\\n(719)\\nCLAM\\nMIL\\nSL\\nCLAM\\nMIL\\nSL\\nCorrect\\nIncorrect\\nTest AUCBalanced errorConfidence\\n0.4\\n0.2\\n0\\n0 0.2 0.4\\n1 – Specificity\\n0.6 0.8 1.0\\n1.0\\n0.8\\n0.96\\n0.90\\n0.84\\n0.78\\n0.72\\n0.04\\n100% train data: AUC = 0.956 ± 0.020\\n75% train data: AUC = 0.951 ± 0.019\\n50% train data: AUC = 0.941 ± 0.020\\n0.10 0.16 0.22 0.28\\n0.6\\nSensitivity0.4\\n0.2\\n0\\n0 0.2 0.4\\n1 – Specificity\\n0.6 0.8 1.0\\n1.0\\n0.8\\n0.94\\n0.88\\n0.82\\n0.76\\n0.70\\n0.03\\n100% train data: AUC = 0.953 ± 0.029\\n75% train data: AUC = 0.930 ± 0.044\\n50% train data: AUC = 0.929 ± 0.028\\n0.08 0.14 0.20 0.25\\n0.6\\nSensitivity0.4\\n0.2\\n0\\n0 0.2 0.4\\n1 – Specificity\\n0.6 0.8 1.0\\nFig. 2 | Performance, data efficiency and comparative analysis. a–i, The 10-fold Monte Carlo cross-validation prediction results and test performance \\nof CLAM models are analysed for RCC subtyping (a,d,g; n\\u2009=\\u200986), nSCLC subtyping (b,e,h; n\\u2009=\\u2009196) and the detection of lymph node metastasis (c,f,i; \\nn\\u2009=\\u200989). a–c, Mean test AUC\\u2009±\\u2009s.d. of CLAM models using 100, 75 and 50% of cases in the training set. The confidence band shows ±1\\u2009s.d. for the averaged \\nreceiver-operating-characteristic curve. For multi-class RCC subtyping, the macro-averaged curve and AUC is reported. Insets: zoomed-in view of the \\ncurves. d–f, The dataset-size-dependent performance of various weakly supervised classification algorithms, in terms of the 10-fold test AUCs (top) and \\nbalanced error scores (middle) is shown using box plots for each training-set size (100, 75, 50, 25 and 10% of cases). The boxes indicate the quartile \\nvalues and the whiskers extend to data points within 1.5× of the interquartile range. Mean confidence (±\\u20091\\u2009s.d.) of the predictions made by the CLAM \\nmodels for correctly and incorrectly classified slides (bottom). g–i, Visualization of the learned slide-level feature space for CLAM models; following \\nPCA, the final slide-level feature representation used for the prediction of the model is plotted for each slide in both the validation and test set for a single \\ncross-validated fold. PC, principal component. d–i, The number of slides used for each training-set size is shown in parentheses.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng558\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nSupplementary Tables 1–3. All of our training data are from publicly \\navailable sources, which, although they represent some of the largest \\npublic WSI datasets, are 5–10× smaller than the proprietary labelled \\ndatasets studied in several recent works5,36. However, despite the \\nmoderate sizes of the datasets used (884, 1,967 and 899 total slides, \\nrespectively, of which only approximately 80% are used for training \\nin each fold), the high performance (>0.95 AUC) on all three tasks \\nindicates that our method can be effectively applied to solve both \\nconventional positive-versus-negative cancer detection binary clas-\\nsification and more general multi-class cancer subtyping problems \\nacross a variety of tissue types.\\nLabelled WSI data are often difficult to acquire, and it may not be \\nfeasible to collect thousands of slides in the context of rare diseases \\n(for example, CRCC), unusual findings or clinical trials. In light of \\nthese limitations, to investigate the data efficiency of our models, \\nwe sequentially sampled subsets of training data equal to 75, 50, 25 \\nand 10% of the total number of cases in each training set created \\nduring cross-validation. For each subsampled training set, its cor-\\nresponding test set was kept the same to investigate the dependency \\nof the performance of the model on the amount of training data \\navailable. We also kept each corresponding validation set constant \\nto avoid the introduction of the model selection criteria as an addi-\\ntional confounding variable to the test performance of a model. \\nWhen supervising CLAM models with the smaller sampled subsets \\nof training data, we observed that the number of slides required to \\nachieve satisfactory performance (AUC > 0.9) varies depending on \\nthe classification task. For example, merely 25% of the total avail-\\nable training cases (which represents an average of approximately \\n170 slides in each cross-validated fold) is sufficient to achieve an \\naverage test AUC above 0.94 on RCC subtyping, whereas 25% of the \\nlung training set (419 slides) and 50% of the lymph-node-metastasis \\ndataset (289 slides) might be needed for NSLCC subtyping and \\nthe detection of lymph node metastasis, respectively. Finally, to \\ninvestigate the value of attention pooling over max-pooling, we \\ncompared the performance of CLAM with MIL and the other \\npopular weakly supervised method of naively assuming the same \\nslide-level label for every patch, denoted as ‘same label’ (SL). We \\nimplemented a multi-class variant of MIL for three-class RCC sub-\\ntyping, which we denote mMIL (see Methods for technical details). \\nIn our comparative study we found that CLAM consistently out-\\nperforms the max-pooling-based algorithms for all tasks and \\ntraining-set sizes (Fig. 2d–f). The AUC difference between CLAM, \\nmax-pooling-based algorithms and SL are more pronounced when \\nfewer slides are used for training. For example, SL demonstrates a \\nreasonable performance for RCC subtyping at 100 and 75% of train-\\ning data, probably because of the high tumour content present in the \\nTCGA RCC dataset, which means most of the training labels used \\nby SL will be correct when assigning the slide-level diagnosis to all \\nregions in each WSI. On the other hand, SL performs poorly in the \\ndetection of lymph node metastasis, given that the areas of metas-\\ntasis can be small and sparse, which leads to a high amount of label \\nnoise when naively assigning the slide-level label to every location \\nof tissue in each slide. Overall, we note that CLAM is data efficient, \\nas it is often able to achieve test AUC > 0.9 using only several hun-\\ndred slides for training. To investigate whether the additional task \\nof instance-level clustering in CLAM contributes to the increased \\ndata efficiency, we conducted ablation studies for all disease models \\nacross training sets of different sizes and observed that the addi-\\ntional instance-level supervision improves model performance over \\nusing bag-level supervision alone when the training-set size is small \\n(Supplementary Table 4).\\nWe also conducted experiments to assess the performance of dif-\\nferent algorithms under data constraint using 60/10/30 and 40/10/50 \\npartitions instead of 80/10/10 train/validate/test partitions, which \\nallows for model evaluation on larger test sets (Supplementary Table \\n5). To enable comparisons with future studies, we conducted addi-\\ntional experiments using the publicly available TCGA, CPTAC and \\nCAMELYON datasets (see Supplementary Table 6 for details).\\nFurthermore, we analysed the performance of CLAM in the \\ncontext of the larger body of related works (Supplementary Table 7)  \\nevaluated on the public datasets that we used for the three dif -\\nferent diagnostic tasks. First, we applied CLAM to the public \\nCAMELYON16 lymph-node-metastasis detection challenge. \\nWe trained on the official training set (without using any of the \\npixel-level annotation provided) after splitting the 270 WSIs into \\napproximately 85% training and 15% validation. Our best model \\nachieved a test AUC of 0.936 (95% confidence interval (CI): 0.890–\\n0.983) on the official test set of 129 WSIs. This is an encouraging \\nresult given that no pixel-level labels were used during training. \\nSimilarly, we trained a CLAM model to perform NSCLC subtyp -\\ning on just TCGA diagnostic WSIs, where 15% of cases (80 LUAD \\nand 81 LUSC WSIs) were held out as the test set and the remaining \\ndata were divided into 85% training and 15% validation. This model \\nachieved a test AUC of 0.963 (95% CI: 0.937–0.990).\\nGeneralization to independent test cohorts. Due to differences \\nin institutional standards and protocols for tissue processing, slide \\npreparation and digitization, WSIs can vary greatly in image appear-\\nance. Therefore, it is important to validate that models trained under \\nthe CLAM weakly supervised framework using publicly available \\ndata sources of a moderate size are robust to data-specific variables \\nand generalize to real-world clinical data from scanners and stain-\\ning protocols that are not encountered during training. We col -\\nlected and scanned a total of 135 RCC (CRCC, 43; CCRCC, 46; and \\nPRCC, 46), 131 NSCLC (LUAD, 63; and LUSC, 68) and 133 lymph \\nnode (negative, 66; and positive, 67) whole slides at the Brigham \\nand Women’s Hospital (BWH) as independent test cohorts to evalu-\\nate the generalization performance of our trained models (further \\nexplained in Methods and Supplementary Table 8). For each task \\nand training-set size, the ten models trained during cross-validation \\non our public datasets were directly evaluated on the completely \\nheld-out independent test set. We observed that for smaller denom-\\ninations of the training set, the variance in the cross-validation \\nperformance of different models were often much higher, in which \\ncase testing using a single best-performing model may give the illu-\\nsion of data efficiency although the performance of the algorithm \\non the independent test set would be inconsistent and vary highly \\nacross models developed using different random splits of training \\ndata. To accommodate this, we used the average performance of \\nall ten models (instead of a single selected model) to estimate the \\nperformance of our algorithm for each training-set size. When test-\\ning on independent test cohorts, the 10-fold cross-validated CLAM \\nmodels trained using 100% of the training set achieved an average \\none-versus-rest AUC (macro-averaged) of 0.972 ± 0.008 on RCC \\nsubtyping, and an average AUC of 0.975 ± 0.007 for NSCLC sub-\\ntyping and 0.940 ± 0.015 for the detection of axillary lymph node \\nmetastasis (Fig. 3a–c). In addition, we observed that even CLAM \\nmodels trained on the smaller subsets of the full training set can \\nachieve respectable performance (test AUC > 0.9) on data from \\nindependent sources after learning from just hundreds of slides \\n(Fig. 3d–f). When compared with mMIL/MIL and SL, CLAM \\ndelivered improved performance across all tasks and training-set \\nsizes (Fig. 3d–f, top and middle) and especially when constrained \\nby limited training data. For example, when trained with 25% of the \\nfull training set, CLAM outperformed MIL/mMIL by 14.2, 5.77 and \\n29.2% in average test AUC on RCC subtyping, NSCLC subtyping \\nand the detection of lymph node metastasis, respectively, and simi-\\nlarly outperformed SL by 7.32, 16.6 and 29.7% in these same respec-\\ntive experiments (for comparisons using additional classification \\nmetrics, see Supplementary Table 9–11). In addition, we observed \\nthat CLAM models became on average less confident as the size of \\nthe training set was reduced (Fig. 3d–f, bottom), which is in general \\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 559\\nArticles NATure BIoMeDICAL eNgINeerINg\\n50% data (347) 25% data (172) \\n100% data (710) 75% data (523) \\n50% data (853) 25% data (419) \\n100% data (1,577) 75% data (1,244) \\n50% data (289) 25% data (189) \\n100% data (719) 75% data (619) i\\na\\nc\\nd g\\nheb\\nf\\n1.0\\n0.8\\n0.97\\n0.91\\n0.85\\n0.79\\n0.73\\n0.02 0.08 0.14 0.20 0.26\\n0.6\\nSensitivity0.4\\n0.2\\n0\\n0 0.2 0.4\\n1 – Specificity\\n0.6 0.8 1.0\\n1.0\\n0.8 0.98\\n0.92\\n0.86\\n0.80\\n0.74\\n0.02 0.08 0.14 0.20 0.26\\n0.6\\nSensitivity0.4\\n0.2\\n0\\n0 0.2 0.4\\n1 – Specificity\\n0.6 0.8 1.0\\n1.0\\n0.8 0.93\\n0.87\\n0.81\\n0.75\\n0.69\\n0.04 0.10 0.16 0.22 0.28\\n0.6\\nSensitivity0.4\\n0.2\\n0\\n0 0.2 0.4\\n1 – Specificity\\n0.6 0.8 1.0\\n1.00\\n0.75\\n0.50\\n0.80\\n0.40\\n0\\n1.00\\n0.75\\n0.50\\n10\\n(70)\\nPercentage of training set used\\n25\\n(172)\\n50\\n(347)\\n75\\n(523)\\n100\\n(710)\\nCLAM\\nMIL\\nSL\\nCLAM\\nMIL\\nSL\\nCorrect\\nIncorrect\\nTest AUCBalanced errorConfidence\\n1.00\\n0.75\\n0.50\\n0.80\\n0.40\\n0\\n1.00\\n0.75\\n0.50\\n10\\n(138)\\nPercentage of training set used\\n25\\n(419)\\n50\\n(853)\\n75\\n(1,244)\\n100\\n(1,577)\\nCLAM\\nMIL\\nSL\\nCLAM\\nMIL\\nSL\\nCorrect\\nIncorrect\\nTest AUCBalanced errorConfidence\\n1.00\\n0.65\\n0.30\\n0.80\\n0.40\\n0\\n1.00\\n0.75\\n0.50\\n10\\n(100)\\nPercentage of training set used\\n25\\n(189)\\n50\\n(289)\\n75\\n(619)\\n100\\n(719)\\nCLAM\\nMIL\\nSL\\nCLAM\\nMIL\\nSL\\nCorrect\\nIncorrect\\nTest AUCBalanced errorConfidence\\n2\\nPC20\\n–2\\n2\\nPC20\\n–1\\n–2 0\\nPC1\\n3 –3 0\\nPC1\\n3\\nCRCC\\nCCRCC\\nPRCC\\nCRCC\\nCCRCC\\nPRCC\\n2\\nPC20\\n–2\\n2\\nPC20\\n–2\\n–3 1\\nPC1\\n4 –3 0\\nPC1\\n3\\n1\\nPC20\\n–1\\n1\\nPC20\\n–1\\n–2 0\\nPC1\\n2 –1 0\\nPC1\\n2\\nCRCC\\nCCRCC\\nPRCC\\nLUAD\\nLUSC\\nLUAD\\nLUSC\\n1\\nPC20\\n–1\\n1\\nPC20\\n–1\\n–2 0\\nPC1\\n2 –2 0\\nPC1\\n3\\n2\\nPC20\\n–2\\n3\\nPC20\\n–2\\n–3 0\\nPC1\\n3 –3 0\\nPC1\\n4\\nLUAD\\nLUSC\\nNegative\\nPositive\\nNegative\\nPositive\\n2\\nPC20\\n–1\\n3\\nPC21\\n–1\\n–2 0\\nPC1\\n3 –2 1\\nPC1\\n4\\nNegative\\nPositive\\nNegative\\nPositive\\nLUAD\\nLUSC\\nCRCC\\nCCRCC\\nPRCC\\n100% train data: AUC = 0.972 ± 0.008\\n75% train data: AUC = 0.973 ± 0.008\\n50% train data: AUC = 0.952 ± 0.011\\n100% train data: AUC = 0.975 ± 0.007\\n75% train data: AUC = 0.970 ± 0.013\\n50% train data: AUC = 0.969 ± 0.009\\n100% train data: AUC = 0.940 ± 0.015\\n75% train data: AUC = 0.922 ± 0.014\\n50% train data: AUC = 0.875 ± 0.015\\nFig. 3 | adaptability to independent test cohorts. a–i, Independent test cohorts from BWH for RCC (a,d,g), nSCLC (b,e,h) and lymph node metastasis \\n(c,f,i) are used to assess and analyse the capability of CLAM models trained on public datasets to generalize to new data sources that are not encountered \\nduring training. a–c, Performance of the CLAM model in terms of 10-fold mean test AUCs\\u2009±\\u2009s.d. for RCC subtyping (n\\u2009=\\u2009135), nSCLC subtyping (n\\u2009=\\u2009131) \\nand the detection of lymph node metastasis (n\\u2009=\\u2009133). Insets: zoomed-in view of the curves. d–f, For each training-set size, the test AUCs (top) and \\nbalanced error scores (middle) of ten models are reported for CLAM, MIL (mMIL for RCC subtyping) and SL using box plots. The boxes indicate the \\nquartile values and the whiskers extend to data points within 1.5×\\u2009of the interquartile range. The results demonstrate that CLAM models can generalize \\nto new data sources after training on a limited number of labelled slides and outperform other weakly supervised baselines with high consistency. Mean \\nconfidence (±1\\u2009s.d.) of CLAM model predictions for correctly and incorrectly classified slides (bottom). In general, CLAM models become less confident \\nwhen trained using fewer data. g–i, Visualization of the slide-level feature space in two dimensions for select models from different training-set sizes.  \\nd–i, The number of slides used for each training-set size is shown in parentheses.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng560\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nmore desirable than having inaccurate but overly confident models \\nthat severely and erroneously overfit on the small training set that \\nthey observe.\\nFor NSCLC and RCC subtyping, the models trained on the \\npublic datasets of TCGA and CPTAC must also adapt to the dif -\\nferent micrometre-per-pixel (m.p.p.) resolution produced by the \\nin-house Hamamatsu scanner compared with the Aperio scan-\\nners used to digitize the training data. Whereas the vast majority \\nof WSIs from TCGA RCC and NSCLC and CPTAC NSCLC had \\na ×20 equivalent m.p.p. close to 0.5, the in-house WSIs had a ×20 \\nequivalent m.p.p. of 0.44. On the in-house NSCLC lung dataset, we \\nalso tested a mechanism to standardize the resolution at test time \\nby downscaling the image patches to an approximate m.p.p. of 0.5 \\nbefore they were embedded by the CNN encoder during data pro-\\ncessing. However, we only observed a small improvement in mean \\ntest AUC to 0.979 ± 0.005 when using this technique. To further \\ninvestigate the impact of variability introduced by different scan-\\nner hardware, we digitized all of the in-house lung-resection slides \\nusing an additional 3DHistech MiraxScan 150 scanner, which pro-\\nduces an m.p.p. of 0.328. We found that our models were able to \\nachieve an average test AUC of 0.910 ± 0.022 when evaluating on \\nthe native scanning resolution of the new scanner despite the dras-\\ntic difference in the m.p.p. resolution of the 3DHistech scanner in \\ncomparison to the Aperio scanners used to digitize the public train-\\ning data (Supplementary Fig. 2). On the other hand, by standard-\\nizing the image patches from the 3DHistech scans to an m.p.p. of \\n0.5, we improved the test AUC to 0.965 ± 0.006. These results rea-\\nsonably demonstrate that our proposed weakly supervised learning \\nframework is quite robust to variation in scanner hardware but also \\nillustrates the potential importance of m.p.p. standardization when \\nevaluating on slides from new data sources, especially when the \\nm.p.p. difference between the training data and test data is large.\\nOverall, the results from our study are highly encouraging and \\nserve as supporting evidence that using CLAM, datasets of a moder-\\nate size curated from multiple institutions (with source-specific vari-\\nability) and a diverse patient distribution (for example, TCGA) are \\nsufficient to develop accurate, weakly supervised computer-aided \\ndiagnostic models capable of generalization. For best performance \\nduring real-world clinical deployment, we additionally propose to \\nensemble the diagnostic predictions from multiple models instead \\nof selecting a single model. This is computationally inexpensive to \\naccomplish as we only have to perform feature extraction on our \\ndata once, unlike methods that require tuning a feature encoder for \\neach model. The ensemble performance (with 95% CI) of trained \\nCLAM models on all independent test cohorts is demonstrated in \\nSupplementary Fig. 3 and Supplementary Tables 12–14.\\nInterpretability and whole-slide attention visualization. Human- \\nreadable interpretability of the trained weakly supervised \\ndeep-learning classifier can validate that the predictive basis of \\nthe model aligns with well-known morphology used by patholo-\\ngists and can also be used to analyse failure cases. In addition, \\nwhole-slide-level heatmaps can be used for artificial-intelligence- \\nassisted human-in-the-loop clinical diagnoses. A CLAM model \\nmakes its slide-level prediction by first identifying and aggre-\\ngating regions in the WSI that are of high diagnostic importance \\n(high attention score) while ignoring regions of low diagnostic rel-\\nevance (low attention score). To visualize and interpret the relative \\nimportance of each region in the WSI, we can generate an atten-\\ntion heatmap by converting the attention scores for the predicted \\nclass of the model into percentiles and mapping the normalized \\nscores to their corresponding spatial location in the original slide. \\nFine-grained attention heatmaps can be created using overlap -\\nping patches (for example, 95% overlap) and averaging the atten-\\ntion scores in the overlapped regions (see Supplementary Fig. 4  \\nfor a discussion on the visual quality of heatmaps for different \\ndegrees of overlap). Although pixel-level or patch-level annota -\\ntion was never used during training to explicitly inform the model \\nwhether each region is tumour tissue (and, if so, which subtype of \\ntumour), we observed that through weakly supervised learning \\nusing slide-level labels only, trained CLAM models are generally \\ncapable of delineating the boundary between tumour and normal \\ntissue (Fig. 4a–c; see the interactive demo at http://clam.mah-\\nmoodlab.org for high-resolution heatmaps). This is an especially \\nwelcoming property given that for RCC and NSCLC subtyping, \\nall training data collected from the TCGA are positive cases and \\ncontain tumour regions. The finding demonstrates that CLAM has \\nthe potential to be used towards meaningful whole-slide-level inter-\\npretability and visualization in cancer subtyping problems for clini-\\ncal or research purposes, without the need to observe negative cases \\nduring training (which would require either collecting slides from \\nadjacent normal tissue or manual annotation of negative regions in \\npositive slides). Of equal importance, high-attention regions gen-\\nerally correspond with morphology already established and recog-\\nnized by pathologists for all of the three classification tasks studied  \\n(Fig. 4a–c). For example, the CLAM model trained for NSCLC sub-\\ntyping highlights prominent intercellular bridges and keratiniza -\\ntion, and uses them as strong evidence (high attention) for LUSC \\n(Fig. 4b), in concordance with human pathology expertise. In addi-\\ntion, we examined the attention heatmap of the model with cor -\\nresponding cytokeratin (AE1/AE3) immunohistochemical staining \\nto further validate its predictive basis for a representative case of \\nlymph node metastasis (Supplementary Fig. 5). These heatmaps \\ncan also be used to analyse and investigate misclassified slides. We \\nobserved challenging cases in our in-house independent test data in \\nwhich the high-attention patches selected by the model for predic-\\ntion failed to clearly indicate the correct class due to poor differen-\\ntiation in the tumour cells or the limited presence of contextual cues \\nto delineate the tumour architecture (Supplementary Fig. 6). For the \\ndetection of lymph node metastasis, false positive predictions typi-\\ncally highlighted large epithelioid histiocytes that mimic tumour \\ncells to some degree, whereas false negatives tended to result from \\nsmall isolated clusters of tumour cells in micro-metastases and \\nisolated tumour cells. Despite their practical usefulness, caution \\nshould be taken to not overly rely on the attention heatmaps with \\nthe expectation that they can serve as pixel-perfect segmentation \\nmasks; intuitively, the attention scores for each region in the slide \\nare relative and simply represent the interpretation by the model \\nof which regions are more important (relative to others) in deter-\\nmining the slide-level prediction. Nonetheless, this simple and \\nintuitive interpretability and visualization technique can provide \\nresearchers insight into the morphological patterns driving the \\npredictions of the model; after further quantitative investigation, \\nwe also found that the attention heatmaps exhibit a high level of \\nagreement with the pathologist annotations of tumour regions \\nacross all tasks when evaluated on our in-house resection slides  \\n(Supplementary Fig. 7).\\nAs a means of enhanced interpretability, we further investi -\\ngated the patch-level feature space learned by the CLAM models. \\nWe randomly sampled a subset of patches from each slide in the \\nindependent test cohorts, reduced their learned instance-level \\n512-dimensional feature representations into two dimensions \\nusing principal component analysis (PCA) and examined their \\nclass predictions assigned by the clustering layers of the network \\n(Supplementary Fig. 8). For the RCC and NSCLC slides, patches \\nof different predicted classes are separated into distinct clusters in \\nthe feature space and exhibit morphology characteristic of their \\nrespective subtype. For the detection of axillary lymph node metas-\\ntasis, sampled patches predicted as the positive cluster include \\ntumour tissue, whereas negative (agnostic) patches capture a wide \\narray of morphologies, including normal tissue and dense immune  \\ncell populations.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 561\\nArticles NATure BIoMeDICAL eNgINeerINg\\nEosinophilic or pale \\ncytoplasm\\nPlant-cell-wall-like \\ncytoplasmic membrane \\nRaisinoid nuclei and \\nbinucleiation\\na\\n3 mm\\nVoluminous clear \\ncytoplasm\\nAlveolar or acinar-like \\npattern\\nThin-walled ‘chicken\\nwire’ vasculature\\nHigh attention\\nLow attention\\nb\\nIntracellular mucin \\ndroplets\\nLumen formation Nuclear atypia and \\nprominent nucleoli\\nc\\nAccumulation of foamy \\nmacrophages\\nPapillary cores lined by \\nneoplastic cells\\nTubulopapillary \\narchitecture\\nProminent intercellular \\nbridges\\nNuclear atypia, \\npleomorphism\\nKeratinization and \\natypical polygonal cells\\nLarger epithelioid cells with nuclear irregularity and increased \\ncytoplasm in a background of small lymphocytes\\n95 µm\\nCRCC CCRCCPRCCLUADLUSCLymph node metastasis\\nFig. 4 | interpretability and visualization. a,b, For RCC (a) and nSCLC (b) subtyping, a representative slide from each subtype was annotated by a \\npathologist (left), who roughly highlighted the tumour tissue regions. c, Similarly, regions of metastasis are highlighted for a case of lymph node metastasis \\n(left). a–c, A whole-slide attention heatmap corresponding to each slide was generated by computing the attention scores for the predicted class of the \\nmodel over patches tiled with a spatial overlap of 25% (second column); the fine-grained ROI heatmap, which highlights parts of the tumour normal \\nboundary, was generated using a 95% overlap and overlaid onto the original H&e image (third column; zoomed-in view of the regions in the black squares in \\nthe images to its left). Patches of the most highly attended regions (red border) generally exhibit well-known tumour morphology and low-attention patches \\n(blue border) include normal tissue among different background artefacts (right). Green arrows highlight specific morphology corresponding to the textual \\ndescription. High-resolution WSIs and heatmaps corresponding to these slides may be viewed in our interactive demo (http:/ / clam.mahmoodlab.org).\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng562\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nAdaptability to smartphone microscopy images. We also explored \\nthe ability of our models (which are trained exclusively on WSIs) \\nto directly adapt to microscopy images captured using a smart-\\nphone camera (commonly known as photomicrographs). In \\nresource-constrained areas with limited access to pathologist exper-\\ntise, consult cases are often imaged using a smartphone attached \\nto a conventional microscope39. Training a deep-learning classifier \\nspecifically based on smartphone microscopy images would prob-\\nably require the time-consuming and laborious process of manually \\ncurating a large set of labelled ROIs. These ROIs should not only \\nbe representative of the underlying pathological conditions but also \\ncapture a wide range of tissue-site and patient-specific appearances \\nand artefacts to ensure that the model can adapt to the heterogeneity \\ninherently found in histopathology slides and WSIs. A robust model \\ntrained on WSIs that is capable of directly adapting to cellphone \\nimages (CPIs) and deliver accurate automated diagnosis is there-\\nfore of tremendous value to the wider adoption of telepathology. \\nAs part of our model adaptability study, 4–8 fields of view (FOVs) \\nfrom each slide in our independent test cohorts were captured using \\na consumer-grade iPhone X smartphone camera and patches from \\nall FOV ROIs were collectively used by the model to predict the \\nslide-level label. A variable number of FOVs were selected from \\neach slide to cover the necessary tissue area relevant for diagnosis. \\nCLAM achieved an average test AUC of 0.873 ± 0.025 on the NSCLC \\nCPI dataset and an average one-versus-rest macro-averaged AUC of \\n0.921 ± 0.023 on the RCC CPI dataset (Fig. 5b,c and Supplementary \\nTables 15 and 16). The drop in performance compared with testing \\non WSIs (Fig. 5d) can probably be attributed to the imperfect con-\\nditions under which CPIs are captured (poor focus, non-uniform \\nillumination, noise artefacts, vignetting, colour shift, magnifica-\\ntion changes and so on). Although some of these adversities can \\npotentially be reduced through the use of both conventional and \\ndeep-learning-based image-processing techniques (for example, \\nstain normalization based on deep convolutional adversarial gen-\\nerative modelling40), we did not attempt to correct or normal -\\nize the images so as to test the robustness and adaptability of our \\nmodels and keep the processing time and computational cost low \\nto potentially allow inference directly on smartphone hardware. \\nDespite these challenging variables, we found that in most cases, \\nthe model still accurately attends to regions in the FOV that exhibit \\nwell-known morphology characteristics of each cancer subtype \\n(Fig. 5e,f). Furthermore, different classes are still visibly separated \\ninto distinct clusters in the feature space that the model has learned \\nfrom WSIs (Fig. 5g,h). These results instil confidence with regards \\nto the potential wider applicability of our weakly supervised learn-\\ning framework to the telepathology domain.\\nAdapting networks trained on resections to biopsies. The publicly \\navailable WSIs that we used for training in our study are all resec-\\ntions. Compared with resected tissue, core-needle-biopsied tissue \\nis generally substantially smaller in size. The limited tissue content \\nas well as the presence of cell distortion due to crush artefact can \\nchallenge the diagnostic ability of the model. Accordingly, given \\nthat we did not use biopsy slides during training, it was impor -\\ntant to investigate whether models trained solely on resections can \\nadapt directly to biopsy slides and make accurate diagnostic pre-\\ndictions. We collected 110 lung (55 LUAD and 55 LUSC) and 92 \\nkidney biopsy slides (53 CCRCC, 26 PRCC and 13 CRCC) at BWH \\nas our independent test cohorts and directly tested our models that \\nhad been trained on the publicly available resection data. Each \\nslide contains a variable number of embedded biopsy specimens, \\nranging from one to six for lung-biopsy WSIs and one to five for \\nkidney-biopsy WSIs (Supplementary Table 17). For each WSI, tissue \\nregions from all biopsy specimens embedded in the slide are pro-\\nvided to the model as input to make a single prediction for evalua-\\ntion at the WSI level. On the lung-biopsy test set, CLAM achieved an \\naverage AUC of 0.902 ± 0.016 and on the kidney-biopsy test set, the \\naverage one-versus-rest macro-averaged test AUC was 0.951 ± 0.011 \\n(Fig. 6b,c and Supplementary Tables 18,19). These results are highly \\nencouraging because many biopsy slides, especially in the case of \\nthe lung-biopsy dataset, contained poorly differentiated tumours, \\nwhich make them extremely difficult or impossible for pathologists \\nto accurately diagnose based on the haematoxylin and eosin (H&E) \\nstains alone (without immunohistochemistry). In addition, to assess \\nthe applicability of our models to potential real-world fully auto-\\nmated computer-aided diagnosis, when testing on biopsy slides, \\nwe did not manually select ROIs that contain high tumour content \\nto avoid exposing the model to non-tumour features (blood ves-\\nsels, inflammation, necrotic regions and so on)35 that might lead \\nto misclassification. We also did not perform any pre-processing \\ntechniques such as stain normalization on our test set and used \\nthe entire tissue region of each slide during evaluation. Using the \\nsame visualization and interpretability technique as before, we \\ngenerated attention heatmaps for each subtype (Fig. 6d,e). We con-\\ntinued to observe a high similarity between the strongly attended \\nregions highlighted by the trained CLAM models and the tumour \\nregions annotated by the pathologist despite the tumours gener-\\nally occupying smaller and more sparse tissue regions than in the  \\nresection slides.\\nDiscussion\\nAltogether, we showed that CLAM addresses several key challenges \\nin computational pathology. Specifically, our analysis demonstrated \\nthat CLAM can be used to train interpretable, high-performance \\ndeep-learning models for both binary and multi-class WSI classifi-\\ncation using only slide-level labels without any additional annota-\\ntion. We are encouraged to note that our approach overcomes the \\nbarrier of time-costly annotation while also being more data effi-\\ncient; we showed that it achieves a strong performance and also has \\nthe ability to generalize to independent test cohorts, smartphone \\nmicroscopy and varying tissue content using a reasonable num-\\nber of slides for training. Using CLAM, we are also able to show-\\ncase high-resolution interpretability heatmaps for the entire WSI, \\nwhich may be used as an interpretability tool in research applica-\\ntions to identify morphological features associated with response \\nand resistance to treatment or as a visualization tool for a second-\\nary opinion in anatomic pathology to highlight ROIs. Although \\nthe use of attention-based pooling in CLAM provides the model \\nwith the flexibility of selectively aggregating information from \\nmultiple relevant ROIs to inform the slide-level diagnosis, a limi-\\ntation of CLAM and MIL-based approaches in general for weakly \\nsupervised classification is that they typically treat different loca-\\ntions in the slide as independent regions and do not learn poten-\\ntial nonlinear interactions between instances, which may help the \\nmodel become more context-aware. One line of future work will \\nfocus on extending the proposed weakly supervised framework \\nto additional problems in computational pathology and develop-\\ning more context-aware approaches. In addition, while fine-tuning \\nthe feature encoder in an end-to-end manner and using exten -\\nsive data augmentation will probably to lead to further improve-\\nment in performance, end-to-end training that involves working \\nwith the original data space of image pixels is expected to drasti-\\ncally increase the total training time and computational resources \\nrequired. In contrast with such a resource-hungry undertaking, the \\nuse of low-dimensional feature representations enables large-scale \\nexperimentation and allows us to conduct a detailed analysis of \\nthe data efficiency of different weakly supervised learning algo-\\nrithms using extensive 10-fold cross-validation across a variety of \\ntasks. However, this leaves room for future methods that will be \\nable to flexibly strike a balance between end-to-end training that  \\nseeks to maximize the expressiveness of the model (especially  \\nwhen large diverse datasets are available to curb overfitting) and \\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 563\\nArticles NATure BIoMeDICAL eNgINeerINg\\ncomputationally efficient usage of feature representations for the \\nweakly supervised learning on gigapixel WSIs. Last, other chal -\\nlenges that remain to be addressed and investigated in future stud-\\nies include developing data-efficient weakly supervised methods for \\nsurvival prediction, learning in the presence of noisy labels, poorly \\ndifferentiated cases, mixed cancer subtypes and from extremely \\nlimited number of labelled data (for example, fewer than ten cases), \\npredictions with uncertainty estimates and human in-the-loop \\ndecision-making.\\nWeakly supervised computational pathology is closer to clini-\\ncal adaption because it only requires slide- or patient-level labels \\nthat are collected for clinical purposes. The improvement in data \\nb d\\nLUSC\\nCCRCC PRCC\\nCRCC\\nc\\ne\\nf\\ng h\\nNSCLC\\nRCC\\n190 µm\\nTrain: resection WSIs Test: CPIs\\nLUAD\\na\\n50 µm\\n1.0\\n0.90\\n0.84\\n0.78\\n0.71\\n0.65\\n0.05 0.11 0.17 0.24 0.30 0.04 0.10 0.16 0.23 0.29\\n0.95\\n0.89\\n0.82\\n0.76\\n0.70\\n1.00\\n0.95\\n0.90\\n0.85\\n0.80\\nWSI CPI\\nWSI CPI\\n1.00\\n0.95\\n0.90\\nTest AUC Test AUC\\n0.85\\n0.80\\n0.8\\n0.6\\n0.4\\nSensitivity\\n1 – Specificity\\n0.2\\n0\\n0\\n–1\\n–1\\n0\\n1\\n0\\nLUAD\\nLUSC\\nCRCC\\nCCRCC\\nPRCC\\nPC1\\nPC2\\n–2\\n0\\n2\\nPC2\\n2 –3 0\\nPC1\\n3\\n0.2 0.4 0.6 0.8 1.0\\n1 – Specificity\\n0 0.2 0.4 0.6 0.8 1.0\\n1.0\\n0.8\\n0.6\\n0.4\\nSensitivity\\n0.2\\n0\\n100% train data: AUC = 0.921 ± 0.023\\n75% train data: AUC = 0.907 ± 0.025\\n50% train data: AUC = 0.900 ± 0.026\\n100% train data: AUC = 0.873 ± 0.025\\n75% train data: AUC = 0.850 ± 0.041\\n50% train data: AUC = 0.837 ± 0.039\\nFig. 5 | adaptability to smartphone microscopy images. a, CLAM models trained on WSIs are adapted to CPIs taken with a consumer-grade smartphone \\ncamera without domain adaptation, stain normalization or further fine-tuning. b,c, An average test AUC of 0.873\\u2009±\\u20090.025 and 0.921\\u2009±\\u20090.023 was achieved \\nfor the BWH nSCLC (b; n\\u2009=\\u2009131) and BWH RCC (c; n = 135) independent test sets, respectively. For each slide, patches extracted from all FOVs are \\ncollectively used by the CLAM model to inform the slide-level diagnosis. Insets: zoomed-in view of the curves. d, A drop in performance is expected when \\ndirectly adapting models trained on data from one imaging modality (WSIs) to another (CPIs). We noted a decrease of 0.102 and 0.051 in the mean test \\nAUC (relative to the performances on the corresponding WSI independent datasets) for nSCLC (top) and RCC (bottom) subtyping, respectively, when \\nevaluating CLAM models (using 100% of the training set) on our CPI datasets. The boxes indicate the quartile values and the whiskers extend to data points \\nwithin 1.5× of the interquartile range. e,f, The attention heatmaps (shown for nSCLC (e) and RCC (f) subtyping) help make model predictions interpretable \\nby highlighting the discriminative regions in each FOV used by the model to make the slide-level diagnostic prediction. We observed that the model attends \\nstrongly to tumour regions and largely ignores normal tissue and background artefacts, as expected. However, due to the circular-shaped cutout of each \\nFOV, patches near the border inevitably encapsulate varying degrees of black space in addition to the tissue content, which can mislead the model towards \\nassigning weaker attention to those regions than it would otherwise. Zoomed-in views of the boxed regions are shown on the right. g,h, As additional \\nvalidation that CLAM models trained on WSIs are directly applicable to the classification of CPIs, we visualized the attention-pooled feature representation \\nof each set of CPIs and observed that there is visible separation between distinct classes in both the nSCLC (g) and RCC (h) smartphone datasets.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng564\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nefficiency brought forth by our approach helps reduce the trade-off \\nbetween weak supervision and the number of labelled whole slides \\nrequired for training. While large diverse datasets are valuable \\nassets for capturing as much heterogeneity within the data distri-\\nbution as possible, data-efficient whole-slide training is essential \\nto enable the applicability of computational pathology for clas -\\nsification in rare conditions as well as patient stratification for  \\nclinical trials where it is valuable to predict the response or resistance \\nto treatment from a small cohort of existing patient cases. Within \\nthe context of our study, we found that CLAM is indeed capable \\nof stratifying patients into predominant and relatively rare classes  \\n(for example, CCRCC versus CRCC). As we look forward to  \\nvalidating CLAM on a wider array of problems, we are also opti -\\nmistic about the potential utility of CLAM in applications beyond \\nb c\\nLUADLUSC\\nCCRCC PRCC\\nCRCC\\n3 mm 35 µm\\nd\\ne\\nf g\\nTrain: resection WSIs Test: biopsy WSIs\\na 1.0\\n0.90\\n0.84\\n0.78\\n0.71\\n0.65\\n0.05 0.11 0.17 0.24 0.30 0.04 0.10 0.16 0.23 0.29\\n0.95\\n0.89\\n0.82\\n0.76\\n0.70\\n0.8\\n0.6\\n0.4\\nSensitivity\\n1 – Specificity\\n0.2\\n0\\n0 0.2 0.4 0.6 0.8 1.0\\n1 – Specificity\\n0 0.2 0.4 0.6 0.8 1.0\\n1.0\\n0.8\\n0.6\\n0.4\\nSensitivity\\n0.2\\n0\\n–1\\n–1\\n0\\n2\\n1\\nLUAD\\nLUSC\\nCRCC\\nCCRCC\\nPRCC\\nPC1\\nPC2\\n–2\\n0\\n3\\nPC2\\n3 –2 0\\nPC1\\n2\\n100% train data: AUC = 0.951 ± 0.011\\n75% train data: AUC = 0.932 ± 0.015\\n50% train data: AUC = 0.921 ± 0.013\\n100% train data: AUC = 0.902 ± 0.016\\n75% train data: AUC = 0.882 ± 0.025\\n50% train data: AUC = 0.885 ± 0.011\\nFig. 6 | adaptability to biopsy slides. a, Compared with resection WSIs, biopsy WSIs generally contain a much lower tissue content (for example, \\nthe average number of patches extracted from the tissue regions of each slide is 820 in our BWH lung biopsy dataset compared with 24,714 in the \\nlung-resection dataset). The presence of crush artefacts as well as poorly differentiated and sparsely distributed tumour cells can further challenge \\naccurate diagnosis. b,c, We observed that CLAM models trained on resections are directly adaptable to biopsy WSIs, achieving a respectable average \\ntest AUC of 0.902\\u2009±\\u20090.016 and 0.951\\u2009±\\u20090.011 on our nSCLC (b; n\\u2009=\\u2009110) and RCC (c; n\\u2009=\\u200992) biopsy independent test cohorts, respectively, without further \\nfine-tuning or ROI extraction. Insets: zoomed-in view of the curves. d,e, Attention heatmap visualization for nSCLC (d) and RCC (e) biopsy slides. H&e \\nslide with annotation by the pathologist for tumour regions (left). Heatmap for patches tiled with a 95% overlap (middle). Zoomed-in view of tumour \\nregions attended by the CLAM model (right). Consistent with our findings on the resection and smartphone datasets, the regions that were most \\nstrongly attended by the model consistently correspond to tumour tissue. The attention heatmaps also tend to clearly highlight the tumour–normal tissue \\nboundaries, despite the fact that no patch-level or pixel-level annotation was required or used during training. f,g, The slide-level feature representations of \\nthe biopsy datasets are visualized in two dimensions using PCA. We observed that the feature space learned by the CLAM model from resections remains \\nvisibly separable among the distinct subtypes when it is adapted to biopsy slides for both nSCLC (f) and RCC (g). A high-resolution version of these \\nbiopsy whole slides and heatmaps may be viewed our interactive demo (http:/ / clam.mahmoodlab.org).\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 565\\nArticles NATure BIoMeDICAL eNgINeerINg\\nthe classification of WSI resections. For instance, we have found \\nthat models trained using CLAM and weak supervision are highly \\nadaptable to independent data sources, biopsy slides, different \\nscanning hardware and smartphone microscopy images without \\nusing any form of domain adaptation or fine-tuning. These impor-\\ntant properties should allow researchers to develop models using \\nresection slides (average tissue coverage: 142 mm2, 11,182 patches), \\nwhich maximizes the diversity of tissue content encountered dur-\\ning training, with the flexibility to later adapt to biopsies (average \\ntissue coverage: 15.6 mm2, 1,225 patches). Similarly, CLAM mod-\\nels trained on WSIs covering large tissue volume can adapt to CPIs \\nwith a limited FOV and have the potential to enable the routine \\nuse of telepathology in remote resource-constrained settings with \\nlimited anatomic pathology expertise, where consult cases are often \\nimaged via a consumer-grade smartphone attached to brightfield \\nmicroscopes. Overall, we hope our study and method will provide \\nresearchers with new ways to solve diagnostic and research prob-\\nlems using whole-slide images of routine histology specimens, \\nthereby improving clinical care and facilitating knowledge discov-\\nery in computational pathology.\\nMethods\\nCLAM. CLAM is a high-throughput deep-learning empowered toolbox designed \\nto solve weakly supervised classification tasks in computational pathology, in \\nwhich each WSI in the training set is a single data point with a known slide-level \\ndiagnosis but for which no class-specific information or annotation is available \\nfor any pixel or region in the slide. CLAM builds on the MIL framework, which \\nviews each WSI (known as a bag) as a collection comprised of many (up to \\nhundreds of thousands) smaller regions or patches (known as instances). The \\nMIL framework typically restricts its scope to binary classification problems of \\na positive and a negative class based on the assumption that if at least one patch \\nbelongs to the positive class, then the entire slide should be classified as positive, \\nwhereas a slide should be classified as negative if all patches are of the negative \\nclass. This assumption is reflected in the rigid non-trainable aggregation function \\nof max-pooling, which simply uses the patch with the highest predicted probability \\nfor the positive class for the slide-level prediction, rendering MIL unsuitable \\nfor both multi-class classification and binary classification problems in which \\nno intrinsic positive/negative assumption can be made. Besides max-pooling, \\nalthough other aggregation functions such as the mean operator, generalized \\nmean, log-sum-exp, the quantile function, noisy-or and noisy-and41–43 can be \\nused, they suffer from limited flexibility for problem and data-specific tuning and \\ndo not offer a simple, intuitive mechanism for model interpretability. In contrast, \\nCLAM is generally applicable to multi-class classification and is built around \\nthe trainable and interpretable attention-based pooling function37 to aggregate \\nslide-level representations from patch-level representations for each class. In \\nour design of multi-class attention pooling, the attention network predicts N \\ndistinct sets of attention scores corresponding to the N classes in a multi-class \\nclassification problem. This enables the network to unambiguously learn which \\nmorphological features should be considered as positive evidence (characteristic \\nof the class) versus negative evidence (non-informative, absent of class-defining \\ncharacteristics) for each class and summarize N unique slide-level representations. \\nSpecifically, for a WSI represented as a bag of K instances (patches), we denote \\nthe instance-level embedding corresponding to the kth patch as zk. In CLAM, \\nthe first fully connected layer W1 2 R512 ´ 1;024\\nI\\n further compresses each fixed \\npatch-level representation zk 2 R1;024\\nI\\n to a 512-dimensional vector hk ¼ W1zk\\nI\\n \\n(for simplicity, all bias terms are implied and not explicitly written). The attention \\nnetwork consists of several stacked fully connected layers; if we consider the first \\ntwo layers of the attention network Ua 2 R256 ´ 512 and Va 2 R256 ´ 512\\nI\\n and W1  \\ncollectively as part of the attention backbone shared by all classes, the attention \\nnetwork then splits into N parallel attention branches Wa;1; ::: ;Wa;N 2 R1 ´ 256\\nI\\n. \\nSimilarly, N parallel independent classifiers (Wc,1, ..., Wc,N) are built to score each \\nclass-specific slide-level representation. Accordingly, the attention score of the kth \\npatch for the ith class, denoted ai,k, is given by equation (1)37 and the slide-level \\nrepresentation aggregated per the attention score distribution for the ith class, \\ndenoted hslide;i 2 R512\\nI\\n, is given by equation (2):\\nai; k ¼ exp Wa;i tanh Vahkð Þ\\ue00c sigm Uahkð ÞðÞ\\n\\ue008\\ue009\\nPK\\nj¼1 exp Wa;i tanh Vahj\\n�\\ue001\\n\\ue00c sigm Uahj\\n� \\ue001 �\\ue001\\ue008\\ue009 ð1Þ\\nhslide;i ¼\\nXK\\nk¼1 ai;khk ð2Þ\\nThe corresponding unnormalized slide-level score sslide,i is given via the \\nclassifier layer Wc;i 2 R1 ´ 512\\nI\\n by sslide;i ¼ Wc;ihslide;i\\nI\\n. We use dropout (P = 0.25) after \\neach layer in the attention backbone of the model for regularization. For inference, \\nthe predicted probability distribution over each class is computed by applying a \\nsoftmax function to the slide-level prediction scores sslide.\\nInstance-level clustering. To further encourage the learning of class-specific features, \\nwe introduce an additional binary clustering objective during training. For each of \\nN classes, we place a fully connected layer after the first layer W1 . If we denote the \\nweight of the clustering layer that corresponds to the ith class as Winst;i 2 R2 ´ 512\\nI\\n, \\nthe cluster assignment scores predicted for the kth patch, denoted by pi,k, is  \\ngiven as:\\npi;k ¼ Winst;ihk ð3Þ\\nGiven that we do not have access to patch-level labels, we use the outputs of \\nthe attention network to generate pseudo labels for each slide in each iteration of \\ntraining to supervise the clustering. Instead of clustering all the patches in the slide, \\nwe only optimize the objective over the subset of the most- and least-attended \\nregions. Let the entire label set be Y¼f 1; :::; Ng\\nI\\n, to avoid confusion, for a \\ngiven slide, with ground-truth class label Y 2Y\\nI\\n, we refer to the attention branch \\nthat corresponds with this ground-truth class (Wa,Y) as ‘in-the-class’ , and the \\nremaining N − 1 attention branches as ‘out-of-the-class’ . If we denote the sorted \\nlist of in-the-class attention scores (in ascending order) as ~aY;1; ::: ;~aY;K\\nI\\n, we take \\nthe B patches with the lowest attention scores and assign them the negative cluster \\nlabel (yY,b = 0, 1 ≤ b ≤ B), while the B patches with the highest in-the-class attention \\nscores receive the positive cluster label (yY,b = 1, B + 1 ≤ b ≤ 2B). Intuitively, because \\neach attention branch is supervised by the slide-level label during training, the \\nB patches with high attention scores (hence the positive cluster) are expected to \\nbe strong positive evidence for class Y, whereas the B patches with low attention \\nscores (hence the negative cluster) are expected to be strong negative evidence for \\nclass Y. Therefore, the clustering task can be intuitively interpreted as constraining \\nthe patch-level feature space hk such that the strong characterizing evidence of \\neach class is linearly separable from its negative evidence. For cancer subtyping \\nproblems, all classes are often assumed to be mutually exclusive (that is, they \\ncannot be present in the same slide), as we cluster the most- and least-attended \\npatches of the in-the-class attention branch into positive and negative evidence, \\nrespectively, it makes sense to also impose additional supervision on the N − 1 \\nout-of-the-class attention branches. Namely, given the ground-truth slide label Y, \\n8i 2 YnfYg\\nI\\n, the B patches with the highest attention scores cannot be positive \\nevidence for class i, provided that we assume none of the patches on the slide \\nis of class i (due to the mutual exclusivity). As a result, in addition to clustering \\nthe 2B patches selected from the in-the-class attention branch, we assign the \\nnegative cluster label to the top B attended patches in all out-of-the-class attention \\nbranches as they are assumed to be false positive evidence. On the other hand, if \\nthe mutual exclusivity assumption does not hold (for example, cancer versus no \\ncancer problem, where a slide can contain patches from both tumour tissue and \\nnormal tissue), then we do not supervise the clustering of highly attended patches \\nfrom out-of-the-class branches as we do not know if they are false positives or not. \\nUsing the aforementioned notations, the full instance-level clustering algorithm is \\nsummarized below in Algorithm 1.\\nAlgorithm 1 instance-level clustering. \\nfunction Cluster((h1, a1), ... , (hK, aK), Y)\\n for i ← 1, 2, ... , N do\\n  if i = Y then                  ⊳in-the-class branch\\n   ðeh1; ~ai;1 Þ; ¼;ðehK ; ~ai;K Þ¼ SortAscending\\nai;k\\nððh1; ai;1 Þ; ¼; ðhk; ai;kÞ; ¼; ðhK ; ai;K ÞÞ\\nI\\n   for  b ← 1, ... , B do\\n    {generate pseudo label for positive and negative evidence}\\n    y i,b = 0                   ⊳negative evidence\\n    y i,b+B = 1                  ⊳positive evidence\\n    {cluster assignment prediction }\\n    pi;b ¼ Winst;iehb\\nI\\n          ⊳prediction for negative evidence\\n    pi;bþB ¼ Winst;iehK�Bþb\\nI\\n      ⊳prediction for positive evidence\\n  else                    ⊳out-of-the-class branch\\n   if classes are mutually exclusive then\\n    ðeh1; ~ai;1 Þ; ¼; ðehK ; ~ai;K Þ¼ SortAscending\\nai;k\\nððh1; ai;1Þ; ¼; ðhk; ai;kÞ; ¼; ðhK ; ai;kÞÞ\\nI\\n    for  b ← 1, ... , B do\\n     {generate pseudo label for false positive evidence}\\n     y i,b = 0                ⊳false positive evidence\\n     {cluster assignment prediction }\\n     pi;b ¼ Winst;iehK�Bþb\\nI\\n    ⊳prediction for false positive evidence\\n   else\\n    pass           ⊳do not supervise out-of-the-class \\nattention branches if not mutually exclusive\\n if classes are mutually exclusive then\\n  return [p 1 , ... ,pN], [y1 , ... , yN]\\n else\\n  return [p Y], [yY]\\nSmooth SVM loss. For the instance-level clustering task, we chose to use the \\nsmooth top-1 SVM loss44, which is based on the well-established multi-class SVM \\nloss45. In a general N-class classification problem, neural network models output a \\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng566\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nvector of prediction scores s, where each entry in s corresponds to the prediction \\nof the model for a single class made. Given the set of all possible ground-truth \\nlabels Y¼ 1; 2; ¼; Nfg\\nI\\n and ground-truth label y 2Y\\nI\\n, the multi-class SVM loss \\npenalizes the classifier linearly in the difference between the prediction score for \\nthe ground-truth class and the highest prediction score for the remaining classes \\nonly if that difference is greater than a specified margin α (equation (4)). The \\nsmoothed variant (equation (5)) adds a temperature scaling τ to the multi-class \\nSVM loss, with which it has been shown to be infinitely differentiable with \\nnon-sparse gradients and suitable for the optimization of deep neural networks \\nwhen the algorithm is implemented efficiently44. The smooth SVM loss can be \\nviewed as a generalization of the widely used cross-entropy classification loss for \\ndifferent choices of finite values for the margin and different temperature scaling.\\nThe introduction of a margin to the loss function has been empirically shown \\nto reduce overfitting when the data labels are noisy or when data are limited. \\nDuring training, the pseudo labels we create to supervise the instance-level \\nclustering task are expected to be noisy. Namely, the top-attended patches \\nmight not necessarily correspond to the ground-truth class and, similarly, the \\nleast-attended patches are also not guaranteed to be actual negative evidence  \\nof the class. Therefore, instead of the widely used cross-entropy loss (which is  \\nused for the slide-level classification task), we apply the binary top-1 smooth  \\nSVM loss to the outputs of the clustering layers of the network. In all our \\nexperiments, α and τ were both set to 1.0.\\nlðs; yÞ¼ max max\\nj2Ynfyg\\nsj þ α\\n/C8/C9\\n� sy ; 0\\n/C26/C27\\nð4Þ\\nL1;τðs; yÞ¼ τlog\\nX\\nj2Y exp 1\\nτ α1ðj≠yÞþ sj � sy\\n�\\ue001\\ue012 \\ue013\\ue014\\ue015\\nð5Þ\\nTraining details. During training, slides are randomly sampled and provided to \\nthe model using a batch size of one. The multinomial sampling probability of each \\nslide is inversely proportional to the frequency of its ground-truth class (that is, \\nslides from under-represented classes are more likely to be sampled relative to \\nothers) to mitigate class imbalance in the training set. Weights and bias parameters \\nof the attention module are initialized randomly and trained end-to-end with \\nthe rest of the model using the slide-level labels as no ground-truth attention is \\navailable. The total loss for a given slide Ltotal\\nI\\n is the sum of both the slide-level \\nclassification loss Lslide\\nI\\n and the instance-level clustering loss Lpatch\\nI\\n with optional \\nscaling via scalar c1  and c2:\\nLtotal ¼ c1Lslide þ c2Lpatch ð6Þ\\nTo compute Lslide\\nI\\n, sslide is compared with the ground-truth slide-level label \\nusing the standard cross-entropy loss, and to compute Lpatch\\nI\\n, the instance-level \\nclustering prediction scores pk for each sampled patch are compared against \\ntheir corresponding pseudo-cluster labels using the binary smooth SVM loss \\n(recall that for non-subtyping problems there are a total of 2B patches sampled \\nfrom the in-the-class branch, whereas for subtyping problems there are 2B \\npatches sampled from the in-the-class branch and B patches sampled via each of \\nN − 1 out-of-the-class attention branches). Unless otherwise specified, for each \\ndisease model, we tuned for B 2 {8, 16, 32, 64, 128} on a single random validation \\nfold by training on a subset of the training data (50% of the full training set). \\nWe considered c1  + c2 = 1 and similarly, tuned for c1  2 {0.3, 0.5, 0.7, 0.9} for the \\nchosen B. Specifically, for the main 10-fold experiments, B = 8 was used for RCC \\nsubtyping, B = 32 for both NSCLC subtyping and the detection of lymph node \\nmetastasis, and c1  = 0.7 was used for all three tasks. However, we did not observe \\na drastic difference in the validation performance for different values of B and \\nc1  (Supplementary Fig. 9). The model parameters are updated via the Adam \\noptimizer with a learning rate of 2 × 10−4 and ℓ2 weight decay of 1 × 10−5. For all \\nexperiments, default coefficient values for computing the running averages of the \\nfirst and second moment of the gradient were used (β1  = 0.9 and β2 = 0.999) and ε \\nterm (for numerical stability) was set to 1 × 10−8 (the default value). A commonly \\nused technique that may help improve model generalization when training data are \\nlimited is data augmentation. However, to investigate the data efficiency of CLAM, \\nwe did not attempt to perform data augmentation.\\nModel selection. All models are trained for at least 50 epochs and up to a maximum \\nof 200 epochs if the early stopping criterion is not met. Namely, the validation loss \\nis monitored each epoch, and when it has not decreased from the previous low for \\nover 20 consecutive epochs, early stopping is used. The saved model, which has the \\nlowest validation loss, is then tested on the test set.\\nComputational hardware and software. We used multiple hard drives to store \\nthe raw files of digitized whole slides. Segmentation and patching of WSIs \\nwere performed on Intel Xeon CPUs (central processing units) and feature \\nextraction using a pre-trained neural network model was accelerated through \\ndata batch parallelization across multiple NVIDIA P100 GPUs on Google \\nCloud Compute instances or 2080 Ti GPUs on local workstations. All weakly \\nsupervised deep-learning models were trained with a total of ten local, consumer \\nworkstation-grade NVIDIA 2080 Ti GPUs by streaming extracted features \\nfrom fast local solid-state-drive storage. Our whole-slide processing pipeline is \\nimplemented in Python (version 3.7.5) and takes advantage of image-processing \\nlibraries, such as openslide (version 3.4.1), opencv (version 4.1.1) and pillow \\n(version 6.2.1). For loading data and training deep-learning models using \\nCLAM, we used the Pytorch (version 1.3.1) deep-learning library. Based on our \\nconsumer-grade hardware, we also analysed the run time of CLAM for performing \\nstreamlined inference on our in-house WSI data. On a single local workstation \\nand using two NVIDIA 2080 Ti GPUs, on average, using non-overlapping patches, \\nCLAM requires 106.26 s (41.46 s for inference and 64.8 s for generating and \\nsaving a heatmap) for a ×20 resection WSI and 15.65 s (4.42 s for inference and \\n11.23 s for heatmap generation) for a ×20 biopsy WSI. Note that the inference \\nspeed includes the time to perform tissue segmentation, extract patches, extract \\nfeatures and perform classification, and heatmaps are generated and saved at \\nthe ×10 magnification. High-overlap (95%) and high-resolution (×10) WSI \\nheatmaps shown in our interactive demo require multiple runs divided into many \\nmini-batches of patches and are created and saved in 5,445 s per ×20 resection slide \\nand 279 s per ×20 biopsy slide. The high compute time associated with generating \\nhigh-resolution heatmaps based on a large number of overlapping patches can \\nprobably be substantially reduced using production-grade hardware and more \\nefficient software parallelization.\\nAll plots were generated using matplotlib (version 3.1.1) and seaborn (version \\n0.8.1). The AUC of the receiver-operating-characteristic curve was estimated \\nusing the Mann–Whitney U statistic, for which the algorithmic implementation \\nis provided in the scikit-learn scientific computing library (version 0.22.1). The \\n95% confidence intervals of the true AUC were computed using DeLong’s method \\nimplemented by pROC (version 1.16.2) in R (version 3.6.1).\\nWSI datasets. A summary of all of the datasets used are included in \\nSupplementary Table 8. For the in-house test data, the BWH pathology archives \\nwere queried and cases were randomly sampled and requested from in-house \\npathology archives (2016–2019). We requested 150 resection cases for each \\nproblem, and 110 biopsy cases each for both NSCLC and RCC subtyping. We \\nreceived slides based on their on-site availability at the time of study, and scanned \\nslides with substantial markings covering the tissue area, damaged slides as well \\nas slides that did not contain tumour (for RCC and NSCLC) were excluded before \\ntesting performance on our models; no other slides were excluded. Further details \\nabout each cohort are given in the following subsections. For model development \\nand evaluation on public datasets using 10-fold Monte Carlo cross-validation, \\nrandom train/validate/test dataset partitions are created where slides from the \\nsame patient case are sampled together to ensure that, for example, different slides \\nfrom the same case are not sampled into both the training and test set. The number \\nof slides available for each patient case can differ, which means that although all ten \\nfolds always have the same number of cases in their train/validate/test set, the exact \\nnumber of slides might differ. For brevity, when we refer to the number of slides in \\nthe training or test set for the cross-validation folds, we refer to the average number \\nof slides across all folds.\\nPublic RCC WSI dataset. Our public RCC dataset consists of a total of 884 \\ndiagnostic WSIs from the TCGA RCC repository under the Kidney Chromophobe \\n(TCGA-KICH), Kidney CCRCC (TCGA-KIRC) and Kidney Renal Papillary Cell \\nCarcinoma (TCGA-KIRP) projects. There are 111 CRCC slides from 99 cases, \\n489 CCRCC slides from 483 cases and 284 PRCC slides from 264 cases. The mean \\nnumber of patches extracted per slide at ×20 magnification is 13,907.\\nIndependent BWH RCC WSI dataset. Our internal RCC dataset consists of a total \\nof 135 WSIs from 133 cases, of which 43 slides are CRCC, 46 are CCRCC and 46 \\nare PRCC. The mean number of patches extracted per slide at ×20 magnification \\nis 20,394. Our RCC biopsy dataset consists of a total of 92 WSIs from 79 cases, of \\nwhich 13 slides are CRCC, 53 are CCRCC and 26 are PRCC. The sample sizes for \\nthe CRCC biopsies were limited by the availability of patient cases for this rare \\ncondition (represents approximately 5% of all RCC cases with only a few biopsy \\ncases). The mean number of patches extracted per slide at ×20 magnification is \\n1,709. Our RCC smartphone dataset comprises 4–8 FOVs per slide for each of the \\n135 slides. The mean number of patches extracted for each set of FOVs is 419. All \\nslides were collected and processed at the BWH between 2016 and 2019.\\nPublic NSCLC WSI dataset. Our public NSCLC dataset consists of 993 \\ndiagnostic WSIs from the TCGA NSCLC repository under the TCGA-LUSC \\nand TCGA-LUAD projects. There are 507 LUAD slides from 444 cases and 486 \\nLUSC slides from 452 cases. In addition, we collected a total of 1,526 WSIs from \\nthe TCIA CPTAC Pathology Portal at the time of study that have lung as the \\ntopological site. From these WSIs, 668 slides from 223 cases are labelled as LUAD \\nand 306 slides from 108 cases are labelled as LUSC. The remaining 552 slides are \\nlabelled as normal tissue and were excluded. Accordingly, our public lung dataset \\ncontains a total of 1,967 WSIs (1,175 LUAD slides from 667 cases and 792 LUSC \\ncases from 560 patients). The mean number of patches extracted per slide at ×20 \\nmagnification is 9,958.\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 567\\nArticles NATure BIoMeDICAL eNgINeerINg\\nIndependent BWH NSCLC WSI dataset. Our internal NSCLC dataset consists  \\nof a total of 131 resection (63 LUAD and 68 LUSC) and 110 biopsy (55 LUAD \\nand 55 LUSC) slides. Each slide comes from a unique case. The mean number of \\npatches extracted per biopsy slide and per resection slide at ×20 magnification is \\n820 and 24,714, respectively. All slides were collected and processed at the BWH \\nbetween 2016 and 2019. Our lung smartphone dataset comprises 4–8 FOVs per \\nslide for each of the 131 resection slides. The mean number of patches extracted \\nfor each set of FOVs is 406. In addition, lung resection slides were scanned with \\na 3DHistech MiraxScan 150 to investigate adaptability to different scanning \\nhardware and varying m.p.p.\\nPublic lymph node WSI dataset. CAMELYON16 and CAMELYON17 \\n(ref. 46) are two of the largest publicly available, annotated breast-cancer \\nlymph-node-metastasis detection datasets. CAMELYON16 consists of 270 \\nannotated whole slides for training and another 129 slides as a held-out official  \\ntest set collected at the Radboud University Medical Center and the University \\nMedical Center Utrecht in the Netherlands. On the other hand, CAMELYON17 \\nconsists of a total of 1,000 slides from five different medical centres in the \\nNetherlands. Because slide-level labels for the 500 slides in the official test set of \\nCAMELYON17 were not yet publicly available, we used just the training portion \\nof CAMELYON17, which consists of 500 slides (with corresponding slide-level \\ndiagnosis) for 100 cases. We combined CAMELYON16 and CAMELYON17  \\ninto a single dataset with a total of 899 slides (591 negative and 308 positive)  \\nfrom 499 cases. The mean number of patches extracted per slide at  \\n×40 magnification is 41,802.\\nIndependent BWH lymph node metastasis (breast cancer) WSI dataset. Our internal \\nbreast-cancer lymph node metastasis dataset consists of a total of 133 WSIs from \\n131 cases (66 negative slides and 67 positive slides). The mean number of patches \\nextracted per slide at ×40 magnification is 51,426. These slides were collected at \\nBWH between 2017 and 2019.\\nWSI processing. Segmentation. For each digitized slide, our pipeline begins with \\nautomated segmentation of the tissue regions. The WSI is read into memory at a \\ndownsampled resolution (for example, 32× downscale), converted from RGB to the \\nHSV colour space. A binary mask for the tissue regions (foreground) is computed \\nbased on thresholding the saturation channel of the image after median blurring to \\nsmooth the edges and is followed by additional morphological closing to fill small \\ngaps and holes. The approximate contours of the detected foreground objects are \\nthen filtered based on an area threshold and stored for downstream processing \\nwhile the segmentation mask for each slide is made available for optional visual \\ninspection. A human-readable text-file is also automatically generated, which \\nincludes the list of files processed along with editable fields containing the set \\nof key segmentation parameters used. Although the default set of parameters \\nare generally sufficient for reliable tissue segmentation, they can also be easily \\nmanually edited for any individual slide should the user find its segmentation \\nresults unsatisfactory.\\nPatching. After segmentation, for each slide, our algorithm exhaustively crops \\n256 × 256 patches from within the segmented foreground contours at the \\nuser-specified magnification and stores stacks of image patches along with their \\ncoordinates and the slide metadata using the hdf5 hierarchical data format. \\nDepending on the size of each WSI and the specified magnification, the number  \\nof patches extracted from each slide can range from hundreds (biopsy slide  \\npatched at ×20 magnification) to hundreds of thousands (large resection slide \\npatched at ×40 magnification).\\nFeature extraction. Following patching, we use a deep CNN to compute a \\nlow-dimensional feature representation for each image patch of each slide. \\nNamely, we take a ResNet50 model pre-trained on ImageNet47 and use adaptive \\nmean-spatial pooling after the third residual block of the network to convert each \\n256 × 256 patch into a 1,024-dimensional feature vector using a batch size of 128 \\nper GPU across multiple GPUs. The benefits of using extracted features as inputs \\nto deep-learning models for supervised learning include a drastically faster training \\ntime and lower computational cost. This enables us to train a deep-learning model \\non thousands of WSIs in a matter of a few hours once the features have been \\nextracted. Compared with using raw pixels, using low-dimensional features also \\nmakes it feasible to fit all patches in a slide (up to 150,000 or more) into memory \\nsimultaneously on a single consumer-grade GPU, thus avoiding the need for \\nsampling patches and using noisy labels.\\nVisualization. Visualizing slide-level feature space. For each public WSI dataset, a \\nmodel trained on one of the ten training sets created for cross-validation was used \\nto compute a 512-dimensional slide-level feature representation for every slide in \\nits corresponding validation and test set for the slide-level prediction of the model. \\nThe resulting set of slide-level feature vectors were reduced to two-dimensional \\nspace for visualization through transformation via PCA and each point was \\nshaded by its ground-truth slide-level label. We then repeated this procedure for \\nthe models trained on 25, 50 and 75% of the same training set. We also performed \\nthe same analysis on the slides in each independent test cohort using the \\nbest-performing model for each training-set size.\\nInterpreting model prediction via attention heatmap. To interpret the relative \\nimportance of different regions in a slide to the final slide-level prediction of the \\nmodel, we computed and saved the unnormalized attention scores (before they \\nwere converted to probability distribution by applying the softmax function) \\nfor all of the patches extracted from the slide, using the attention branch that \\ncorresponded to the predicted class of the model. These attention scores were \\nconverted to percentile scores and scaled to between zero and 1.0 (with 1.0 \\nbeing most attended and zero being least attended). The normalized scores were \\nconverted to RGB colours using a diverging colourmap and displayed on top \\nof their respective spatial locations in the slide to visually identify and interpret \\nregions of high attention displayed in red (positive evidence, high contribution to \\nthe prediction of the model relative to other patches) and low attention displayed \\nin blue (low contribution to prediction of the model relative to other patches). \\nTo create more fine-grained heatmaps, we tiled the slides or smaller ROIs (for \\nexample, 8,000 × 8,000) into 256 × 256 patches using an overlap and calculated \\nthe raw attention score for each patch. We then followed a similar procedure and \\nused the same colourmap as above to convert the raw score of each patch in the \\nROI to RGB colours. To ensure that the normalized attention scores computed for \\npatches produced with an overlap were directly comparable to those for the set \\nof non-overlapping patches used by the model for prediction, we referred to the \\nset of unnormalized attention scores over the entire slide (without overlap) when \\ncalculating the percentile score of each patch. The ROI heatmaps were overlaid \\nover the original WSI with a transparency value of 0.5 to simultaneously visualize \\nthe underlying morphological structures in the original H&E slide. Biopsy and \\nROI heatmaps were produced with an overlap of 95%. To produce fine-grained \\nheatmaps for CPIs, a 95% overlap was used and attention scores were normalized \\nover each image.\\nVisualizing patch-level feature space. For each slide in the independent  \\ntest cohort, we uniformly randomly sampled 2% of its tissue patches and  \\nrecorded their clustering probability predictions, made by each of the N clustering \\nbranches in addition to their 512-dimensional feature representations after  \\nthe first fully connected layer. For subtyping problems, patches for which  \\nall clustering branches predicted a positive probability of less than 0.5  \\n(in other words, the clustering branch of every class considers them as negative \\nevidence for its respective class) were labelled as class-agnostic, whereas  \\nthe remaining patches were labelled with the class for which its positive  \\nprobability is the highest. For metastasis detection in axillary lymph nodes, the \\nclustering branch corresponding to the positive class was used to label patches \\nas positive (positive probability greater than or equal to 0.5) and class-agnostic \\n(positive probability less than 0.5). Using the same technique above for visualizing \\nthe slide-level feature space, we reduced each patch-level feature vector to  \\ntwo dimensions using PCA.\\nQuantitative evaluation of attention heatmaps. While the attention heatmaps \\ngenerated from CLAM models trained in a weakly supervised learning fashion \\nare not designed or intended to perform pixel-level annotation of ROIs, to assess \\nthe possibility of using the heatmaps as an assistive annotator in a clinical or \\nresearch setting as well as the correctness of attention, we evaluated the predicted \\nattention heatmaps produced by a single CLAM model against the pathologist \\nannotations using quantitative metrics including the Dice score, intersection \\nover union and Cohen’s κ for each disease model. For all resection slides in the \\nin-house dataset of each disease model, two anatomic pathologists were asked to \\nindependently and exhaustively annotate the tumour regions in all slides using the \\nannotation tool Automated Slide Analysis Platform (ASAP). No time constraints \\nwere placed and for the annotation of metastasis in axillary lymph nodes, AE1/\\nAE3 immunohistochemistry were used to assist in the annotation and ensure \\nsmall tumour regions (micro-metastasis) were not missed. For evaluation, all \\nheatmaps were generated by tiling patches at a 75% overlap. Binary masks were \\nproduced from heatmaps after dynamic thresholding, in concordance with the \\nprobable real-world scenario where a human operator can freely adjust the display \\nthreshold for the desired range to identify contiguous and dense regions of high \\nattention. Each heatmap was thresholded without assistance from the pathologist \\nannotations. Following binarization, simple post processing techniques including \\nmorphological closing and opening are applied to reduce fragmentation, close \\nsmall cavities and suppress small artefacts. We did not apply closing and opening \\nfor lymph node metastasis, due to the presence of micro-metastasis, which can \\nmake up pixel-islands of extremely small area that are easily destroyed by such \\noperations. We instead slightly dilated the foreground to connect neighbouring \\nfragments and filter out pixel-islands for which all pixels have an attention of less \\nthan 0.95. Finally, despite extensive thoroughness, it is not possible to exclude all \\nnegative pixels that are present inside regions of tumour, thus we apply a tissue \\nsegmentation algorithm to detect large cavities inside the tissue and exclude such \\nregions from the evaluation of the heatmaps. However, we note that this cannot \\nautomatically identify all regions of cavity, especially if they are small, and also \\ndoes not take into account small areas of normal tissue inside an annotated tumour \\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng568\\nArticlesNATure BIoMeDICAL eNgINeerINg\\nregion. The results for both sets of pathologist annotations for all disease models \\nare summarized in Supplementary Fig. 7.\\nComparative analysis using MIL. The most well-known MIL decision rule \\ninvolves a diagnostic model making a prediction for every patch in a whole  \\nslide and the patch with the highest predicted probability for the positive  \\nclass is selected to both inform the final diagnostic decision for the entire slide  \\nas well as gradient signals during training. In addition to using MIL, which  \\nsimply takes the highest probability patch, the authors of a recent study36 \\nintroduced a recurrent-neural-network-based aggregation that sequentially  \\npasses the top S patches ranked on the basis of their predicted probability  \\nfor the positive class through a recurrent neural network to obtain the final \\nslide-level prediction. However, on three different large datasets (prostate cancer, \\nskin cancer basal cell carcinoma and lymph node metastasis detection), they  \\nnoted a test AUC ranging from marginal improvement to no improvement  \\nusing recurrent-neural-network-based aggregation. In light of these findings,  \\nwe used the widely adopted max-pooling MIL formulation as our baseline  \\nfor comparison.\\nFor each slide, during training, feature embeddings of all patches in the slide \\nare read into memory at once, which corresponds to an input into the MIL network \\nof shape K × 1,024. K is the number of patches (known as the bag size), which \\nvaries from slide to slide, and each patch is described by a fixed 1,024-dimensional \\nvector representation zk, produced previously in the feature-extraction step  \\nusing a pre-trained ResNet50 model. The MIL network has one fully connected \\nlayer with 512 hidden units and is followed by rectified linear unit (ReLU) \\nactivation and the classification layer. If we denote the weights and bias of each \\nlayer as W1 2 R512 ´ 1;024\\nI\\n, b1 2 R512\\nI\\n and W2 2 R2 ´ 512\\nI\\n, b2 2 R2\\nI\\n, respectively,  \\nthe unnormalized prediction score sk, 1 ≤ k ≤ K for each patch can therefore  \\nbe defined as:\\nsk ¼ W2ð ReLU ðW1zk þ b1ÞÞ þb2 ð7Þ\\nAccording to the max-pooling aggregation rule, the patch whose predicted \\nprobability score for the positive class is the highest is then selected to represent the \\nfinal slide-level prediction.\\nAs previously mentioned, this MIL algorithm was designed specifically for \\nbinary classification. To compare the performance of CLAM against MIL in the \\nmulti-class setting, we also implemented a multi-class variant of MIL, which \\nwe call mMIL; mMIL has a fully connected layer of the same dimension as our \\nbinary MIL network but we adjust the binary classification layer to instead be \\nW2 2 RN ´ 512\\nI\\n to predict the N-class probability distribution of every patch in the \\nslide. Similar to performing max-pooling in the binary case, based on the raw \\nscores, we select the patch with the highest single class probability score across all \\nclasses as the slide-level prediction. For both the MIL and mMIL models, we used \\ndropout (P = 0.25) after the model hidden layer for regularization.\\nTraining details. During training, for each slide, scores of the patch selected \\nvia max-pooling are passed to the cross-entropy loss function and the model \\nparameters are optimized via stochastic gradient descent using a batch size of one \\nand the Adam optimizer with the same hyperparameters as CLAM. Namely, we  \\nuse a learning rate of 2 × 10−4, weight decay of 1 × 10−5, with β1  set to 0.9, β2 set to \\n0.999 and ε set to 1 × 10−8. Similarly, we use the same mini-batch sampling strategy \\nand early stopping and model selection criteria for MIL/mMIL as for CLAM.  \\nFor inference, the predicted probability distribution over each class is computed  \\nby normalizing the raw predicted scores of the max-pooled patch using the \\nsoftmax function.\\nComparative analysis using the slide-level label assigned to every patch. \\nAnother weakly supervised learning framework used in computational pathology \\nwhen pixel- or ROI-level annotations are not available is to simply sample patches \\nfrom the tissue regions of each WSI and assign the slide-level label to each and \\nevery patch retrieved from that slide. We refer to this technique as SL in this study. \\nBy following this procedure, patches sampled from all WSIs in the training set can \\nsimply be treated as independent labelled data points during training. Without any \\nannotation to guide the sampling process, this procedure implies that it is possible \\nto infer the slide-level label from every patch sampled from that slide, which \\ncannot be reasonably substantiated in most classification problems performed on \\nWSIs and results in noisy labels. For example, in a positive lymph node containing \\na micro-metastasis, only a tiny fraction of the patches sampled from the slide \\nwould contain tumour and hence be responsible for the slide-level label, while \\nall remaining negative patches will be mislabelled as positive for the purpose of \\ntraining. Therefore, one would expect the performance of SL to be limited by  \\nthe level of label noise, which is closely related to the signal-to-noise ratio of \\npatches in each WSI.\\nWe used the 1,024-dimensional feature vector representation for each  \\npatch in our datasets as per CLAM and MIL/mMIL. The SL models consist  \\nof a stack of fully connected layers W1 2 R512 ´ 1;024\\nI\\n and W2 2 RN ´ 512\\nI\\n (with  \\nthe same dimensions as those in the MIL/mMIL networks) for mapping each  \\npatch embedding into N-class probability scores following softmax activation. \\nConsistent with the CLAM and MIL/mMIL models, we used dropout (P = 0.25) \\nafter the hidden layer of the SL model.\\nTraining details. During training, patches are randomly sampled from slides in \\nthe training set using a batch size of 512. For inference during validation and test \\ntime, to get the slide-level prediction, we followed a previous study35 by using the \\nmodel to first make a prediction for every patch in the slide and then averaging \\ntheir probability scores. We validate the model after every 100,000 patches and \\nuse early stopping on the model when the validation loss does not decrease \\nfor 20 consecutive validation epochs. The model checkpoint with the lowest \\nvalidation loss is used for evaluation on the test set, which is consistent with the \\nmodel selection criteria we use for MIL/mMIL and CLAM. Similarly, we use the \\ncross-entropy loss function, and the model parameters are optimized via stochastic \\ngradient descent using the Adam optimizer with a learning rate of 2 × 10−4 and \\nweight decay of 1 × 10−5, with β1  = 0.9, β2 = 0.999 and an ε value of 1 × 10−8.\\nEthics statement. The study was approved by the Mass General Brigham (MGB) \\nIRB office under protocol 2020P000233.\\nReporting Summary. Further information on research design is available in the \\nNature Research Reporting Summary linked to this article.\\nData availability\\nThe TCGA diagnostic whole-slide data (NSCLC, RCC) and corresponding labels \\nare available from the NIH genomic data commons (https://portal.gdc.cancer.\\ngov). The CPTAC whole-slide data (NSCLC) and the corresponding labels are \\navailable from the NIH cancer imaging archive (https://cancerimagingarchive.net/\\ndatascope/cptac). Metastatic-lymph-node data are publicly available from  \\nthe CAMELYON16 and CAMELYON17 website (https://camelyon17.\\ngrand-challenge.org/Data). We included links to all public data in Supplementary \\nTable 20. All reasonable requests for academic use of in-house raw and analysed \\ndata can be addressed to the corresponding author. All requests will be promptly \\nreviewed to determine whether the request is subject to any intellectual  \\nproperty or patient-confidentiality obligations, will be processed in concordance \\nwith institutional and departmental guidelines and will require a material  \\ntransfer agreement.\\nCode availability\\nAll code was implemented in Python using PyTorch as the primary deep-learning \\nlibrary. The complete pipeline for processing WSIs as well as training and \\nevaluating the deep-learning models is available at https://github.com/\\nmahmoodlab/CLAM and can be used to reproduce the experiments of this paper. \\nAll source code has been released under the GNU GPLv3 free software license.\\nReceived: 23 April 2020; Accepted: 22 December 2020;  \\nPublished online: 1 March 2021\\nReferences\\n 1. Bera, K., Schalper, K. A. & Madabhushi, A. Artificial intelligence in digital \\npathology-new tools for diagnosis and precision oncology. Nat. Rev. Clin. \\nOncol. 16, 703–715 (2019).\\n 2. Niazi, M. K. K., Parwani, A. V . & Gurcan, M. N. Digital pathology and \\nartificial intelligence. Lancet Oncol. 20, e253–e261 (2019).\\n 3. Hollon, T. C. et al. Near real-time intraoperative brain tumor diagnosis  \\nusing stimulated raman histology and deep neural networks. Nat. Med. 26, \\n52–58 (2020).\\n 4. Kather, J. N. et al. Deep learning can predict microsatellite instability directly \\nfrom histology in gastrointestinal cancer. Nat. Med. 25, 1054–1056 (2019).\\n 5. Bulten, W . et al. Automated deep-learning system for gleason grading of \\nprostate cancer using biopsies: a diagnostic study. Lancet Oncol. 21,  \\n233–241 (2020).\\n 6. Ström, P . et al. Artificial intelligence for diagnosis and grading of prostate \\ncancer in biopsies: a population-based, diagnostic study. Lancet Oncol. 21, \\n222–232 (2020).\\n 7. Schapiro, D. et al. histoCAT: analysis of cell phenotypes and interactions in \\nmultiplex image cytometry data. Nat. Methods 14, 873–876 (2017).\\n 8. Moen, E. et al. Deep learning for cellular image analysis. Nat. Methods 16, \\n1233–1246 (2019).\\n 9. Mahmood, F . et al. Deep adversarial training for multi-organ nuclei \\nsegmentation in histopathology images. IEEE Trans. Med. Imaging 39, \\n3257–3267 (2019).\\n 10. Graham, S. et al. Hover-net: simultaneous segmentation and classification of \\nnuclei in multi-tissue histology images. Med. Image Anal. 58, 101563 (2019).\\n 11. Saltz, J. et al. Spatial organization and molecular correlation of \\ntumor-infiltrating lymphocytes using deep learning on pathology images.  \\nCell Rep. 23, 181–193 (2018).\\n 12. Javed, S. et al. Cellular community detection for tissue phenotyping in \\ncolorectal cancer histology images. Med. Image Anal. 63, 101696 (2020).\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng 569\\nArticles NATure BIoMeDICAL eNgINeerINg\\n 13. Mobadersany, P . et al. Predicting cancer outcomes from histology and \\ngenomics using convolutional networks. Proc. Natl Acad. Sci. USA 115, \\nE2970–E2979 (2018).\\n 14. Heindl, A. et al. Microenvironmental niche divergence shapes \\nbrca1-dysregulated ovarian cancer morphological plasticity. Nat. Commun. 9, \\n3917 (2018).\\n 15. Yuan, Y . et al. Quantitative image analysis of cellular heterogeneity in breast \\ntumors complements genomic profiling. Sci. Transl. Med. 4, 157ra143 (2012).\\n 16. Lazar, A. J. et al. Comprehensive and integrated genomic characterization of \\nadult soft tissue sarcomas. Cell 171, 950–965 (2017).\\n 17. Fu, Y . et al. Pan-cancer computational histopathology reveals mutations, \\ntumor composition and prognosis. Nat. Cancer 1, 800–810 (2020).\\n 18. Kather, J. N. et al. Pan-cancer image-based detection of clinically actionable \\ngenetic alterations. Nat. Cancer 1, 789–799 (2020).\\n 19. Chen, R. J. et al. Pathomic fusion: an integrated framework for fusing \\nhistopathology and genomic features for cancer diagnosis and prognosis. \\nIEEE Trans. Med. Imaging https://doi.org/10.1109/TMI.2020.3021387 (2020).\\n 20. Beck, A. H. et al. Systematic analysis of breast cancer morphology  \\nuncovers stromal features associated with survival. Sci. Transl Med. 3, \\n108ra113 (2011).\\n 21. Y amamoto, Y . et al. Automated acquisition of explainable knowledge from \\nunannotated histopathology images. Nat. Commun. 10, 5642 (2019).\\n 22. Pell, R. et al. The use of digital pathology and image analysis in clinical trials. \\nJ. Pathol. Clin. Res. 5, 81–90 (2019).\\n 23. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521,  \\n436–444 (2015).\\n 24. Esteva, A. et al. A guide to deep learning in healthcare. Nat. Med. 25,  \\n24–29 (2019).\\n 25. Esteva, A. et al. Dermatologist-level classification of skin cancer with deep \\nneural networks. Nature 542, 115–118 (2017).\\n 26. Poplin, R. et al. Prediction of cardiovascular risk factors from retinal fundus \\nphotographs via deep learning. Nat. Biomed. Eng. 2, 158–164 (2018).\\n 27. McKinney, S. M. et al. International evaluation of an ai system for breast \\ncancer screening. Nature 577, 89–94 (2020).\\n 28. Mitani, A. et al. Detection of anaemia from retinal fundus images via deep \\nlearning. Nat. Biomed. Eng. 4, 18–27 (2020).\\n 29. Shen, L., Zhao, W . & Xing, L. Patient-specific reconstruction of volumetric \\ncomputed tomography images from a single projection view via deep \\nlearning. Nat. Biomed. Eng. 3, 880–888 (2019).\\n 30. Tellez, D., Litjens, G., van der Laak, J. & Ciompi, F . Neural image \\ncompression for gigapixel histopathology image analysis. IEEE Trans. Pattern \\nAnal. Mach. Intell. 43, 567–578 (2019).\\n 31. Bejnordi, B. E. et al. Diagnostic assessment of deep learning algorithms for \\ndetection of lymph node metastases in women with breast cancer. JAMA 318, \\n2199–2210 (2017).\\n 32. Chen, P .-H. C. et al. An augmented reality microscope with real-time \\nartificial intelligence integration for cancer diagnosis. Nat. Med. 25, \\n1453–1457 (2019).\\n 33. Nagpal, K. et al. Development and validation of a deep learning  \\nalgorithm for improving gleason scoring of prostate cancer. npj Digit.  \\nMed. 2, 48 (2019).\\n 34. Wang, S. et al. RMDL: recalibrated multi-instance deep learning for whole \\nslide gastric image classification. Med. Image Anal. 58, 101549 (2019).\\n 35. Coudray, N. et al. Classification and mutation prediction from non-small cell \\nlung cancer histopathology images using deep learning. Nat. Med. 24, \\n1559–1567 (2018).\\n 36. Campanella, G. et al. Clinical-grade computational pathology using  \\nweakly supervised deep learning on whole slide images. Nat. Med. 25, \\n1301–1309 (2019).\\n 37. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple  \\ninstance learning. In International Conference on Machine Learning  \\n(eds Lawrence, M. & Reid, M.) 2132–2141 (PMLR, 2018).\\n 38. Maron, O. & Lozano-Pérez, T. A framework for multiple-instance learning.  \\nIn Advances in Neural Information Processing Systems (eds Jordan, M. I. et al.) \\n570–576 (Citeseer, 1998).\\n 39. Schaumberg, A. J. et al. Interpretable multimodal deep learning for real-time \\npan-tissue pan-disease pathology search on social media. Mod. Pathol. 33, \\n2169–2185 (2020).\\n 40. BenTaieb, A. & Hamarneh, G. Adversarial stain transfer for histopathology \\nimage analysis. IEEE Trans. Med. Imaging 37, 792–802 (2017).\\n 41. Couture, H. D., Marron, J. S., Perou, C. M., Troester, M. A. &  \\nNiethammer, M. Multiple instance learning for heterogeneous images: \\ntraining a CNN for histopathology. In International Conference on Medical \\nImage Computing and Computer-Assisted Intervention (eds Frangi, A. F . et al.) \\n254–262 (Springer, 2018).\\n 42. Kraus, O. Z., Ba, J. L. & Frey, B. J. Classifying and segmenting microscopy \\nimages with deep multiple instance learning. Bioinformatics 32,  \\ni52–i59 (2016).\\n 43. Zhang, C., Platt, J. C. & Viola, P . A. Multiple instance boosting for  \\nobject detection. In Advances in Neural Information Processing Systems  \\n(eds Weiss, Y . et al.) 1417–1424 (Citeseer, 2006).\\n 44. Berrada, L., Zisserman, A. & Kumar, M. P . Smooth loss functions for  \\ndeep top-k classification. In International Conference on Learning \\nRepresentations (2018).\\n 45. Crammer, K. & Singer, Y . On the algorithmic implementation of multiclass \\nkernel-based vector machines. J. Mach. Learn. Res. 2, 265–292 (2001).\\n 46. Litjens, G. et al. 1399 H&E-stained sentinel lymph node sections of breast \\ncancer patients: the CAMELYON dataset. GigaScience 7, giy065 (2018).\\n 47. Russakovsky, O. et al. ImageNet large scale visual recognition challenge.  \\nInt. J. Comput. Vis. 115, 211–252 (2015).\\nacknowledgements\\nThe authors thank A. Bruce for scanning internal cohorts of patient histology slides at \\nBWH; J. Wang, K. Bronstein, L. Cirelli and S. Sahai for querying the BWH slide database \\nand retrieving archival slides; M. Bragg, S. Zimmet and T. Mellen for administrative \\nsupport; and Z. Noor for developing the interactive demo website. This work was \\nsupported in part by internal funds from BWH Pathology, the NIH National Institute of \\nGeneral Medical Sciences (NIGMS) grant no. R35GM138216A (to F .M.), a Google Cloud \\nResearch Grant and the Nvidia GPU Grant Program. R.J.C. was additionally supported \\nby the NSF Graduate Research Fellowship and NIH National Human Genome Research \\nInstitute (NHGRI) grant no. T32HG002295. The content is solely the responsibility of \\nthe authors and does not reflect the official views of the National Institute of Health, \\nNational Institute of General Medical Sciences, National Human Genome Research \\nInstitute and the National Science Foundation.\\nauthor contributions\\nM.Y .L. and F .M. conceived the study and designed the experiments. M.Y .L. performed \\nthe experimental analysis. D.F .K.W . and T.Y .C. curated the in-house datasets and \\ncollected smartphone microscopy data. M.Y .L., R.J.C and M.B. developed and tested  \\nthe CLAM Python package. M.Y .L. and F .M. prepared the manuscript. F .M. supervised \\nthe research.\\nCompeting interests\\nThe authors declare no competing interests.\\nadditional information\\nSupplementary information The online version contains supplementary material \\navailable at https://doi.org/10.1038/s41551-020-00682-w.\\nCorrespondence and requests for materials should be addressed to F .M.\\nPeer review information Nature Biomedical Engineering thanks Anant Madabhushi, \\nGeert Litjens and the other, anonymous, reviewer(s) for their contribution to the peer \\nreview of this work.\\nReprints and permissions information is available at www.nature.com/reprints.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\n© The Author(s), under exclusive licence to Springer Nature Limited 2021\\nNaTuRe BioMeDiCaL eNgiNeeRiNg | VOL 5 | JUne 2021 | 555–570 | www.nature.com/ natbiomedeng570\\n\\n2 nature research  |  reporting summaryApril 2020\\nMetastatic-lymph-node data are publicly available from the Camelyon16 and Camelyon17 website (https://camelyon17.grand-challenge.org/Data). We included \\nlinks to all public data in Supplementary Table 20. All reasonable requests for academic use of in-house raw and analysed data can be addressed to the \\ncorresponding author. All requests will be promptly reviewed to determine whether the request is subject to any intellectual property or patient-confidentiality \\nobligations, will be processed in concordance with institutional and departmental guidelines, and will require a material transfer agreement.\\nField-specific reporting\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\\nLife sciences study design\\nAll studies must disclose on these points even when the disclosure is negative.\\nSample size No statistical methods were used to predetermine sample size. We used all available data from publicly available repositories for model \\ndevelopment. Details are given below. \\n \\nPublic Datasets: All publicly available data was used for NSCLC (TCGA+CPTAC), RCC (TCGA), Lymph Node Met. (Camelyon 16, 17) was \\nrandomly split into training (80%), validation (10%) and test (10%). \\nBWH Independent test cohorts: For testing, the BWH pathology archives were queried, and cases were randomly sampled and requested \\nfrom in-house pathology archives (2016–2019). We requested 150 resection cases for each problem, and 110 biopsy cases for NSCLC and RCC \\nsubtyping that were available in-house. Based on the availability of slides and after excluding slides with no tumor content we retrieved: \\nNSCLC (n=131): Adenocarcinoma (n=63); Squamous Cell Carcinoma (n=68) [imaged using both WSI scanner and using a cellphone microscope] \\nRCC (n=135): Clear Cell (n=46); Papillary (n=46); Chromophobe (n=43) [imaged using both WSI scanner and using a cellphone microscope] \\nBRCA Lymph Node Mets. (n=133): Positive (n=67); Negative (n=66)  \\nNSCLC Biopsies (n=110): Adenocarcinoma (n=55); Squamous Cell Carcinoma (n=55) \\nRCC Biopsies (n=92): Clear Cell (n=53); Papillary (n=26); Chromophobe (n=13) \\nSample size for Chromophobe RCC biopsies was limited by the number of patient cases available for the rare condition (since it represents \\n<5% of all RCC cases with only a few biopsy cases). A more detailed dataset summary is given in Supplementary table 8.\\nData exclusions Pre-established exclusion criteria include slides with significant marking covering the tissue area, damaged and missing tissue slides, and slides \\nwith no tumor content (for NSCLC and RCC subtyping). Slides with markings that do not predominantly cover tissue regions were not \\nexcluded. Additionally, fewer than 5 digitized slides downloaded from the TCGA were removed because the image files could not be \\nprocessed on our workstation hardware using openslide owing to either file corruption or the absence of any low-resolution downsamples.\\nReplication For each problem and for each training set size, denomination models were trained 10 times during cross-validation. For testing, replication \\nwas successful under all conditions for which results were reported. \\nRandomization For the TCGA, CPTAC and Camelyon data, patients were randomly divided into three groups: training, validation, and test sets. No other \\ncovariates were controlled for.\\nBlinding Blinding was not necessary because our experiments were based on digitized histology slides.\\nReporting for specific materials, systems and methods\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \\nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \\nMaterials & experimental systems\\nn/a Involved in the study\\nAntibodies\\nEukaryotic cell lines\\nPalaeontology and archaeology\\nAnimals and other organisms\\nHuman research participants\\nClinical data\\nDual use research of concern\\nMethods\\nn/a Involved in the study\\nChIP-seq\\nFlow cytometry\\nMRI-based neuroimaging\\n3 nature research  |  reporting summaryApril 2020\\nHuman research participants\\nPolicy information about studies involving human research participants\\nPopulation characteristics Public Data: TCGA and CPTAC contain data from a diverse population representing multiple hospitals. Camelyon data were \\ncollected from five different hospitals.  \\nIn-house BWH Data: All patient cases between 2016–2019 were queried from the pathology database, and the test set was \\nrandomly selected from that patient population. \\nRecruitment No patient recruitment was necessary for the use of histology whole-slide images retrospectively.  \\nEthics oversight The Mass General Brigham IRB committee approved the study (approval 2020P000233).\\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\n'}, {'file_name': 'rosettafold', 'text': 'RESEARCH ARTICLE\\n◥\\nPROTEIN FOLDING\\nAccurate prediction of protein structures and\\ninteractions using a three-track neural network\\nMinkyung Baek1,2, Frank DiMaio1,2, Ivan Anishchenko1,2, Justas Dauparas1,2, Sergey Ovchinnikov3,4,\\nGyu Rie Lee1,2, Jue Wang1,2, Qian Cong5,6, Lisa N. Kinch7, R. Dustin Schaeffer6, Claudia Millán8,\\nHahnbeom Park1,2, Carson Adams1,2, Caleb R. Glassman9,10,11, Andy DeGiovanni12, Jose H. Pereira12,\\nAndria V. Rodrigues12, Alberdina A. van Dijk13, Ana C. Ebrecht13, Diederik J. Opperman14,\\nTheo Sagmeister15, Christoph Buhlheller15,16, Tea Pavkov-Keller15,17, Manoj K. Rathinaswamy18,\\nUdit Dalwadi19, Calvin K. Yip19, John E. Burke18, K. Christopher Garcia9,10,11,20, Nick V. Grishin6,7,21,\\nPaul D. Adams12,22, Randy J. Read8, David Baker1,2,23*\\nDeepMind presented notably accurate predictions at the recent 14th Critical Assessment of\\nStructure Prediction (CASP14) conference. We explored network architectures that incorporate related\\nideas and obtained the best performance with a three-track network in which information at the\\none-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is\\nsuccessively transformed and integrated. The three-track network produces structure predictions\\nwith accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging\\nx-ray crystallography and cryo–electron microscopy structure modeling problems, and provides insights\\ninto the functions of proteins of currently unknown structure. The network also enables rapid generation\\nof accurate protein-protein complex models from sequence information alone, short-circuiting traditional\\napproaches that require modeling of individual subunits followed by docking. We make the method\\navailable to the scientific community to speed biological research.\\nT\\nhe prediction of protein structure from\\namino acid sequence information alone\\nhas been a long-standing challenge. The\\nbiannual Critical Assessment of Struc-\\nture Prediction (CASP) meetings have\\ndemonstrated that deep-learning methods\\nsuch as AlphaFold (1, 2) and trRosetta (3),\\nwhich extract information from the large data-\\nbase of known protein structures in the Protein\\nData Bank (PDB), outperform more traditional\\napproaches that explicitly model the folding pro-\\ncess. The outstanding performance of DeepMind’s\\nAlphaFold2 in the recent 14th CASP (CASP14)\\nmeeting (https://predictioncenter.org/casp14/\\nzscores_final.cgi) leftthe scientific community\\neager to learn details beyond the overall frame-\\nwork that was presented and raised the ques-\\ntion of whether such accuracy could be achieved\\noutside of a world-leading deep-learning com-\\npany. As described at the CASP14 conference,\\nthe AlphaFold2 methodological advances in-\\ncluded (i) starting from multiple sequence align-\\nments (MSAs) rather than from more-processed\\nfeatures such as inverse covariance matrices\\nderived from MSAs, (ii) replacement of two-\\ndimensional (2D) convolution with an attention\\nmechanism that better represents interactions\\nbetween residues distant along the sequence,\\n(iii) use of a two-track network architecture in\\nwhich information at the 1D sequence level\\nand the 2D distance map level is iteratively\\ntransformed and passed back and forth, (iv)\\nuse of an SE(3)-equivariant Transformer net-\\nwork to directly refine atomic coordinates\\n(rather than 2D distance maps as in previous\\napproaches) generated from the two-track net-\\nwork, and (v) end-to-end learning in which\\nall network parameters are optimized by back-\\npropagation from the final generated 3D co-\\nordinates through all network layers back\\nto the input sequence.\\nNetwork architecture development\\nIntrigued by the DeepMind results, and with\\nthe goal of increasing protein structure predic-\\ntion accuracy for structural biology research\\nand advancing protein design (4), we explored\\nnetwork architectures that incorporate differ-\\nent combinations of these five properties. In\\nthe absence of a published method, we experi-\\nmented with a wide variety of approaches for\\npassing information between different parts of\\nthe networks, as summarized in the methods\\nand table S1. We succeeded in producing a\\n“two-track” network with information flow-\\ning in parallel along a 1D sequence alignment\\ntrack and a 2D distance matrix track with con-\\nsiderably better performance than trRosetta\\n(BAKER-ROSETTASERVER and BAKER in\\nFig. 1B), the next-best method after AlphaFold2\\nin CASP14 (https://predictioncenter.org/casp14/\\nzscores_final.cgi).\\nWe reasoned that better performance could\\nbe achieved by extending to a third track oper-\\nating in 3D coordinate space to provide a tighter\\nconnection between sequence, residue-residue\\ndistances and orientations, and atomic coor-\\ndinates. We constructed architectures with the\\ntwo levels of the two-track model augmented\\nwith a third parallel structure track operating\\non 3D backbone coordinates, as depicted in\\nFig. 1A (see methods and fig. S1 for details). In\\nthis architecture, information flows back and\\nforth between the 1D amino acid sequence in-\\nformation, the 2D distance map, and the 3D\\ncoordinates, allowing the network to collect-\\nively reason about relationships within and\\nbetween sequences, distances, and coordinates.\\nBy contrast, reasoning about 3D atomic coor-\\ndinates in the two-track AlphaFold2 architec-\\nture happens after processing of the 1D and 2D\\ninformation is complete (although end-to-end\\ntraining does link parameters to some extent).\\nBecause of computer hardware memory lim-\\nitations, we could not train models on large\\nproteins directly because the three-track models\\nhave many millions of parameters; instead, we\\npresented to the network many discontinu-\\nous crops of the input sequence consisting of\\ntwo discontinuous sequence segments span-\\nning a total of 260 residues. To generate final\\nmodels, we combined and averaged the 1D fea-\\ntures and 2D distance and orientation predic-\\ntions produced for each of the crops and then\\nused two approaches to generate final 3D\\nstructures. In the first, the predicted residue-\\nresidue distance and orientation distributions\\nare fed into pyRosetta (5)t og e n e r a t ea l l - a t o m\\nmodels. In the second, the averaged 1D and 2D\\nRESEARCH\\nBaek et al., Science 373, 871– 876 (2021) 20 August 2021 1o f6\\n1Department of Biochemistry, University of Washington, Seattle, WA 98195, USA.2Institute for Protein Design, University of Washington, Seattle, WA 98195, USA.3Faculty of Arts and Sciences,\\nDivision of Science, Harvard University, Cambridge, MA 02138, USA.4John Harvard Distinguished Science Fellowship Program, Harvard University, Cambridge, MA 02138, USA.5Eugene\\nMcDermott Center for Human Growth and Development, University of Texas Southwestern Medical Center, Dallas, TX, USA.6Department of Biophysics, University of Texas Southwestern Medical\\nCenter, Dallas, TX, USA.7Howard Hughes Medical Institute, University of Texas Southwestern Medical Center, Dallas, TX, USA.8Department of Haematology, Cambridge Institute for Medical\\nResearch, University of Cambridge, Cambridge, UK.9Program in Immunology, Stanford University School of Medicine, Stanford, CA 94305, USA.10Department of Molecular and Cellular\\nPhysiology, Stanford University School of Medicine, Stanford, CA 94305, USA.11Department of Structural Biology, Stanford University School of Medicine, Stanford, CA 94305, USA.12Molecular\\nBiophysics & Integrated Bioimaging Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA.13Department of Biochemistry, Focus Area Human Metabolomics, North-West University,\\n2531 Potchefstroom, South Africa.14Department of Biotechnology, University of the Free State, 205 Nelson Mandela Drive, Bloemfontein 9300, South Africa.15Institute of Molecular Biosciences,\\nUniversity of Graz, Humboldtstrasse 50, 8010 Graz, Austria.16Medical University of Graz, Graz, Austria.17BioTechMed-Graz, Graz, Austria.18Department of Biochemistry and Microbiology,\\nUniversity of Victoria, Victoria, BC, Canada.19Life Sciences Institute, Department of Biochemistry and Molecular Biology, The University of British Columbia, Vancouver, BC, Canada.20Howard\\nHughes Medical Institute, Stanford University School of Medicine, Stanford, CA 94305, USA.21Department of Biochemistry, University of Texas Southwestern Medical Center, Dallas, TX, USA.\\n22Department of Bioengineering, University of California, Berkeley, Berkeley, CA 94720, USA.23Howard Hughes Medical Institute, University of Washington, Seattle, WA 98195, USA.\\n*Corresponding author. Email: dabaker@uw.edu\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nfeatures are fed into a final SE(3)-equivariant\\nlayer (6), and, after end-to-end training from\\namino acid sequence to 3D coordinates, back-\\nbone coordinates are generated directly by\\nthe network (see methods). We refer to these\\nnetworks, which also generate per-residue ac-\\ncuracy predictions, as RoseTTAFold. The first\\nhas the advantages of requiring lower-memory\\ngraphics processing units (GPUs) at inference\\ntime [for proteins with more than 400 resi-\\ndues, 8 gigabytes (GB) rather than 24 GB] and\\nof producing full side-chain models but re-\\nquires central processing unit (CPU) time for\\nthe pyRosetta structure modeling step.\\nThe three-track models with attention oper-\\nating at the 1D, 2D, and 3D levels and infor-\\nmation flowing between the three levels were\\nthe best models we tested (Fig. 1B), clearly out-\\nperforming the top two server groups (Zhang-\\nserver and BAKER-ROSETTASERVER), BAKER\\nhuman group (ranked second among all groups),\\nand our two-track attention models on CASP14\\ntargets. As in the case of AlphaFold2, the cor-\\nrelation between MSA depth and model accu-\\nracy is lower for RoseTTAFold than for trRosetta\\nand other methods tested at CASP14 (fig. S2).\\nT h ep e r f o r m a n c eo ft h et h r e e - t r a c km o d e l\\non the CASP14 targets was still not as good\\nas AlphaFold2 (Fig. 1B). This could reflect\\nhardware limitations that limited the size\\nof the models we could explore, alternative\\narchitectures or loss formulations, or more\\nintensive use of the network for inference.\\nDeepMind reported using several GPUs for\\ndays to make individual predictions, whereas\\nour predictions are made in a single pass\\nthrough the network in the same manner that\\nwould be used for a server; after sequence and\\ntemplate search (~1.5 hours), the end-to-end\\nversion of RoseTTAFold requires ~10 min on an\\nRTX2080 GPU to generate backbone coordi-\\nnates for proteins with fewer than 400 resi-\\ndues, and the pyRosetta version requires 5 min\\nfor network calculations on a single RTX2080\\nGPU and an hour for all-atom structure gen-\\neration with 15 CPU cores. Incomplete opti-\\nmization due to computer memory limitations\\nand neglect of side-chain information likely\\nexplain the poorer performance of the end-\\nto-end version compared with the pyRosetta\\nversion (Fig. 1B; the latter incorporates side-\\nchain information at the all-atom relaxation\\nstage); because SE(3)-equivariant layers are\\nused in the main body of the three-track model,\\nthe added gain from the final SE(3) layer is\\nlikely less than that in the AlphaFold2 case.\\nWe expect the end-to-end approach to ulti-\\nmately be at least as accurate once the com-\\nputer hardware limitations are overcome and\\nside chains are incorporated.\\nThe improved performance of the three-track\\nmodels over the two-track model with identical\\ntraining sets, similar attention-based architec-\\ntures for the 1D and 2D tracks, and similar\\noperations in inference (prediction) mode\\nsuggests that simultaneously reasoning at\\nthe MSA, distance map, and 3D coordinate\\nrepresentations can more effectively extract\\nsequence-structure relationships than reason-\\ning over only MSA and distance map informa-\\ntion. The relatively low computational cost\\nmakes it straightforward to incorporate the\\nmethods in a public server and predict struc-\\ntures for large sets of proteins, for example,\\nall human G protein–coupled receptors (GPCRs),\\nas described below.\\nBlind structure prediction tests are needed\\nto assess any new protein structure predic-\\ntion method, but CASP is held only once every\\n2y e a r s .F o r t u n a t e l y ,t h eC o n t i n u o u sA u t o m a t e d\\nModel Evaluation (CAMEO) experiment (7)\\ntests structure prediction servers blindly on\\nprotein structures as they are submitted to the\\nPDB. RoseTTAFold has been evaluated since\\n15 May 2021 on CAMEO; over the 69 me-\\ndium and hard targets released during this\\ntime (15 May 2021 to 19 June 2021), it out-\\nperformed all other servers evaluated in the\\nexperiment, including Robetta (3), IntFold6-\\nTS (8), BestSingleTemplate (9), and SWISS-\\nMODEL (10)( F i g .1 C ) .\\nWe experimented with approaches for fur-\\nther improving accuracy by more intensive\\nuse of the network during sampling. Because\\nthe network can take templates of known\\nstructures as input, we experimented with a\\nBaek et al., Science 373, 871– 876 (2021) 20 August 2021 2o f6\\nFig. 1. Network architecture and\\nperformance. (A) RoseTTAFold\\narchitecture with 1D, 2D, and\\n3D attention tracks. Multiple\\nconnections between tracks allow\\nthe network to simultaneously\\nlearn relationships within and\\nbetween sequences, distances,\\nand coordinates (see methods and\\nfig. S1 for details). (B) Average\\nTM-score of prediction methods on\\nthe CASP14 targets. Zhang-server\\nand BAKER-ROSETTASERVER\\nwere the top two server groups,\\nwhereas AlphaFold2 and BAKER\\nwere the top two human groups in\\nCASP14; BAKER-ROSETTASERVER\\nand BAKER predictions were based\\non trRosetta. Predictions with the\\ntwo-track model and RoseTTAFold\\n(both end-to-end and pyRosetta\\nversion) were completely automated.\\n(C) Blind benchmark results on\\nCAMEO medium and hard targets;\\nmodel accuracies are TM-score\\nvalues from the CAMEO website\\n(https://cameo3d.org/). In (B) and\\n(C), the error bars represent a 95%\\nconfidence interval.\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nfurther coupling of 3D structural information\\nand 1D sequence information by iteratively\\nfeeding the predicted structures back into\\nthe network as templates and random sub-\\nsampling from the MSAs to sample a broader\\nrange of models. These approaches generated\\nensembles that contained higher-accuracy\\nmodels, but the accuracy predictor was not\\nable to consistently identify models better\\nthan those generated by the rapid single-pass\\nmethod (fig. S3). Nevertheless, we suspect that\\nthese approaches can improve model perform-\\nance, and we are carrying out further investi-\\ngations along these lines.\\nIn developing RoseTTAFold, we found that\\ncombining predictions from multiple discontin-\\nuous crops generated more-accurate structures\\nthan predicting the entire structure at once (fig.\\nS4A). We hypothesized that this arises from\\nselecting the most relevant sequences for each\\nregion from the very large number of aligned\\nsequences that are often available (fig. S4B).\\nTo enable the network to focus on the most\\nrelevant sequence information for each re-\\ngion while keeping access to the full MSA in a\\nmore memory-efficient way, we experimented\\nwith the Perceiver architecture (11), updating\\nsmaller-seed MSAs (up to 100 sequences) with\\nextra sequences (thousands of sequences)\\nthrough cross-attention (fig. S4C). As of now,\\nRoseTTAFold only uses the top 1000 sequences\\nbecause of memory limitations; with this addi-\\ntion, all available sequence information can\\nbe used (often more than 10,000 sequences).\\nInitial results are promising (fig. S4D), but\\nmore training will be required for rigorous\\ncomparison.\\nEnabling experimental protein\\nstructure determination\\nWith the recent considerable progress in pro-\\ntein structure prediction, a key question is\\nwhat accurate protein structure models can\\nbe used for. We investigated the utility of the\\nRoseTTAFold to facilitate experimental struc-\\nture determination by x-ray crystallography\\nand cryo– electron microscopy (cryo-EM) and\\nto build models that provide biological in-\\nsights for key proteins of currently unknown\\nstructures.\\nSolution of x-ray structures by molecular\\nreplacement (MR) often requires quite accu-\\nrate models. The much higher accuracy of the\\nRoseTTAFold method compared with currently\\navailable methods prompted us to test whether\\nit could help solve previously unsolved chal-\\nlenging MR problems and improve the solution\\nof borderline cases. Fourrecent crystallographic\\ndatasets (summarized, including resolution\\nlimits, in table S2), which had eluded solution\\nby MR using models available in the PDB, were\\nreanalyzed using RoseTTAFold models: glycine\\nN-acyltransferase (GLYAT) fromBos taurus\\n(fig. S5A), a bacterial oxidoreductase (fig. S5B),\\na bacterial surface layer protein (SLP) (Fig. 2A),\\nand the secreted protein Lrbp from the fungus\\nPhanerochaete chrysosporium(Fig. 2B and fig.\\nS5C). In all four cases, the predicted models\\nhad sufficient structural similarity to the true\\nstructures that enabled solution of the struc-\\ntures by MR [see methods for details; the per-\\nresidue error estimates by DeepAccNet (12)\\nallowed the more accurate parts to be weighted\\nmore heavily]. The increased prediction accu-\\nracy was critical for success in all cases; models\\nmade with trRosetta didnot yield MR solutions.\\nTo determine why the RoseTTAFold models\\nwere successful where PDB structures had pre-\\nviously failed, we compared the models to the\\ncrystal structures we obtained. The images in\\nFig. 2A and fig. S5 show that in each case, the\\nclosest homolog of the known structure was a\\nmuch poorer model than the RoseTTAFold\\nm o d e l ;i nt h ec a s eo fS L P ,o n l yad i s t a n tm o d e l\\ncovering part of the N-terminal domain (38%\\nof the sequence) was available in the PDB,\\nwhereas no homologs of the C-terminal domain\\nof SLP or any portion of Lrbp could be detected\\nusing HHsearch (13).\\nBuilding atomic models of protein assem-\\nblies from cryo-EM maps can be challenging in\\nthe absence of homologs with known structures.\\nWe used RoseTTAFold to predict the p101 Gbg\\nbinding domain (GBD) structure in a hetero-\\ndimeric PI3Kg complex. The top HHsearch hit\\nhas a statistically insignificant E-value of 40\\nand only covers 14 out of 167 residues. The\\npredicted structure could readily fit into the\\nelectron density map despite the low local res-\\nolution [Fig. 2C, top; trRosetta failed to predict\\nt h ec o r r e c tf o l dw i t ht h es a m eM S Ai n p u t( f i g .\\nS6)]. The Ca-RMSD (root mean square de-\\nviation) between the predicted and the final\\nrefined structure is 3.0 Å for the coreb sheets\\n(Fig. 2C, bottom).\\nProviding insights into biological function\\nExperimental structure determination can pro-\\nvide considerable insight into biological func-\\ntion and mechanism. We investigated whether\\nstructures generated by RoseTTAFold could\\nsimilarly provide new insights into function.\\nWe focused on two sets of proteins: first, GPCRs\\nof currently unknown structure; and second,\\na set of human proteins implicated in disease.\\nBenchmark tests on GPCR sequences with deter-\\nmined structures showed that RoseTTAFold\\nmodels for both active and inactive states can\\nbe quite accurate even in the absence of close\\nhomologs with known structures [and better\\nthan those in current GPCR model databases\\n(14, 15); fig. S7] and that the DeepAccNet model\\nquality predictor (12) provides a good measure\\nof actual model accuracy (fig. S7D). We provide\\nBaek et al., Science 373, 871– 876 (2021) 20 August 2021 3o f6\\nFig. 2. Enabling experimental structure determination with RoseTTAFold.(A and B) Successful\\nmolecular replacement with RoseTTAFold models. SLP is shown in (A). The C-terminal domain is shown\\nat the top, with a comparison of final refined structure (gray) to RoseTTAFold model (blue); there are no\\nhomologs with known structure. The N-terminal domain is shown at the bottom; the refined structure is in\\ngray, and the RoseTTAFold model is colored by the estimated root mean square (RMS) error (ranging\\nfrom blue for 0.67 Å to red for 2 Å or greater). Ninety-five Ca atoms of the RoseTTAFold model can be\\nsuperimposed within 3 Å of Ca atoms in the final structure, yielding a Ca-RMSD of 0.98 Å. By contrast, only\\n54 Ca atoms of the closest template (4l3a, brown) can be superimposed (with a Ca-RMSD of 1.69 Å). In (B),\\nthe refined structure of Lrbp (gray) with the closest RoseTTAFold model (blue) superimposed is shown;\\nresidues having an estimated RMS error greater than 1.3 Å are omitted (full model is in fig. S5C). (C) Cryo-EM\\nstructure determination of the p101 GBD in a heterodimeric PI3Kg complex using RoseTTAFold. At the\\ntop, RoseTTAFold models colored in a rainbow from the N terminus (blue) to the C terminus (red) have\\na consistent all-b topology with a clear correspondence to the density map. Shown at the bottom is a\\ncomparison of the final refined structure to the RoseTTAFold model colored by predicted RMS error ranging\\nfrom blue for 1.5 Å or less to red for 3 Å or greater. The actual Ca-RMSD between the predicted structure\\nand final refined structure is 3.0 Å over theb sheets. The figure was prepared with ChimeraX (35).\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nRoseTTAFold models and accompanying ac-\\ncuracy predictions for closed and open states\\nof all human GPCRs of currently unknown\\nstructure.\\nProtein structures can provide insight into\\nhow mutations lead to human disease. We iden-\\ntified human proteins without close homologs of\\nknown structure that contain multiple disease-\\ncausing mutations or haveb e e nt h es u b j e c to f\\nintensive experimental investigation (see meth-\\nods). We used RoseTTAFold to generate mod-\\nels for 693 domains from such proteins. More\\nthan one-third of these models have a predicted\\nlocal distance difference test (lDDT) >0.8,\\nwhich corresponded to an average Ca-RMSD\\nof 2.6 Å on CASP14 targets (fig. S8). Here, we\\nfocus on three examples that illustrate the\\ndifferent ways in which structure models can\\nprovide insight into the function or mecha-\\nnisms of diseases.\\nDeficiencies in TANGO2 (transport and Golgi\\norganization protein 2) lead to metabolic dis-\\norders, and the protein plays an unknown role\\nin Golgi membrane redistribution into the en-\\ndoplasmic reticulum (16, 17). The RoseTTAFold\\nmodel of TANGO2 adopts an N-terminal nu-\\ncleophile aminohydrolase (Ntn) fold (Fig. 3A)\\nwith well-aligned active-site residues that are\\nconserved in TANGO2 orthologs (Fig. 3B). Ntn\\nsuperfamily members with structures sim-\\nilar to the RoseTTAFold model suggest that\\nTANGO2 may function as an enzyme that\\nhydrolyzes a carbon-nitrogen bond in a mem-\\nbrane component (18). Based on the model,\\nknown mutations that cause disease (magenta\\nspheres in Fig. 3A) could act by hindering cat-\\nalysis [Arg26→Lys (R26K), Arg32→Gln (R32Q),\\nand Leu50→Pro (L50P), near thea c t i v es i t e ]o r\\nproduce steric clashes [Gly154→Arg (G154R)]\\n(19) in the hydrophobic core. By comparison, a\\nhomology model based on very distant (<15%\\nsequence identity) homologs had multiple\\nalignment shifts that misplace key conserved\\nresidues (fig. S9 and table S3)\\nThe ADAM (a disintegrin and metallopro-\\ntease) and ADAMTS (a disintegrin and metal-\\nloproteinase with thrombospondin motifs)\\nfamilies of metalloproteases are encoded by\\nmore than 40 human genes, mediate cell-cell\\nand cell-matrix interactions (20, 21), and are\\ninvolved in a range of human diseases, includ-\\ning cancer metastasis, inflammatory disorders,\\nneurological diseases, and asthma (21, 22). The\\nADAMs contain prodomain and metallopro-\\ntease domains; the fold of the metalloprotease\\nis known (23, 24), but the fold of the prodomain,\\nwhich has no homologs of known structure, is\\nnot. The RoseTTAFold-predicted structure of\\nthe ADAM33 prodomainhas a lipocalin-like\\nb-barrel fold (Fig. 3C) that belongs to an ex-\\ntended superfamily that includes metallopro-\\ntease inhibitors (25). There is a cysteine in an\\nextension following the predicted prodomain\\nbarrel; taken together, these data are consist-\\nent with experimental data that suggest that\\nthe ADAM prodomain inhibits metalloprotease\\nactivity using a cysteine switch (26). Conserved\\nresidues within ADAM33 orthologs line one\\nside of the barrel and likely interact with the\\nmetalloprotease (Fig. 3D).\\nTransmembrane spanning ceramide synthase\\n(CERS1) is a key enzyme in sphingolipid metab-\\nolism that uses acyl– coenzyme A (acyl-CoA) to\\ngenerate ceramides with various acyl chain\\nlengths that regulate differentiation, prolifer-\\nation, and apoptosis (27). Structure information\\nis not available for any of the ceramide synthase\\nenzymes or their homologs, and the number\\nand orientation of transmembrane helices\\n(TMH) are not known (28). The RoseTTAFold\\nCERS1 model for residues 98 to 304 (Pfam TLC\\ndomain) (29) includes six TMH that traverse\\nthe membrane in an up and down arrange-\\nment (Fig. 3E). A central crevice extends into\\nthe membrane and is lined with residues re-\\nquired for activity (His182 and Asp213)( 30)o r\\nconserved (Trp298), as well as a pathogenic mu-\\ntation [His183→Gln (H183Q)] found in progres-\\nsive myoclonus epilepsy and dementia that\\ndecreases ceramide levels (31). This active-site\\ncomposition (His182,A s p213, and potentially a\\nneighboring Ser212) suggests testable reaction\\nmechanisms for the enzyme (Fig. 3F).\\nDirect generation of protein-protein\\ncomplex models\\nThe final layer of the end-to-end version of our\\nthree-track network generates 3D structure\\nmodels by combining features from discontin-\\nuous crops of the protein sequence (two seg-\\nments of the protein with a chain break between\\nthem). We reasoned that because the network\\ncan seamlessly handle chain breaks, it might be\\nable to predict the structure of protein-protein\\ncomplexes directly from sequence information.\\nRather than providing the network with the\\nsequence of a single protein, with or without\\npossible template structures, two or more se-\\nquences (and possible templates for these) can\\nbe input, with the output being the backbone\\ncoordinates of two or more protein chains.\\nThus, the network enables the direct building\\nof structure models for protein-protein com-\\nplexes from sequence information, short-\\ncircuiting the standard procedure of building\\nmodels for individual subunits and then carry-\\ning out rigid-body docking. In addition to the\\ngreat reduction in compute time required\\nBaek et al., Science 373, 871– 876 (2021) 20 August 2021 4o f6\\nFig. 3. RoseTTAFold models provide insights into function.(A) TANGO2 model, colored in a rainbow from\\nthe N terminus (blue) to the C terminus (red), adopts an Ntn hydrolase fold. Pathogenic mutation sites\\nare represented by magenta spheres. (B) Predicted TANGO2 active site colored by ortholog conservation\\nfrom variable (blue) to conserved (red), with conserved residues shown in stick format and labeled.\\nPathogenic mutations (spheres with wild-type side chains in sticks) are labeled in magenta; select\\nneighboring residues are depicted in sticks. (C) ADAM33 prodomain adopts a lipocalin-like barrel (blue,\\nN terminus; red, C terminus). (D) ADAM33 model surface rendering colored by ortholog conservation from\\nblue (variable) to red (conserved), highlighting a conserved surface patch. (E) CERS1 transmembrane\\nstructure prediction colored from N terminus (blue) to C terminus (red), with a pathogenic mutation in TMH2 near\\na central cavity represented by a magenta sphere. (F) Zoom-in of the CERS1 active site, with residues colored\\nby ortholog conservation from variable (blue) to conserved (red). Residues that contribute to catalysis (His182 and\\nAsp213) or are conserved (Trp298 and Asp213) line the cavity. The conserved pathogenic mutation H183Q is\\nadjacent to the active site. D, Asp; H, His; Q, Gln; R, Arg; W, Trp.\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\n(complex models are generated from sequence\\ni n f o r m a t i o ni n~ 3 0m i no na2 4 - G BT I T A NR T X\\nGPU), this approach implements“flexible back-\\nbone” docking almost by construction because\\nthe structures of the chains are predicted in\\nthe context of each other. We tested the end-to-\\ne n dt h r e e - t r a c kn e t w o r ko np a i r e ds e q u e n c e\\nalignments for complexes of known structures\\n(32)( s e em e t h o d sa n dt a b l eS 4f o rd e t a i l s )\\ncontaining two (Fig. 4A) or three (Fig. 4B)\\nchains, and in many cases, the resulting models\\nwere very close to the actual structures [tem-\\nplate modeling score (TM-score) (33)> 0 . 8 ] .\\nInformation on residue-residue coevolution\\nbetween the paired sequences likely contrib-\\nutes to the accuracy of the rigid-body placement\\nbecause more-accurate complex structures were\\ngenerated when more sequences were avail-\\nable (fig. S10). The network was trained on\\nmonomeric proteins, not complexes, so there\\nm a yb es o m et r a i n i n g - s e tb i a si nt h em o n o m e r\\nstructures, but there is none for the complexes.\\nTo illustrate the application of RoseTTAFold\\nto complexes of unknown structure with more\\nthan three chains, we used it to generate\\nmodels of the complete four-chain human\\ninterleukin-12 receptor– interleukin-12 (IL-12R–\\nIL-12) complex (Fig. 4C and fig. S11). A pre-\\nviously published cryo-EM map of the IL-12\\nreceptor complex indicated a similar topol-\\nogy to that of the IL-23 receptor; however, the\\nresolution was not sufficient to observe the\\ndetailed interaction between IL-12Rb2a n d\\nIL-12p35 (34). Such an understanding is im-\\nportant for dissecting the specific actions of\\nIL-12 and IL-23 and generating inhibitors that\\nblock IL-12 without affecting IL-23 signaling.\\nThe RoseTTAFold model fits the experimental\\ncryo-EM density well and identified a shared\\ninteraction between Tyr189 in IL-12p35 and\\nGly115 in IL-12Rb2 analogous to the packing be-\\ntween Trp156 in IL-23p19 with Gly116 in IL-23R.\\nIn addition, the model suggests a role for the\\nIL-12Rb2 N-terminal peptide (residues 24 to 31)\\nin IL-12 binding (IL-12Rb2A s p26 may interact\\nwith nearby Lys190 and Lys194 in IL-12p35),\\nwhich may provide an avenue to specifically\\ntarget the IL-12Rb2– IL-12 interaction.\\nConclusions\\nRoseTTAFold enables solutions of challenging\\nx-ray crystallography and cryo-EM modeling\\nproblems, provides insight into protein function\\nin the absence of experimentally determined\\nstructures, and rapidly generates accurate mod-\\nels of protein-protein complexes. Further train-\\ning on protein-protein complex datasets will\\nlikely further improve the modeling of the\\nstructures of multiprotein assemblies. The\\napproach can be readily coupled with existing\\nsmall-molecule and protein binder design meth-\\nodology to improve computational discovery\\nof new protein and small-molecule ligands for\\ntargets of interest. The simultaneous processing\\nof sequence, distance, and coordinate informa-\\ntion by the three-track architecture opens the\\ndoor to new approaches that incorporate con-\\nstraints and experimental information at all\\nthree levels for problems ranging from cryo-EM\\nstructure determination to protein design.\\nREFERENCES AND NOTES\\n1. A. W. Senior et al., Nature 577, 706–710 (2020).\\n2. J. Jumper et al., in Fourteenth Critical Assessment of\\nTechniques for Protein Structure Prediction: CASP14 Abstract\\nBook (Protein Structure Prediction Center, 2020), pp. 22–24.\\n3. J. Yang et al., Proc. Natl. Acad. Sci. U.S.A.117, 1496–1503\\n(2020).\\n4. I. Anishchenko, T. M. Chidyausiku, S. Ovchinnikov, S. J. Pellock,\\nD. Baker, bioRxiv 2020.07.22.211482 [Preprint] (2020);\\nhttps://doi.org/10.1101/2020.07.22.211482.\\n5. S. Chaudhury, S. Lyskov, J. J. Gray,Bioinformatics 26, 689–691\\n(2010).\\n6. F. B. Fuchs, D. E. Worrall, V. Fischer, M. Welling,\\narXiv:2006.10503 [cs.LG] (2020).\\n7. J. Haas et al., Proteins 86, 387–398 (2018).\\n8. L. J. McGuffin et al., Nucleic Acids Res.47, W408–W413\\n(2019).\\n9. J. Haas et al., Proteins 87, 1378–1387 (2019).\\n10. A. Waterhouseet al., Nucleic Acids Res.46,W 2 9 6–W303 (2018).\\n11. A. Jaegle et al., arXiv:2103.03206 [cs.CV] (2021).\\n12. N. Hiranuma et al., Nat. Commun. 12, 1340 (2021).\\n13. M. Steinegger et al., BMC Bioinformatics 20, 473 (2019).\\n14. A. J. Kooistraet al., Nucleic Acids Res.49, D335–D343 (2021).\\n15. B. J. Bender, B. Marlow, J. Meiler,PLOS Comput. Biol.16,\\ne1007597 (2020).\\n16. L. S. Kremeret al., Am. J. Hum. Genet.98, 358–362 (2016).\\n17. C. Rabouille, V. Kondylis,Genome Biol. 7, 213 (2006).\\n18. M. P. Milevet al., J. Inherit. Metab. Dis.44, 426–437 (2021).\\n19. S. R. Lalaniet al., Am. J. Hum. Genet.98, 347–357 (2016).\\n20. T. G. Wolfsberg, P. Primakoff, D. G. Myles, J. M. White,\\nJ. Cell Biol.131, 275–278 (1995).\\n21. T. Klein, R. Bischoff,J. Proteome Res.10,1 7–33 (2011).\\nBaek et al., Science 373, 871– 876 (2021) 20 August 2021 5o f6\\nFig. 4. Complex structure\\nprediction using RoseTTAFold.\\n(A and B) Prediction of\\nstructures of Escherichia coli\\nprotein complexes from\\nsequence information.\\nExperimentally determined\\nstructures are on the left,\\nand RoseTTAFold models are on\\nthe right; the TM-scores\\nbelow indicate the extent of\\nstructural similarity. Two\\nchain complexes are shown in\\n(A). The first subunit is colored\\nin gray, and the second\\nsubunit is colored in a rainbow\\nfrom blue (N terminus) to red\\n(C terminus). Three chain\\ncomplexes are shown in (B).\\nSubunits are colored in gray,\\ncyan, and magenta. (C) IL-12R–\\nIL-12 complex structure\\ngenerated by RoseTTAFold\\nfits the previously published\\ncryo-EM density (EMD-21645).\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\n22. S. Zhong, R. A. Khalil,Biochem. Pharmacol. 164, 188–204\\n(2019).\\n23. P. Orthet al., J. Mol. Biol.335, 129–137 (2004).\\n24. S. Takeda, T. Igarashi, H. Mori, S. Araki,EMBO J. 25,\\n2388–2396 (2006).\\n25. D. R. Flower, A. C. North, C. E. Sansom,Biochim. Biophys. Acta\\n1482,9 –24 (2000).\\n26. H. E. Van Wart, H. Birkedal-Hansen,Proc. Natl. Acad. Sci. U.S.A.\\n87, 5578–5582 (1990).\\n27. M. Levy, A. H. Futerman,IUBMB Life 62, 347–356 (2010).\\n28. J. L. Kim, B. Mestre, S.-H. Shin, A. H. Futerman,Cell. Signal.\\n82, 109958 (2021).\\n29. E. Winter, C. P. Ponting,Trends Biochem. Sci.27, 381–383\\n(2002).\\n30. S. Spassievaet al., J. Biol. Chem.281, 33931–33938 (2006).\\n31. N. Vanni et al., Ann. Neurol. 76, 206–212 (2014).\\n32. Q. Cong, I. Anishchenko, S. Ovchinnikov, D. Baker,Science 365,\\n185–189 (2019).\\n33. Y. Zhang, J. Skolnick,Proteins 57, 702–710 (2004).\\n34. C. R. Glassmanet al., Cell 184, 983–999.e24 (2021).\\n35. E. F. Pettersenet al., Protein Sci. 30,7 0–82 (2021).\\n36. M. Baeket al., RoseTTAFold: The first release of RoseTTAFold.\\nZenodo (2021); https://zenodo.org/record/5068265.\\nACKNOWLEDGMENTS\\nWe thank E. Horvitz, N. Hiranuma, D. Juergens, S. Mansoor,\\nand D. Tischer for helpful discussions; D. E. Kim for web-server\\nconstruction; and L. Goldschmidt for computing resource\\nmanagement. T.P.-K. thanks B. Nidetzky and M. Monschein from\\nGraz University of Technology for providing protein samples for\\ncrystallization. D.J.O. acknowledges assistance with data collection\\nfrom scientists of Diamond Light Source beamline I04 under\\nproposal mx20303. T.S., C.B., and T.P.-K. acknowledge the ESRF\\n(ID30-3, Grenoble, France) and DESY (P11, PETRAIII, Hamburg,\\nGermany) for provision of synchrotron-radiation facilities and\\nsupport during data collection. P.D.A., J.H.P., A.D., and A.V.R.\\nacknowledge support from the Joint BioEnergy Institute, which is\\nsupported by the US Department of Energy, Office of Science,\\nOffice of Biological and Environmental Research under contract\\nno. DE-AC02-05CH11231 between LBNL and the US Department of\\nEnergy. Funding: This work was supported by Microsoft (M.B.,\\nD.B., and generous gifts of Azure compute time and expertise);\\nOpen Philanthropy (D.B. and G.R.L.); E. and W. Schmidt by\\nrecommendation of the Schmidt Futures program (F.D. and H.P.);\\nThe Washington Research Foundation (M.B., G.R.L., and J.W.);\\nthe National Science Foundation Cyberinfrastructure for Biological\\nResearch, award no. DBI 1937533 (I.A.); Wellcome Trust grant\\nnumber 209407/Z/17/Z (R.J.R.); the National Institutes of Health,\\ngrant numbers P01GM063210 (P.D.A. and R.J.R.), DP5OD026389\\n(S.O.), RO1-AI51321 (K.C.G.), and GM127390 (N.V.G.); the Mathers\\nFoundation (K.C.G.); the Canadian Institute of Health Research (CIHR)\\nProject Grant, grant numbers 168998 (J.E.B.) and 168907 (C.K.Y.);\\nthe Welch Foundation I-1505 (N.V.G.); the Global Challenges Research\\nFund (GCRF) through Science & Technology Facilities Council (STFC),\\ngrant number ST/R002754/1: Synchrotron Techniques for African\\nResearch and Technology (START) (D.J.O., A.A.v.D., and A.C.E.); and\\nthe Austrian Science Fund (FWF), projects P29432 and DOC50\\n(doc.fund Molecular Metabolism) (T.S., C.B., and T.P.-K.).Author\\ncontributions:M.B., F.D., and D.B. designed the research; M.B., F.D.,\\nI.A., J.D., S.O., and J.W. developed the deep-learning network; G.R.L.\\nand H.P. analyzed GPCR modeling results; Q.C., L.N.K., R.D.S., and\\nN.V.G. analyzed modeling results for proteins related to the human\\ndiseases; C.R.G. and K.C.G. analyzed modeling results for the\\nIL-12R–IL-12 complex; P.D.A., R.J.R., C.A., F.D., and C.M. worked\\non structure determination; A.A.v.D., A.C.E., D.J.O., T.S., C.B., T.P.-K.,\\nM.K.R., U.D., C.K.Y., J.E.B., A.D., J.H.P., and A.V.R. provided\\nexperimental data; M.B., F.D., G.R.L., Q.C., L.N.K., H.P., C.R.G.,\\nP.D.A., R.J.R., and D.B. wrote the manuscript; and all authors\\ndiscussed the results and commented on the manuscript.Competing\\ninterests: The authors declare that they have no competing\\ninterests. Data and materials availability:The GPCR models of\\nunknown structures have been deposited to http://files.ipd.uw.\\nedu/pub/RoseTTAFold/all_human_GPCR_unknown_models.tar.gz\\nand http://files.ipd.uw.edu/pub/RoseTTAFold/GPCR_benchmark_\\none_state_unknown_models.tar.gz. The model structures for\\nstructurally uncharacterized human proteins have been deposited\\nto http://files.ipd.uw.edu/pub/RoseTTAFold/human_prot.tar.gz.\\nCoordinates for the full PI3K complex structure determined by\\ncryo-EM are available at the PDB with accession code PDB: 7MEZ.\\nModel structures used for molecular replacement are available\\nat http://files.ipd.uw.edu/pub/RoseTTAFold/MR_models.tar.gz.\\nThe refined structures for GLYAT, oxidoreductase, SLP, and Lrbp\\nproteins will be deposited in the PDB when final processing is\\ncompleted. The method is available as a server at https://robetta.\\nbakerlab.org (RoseTTAFold option), and the source code\\nand model parameters are available at https://github.com/\\nRosettaCommons/RoseTTAFold or Zenodo (36). This research\\nwas funded in whole or in part by Wellcome Trust, grant #209407/\\nZ/17/Z, a cOAlition S organization. The author will make the\\nAuthor Accepted Manuscript (AAM) version available under\\na CC BY public copyright license.\\nSUPPLEMENTARY MATERIALS\\nscience.sciencemag.org/content/373/6557/871/suppl/DC1\\nMaterials and Methods\\nFigs. S1 to S17\\nTables S1 to S4\\nReferences (37–82)\\nMDAR Reproducibility Checklist\\nView/request a protocol for this paper fromBio-protocol.\\n7 June 2021; accepted 7 July 2021\\nPublished online 15 July 2021\\n10.1126/science.abj8754\\nBaek et al., Science 373, 871– 876 (2021) 20 August 2021 6o f6\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\n'}, {'file_name': 'alphafold', 'text': \"Nature | Vol 596 | 26 August 2021 | 583\\nArticle\\nHighly accurate protein structure prediction \\nwith AlphaFold\\nJohn Jumper1,4\\u2009✉, Richard Evans1,4, Alexander Pritzel1,4, Tim Green1,4, Michael Figurnov1,4, \\nOlaf Ronneberger1,4, Kathryn Tunyasuvunakool1,4, Russ Bates1,4, Augustin Žídek1,4, \\nAnna Potapenko1,4, Alex Bridgland1,4, Clemens Meyer1,4, Simon A. A. Kohl1,4, \\nAndrew J. Ballard1,4, Andrew Cowie1,4, Bernardino Romera-Paredes1,4, Stanislav Nikolov1,4, \\nRishub Jain1,4, Jonas Adler1, Trevor Back1, Stig Petersen1, David Reiman1, Ellen Clancy1, \\nMichal Zielinski1, Martin Steinegger2,3, Michalina Pacholska1, Tamas Berghammer1, \\nSebastian Bodenstein1, David Silver1, Oriol Vinyals1, Andrew W. Senior1, Koray Kavukcuoglu1, \\nPushmeet Kohli1 & Demis Hassabis1,4\\u2009✉\\nProteins are essential to life, and understanding their structure can facilitate a \\nmechanistic understanding of their function. Through an enormous experimental \\neffort1–4, the structures of around 100,000 unique proteins have been determined5, but \\nthis represents a small fraction of the billions of known protein sequences6,7. Structural \\ncoverage is bottlenecked by the months to years of painstaking effort required to \\ndetermine a single protein structure. Accurate computational approaches are needed \\nto address this gap and to enable large-scale structural bioinformatics. Predicting the \\nthree-dimensional structure that a protein will adopt based solely on its amino acid \\nsequence—the structure prediction component of the ‘protein folding problem’8—has \\nbeen an important open research problem for more than 50\\xa0years9. Despite recent \\nprogress10–14, existing methods fall far\\xa0short of atomic accuracy, especially when no \\nhomologous structure is available. Here we provide the first computational method \\nthat can regularly predict protein structures with atomic accuracy even in cases in which \\nno similar structure is known. We validated an entirely redesigned version of our neural \\nnetwork-based model, AlphaFold, in the challenging 14th Critical Assessment of protein \\nStructure Prediction (CASP14)15, demonstrating accuracy competitive with \\nexperimental structures in a majority of cases and greatly outperforming other \\nmethods. Underpinning the latest version of AlphaFold is a novel machine learning \\napproach that incorporates physical and biological knowledge about protein structure, \\nleveraging multi-sequence alignments, into the design of the deep learning algorithm.\\nThe development of computational methods to predict \\nthree-dimensional (3D) protein structures from the protein sequence \\nhas proceeded along two complementary paths that focus on either the \\nphysical interactions or the evolutionary history. The physical interac-\\ntion programme heavily integrates our understanding of molecular \\ndriving forces into either thermodynamic or kinetic simulation of pro-\\ntein physics16 or statistical approximations thereof17. Although theoreti-\\ncally very appealing, this approach has proved highly challenging for \\neven moderate-sized proteins due to the computational intractability \\nof molecular simulation, the context dependence of protein stability \\nand the difficulty of producing sufficiently accurate models of protein \\nphysics. The evolutionary programme has provided an alternative in \\nrecent years, in which the constraints on protein structure are derived \\nfrom bioinformatics analysis of the evolutionary history of proteins, \\nhomology to solved structures18,19 and pairwise evolutionary correla-\\ntions20–24. This bioinformatics approach has benefited greatly from \\nthe steady growth of experimental protein structures deposited in \\nthe Protein Data Bank (PDB)5, the explosion of genomic sequencing \\nand the rapid development of deep learning techniques to interpret \\nthese correlations. Despite these advances, contemporary physical \\nand evolutionary-history-based approaches produce predictions that \\nare far\\xa0short of experimental accuracy in the majority of cases in which \\na close homologue has not been solved experimentally and this has \\nlimited their utility for many biological applications.\\nIn this study, we develop the first, to our knowledge, computational \\napproach capable of predicting protein structures to near experimental \\naccuracy in a majority of cases. The neural network AlphaFold that we \\ndeveloped was entered into the CASP14 assessment (May–July\\xa02020; \\nentered under the team name ‘ AlphaFold2’ and a completely different \\nmodel from our CASP13 AlphaFold system10). The CASP assessment is \\ncarried out biennially using recently solved structures that have not \\nbeen deposited in the PDB or publicly disclosed so that it is a blind test \\nhttps://doi.org/10.1038/s41586-021-03819-2\\nReceived: 11 May 2021\\nAccepted: 12 July 2021\\nPublished online: 15 July 2021\\nOpen access\\n Check for updates\\n1DeepMind, London, UK. 2School of Biological Sciences, Seoul National University, Seoul, South Korea. 3Artificial Intelligence Institute, Seoul National University, Seoul, South Korea. 4These \\nauthors contributed equally: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna \\nPotapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Demis Hassabis.  \\n✉e-mail: jumper@deepmind.com; dhcontact@deepmind.com\\n584 | Nature | Vol 596 | 26 August 2021\\nArticle\\nfor the participating methods, and has long served as the gold-standard \\nassessment for the accuracy of structure prediction25,26.\\nIn CASP14, AlphaFold structures were vastly more accurate than \\ncompeting methods. AlphaFold structures had a median backbone \\naccuracy of 0.96\\xa0Å r.m.s.d.95 (Cα root-mean-square deviation at 95% \\nresidue coverage) (95% confidence interval\\xa0=\\xa00.85–1.16\\xa0Å) whereas \\nthe next best performing method had a median backbone accuracy \\nof 2.8\\xa0Å r.m.s.d.95 (95% confidence interval\\xa0=\\xa02.7–4.0\\xa0Å) (measured on \\nCASP domains; see Fig.\\xa01a for backbone accuracy and Supplementary \\nFig.\\xa014 for all-atom accuracy). As a comparison point for this accuracy, \\nthe width of a carbon atom is approximately 1.4\\xa0Å. In addition to very \\naccurate domain structures (Fig.\\xa0 1b), AlphaFold is able to produce \\nhighly accurate side chains (Fig.\\xa01c) when the backbone is highly accu-\\nrate and considerably improves over template-based methods even \\nwhen strong templates are available. The all-atom accuracy of Alpha-\\nFold was 1.5\\xa0Å r.m.s.d.95 (95% confidence interval\\xa0=\\xa01.2–1.6\\xa0Å) compared \\nwith the 3.5\\xa0Å r.m.s.d.95 (95% confidence interval\\xa0=\\xa03.1–4.2\\xa0Å) of the best \\nalternative method. Our methods are scalable to very long proteins with \\naccurate domains and domain-packing (see Fig.\\xa01d for the prediction \\nof a 2,180-residue protein with no structural homologues). Finally, the \\nmodel is able to provide precise, per-residue estimates of its reliability \\nthat should enable the confident use of these predictions.\\nWe demonstrate in Fig.\\xa02a that the high accuracy that AlphaFold dem-\\nonstrated in CASP14 extends to a large sample of recently released PDB \\nstructures; in this dataset, all structures were deposited in the PDB after \\nour training data cut-off and are analysed as full chains (see\\xa0Methods, \\nSupplementary Fig.\\xa015 and Supplementary Table\\xa06 for more details). \\nFurthermore, we observe high side-chain accuracy when the back -\\nbone prediction is accurate (Fig.\\xa02b) and we show that our confidence \\nmeasure, the predicted local-distance difference test (pLDDT), reliably \\npredicts the Cα local-distance difference test (lDDT-Cα) accuracy of the \\ncorresponding prediction (Fig.\\xa02c). We also find that the global super-\\nposition metric template modelling score (TM-score)27 can be accu-\\nrately estimated (Fig.\\xa02d). Overall, these analyses validate that the high \\naccuracy and reliability of AlphaFold on CASP14 proteins also transfers \\nto an uncurated collection of recent PDB submissions, as would be \\nexpected (see\\xa0Supplementary Methods 1.15 and Supplementary Fig.\\xa011 \\nfor confirmation that this high accuracy extends to new folds).\\nThe AlphaFold network\\nAlphaFold greatly improves the accuracy of structure prediction by \\nincorporating novel neural network architectures and training proce-\\ndures based on the evolutionary, physical and geometric constraints \\nof protein structures. In particular, we demonstrate a new architecture \\nto jointly embed multiple sequence alignments (MSAs) and pairwise \\nfeatures, a new output representation and associated loss that enable \\naccurate end-to-end structure prediction, a new equivariant attention \\na\\nG427\\nAlphaFold\\nG009\\nG473\\nG129\\nG403\\nG032\\nG420\\nG480\\nG498\\nG488\\nG368\\nG324\\nG362\\nG253\\nG216\\n0\\n1\\n2\\n3\\n4\\nMedian C/uni03B1 r.m.s.d.95 (Å)\\nb\\nC terminus\\nN terminus\\nAlphaFold Experiment\\nr.m.s.d.95 = 0.8Å ; TM-score = 0.93\\nc\\nAlphaFold Experiment\\nr.m.s.d. = 0.59 Å within 8Å of Zn\\nd\\nAlphaFold Experiment\\nr.m.s.d.95 = 2.2Å ; TM-score = 0.96\\nMSA\\n3D structure\\nLow \\ncon/f_idence\\nHigh \\ncon/f_idence\\nTemplates\\nInput sequence\\nMSA \\nrepresentation\\n(s,r,c)\\nEvoformer \\n(48 blocks)\\nStructure \\nmodule\\n (8 blocks)\\n+\\n+\\n/uni2190 Recycling (three times)\\nPairing\\nPair \\nrepresentation\\n(r,r,c)\\nPair \\nrepresentation\\n(r,r,c)\\nGenetic \\ndatabase\\nsearch\\nStructure \\ndatabase\\nsearch\\ne\\nSingle repr. (r,c)\\nFig. 1 | AlphaFold produces highly accurate structures.  a, The performance \\nof AlphaFold on the CASP14 dataset ( n\\xa0=\\xa087 protein domains) relative to the top-\\n15 entries (out of 146 entries), group numbers correspond to the numbers \\nassigned to entrants by CASP . Data are median and the 95% confidence interval \\nof the median, estimated from 10,000 bootstrap samples. b, Our prediction of \\nCASP14 target T1049 (PDB\\xa06Y4F ,\\xa0blue) compared with the true (experimental) \\nstructure (green). Four residues in the C terminus of the crystal structure are \\nB-factor outliers and are not depicted. c, CASP14 target T1056 (PDB 6YJ1).  \\nAn example of a well-predicted zinc-binding site (AlphaFold has accurate side \\nchains even though it does not explicitly predict the zinc ion). d, CASP target \\nT1044\\xa0(PDB 6VR4)—a 2,180-residue single chain—was predicted with correct \\ndomain packing (the prediction was made after CASP using AlphaFold without \\nintervention). e, Model architecture. Arrows show the information flow among \\nthe various components described in this paper. Array shapes are shown in \\nparentheses with s, number of sequences ( Nseq in the main text); r, number of \\nresidues (Nres in the main text); c, number of channels.\\nNature | Vol 596 | 26 August 2021 | 585\\narchitecture, use of intermediate losses to achieve iterative refinement \\nof predictions, masked MSA loss to jointly train with the structure, \\nlearning from unlabelled protein sequences using self-distillation and \\nself-estimates of accuracy.\\nThe AlphaFold network directly predicts the 3D coordinates of all \\nheavy atoms for a given protein using the primary amino acid sequence \\nand aligned sequences of homologues as inputs (Fig.\\xa01e; see\\xa0Methods \\nfor details of inputs including databases, MSA construction and use of \\ntemplates). A description of the most important ideas and components \\nis provided below. The full network architecture and training procedure \\nare provided in the\\xa0Supplementary Methods.\\nThe network comprises two main stages. First, the trunk of the net-\\nwork processes the inputs through repeated layers of a novel neural \\nnetwork block that we term Evoformer to produce an Nseq\\xa0×\\xa0Nres array \\n(Nseq, number of sequences; Nres, number of residues) that represents \\na processed MSA and an Nres\\xa0×\\xa0Nres array that represents residue pairs. \\nThe MSA representation is initialized with the raw MSA (although \\nsee\\xa0Supplementary Methods 1.2.7 for details of handling very deep \\nMSAs). The Evoformer blocks contain a number of attention-based \\nand non-attention-based components. We show evidence in ‘Interpret-\\ning the neural network’ that a concrete structural hypothesis arises \\nearly within the Evoformer blocks and is continuously refined. The key \\ninnovations in the Evoformer block are new mechanisms to exchange \\ninformation within the MSA and pair representations that enable direct \\nreasoning about the spatial and evolutionary relationships.\\nThe trunk of the network is followed by the structure module that \\nintroduces an explicit 3D structure in the form of a rotation and transla-\\ntion for each residue of the protein (global rigid body frames). These \\nrepresentations are initialized in a trivial state with all rotations set to \\nthe identity and all positions set to the origin, but rapidly develop and \\nrefine a highly accurate protein structure with precise atomic details. \\nKey innovations in this section of the network include breaking the \\nchain structure to allow simultaneous local refinement of all parts of \\nthe structure, a novel equivariant transformer to allow the network to \\nimplicitly reason about the unrepresented side-chain atoms and a loss \\nterm that places substantial weight on the orientational correctness \\nof the residues. Both within the structure module and throughout \\nthe whole network, we reinforce the notion of iterative refinement \\nby repeatedly applying the final loss to outputs and then feeding the \\noutputs recursively into the same modules. The iterative refinement \\nusing the whole network (which we term ‘recycling’ and is related to \\napproaches in computer vision28,29) contributes markedly to accuracy \\nwith minor extra training time (see\\xa0Supplementary Methods 1.8 for \\ndetails).\\nEvoformer\\nThe key principle of the building block of the network—named Evo-\\nformer (Figs.\\xa01e, 3a)—is to view the prediction of protein structures \\nas a graph inference problem in 3D space in which the edges of the \\ngraph are defined by residues in proximity. The elements of the pair \\nrepresentation encode information about the relation between the \\nresidues (Fig.\\xa03b). The columns of the MSA representation encode the  \\nindividual residues of the input sequence while the rows represent  \\nthe sequences in which those residues appear. Within this framework, \\nwe define a number of update operations that are applied in each block \\nin which the different update operations are applied in series.\\nThe MSA representation updates the pair representation through an \\nelement-wise outer product that is summed over the MSA sequence \\ndimension. In contrast to previous work30, this operation is applied \\nwithin every block rather than once in the network, which enables the \\ncontinuous communication from the evolving MSA representation to \\nthe pair representation.\\nWithin the pair representation, there are two different update pat-\\nterns. Both are inspired by the necessity of consistency of the pair \\n0–0.5\\n0.5–1\\n1–2\\n2–4\\n4–8\\n>8\\nFull chain C/uni03B1 r.m.s.d.95 (Å)\\n0\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30Fraction of proteins\\na\\n20 40 60 80 100\\nlDDT-C/uni03B1 of a residue\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nFraction of correct F1 rotamers\\nb\\n0 20 40 60 80 100\\nAverage pLDDT on the resolved region\\n0\\n20\\n40\\n60\\n80\\n100\\nlDDT-C/uni03B1\\n80 90 100\\n80\\n90\\n100\\nc\\n0 0.2 0.4 0.6 0.8 1.0\\npTM on the resolved region\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTM-score\\n0.8 0.9 1.0\\n0.8\\n0.9\\n1.0\\nd\\nFig. 2 | Accuracy of AlphaFold on recent PDB structures. The analysed \\nstructures are newer than any structure in the training set. Further filtering is \\napplied to reduce redundancy (see\\xa0Methods). a, Histogram of backbone \\nr.m.s.d. for full chains (Cα r.m.s.d. at 95% coverage). Error bars are 95% \\nconfidence intervals (Poisson). This dataset excludes proteins with a template \\n(identified by hmmsearch) from the training set with more than 40% sequence \\nidentity covering more than 1% of the chain (n\\xa0=\\xa03,144 protein chains). The \\noverall median is 1.46\\xa0Å (95% confidence interval\\xa0=\\xa01.40–1.56\\xa0Å). Note that this \\nmeasure will be highly sensitive to domain packing and domain accuracy; a \\nhigh r.m.s.d. is expected for some chains with uncertain packing or packing \\nerrors. b, Correlation between backbone accuracy and side-chain accuracy. \\nFiltered to structures with any observed side chains and resolution better than \\n2.5\\xa0Å (n\\xa0=\\xa05,317 protein chains); side chains were further filtered to \\nB-factor\\xa0<30\\xa0Å 2. A rotamer is classified as correct if the predicted torsion angle \\nis within 40°. Each point aggregates a range of lDDT-Cα, with a bin size of 2 units \\nabove 70 lDDT-Cα and 5 units otherwise. Points correspond to the mean \\naccuracy; error bars are 95% confidence intervals (Student t-test) of the mean \\non a per-residue basis. c, Confidence score compared to the true accuracy on \\nchains. Least-squares linear fit lDDT-Cα\\xa0=\\xa00.997\\xa0×\\xa0pLDDT\\xa0−\\xa01.17 (Pearson’s \\nr\\xa0=\\xa00.76). n\\xa0=\\xa010,795 protein chains. The shaded region of the linear fit \\nrepresents a 95% confidence interval estimated from 10,000 bootstrap \\nsamples. In the companion paper39, additional quantification of the reliability \\nof pLDDT as a confidence measure is provided. d , Correlation between pTM \\nand full chain TM-score. Least-squares linear fit TM-score\\xa0=\\xa00.98\\xa0×\\xa0pTM\\xa0+\\xa00.07 \\n(Pearson’s r\\xa0=\\xa00.85). n\\xa0=\\xa010,795 protein chains. The shaded region of the linear fit \\nrepresents a 95% confidence interval estimated from 10,000 bootstrap \\nsamples.\\n586 | Nature | Vol 596 | 26 August 2021\\nArticle\\nrepresentation—for a pairwise description of amino acids to be represent-\\nable as a single 3D structure, many constraints must be satisfied including \\nthe triangle inequality on distances. On the basis of this intuition, we \\narrange the update operations on the pair representation in terms of \\ntriangles of edges involving three different nodes (Fig.\\xa03c). In particular, \\nwe add an extra logit bias to axial attention31 to include the ‘missing edge’ \\nof the triangle and we define a non-attention update operation ‘triangle \\nmultiplicative update’ that uses two edges to update the missing third \\nedge (see\\xa0Supplementary Methods 1.6.5 for details). The triangle multipli-\\ncative update was developed originally as a more symmetric and cheaper \\nreplacement for the attention, and networks that use only the attention or \\nmultiplicative update are both able to produce high-accuracy structures. \\nHowever, the combination of the two updates is more accurate.\\nWe also use a variant of axial attention within the MSA representation. \\nDuring the per-sequence attention in the MSA, we project additional \\nlogits from the pair stack to bias the MSA attention. This closes the loop \\nby providing information flow from the pair representation back into \\nthe MSA representation, ensuring that the overall Evoformer block is \\nable to fully mix information between the pair and MSA representations \\nand prepare for structure generation within the structure module.\\n \\nEnd-to-end structure prediction\\nThe structure module (Fig.\\xa03d) operates on a concrete 3D backbone \\nstructure using the pair representation and the original sequence row \\n(single representation) of the MSA representation from the trunk. The \\n3D backbone structure is represented as Nres independent rotations \\nand translations, each with respect to the global frame (residue gas) \\n(Fig.\\xa03e). These rotations and translations—representing the geometry \\nof the N-Cα-C atoms—prioritize the orientation of the protein back-\\nbone so that the location of the side chain of each residue is highly \\nconstrained within that frame. Conversely, the peptide bond geometry \\nis completely unconstrained and the network is observed to frequently \\nviolate the chain constraint during the application of the structure mod-\\nule as breaking this constraint enables the local refinement of all parts \\nof the chain without solving complex loop closure problems. Satisfac-\\ntion of the peptide bond geometry is encouraged during fine-tuning \\nby a violation loss term. Exact enforcement of peptide bond geometry \\nis only achieved in the post-prediction relaxation of the structure by \\ngradient descent in the Amber32 force field. Empirically, this final relaxa-\\ntion does not improve the accuracy of the model as measured by the \\nf\\nBackbone frames\\n(r, 3×3) and (r,3)\\nBackbone frames\\n(r, 3×3) and (r,3) \\n(initially all at the origin)\\nPredict relative \\nrotations and \\ntranslations\\nPredict F angles \\nand compute all \\natom positions (Rk, t\\nk)\\nxi\\n8 blocks (shared weights)d\\nPair \\nrepresentation\\n(r,r,c)\\nSingle repr. (r,c) \\nSingle repr. (r,c) \\nMSA \\nrepresentation \\n(s,r,c)\\nPair \\nrepresentation\\n(r,r,c)\\nRow-wise \\ngated \\nself-\\nattention \\nwith pair \\nbias\\nColumn-\\nwise \\ngated \\nself-\\nattention \\n+ Tran-\\nsition\\nOuter \\nproduct \\nmean\\n+\\n+\\nTriangle \\nupdate \\nusing \\noutgoing\\nedges\\nTriangle \\nself-\\nattention \\naround \\nstarting \\nnode\\nTriangle \\nself-\\nattention \\naround \\nending \\nnode\\nTriangle \\nupdate \\nusing \\nincoming\\nedges\\nTran-\\nsition\\nMSA \\nrepresentation \\n(s,r,c)\\nPair \\nrepresentation\\n(r,r,c)\\na\\n+\\n+ + + + +\\nTriangle self-attention around \\nstarting node\\nTriangle self-attention around \\nending node\\nki\\nij\\nkjjk\\nij\\nik\\ni\\nk\\nj i j\\nk\\nki\\nij\\nkj\\ni j\\nk\\nik\\nij\\njk\\ni j\\nk\\nTriangle multiplicative update\\nusing ‘outgoing’ edges\\nTriangle multiplicative update\\nusing ‘incoming’ edges\\nc\\nIPA\\nmodule +\\nPair representation \\n(r,r,c)\\nij\\ni\\n j\\n k\\nik\\ni\\nj\\nk\\njk\\nkj\\nji\\nki\\ni j\\nk\\nij\\nji\\njk\\nkjki\\nik\\nCorresponding edges \\nin a graph\\nb\\n48 blocks (no shared weights)\\ne\\n(Rk, t\\nk)\\n~ ~\\nxi\\n~\\nFig. 3 | Architectural details. a, Evoformer block. Arrows show the information \\nflow. The shape of the arrays is shown in parentheses. b, The pair representation \\ninterpreted as directed edges in a graph. c, Triangle multiplicative update and \\ntriangle self-attention. The circles represent residues. Entries in the pair \\nrepresentation are illustrated as directed edges and in each diagram, the edge \\nbeing updated is ij. d, Structure module\\xa0including Invariant point attention (IPA) \\nmodule. The single representation is a copy of the first row of the MSA \\nrepresentation. e, Residue gas: a representation of each residue as one \\nfree-floating rigid body for the backbone (blue triangles) and χ angles for the \\nside chains (green circles). The corresponding atomic structure is shown below. \\nf, Frame aligned point error (FAPE). Green, predicted structure; grey, true \\nstructure; (Rk,\\xa0tk), frames; xi, atom positions.\\nNature | Vol 596 | 26 August 2021 | 587\\nglobal distance test (GDT)33 or lDDT-Cα34 but does remove distracting \\nstereochemical violations without the loss of accuracy.\\nThe residue gas representation is updated iteratively in two stages \\n(Fig.\\xa03d). First, a geometry-aware attention operation that we term \\n‘invariant point attention’ (IPA) is used to update an Nres set of neural \\nactivations (single representation) without changing the 3D positions, \\nthen an equivariant update operation is performed on the residue gas \\nusing the updated activations. The IPA augments each of the usual \\nattention queries, keys and values with 3D points that\\xa0are produced \\nin the local frame of each residue such that the final value is invariant \\nto global rotations and translations (see\\xa0Methods ‘IPA’ for details). The \\n3D queries and keys also impose a strong spatial/locality bias on the \\nattention, which is well-suited to the iterative refinement of the protein \\nstructure. After each attention operation and element-wise transition \\nblock, the module computes an update to the rotation and translation \\nof each backbone frame. The application of these updates within the \\nlocal frame of each residue makes the overall attention and update \\nblock an equivariant operation on the residue gas.\\nPredictions of side-chain χ angles as well as the final, per-residue \\naccuracy of the structure (pLDDT) are computed with small per-residue \\nnetworks on the final activations at the end of the network. The estimate \\nof the TM-score (pTM) is obtained from a pairwise error prediction that \\nis computed as a linear projection from the final pair representation. The \\nfinal loss (which we term the frame-aligned point error (FAPE) (Fig.\\xa03f)) \\ncompares the predicted atom positions to the true positions under \\nmany different alignments. For each alignment, defined by aligning \\nthe predicted frame (Rk,\\xa0tk) to the corresponding true frame, we com-\\npute the distance of all predicted atom positions xi from the true atom \\npositions. The resulting Nframes\\xa0×\\xa0Natoms distances are penalized with a \\nclamped L1 loss. This creates a strong bias for atoms to be correct relative \\nto the local frame of each residue and hence correct with respect to its \\nside-chain interactions, as well as providing the main source of chirality \\nfor AlphaFold (Supplementary Methods 1.9.3 and Supplementary Fig.\\xa09).\\nTraining with labelled and unlabelled data\\nThe AlphaFold architecture is able to train to high accuracy using only \\nsupervised learning on PDB data, but we are able to enhance accuracy \\n(Fig.\\xa04a) using an approach similar to noisy student self-distillation35. \\nIn this procedure, we use a trained network to predict the structure of \\naround 350,000 diverse sequences from Uniclust3036 and make a new \\ndataset of predicted structures filtered to a high-confidence subset. We \\nthen train the same architecture again from scratch using a mixture of \\nPDB data and this new dataset of predicted structures as the training \\ndata, in which the various training data augmentations such as crop-\\nping and MSA subsampling make it challenging for the network to \\nrecapitulate the previously predicted structures. This self-distillation \\nprocedure makes effective use of the unlabelled sequence data and \\nconsiderably improves the accuracy of the resulting network.\\nAdditionally, we randomly mask out or mutate individual residues \\nwithin the MSA and have a Bidirectional Encoder Representations from \\nTransformers (BERT)-style37 objective to predict the masked elements of \\nthe MSA sequences. This objective encourages the network to learn to \\ninterpret phylogenetic and covariation relationships without hardcoding \\na particular correlation statistic into the features. The BERT objective is \\ntrained jointly with the normal PDB structure loss on the same training \\nexamples and is not pre-trained, in contrast to recent independent work38.\\nInterpreting the neural network\\nT o understand how AlphaFold predicts protein structure, we trained \\na separate structure module for each of the 48 Evoformer blocks in \\nthe network while keeping all parameters of the main network fro -\\nzen (Supplementary Methods 1.14). Including our recycling stages, \\nthis provides a trajectory of 192 intermediate structures—one per full \\nEvoformer block—in which each intermediate represents the belief of \\nthe network of the most likely structure at that block. The resulting \\ntrajectories are surprisingly smooth after the first few blocks, show-\\ning that AlphaFold makes constant incremental improvements to the \\nstructure until it can no longer improve (see Fig.\\xa04b for a trajectory of \\naccuracy). These trajectories also illustrate the role of network depth. \\nFor very challenging proteins such as ORF8 of SARS-CoV-2 (T1064), \\nthe network searches and rearranges secondary structure elements \\nfor many layers before settling on a good structure. For other proteins \\nsuch as LmrP (T1024), the network finds the final structure within the \\nfirst few layers. Structure trajectories of CASP14 targets T1024, T1044, \\nT1064 and T1091 that demonstrate a clear iterative building process \\nfor a range of protein sizes and difficulties are shown in Supplementary \\nVideos\\xa01–4. In\\xa0Supplementary Methods 1.16 and Supplementary Figs.\\xa012, \\n13, we interpret the attention maps produced by AlphaFold layers.\\nFigure\\xa04a contains detailed ablations of the components of AlphaFold \\nthat demonstrate that a variety of different mechanisms contribute \\nto AlphaFold accuracy. Detailed descriptions of each ablation model, \\ntheir training details, extended discussion of ablation results and the \\n–20 –10 0\\nGDT difference compared\\nwith baseline\\nNo IPA and no recycling\\nNo end-to-end structure gradients\\n(keep auxiliary heads)\\nNo triangles, biasing or gating\\n(use axial attention)\\nNo recycling\\nNo auxiliary masked MSA head\\nNo IPA (use direct projection)\\nNo raw MSA\\n(use MSA pairwise frequencies)\\nNo auxiliary distogram head\\nNo templates\\nBaseline\\nWith self-distillation training\\nTest set of CASP14 domains\\n–4 –2 0 2\\nlDDT-C/uni03B1 difference\\ncompared with baseline\\nTest set of PDB chainsa\\n0 48 96 144 192\\nEvoformer block\\n0\\n20\\n40\\n60\\n80\\n100Domain GDT\\nT1024 D1\\nT1024 D2\\nT1064 D1\\nb\\nFig. 4 | Interpreting the neural network. a, Ablation results on two target sets: \\nthe CASP14 set of domains ( n\\xa0=\\xa087 protein domains) and the PDB test set of \\nchains with template coverage of ≤30% at 30% identity ( n\\xa0=\\xa02,261 protein \\nchains). Domains are scored with GDT and chains are scored with lDDT-Cα. The \\nablations are reported as a difference compared with the average of the three \\nbaseline seeds. Means (points) and 95% bootstrap percentile intervals (error \\nbars) are computed using bootstrap estimates of 10,000 samples. b, Domain \\nGDT trajectory over 4 recycling iterations and 48 Evoformer blocks on CASP14 \\ntargets LmrP (T1024) and Orf8 (T1064) where D1 and D2 refer to the individual \\ndomains as defined by the CASP assessment. Both T1024 domains obtain the \\ncorrect structure early in the network, whereas the structure of T1064 changes \\nmultiple times and requires nearly the full depth of the network to reach the \\nfinal structure. Note, 48 Evoformer blocks comprise one recycling iteration.\\n588 | Nature | Vol 596 | 26 August 2021\\nArticle\\neffect of MSA depth on each ablation are provided in\\xa0Supplementary \\nMethods 1.13 and Supplementary Fig.\\xa010.\\nMSA depth and cross-chain contacts\\nAlthough AlphaFold has a high accuracy across the vast majority of \\ndeposited PDB structures, we note that there are still factors that affect \\naccuracy or limit the applicability of the model. The model uses MSAs \\nand the accuracy decreases substantially when the median alignment \\ndepth is less than around 30\\xa0sequences (see Fig.\\xa05a for details). We \\nobserve a threshold effect where improvements in MSA depth over \\naround 100\\xa0sequences lead to small gains. We hypothesize that the MSA \\ninformation is needed to coarsely find the correct structure within the \\nearly stages of the network, but refinement of that prediction into a \\nhigh-accuracy model does not depend crucially on the MSA information. \\nThe other substantial limitation that we have observed is that AlphaFold \\nis much weaker for proteins that have few intra-chain or homotypic con-\\ntacts compared to the number of heterotypic contacts (further details \\nare provided in a companion paper39). This typically occurs for bridging \\ndomains within larger complexes in which the shape of the protein is \\ncreated almost entirely by interactions with other chains in the complex. \\nConversely, AlphaFold is often able to give high-accuracy predictions for \\nhomomers, even when the chains are substantially intertwined (Fig.\\xa05b). \\nWe expect that the ideas of AlphaFold are readily applicable to predicting \\nfull hetero-complexes in a future system and that this will remove the dif-\\nficulty with protein chains that have a large number of hetero-contacts.\\nRelated work\\nThe prediction of protein structures has had a long and varied develop-\\nment, which is extensively covered in a number of reviews14,40–43. Despite \\nthe long history of applying neural networks to structure prediction14,42,43, \\nthey have only recently come to improve structure prediction10,11,44,45. \\nThese approaches effectively leverage the rapid improvement in com-\\nputer vision systems46 by treating the problem of protein structure \\nprediction as converting an ‘image’ of evolutionary couplings22–24 to an \\n‘image’ of the protein distance matrix and then integrating the distance \\npredictions into a heuristic system that produces the final 3D coordinate \\nprediction. A few recent studies have been developed to predict the 3D \\ncoordinates directly47–50, but the accuracy of these approaches does not \\nmatch traditional, hand-crafted structure prediction pipelines51. In paral-\\nlel, the success of attention-based networks for language processing52 \\nand, more recently, computer vision31,53 has inspired the exploration of \\nattention-based methods for interpreting protein sequences54–56.\\nDiscussion\\nThe methodology that we have taken in designing AlphaFold is a combi-\\nnation of the bioinformatics and physical approaches: we use a physical \\nand geometric inductive bias to build components that learn from PDB \\ndata with minimal imposition of handcrafted features (for example, \\nAlphaFold builds hydrogen bonds effectively without a hydrogen bond \\nscore function). This results in a network that learns far more efficiently \\nfrom the limited data in the PDB but is able to cope with the complexity \\nand variety of structural data.\\nIn particular, AlphaFold is able to handle missing the physical context \\nand produce accurate models in challenging cases such as intertwined \\nhomomers or proteins that only fold in the presence of an unknown \\nhaem group. The ability to handle underspecified structural conditions \\nis essential to learning from PDB structures as the PDB represents the \\nfull range of conditions in which structures have been solved. In gen-\\neral, AlphaFold is trained to produce the protein structure most likely \\nto appear as part of a PDB structure. For example, in cases in which a \\nparticular stochiometry, ligand or ion is predictable from the sequence \\nalone, AlphaFold is likely to produce a structure that respects those \\nconstraints implicitly.\\nAlphaFold has already demonstrated its utility to the experimental \\ncommunity, both for molecular replacement57 and for interpreting \\ncryogenic electron microscopy maps58. Moreover, because AlphaFold \\noutputs protein coordinates directly, AlphaFold produces predictions \\nin graphics processing unit (GPU) minutes to GPU hours depending on \\nthe length of the protein sequence (for example, around one GPU min-\\nute per model for 384 residues; see\\xa0Methods for details). This opens up \\nthe exciting possibility of predicting structures at the proteome-scale \\nand beyond—in a companion paper39, we demonstrate the application \\nof AlphaFold to the entire human proteome39.\\nThe explosion in available genomic sequencing techniques and data \\nhas revolutionized bioinformatics but the intrinsic challenge of experi-\\nmental structure determination has prevented a similar expansion in \\nour structural knowledge. By developing an accurate protein structure \\n100 101 102 103 104\\nMedian per-residue Neff for the chain\\n0\\n20\\n40\\n60\\n80\\n100lDDT-C/uni03B1\\nCoverage < 0.3\\nCoverage > 0.6\\na\\nAlphaFold Experiment\\nb\\nFig. 5 | Effect of MSA depth and cross-chain contacts. a, Backbone accuracy \\n(lDDT-Cα) for the redundancy-reduced set of the PDB after our training data \\ncut-off, restricting to proteins in which at most 25% of the long-range contacts \\nare between different heteromer chains. We further consider two groups of \\nproteins based on template coverage at 30% sequence identity: covering more \\nthan 60% of the chain (n\\xa0=\\xa06,743 protein chains) and covering less than 30% of \\nthe chain (n\\xa0=\\xa01,596 protein chains). MSA depth is computed by counting the \\nnumber of non-gap residues for each position in the MSA (using the N eff \\nweighting scheme; see\\xa0Methods for details) and taking the median across \\nresidues. The curves are obtained through Gaussian kernel average smoothing \\n(window size is 0.2 units in log10(Neff )); the shaded area is the 95% confidence \\ninterval estimated using bootstrap of 10,000 samples. b , An intertwined \\nhomotrimer (PDB 6SK0) is correctly predicted without input stoichiometry \\nand only a weak template (blue is predicted and green is experimental).\\nNature | Vol 596 | 26 August 2021 | 589\\nprediction algorithm, coupled with existing large and well-curated \\nstructure and sequence databases assembled by the experimental \\ncommunity, we hope to accelerate the advancement of structural \\nbioinformatics that can keep pace with the genomics revolution. We \\nhope that AlphaFold—and computational approaches that apply its \\ntechniques for other biophysical problems—will become essential \\ntools of modern biology.\\nOnline content\\nAny methods, additional references, Nature Research reporting sum-\\nmaries, source data, extended data, supplementary information, \\nacknowledgements, peer review information; details of author con-\\ntributions and competing interests; and statements of data and code \\navailability are available at https://doi.org/10.1038/s41586-021-03819-2.\\n1. Thompson, M. C., Yeates, T. O. & Rodriguez, J. A. Advances in methods for atomic \\nresolution macromolecular structure determination. F1000Res. 9, 667 (2020).\\n2. Bai, X.-C., McMullan, G. & Scheres, S. H. W. How cryo-EM is revolutionizing structural \\nbiology. Trends Biochem. Sci. 40, 49–57 (2015).\\n3. Jaskolski, M., Dauter, Z. & Wlodawer, A. A brief history of macromolecular crystallography, \\nillustrated by a family tree and its Nobel fruits. FEBS J. 281, 3985–4009 (2014).\\n4. Wüthrich, K. The way to NMR structures of proteins. Nat. Struct. Biol. 8, 923–925 (2001).\\n5. wwPDB Consortium. Protein Data Bank: the single global archive for 3D macromolecular \\nstructure data. Nucleic Acids Res. 47, D520–D528 (2018).\\n6. Mitchell, A. L. et\\xa0al. MGnify: the microbiome analysis resource in 2020. Nucleic Acids Res. \\n48, D570–D578 (2020).\\n7. Steinegger, M., Mirdita, M. & Söding, J. Protein-level assembly increases protein sequence \\nrecovery from metagenomic samples manyfold. Nat. Methods 16, 603–606 (2019).\\n8. Dill, K. A., Ozkan, S. B., Shell, M. S. & Weikl, T. R. The protein folding problem. Annu. Rev. \\nBiophys. 37, 289–316 (2008).\\n9. Anfinsen, C. B. Principles that govern the folding of protein chains. Science 181, 223–230 \\n(1973).\\n10. Senior, A. W. et\\xa0al. Improved protein structure prediction using potentials from deep \\nlearning. Nature 577, 706–710 (2020).\\n11. Wang, S., Sun, S., Li, Z., Zhang, R. & Xu, J. Accurate de\\xa0novo prediction of protein contact \\nmap by ultra-deep learning model. PLOS Comput. Biol. 13, e1005324 (2017).\\n12. Zheng, W. et\\xa0al. Deep-learning contact-map guided protein structure prediction in \\nCASP13. Proteins 87, 1149–1164 (2019).\\n13. Abriata, L. A., Tamò, G. E. & Dal Peraro, M. A further leap of improvement in tertiary \\nstructure prediction in CASP13 prompts new routes for future assessments. Proteins 87, \\n1100–1112 (2019).\\n14. Pearce, R. & Zhang, Y. Deep learning techniques have significantly impacted protein \\nstructure prediction and protein design. Curr. Opin. Struct. Biol. 68, 194–207 (2021).\\n15. Moult, J., Fidelis, K., Kryshtafovych, A., Schwede, T. & Topf, M. Critical assessment of \\ntechniques for protein structure prediction, fourteenth round. CASP 14 Abstract Book \\nhttps://www.predictioncenter.org/casp14/doc/CASP14_Abstracts.pdf (2020).\\n16. Brini, E., Simmerling, C. & Dill, K. Protein storytelling through physics. Science 370, \\neaaz3041 (2020).\\n17. Sippl, M. J. Calculation of conformational ensembles from potentials of mean force.  \\nAn approach to the knowledge-based prediction of local structures in globular proteins. \\nJ. Mol. Biol. 213, 859–883 (1990).\\n18. Šali, A. & Blundell, T. L. Comparative protein modelling by satisfaction of spatial \\nrestraints. J. Mol. Biol. 234, 779–815 (1993).\\n19. Roy, A., Kucukural, A. & Zhang, Y. I-TASSER: a unified platform for automated protein \\nstructure and function prediction. Nat. Protocols 5, 725–738 (2010).\\n20. Altschuh, D., Lesk, A. M., Bloomer, A. C. & Klug, A. Correlation of co-ordinated amino acid \\nsubstitutions with function in viruses related to tobacco mosaic virus. J. Mol. Biol. 193, \\n693–707 (1987).\\n21. Shindyalov, I. N., Kolchanov, N. A. & Sander, C. Can three-dimensional contacts in protein \\nstructures be predicted by analysis of correlated mutations? Protein Eng. 7, 349–358 (1994).\\n22. Weigt, M., White, R. A., Szurmant, H., Hoch, J. A. & Hwa, T. Identification of direct residue \\ncontacts in protein–protein interaction by message passing. Proc. Natl Acad. Sci. USA \\n106, 67–72 (2009).\\n23. Marks, D. S. et\\xa0al. Protein 3D structure computed from evolutionary sequence variation. \\nPLoS ONE 6, e28766 (2011).\\n24. Jones, D. T., Buchan, D. W. A., Cozzetto, D. & Pontil, M. PSICOV: precise structural contact \\nprediction using sparse inverse covariance estimation on large multiple sequence \\nalignments. Bioinformatics 28, 184–190 (2012).\\n25. Moult, J., Pedersen, J. T., Judson, R. & Fidelis, K. A large-scale experiment to assess protein \\nstructure prediction methods. Proteins 23, ii–iv (1995).\\n26. Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K. & Moult, J. Critical assessment of \\nmethods of protein structure prediction (CASP)-round XIII. Proteins 87, 1011–1020 (2019).\\n27. Zhang, Y. & Skolnick, J. Scoring function for automated assessment of protein structure \\ntemplate quality. Proteins 57, 702–710 (2004).\\n28. Tu, Z. & Bai, X. Auto-context and its application to high-level vision tasks and 3D brain \\nimage segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 32, 1744–1757 (2010).\\n29. Carreira, J., Agrawal, P., Fragkiadaki, K. & Malik, J. Human pose estimation with iterative \\nerror feedback. In Proc. IEEE Conference on Computer Vision and Pattern Recognition \\n4733–4742 (2016).\\n30. Mirabello, C. & Wallner, B. rawMSA: end-to-end deep learning using raw multiple \\nsequence alignments. PLoS ONE 14, e0220182 (2019).\\n31. Huang, Z. et\\xa0al. CCNet: criss-cross attention for semantic segmentation. In Proc. IEEE/CVF \\nInternational Conference on Computer Vision 603–612 (2019).\\n32. Hornak, V. et\\xa0al. Comparison of multiple Amber force fields and development of \\nimproved protein backbone parameters. Proteins 65, 712–725 (2006).\\n33. Zemla, A. LGA: a method for finding 3D similarities in protein structures. Nucleic Acids \\nRes. 31, 3370–3374 (2003).\\n34. Mariani, V., Biasini, M., Barbato, A. & Schwede, T. lDDT: a local superposition-free score for \\ncomparing protein structures and models using distance difference tests. Bioinformatics \\n29, 2722–2728 (2013).\\n35. Xie, Q., Luong, M.-T., Hovy, E. & Le, Q. V. Self-training with noisy student improves \\nimagenet classification. In Proc. IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition 10687–10698 (2020).\\n36. Mirdita, M. et\\xa0al. Uniclust databases of clustered and deeply annotated protein \\nsequences and alignments. Nucleic Acids Res. 45, D170–D176 (2017).\\n37. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional \\ntransformers for language understanding. In Proc. 2019 Conference of the North \\nAmerican Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies 1, 4171–4186 (2019).\\n38. Rao, R. et\\xa0al. MSA transformer. In Proc. 38th International Conference on Machine \\nLearning PMLR 139, 8844–8856 (2021).\\n39. Tunyasuvunakool, K. et\\xa0al. Highly accurate protein structure prediction for the human \\nproteome. Nature https://doi.org/10.1038/s41586-021-03828-1 (2021).\\n40. Kuhlman, B. & Bradley, P. Advances in protein structure prediction and design. Nat. Rev. \\nMol. Cell Biol. 20, 681–697 (2019).\\n41. Marks, D. S., Hopf, T. A. & Sander, C. Protein structure prediction from sequence variation. \\nNat. Biotechnol. 30, 1072–1080 (2012).\\n42. Qian, N. & Sejnowski, T. J. Predicting the secondary structure of globular proteins using \\nneural network models. J. Mol. Biol. 202, 865–884 (1988).\\n43. Fariselli, P., Olmea, O., Valencia, A. & Casadio, R. Prediction of contact maps with neural \\nnetworks and correlated mutations. Protein Eng. 14, 835–843 (2001).\\n44. Yang, J. et\\xa0al. Improved protein structure prediction using predicted interresidue \\norientations. Proc. Natl Acad. Sci. USA 117, 1496–1503 (2020).\\n45. Li, Y. et\\xa0al. Deducing high-accuracy protein contact-maps from a triplet of coevolutionary \\nmatrices through deep residual convolutional networks. PLOS Comput. Biol. 17, \\ne1008865 (2021).\\n46. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In  \\nProc. IEEE Conference on Computer Vision and Pattern Recognition 770–778 (2016).\\n47. AlQuraishi, M. End-to-end differentiable learning of protein structure. Cell Syst. 8,  \\n292–301 (2019).\\n48. Senior, A. W. et\\xa0al. Protein structure prediction using multiple deep neural networks in the \\n13th Critical Assessment of Protein Structure Prediction (CASP13). Proteins 87, 1141–1148 \\n(2019).\\n49. Ingraham, J., Riesselman, A. J., Sander, C. & Marks, D. S. Learning protein structure with a \\ndifferentiable simulator. in Proc. International Conference on Learning Representations \\n(2019).\\n50. Li, J. Universal transforming geometric network. Preprint at https://arxiv.org/\\nabs/1908.00723 (2019).\\n51. Xu, J., McPartlon, M. & Li, J. Improved protein structure prediction by deep learning \\nirrespective of co-evolution information. Nat. Mach. Intell. 3, 601–609 (2021).\\n52. Vaswani, A. et\\xa0al. Attention is all you need. In Advances in Neural Information Processing \\nSystems 5998–6008 (2017).\\n53. Wang, H. et\\xa0al. Axial-deeplab: stand-alone axial-attention for panoptic segmentation. in \\nEuropean Conference on Computer Vision 108–126 (Springer, 2020).\\n54. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M. Unified rational \\nprotein engineering with sequence-based deep representation learning. Nat. Methods 16, \\n1315–1322 (2019).\\n55. Heinzinger, M. et\\xa0al. Modeling aspects of the language of life through transfer-learning \\nprotein sequences. BMC Bioinformatics 20, 723 (2019).\\n56. Rives, A. et\\xa0al. Biological structure and function emerge from scaling unsupervised \\nlearning to 250 million protein sequences. Proc. Natl Acad. Sci. USA 118, e2016239118 \\n(2021).\\n57. Pereira, J. et\\xa0al. High-accuracy protein structure prediction in CASP14. Proteins https://doi.\\norg/10.1002/prot.26171 (2021).\\n58. Gupta, M. et\\xa0al. CryoEM and AI reveal a structure of SARS-CoV-2 Nsp2, a multifunctional \\nprotein involved in key host processes. Preprint at https://doi.org/10.1101/2021.05.10. \\n443524 (2021).\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution \\n4.0 International License, which permits use, sharing, adaptation, distribution \\nand reproduction in any medium or format, as long as you give appropriate \\ncredit to the original author(s) and the source, provide a link to the Creative Commons license, \\nand indicate if changes were made. The images or other third party material in this article are \\nincluded in the article’s Creative Commons license, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons license and your \\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a copy of this license, \\nvisit http://creativecommons.org/licenses/by/4.0/.\\n© The Author(s) 2021\\nArticle\\nMethods\\nFull algorithm details\\nExtensive explanations of the components and their motivations are \\navailable in\\xa0Supplementary Methods 1.1–1.10, in addition, pseudocode \\nis available in Supplementary\\xa0Information Algorithms 1–32, network \\ndiagrams in Supplementary Figs.\\xa01–8, input features in Supplementary \\nTable\\xa01 and additional details are provided in Supplementary Tables\\xa02, 3. \\nTraining and inference details are provided in\\xa0Supplementary Methods \\n1.11–1.12 and Supplementary Tables\\xa04, 5.\\nIPA\\nThe IPA module combines the pair representation, the single repre-\\nsentation and the geometric representation to update the single rep-\\nresentation (Supplementary Fig.\\xa08). Each of these representations \\ncontributes affinities to the shared attention weights and then uses \\nthese weights to map its values to the output. The IPA operates in 3D \\nspace. Each residue produces query points, key points and value points \\nin its local frame. These points are projected into the global frame using \\nthe backbone frame of the residue in which they interact with each \\nother. The resulting points are then projected back into the local frame. \\nThe affinity computation in the 3D space uses squared distances and \\nthe coordinate transformations ensure the invariance of this module \\nwith respect to the global frame (see\\xa0Supplementary Methods 1.8.2 \\n‘Invariant point attention (IPA)’ for the algorithm, proof of invariance \\nand a description of the full multi-head version). A related construc-\\ntion that uses classic geometric invariants to construct pairwise fea-\\ntures in place of the learned 3D points has been applied to protein  \\ndesign59.\\nIn addition to the IPA, standard dot product attention is computed \\non the abstract single representation and a special attention on the pair \\nrepresentation. The pair representation augments both the logits and \\nthe values of the attention process, which is the primary way in which \\nthe pair representation controls the structure generation.\\nInputs and data sources\\nInputs to the network are the primary sequence, sequences from evo-\\nlutionarily related proteins in the form of a MSA created by standard \\ntools including jackhmmer60 and HHBlits61, and 3D atom coordinates \\nof a small number of homologous structures (templates) where avail-\\nable. For both the MSA and templates, the search processes are tuned \\nfor high recall; spurious matches will probably appear in the raw MSA \\nbut this matches the training condition of the network.\\nOne of the sequence databases used, Big Fantastic Database (BFD), \\nwas custom-made and released publicly (see ‘Data availability’) and \\nwas used by several CASP teams. BFD is one of the largest publicly avail-\\nable collections of protein families. It consists of 65,983,866 families \\nrepresented as MSAs and hidden Markov models (HMMs) covering \\n2,204,359,010 protein sequences from reference databases, metage-\\nnomes and metatranscriptomes.\\nBFD was built in three steps. First, 2,423,213,294 protein sequences \\nwere collected from UniProt (Swiss-Prot&TrEMBL, 2017-11)62, a soil refer-\\nence protein catalogue and the marine eukaryotic reference catalogue7, \\nand clustered to 30% sequence identity, while enforcing a 90% align-\\nment coverage of the shorter sequences using MMseqs2/Linclust63. \\nThis resulted in 345,159,030 clusters. For computational efficiency, \\nwe removed all clusters with less than three members, resulting in \\n61,083,719 clusters. Second, we added 166,510,624 representative pro-\\ntein sequences from Metaclust NR (2017-05; discarding all sequences \\nshorter than 150 residues)63 by aligning them against the cluster rep-\\nresentatives using MMseqs264. Sequences that fulfilled the sequence \\nidentity and coverage criteria were assigned to the best scoring cluster. \\nThe remaining 25,347,429 sequences that could not be assigned were \\nclustered separately and added as new clusters, resulting in the final \\nclustering. Third, for each of the clusters, we computed an MSA using \\nFAMSA65 and computed the HMMs following the Uniclust HH-suite \\ndatabase protocol36.\\nThe following versions of public datasets were used in this study. Our \\nmodels were trained on a copy of the PDB5 downloaded on 28\\xa0August \\n2019. For finding template structures at prediction time, we used a copy \\nof the PDB downloaded on 14\\xa0May 2020, and the PDB7066 clustering \\ndatabase downloaded on 13\\xa0May 2020. For MSA lookup at both training \\nand prediction time, we used Uniref9067 v.2020_01, BFD, Uniclust3036 \\nv.2018_08 and MGnify6 v.2018_12. For sequence distillation, we used \\nUniclust3036 v.2018_08 to construct a distillation structure dataset. \\nFull details are provided in\\xa0Supplementary Methods 1.2.\\nFor MSA search on BFD\\xa0+\\xa0Uniclust30, and template search against \\nPDB70, we used HHBlits61 and HHSearch66 from hh-suite v.3.0-beta.3 \\n(version 14/07/2017). For MSA search on Uniref90 and clustered MGnify, \\nwe used jackhmmer from HMMER368. For constrained relaxation of \\nstructures, we used OpenMM v.7.3.169 with the Amber99sb force field32. \\nFor neural network construction, running and other analyses, we used \\nT ensorFlow70, Sonnet71, NumPy72, Python73 and Colab74.\\nT o quantify the effect of the different sequence data sources, we \\nre-ran the CASP14 proteins using the same models but varying how the \\nMSA was constructed. Removing BFD reduced the mean accuracy by \\n0.4\\xa0GDT, removing Mgnify reduced the mean accuracy by 0.7\\xa0GDT, and \\nremoving both reduced the mean accuracy by 6.1\\xa0GDT. In each case, we \\nfound that most targets had very small changes in accuracy but a few \\noutliers had very large (20+\\xa0GDT) differences. This is consistent with the \\nresults in Fig.\\xa05a in which the depth of the MSA is relatively unimportant \\nuntil it approaches a threshold value of around 30 sequences when the \\nMSA size effects become quite large. We observe mostly overlapping \\neffects between inclusion of BFD and Mgnify, but having at least one \\nof these metagenomics databases is very important for target classes \\nthat are poorly represented in UniRef, and having both was necessary \\nto achieve full CASP accuracy.\\nTraining regimen\\nT o train, we use structures from the PDB with a maximum release date \\nof 30\\xa0April 2018. Chains are sampled in inverse proportion to cluster \\nsize of a 40% sequence identity clustering. We then randomly crop \\nthem to 256 residues and assemble into batches of size 128. We train the \\nmodel on T ensor Processing Unit (TPU) v3 with a batch size of 1 per TPU \\ncore, hence the model uses 128 TPU v3 cores. The model is trained until \\nconvergence (around 10\\xa0million samples) and further fine-tuned using \\nlonger crops of 384 residues, larger MSA stack and reduced learning \\nrate (see\\xa0Supplementary Methods 1.11 for the exact configuration). The \\ninitial training stage takes approximately 1\\xa0week, and the fine-tuning \\nstage takes approximately 4\\xa0additional days.\\nThe network is supervised by the FAPE\\xa0loss and a number of auxil-\\niary losses. First, the final pair representation is linearly projected to \\na binned distance distribution (distogram) prediction, scored with \\na cross-entropy loss. Second, we use random masking on the input \\nMSAs and require the network to reconstruct the masked regions \\nfrom the output MSA representation using a BERT-like loss37. Third, \\nthe output single representations of the structure module are used to \\npredict binned per-residue lDDT-Cα values. Finally, we use an auxiliary \\nside-chain loss during training, and an auxiliary structure violation loss \\nduring fine-tuning. Detailed descriptions and weighting are provided \\nin the\\xa0Supplementary Information.\\nAn initial model trained with the above objectives was used to make \\nstructure predictions for a Uniclust dataset of 355,993 sequences with \\nthe full MSAs. These predictions were then used to train a final model \\nwith identical hyperparameters, except for sampling examples 75% of \\nthe time from the Uniclust prediction set, with sub-sampled MSAs, and \\n25% of the time from the clustered PDB set.\\nWe train five different models using different random seeds, some \\nwith templates and some without, to encourage diversity in the predic-\\ntions (see Supplementary Table\\xa05 and Supplementary Methods 1.12.1 \\nfor details). We also fine-tuned these models after CASP14 to add a \\npTM prediction objective (Supplementary Methods 1.9.7) and use the \\nobtained models for Fig.\\xa02d.\\nInference regimen\\nWe inference the five trained models and use the predicted confidence \\nscore to select the best model per target.\\nUsing our CASP14 configuration for AlphaFold, the trunk of the net-\\nwork is run multiple times with different random choices for the MSA \\ncluster centres (see\\xa0Supplementary Methods 1.11.2 for details of the \\nensembling procedure). The full time to make a structure prediction \\nvaries considerably depending on the length of the protein. Repre-\\nsentative timings for the neural network using a single model on V100 \\nGPU are 4.8\\xa0min with 256\\xa0residues, 9.2\\xa0min with 384\\xa0residues and 18\\xa0h \\nat 2,500\\xa0residues. These timings are measured using our open-source \\ncode, and the open-source code is notably faster than the version we \\nran in CASP14 as we now use the XLA compiler75.\\nSince CASP14, we have found that the accuracy of the network with-\\nout ensembling is very close or equal to the accuracy with ensembling \\nand we turn off ensembling for most inference. Without ensembling, \\nthe network is 8× faster and the representative timings for a single \\nmodel are 0.6\\xa0min with 256\\xa0residues, 1.1\\xa0min with 384 residues and \\n2.1\\xa0h with 2,500 residues.\\nInferencing large proteins can easily exceed the memory of a single \\nGPU. For a V100 with 16\\xa0GB of memory, we can predict the structure \\nof proteins up to around 1,300\\xa0residues without ensembling and the \\n256- and 384-residue inference times are using the memory of a single \\nGPU. The memory usage is approximately quadratic in the number of \\nresidues, so a 2,500-residue protein involves using unified memory so \\nthat we can greatly exceed the memory of a single V100. In our cloud \\nsetup, a single V100 is used for computation on a 2,500-residue protein \\nbut we requested four GPUs to have sufficient memory.\\nSearching genetic sequence databases to prepare inputs and final \\nrelaxation of the structures take additional central processing unit \\n(CPU) time but do not require a GPU or TPU.\\nMetrics\\nThe predicted structure is compared to the true structure from the \\nPDB in terms of lDDT metric34, as this metric reports the domain accu-\\nracy without requiring a domain segmentation of chain structures. \\nThe distances are either computed between all heavy atoms (lDDT) \\nor only the Cα atoms to measure the backbone accuracy (lDDT-Cα). \\nAs lDDT-Cα only focuses on the Cα atoms, it does not include the pen-\\nalty for structural violations and clashes. Domain accuracies in CASP \\nare reported as GDT33 and the TM-score27 is used as a full chain global \\nsuperposition metric.\\nWe also report accuracies using the r.m.s.d.95 (Cα r.m.s.d. at 95% cov-\\nerage). We perform five iterations of (1) a least-squares alignment of the \\npredicted structure and the PDB structure on the currently chosen Cα \\natoms (using all Cα atoms in the first iteration); (2) selecting the 95% \\nof\\xa0Cα atoms with the lowest alignment error. The r.m.s.d. of the atoms \\nchosen for the final iterations is the r.m.s.d.95. This metric is more robust \\nto apparent errors that can originate from crystal structure artefacts, \\nalthough in some cases the removed 5% of residues will contain genuine \\nmodelling errors.\\nTest set of recent PDB sequences\\nFor evaluation on recent PDB sequences (Figs.\\xa02a–d, 4a, 5a), we used \\na copy of the PDB downloaded 15 February 2021. Structures were fil-\\ntered to those with a release date after 30\\xa0April 2018 (the date limit for \\ninclusion in the training set for AlphaFold). Chains were further filtered \\nto remove sequences that consisted of a single amino acid as well as \\nsequences with an ambiguous chemical component at any residue \\nposition. Exact duplicates were removed, with the chain with the most \\nresolved Cα atoms used as the representative sequence. Subsequently, \\nstructures with less than 16 resolved residues, with unknown residues \\nor solved by NMR methods were removed. As the PDB contains many \\nnear-duplicate sequences, the chain with the highest resolution was \\nselected from each cluster in the PDB 40% sequence clustering of the \\ndata. Furthermore, we removed all sequences for which fewer than  \\n80 amino acids had the alpha carbon resolved and removed chains with \\nmore than 1,400 residues. The final dataset contained 10,795 protein \\nsequences.\\nThe procedure for filtering the recent PDB dataset based on prior \\ntemplate identity was as follows. Hmmsearch was run with default \\nparameters against a copy of the PDB SEQRES fasta downloaded  \\n15 February 2021. T emplate hits were accepted if the associated struc-\\nture had a release date earlier than 30\\xa0April 2018. Each residue position \\nin a query sequence was assigned the maximum identity of any template \\nhit covering that position. Filtering then proceeded as described in \\nthe individual figure legends, based on a combination of maximum \\nidentity and sequence coverage.\\nThe MSA depth analysis was based on computing the normalized \\nnumber of effective sequences (N eff ) for each position of a query \\nsequence. Per-residue Neff values were obtained by counting the num-\\nber of non-gap residues in the MSA for this position and weighting the \\nsequences using the Neff scheme76 with a threshold of 80% sequence \\nidentity measured on the region that is non-gap in either sequence.\\nReporting summary\\nFurther information on research design is available in the\\xa0Nature \\nResearch Reporting Summary linked to this paper.\\nData availability\\nAll input data are freely available from public sources.\\nStructures from the PDB were used for training and as templates \\n(https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated \\nsequence data and 40% sequence clustering see also https://ftp.wwpdb.\\norg/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/\\nsequence/clusters/bc-40.out). Training used a version of the PDB \\ndownloaded 28\\xa0August 2019, while the CASP14 template search used \\na version downloaded 14\\xa0May 2020. The template search also used the \\nPDB70 database, downloaded 13\\xa0May 2020 (https://wwwuser.gwdg.\\nde/~compbiol/data/hhsuite/databases/hhsuite_dbs/).\\nWe show experimental structures from the PDB with accession num-\\nbers 6Y4F77, 6YJ178, 6VR479, 6SK080, 6FES81, 6W6W82, 6T1Z83 and 7JTL84.\\nFor MSA lookup at both the training and prediction time, we used \\nUniRef90 v.2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/\\nprevious_releases/release-2020_01/uniref/), BFD (https://bfd.mmseqs.\\ncom), Uniclust30 v.2018_08 (https://wwwuser.gwdg.de/~compbiol/\\nuniclust/2018_08/) and MGnify clusters v.2018_12 (https://ftp.ebi.ac.uk/\\npub/databases/metagenomics/peptide_database/2018_12/). Uniclust30 \\nv.2018_08 was also used as input for constructing a distillation structure \\ndataset.\\nCode availability\\nSource code for the AlphaFold model, trained weights and inference \\nscript are available under an open-source license at https://github.\\ncom/deepmind/alphafold.\\nNeural networks were developed with T ensorFlow v.1 (https://github.\\ncom/tensorflow/tensorflow), Sonnet v.1 (https://github.com/deep-\\nmind/sonnet), JAX v.0.1.69 (https://github.com/google/jax/) and Haiku \\nv.0.0.4 (https://github.com/deepmind/dm-haiku). The XLA compiler is \\nbundled with JAX and does not have a separate version number.\\nFor MSA search on BFD+Uniclust30, and for template search against \\nPDB70, we used HHBlits and HHSearch from hh-suite v.3.0-beta.3 \\nrelease 14/07/2017 (https://github.com/soedinglab/hh-suite). For MSA \\nsearch on UniRef90 and clustered MGnify, we used jackhmmer from \\nArticle\\nHMMER v.3.3 (http://eddylab.org/software/hmmer/). For constrained \\nrelaxation of structures, we used OpenMM v.7.3.1 (https://github.com/\\nopenmm/openmm) with the Amber99sb force field.\\nConstruction of BFD used MMseqs2 v.925AF (https://github.\\ncom/soedinglab/MMseqs2) and FAMSA v.1.2.5 (https://github.com/\\nrefresh-bio/FAMSA).\\nData analysis used Python v.3.6 (https://www.python.org/), NumPy \\nv.1.16.4 (https://github.com/numpy/numpy), SciPy v.1.2.1 (https://www.\\nscipy.org/), seaborn v.0.11.1 (https://github.com/mwaskom/seaborn), \\nMatplotlib v.3.3.4 (https://github.com/matplotlib/matplotlib), bokeh \\nv.1.4.0 (https://github.com/bokeh/bokeh), pandas v.1.1.5 (https://\\ngithub.com/pandas-dev/pandas), plotnine v.0.8.0 (https://github.\\ncom/has2k1/plotnine), statsmodels v.0.12.2 (https://github.com/\\nstatsmodels/statsmodels) and Colab (https://research.google.com/\\ncolaboratory). TM-align v.20190822 (https://zhanglab.dcmb.med.\\numich.edu/TM-align/) was used for computing TM-scores. Structure \\nvisualizations were created in Pymol v.2.3.0 (https://github.com/schro-\\ndinger/pymol-open-source).\\n \\n59. Ingraham, J., Garg, V. K., Barzilay, R. & Jaakkola, T. Generative models for graph-based \\nprotein design. in Proc. 33rd Conference on Neural Information Processing Systems \\n(2019).\\n60. Johnson, L. S., Eddy, S. R. & Portugaly, E. Hidden Markov model speed heuristic and \\niterative HMM search procedure. BMC Bioinformatics 11, 431 (2010).\\n61. Remmert, M., Biegert, A., Hauser, A. & Söding, J. HHblits: lightning-fast iterative protein \\nsequence searching by HMM-HMM alignment. Nat. Methods 9, 173–175 (2012).\\n62. The UniProt Consortium. UniProt: the universal protein knowledgebase in 2021. Nucleic \\nAcids Res. 49, D480–D489 (2020).\\n63. Steinegger, M. & Söding, J. Clustering huge protein sequence sets in linear time. Nat. \\nCommun. 9, 2542 (2018).\\n64. Steinegger, M. & Söding, J. MMseqs2 enables sensitive protein sequence searching for \\nthe analysis of massive data sets. Nat. Biotechnol. 35, 1026–1028 (2017).\\n65. Deorowicz, S., Debudaj-Grabysz, A. & Gudyś, A. FAMSA: fast and accurate multiple \\nsequence alignment of huge protein families. Sci. Rep. 6, 33964 (2016).\\n66. Steinegger, M. et\\xa0al. HH-suite3 for fast remote homology detection and deep protein \\nannotation. BMC Bioinformatics 20, 473 (2019).\\n67. Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B. & Wu, C. H. UniRef clusters: a \\ncomprehensive and scalable alternative for improving sequence similarity searches. \\nBioinformatics 31, 926–932 (2015).\\n68. Eddy, S. R. Accelerated profile HMM searches. PLOS Comput. Biol. 7, e1002195 (2011).\\n69. Eastman, P. et\\xa0al. OpenMM 7: rapid development of high performance algorithms for \\nmolecular dynamics. PLOS Comput. Biol. 13, e1005659 (2017).\\n70. Ashish, A. M. A. et\\xa0al. TensorFlow: large-scale machine learning on heterogeneous \\nsystems. Preprint at https://arxiv.org/abs/1603.04467 (2015).\\n71. Reynolds, M. et\\xa0al. Open sourcing Sonnet – a new library for constructing neural \\nnetworks. DeepMind https://deepmind.com/blog/open-sourcing-sonnet/ (7 April 2017).\\n72. Harris, C. R. et\\xa0al. Array programming with NumPy. Nature 585, 357–362 (2020).\\n73. Van Rossum, G. & Drake, F. L. Python 3 Reference Manual (CreateSpace, 2009).\\n74. Bisong, E. in Building Machine Learning and Deep Learning Models on Google Cloud \\nPlatform: A Comprehensive Guide for Beginners 59–64 (Apress, 2019).\\n75. TensorFlow. XLA: Optimizing Compiler for TensorFlow. https://www.tensorflow.org/xla \\n(2018).\\n76. Wu, T., Hou, J., Adhikari, B. & Cheng, J. Analysis of several key factors influencing deep \\nlearning-based inter-residue contact prediction. Bioinformatics 36, 1091–1098 (2020).\\n77. Jiang, W. et\\xa0al. MrpH, a new class of metal-binding adhesin, requires zinc to mediate \\nbiofilm formation. PLoS Pathog. 16, e1008707 (2020).\\n78. Dunne, M., Ernst, P., Sobieraj, A., Pluckthun, A. & Loessner, M. J. The M23 peptidase \\ndomain of the Staphylococcal phage 2638A endolysin. PDB https://doi.org/10.2210/\\npdb6YJ1/pdb (2020).\\n79. Drobysheva, A. V. et\\xa0al. Structure and function of virion RNA polymerase of a crAss-like \\nphage. Nature 589, 306–309 (2021).\\n80. Flaugnatti, N. et\\xa0al. Structural basis for loading and inhibition of a bacterial T6SS \\nphospholipase effector by the VgrG spike. EMBO J. 39, e104129 (2020).\\n81. ElGamacy, M. et\\xa0al. An interface-driven design strategy yields a novel, corrugated protein \\narchitecture. ACS Synth. Biol. 7, 2226–2235 (2018).\\n82. Lim, C. J. et\\xa0al. The structure of human CST reveals a decameric assembly bound to \\ntelomeric DNA. Science 368, 1081–1085 (2020).\\n83. Debruycker, V. et\\xa0al. An embedded lipid in the multidrug transporter LmrP suggests a \\nmechanism for polyspecificity. Nat. Struct. Mol. Biol. 27, 829–835 (2020).\\n84. Flower, T. G. et\\xa0al. Structure of SARS-CoV-2 ORF8, a rapidly evolving immune evasion \\nprotein. Proc. Natl Acad. Sci. USA 118, e2021785118 (2021).\\nAcknowledgements We thank A. Rrustemi, A. Gu, A. Guseynov, B. Hechtman, C. Beattie,  \\nC. Jones, C. Donner, E. Parisotto, E. Elsen, F. Popovici, G. Necula, H. Maclean, J. Menick,  \\nJ. Kirkpatrick, J. Molloy, J. Yim, J. Stanway, K. Simonyan, L. Sifre, L. Martens, M. Johnson,  \\nM. O’Neill, N. Antropova, R. Hadsell, S. Blackwell, S. Das, S. Hou, S. Gouws, S. Wheelwright,  \\nT. Hennigan, T. Ward, Z. Wu, Ž. Avsec and the Research Platform Team for their contributions; \\nM. Mirdita for his help with the datasets; M. Piovesan-Forster, A. Nelson and R. Kemp for their \\nhelp managing the project; the JAX, TensorFlow and XLA teams for detailed support and \\nenabling machine learning models of the complexity of AlphaFold; our colleagues at \\nDeepMind, Google and Alphabet for their encouragement and support; and J. Moult and  \\nthe CASP14 organizers, and the experimentalists whose structures enabled the assessment.  \\nM.S. acknowledges support from the National Research Foundation of Korea grant \\n(2019R1A6A1A10073437, 2020M3A9G7103933) and the Creative-Pioneering Researchers \\nProgram through Seoul National University.\\nAuthor contributions J.J. and D.H. led the research. J.J., R.E., A. Pritzel, M.F., O.R., R.B.,  \\nA. Potapenko, S.A.A.K., B.R.-P., J.A., M.P., T. Berghammer and O.V. developed the neural network  \\narchitecture and training. T.G., A.Ž., K.T., R.B., A.B., R.E., A.J.B., A.C., S.N., R.J., D.R., M.Z. and S.B. \\ndeveloped the data, analytics and inference systems. D.H., K.K., P.K., C.M. and E.C. managed \\nthe research. T.G. led the technical platform. P.K., A.W.S., K.K., O.V., D.S., S.P. and T. Back \\ncontributed technical advice and ideas. M.S. created the BFD genomics database and \\nprovided technical assistance on HHBlits. D.H., R.E., A.W.S. and K.K. conceived the AlphaFold \\nproject. J.J., R.E. and A.W.S. conceived the end-to-end approach. J.J., A. Pritzel, O.R.,  \\nA. Potapenko, R.E., M.F., T.G., K.T., C.M. and D.H. wrote the paper.\\nCompeting interests J.J., R.E., A. Pritzel, T.G., M.F., O.R., R.B., A.B., S.A.A.K., D.R. and A.W.S. have \\nfiled non-provisional patent applications 16/701,070 and PCT/EP2020/084238, and provisional \\npatent applications 63/107,362, 63/118,917, 63/118,918, 63/118,921 and 63/118,919, each in the \\nname of DeepMind Technologies Limited, each pending, relating to machine learning for \\npredicting protein structures. The other authors declare no competing interests.\\nAdditional information\\nSupplementary information The online version contains supplementary material available at \\nhttps://doi.org/10.1038/s41586-021-03819-2.\\nCorrespondence and requests for materials should be addressed to J.J. or D.H.\\nPeer review information Nature thanks Mohammed AlQuraishi, Charlotte Deane and Yang \\nZhang for their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.\\n1 nature research  |  reporting summaryApril 2020\\nCorresponding author(s): John Jumper, Demis Hassabis\\nLast updated by author(s): Jul 11, 2021\\nReporting Summary\\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \\nin reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.\\nStatistics\\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\\nn/a Confirmed\\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\\nThe statistical test(s) used AND whether they are one- or two-sided \\nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\\nA description of all covariates tested\\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \\nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \\nGive P values as exact values whenever suitable.\\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\\nOur web collection on statistics for biologists contains articles on many of the points above.\\nSoftware and code\\nPolicy information about availability of computer code\\nData collection Source code for the AlphaFold model, trained weights, and inference script will be made available under an open-source license at https://\\ngithub.com/deepmind/ upon publication. \\n \\nNeural networks were developed with TensorFlow v1 (https://github.com/tensorflow/tensorflow), Sonnet v1 (https://github.com/deepmind/\\nsonnet), JAX v0.1.69 (https://github.com/google/jax/), and Haiku v0.0.4 (https://github.com/deepmind/dm-haiku). The XLA compiler is \\nbundled with JAX and does not have a separate version number. \\n \\nFor MSA search on BFD+Uniclust30, and for template search against PDB70, we used HHBlits and HHSearch from hh-suite v3.0-beta.3 \\n14/07/2017 (https://github.com/soedinglab/hh-suite). For MSA search on UniRef90 and clustered MGnify, we used jackhmmer from HMMER \\nv3.3 (http://eddylab.org/software/hmmer/). For constrained relaxation of structures, we used OpenMM v7.3.1 (https://github.com/openmm/\\nopenmm) with the Amber99sb force field.  \\n \\nConstruction of BFD used MMseqs2 version 925AF (https://github.com/soedinglab/MMseqs2) and FAMSA v1.2.5 (https://github.com/refresh-\\nbio/FAMSA). \\nData analysis Data analysis used Python v3.6 (https://www.python.org/), NumPy v1.16.4 (https://github.com/numpy/numpy), SciPy v1.2.1 (https://\\nwww.scipy.org/), seaborn v0.11.1 (https://github.com/mwaskom/seaborn), Matplotlib v3.3.4 (https://github.com/matplotlib/matplotlib), \\nbokeh v1.4.0 (https://github.com/bokeh/bokeh), pandas v1.1.5 (https://github.com/pandas-dev/pandas), plotnine v0.8.0 (https://github.com/\\nhas2k1/plotnine), statsmodels v0.12.2 (https://github.com/statsmodels/statsmodels) and Colab (https://research.google.com/colaboratory). \\nTM-align v20190822 (https://zhanglab.dcmb.med.umich.edu/TM-align/) was used for computing TM-scores. Structure visualizations were \\ncreated in Pymol v2.3.0 (https://github.com/schrodinger/pymol-open-source).\\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \\nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\\n2 nature research  |  reporting summaryApril 2020\\nData\\nPolicy information about availability of data\\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \\n- Accession codes, unique identifiers, or web links for publicly available datasets \\n- A list of figures that have associated raw data \\n- A description of any restrictions on data availability\\nAll input data are freely available from public sources. \\n \\nStructures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence \\nclustering see also https://ftp.wwpdb.org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/sequence/clusters/bc-40.out). Training used a version of the \\nPDB downloaded 28/08/2019, while CASP14 template search used a version downloaded 14/05/2020. Template search also used the PDB70 database, downloaded \\n13/05/2020 (https://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/). \\n \\nWe show experimental structures from the PDB with accessions 6Y4F77, 6YJ178, 6VR479, 6SK080, 6FES81, 6W6W82, 6T1Z83, and 7JTL84. \\n \\nFor MSA lookup at both training and prediction time, we used UniRef90 v2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/\\nrelease-2020_01/uniref/), BFD (https://bfd.mmseqs.com), Uniclust30 v2018_08 (https://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/), and MGnify clusters \\nv2018_12 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/).  \\nUniclust30 v2018_08 was further used as input for constructing a distillation structure dataset.\\nField-specific reporting\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\\nLife sciences study design\\nAll studies must disclose on these points even when the disclosure is negative.\\nSample size No sample size was chosen; the method was evaluated on the full CASP14 benchmark set, and all PDB chains not in the training set (subject to \\nthe exclusions noted below). \\nData exclusions The recent PDB set was filtered (see Methods for full details). Briefly this excludes chains with too few resolved residues, longer than 1400 \\nresidues, solved by NMR or with unknown/ambiguous residues. This set was also redundancy reduced (by taking representatives from a \\nsequence clustering), and for some figures a sequence similarity-based filter was applied to remove entries too similar to the training set (see \\nMethods and figure legends for details).\\nReplication Not applicable, no experimental work is described in this study. The results are the output of a computational method which will be made \\navailable.\\nRandomization Not applicable, we are not making a comparison between two groups\\nBlinding Not applicable, we are not making a comparison between two groups\\nReporting for specific materials, systems and methods\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \\nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \\nMaterials & experimental systems\\nn/a Involved in the study\\nAntibodies\\nEukaryotic cell lines\\nPalaeontology and archaeology\\nAnimals and other organisms\\nHuman research participants\\nClinical data\\nDual use research of concern\\nMethods\\nn/a Involved in the study\\nChIP-seq\\nFlow cytometry\\nMRI-based neuroimaging\\n\"}, {'file_name': 'esm-2', 'text': 'STRUCTURE PREDICTION\\nEvolutionary-scale prediction of atomic-level protein\\nstructure with a language model\\nZeming Lin1,2†, Halil Akin1†, Roshan Rao1†, Brian Hie1,3†, Zhongkai Zhu1, Wenting Lu1,\\nNikita Smetanin1, Robert Verkuil1, Ori Kabeli1, Yaniv Shmueli1, Allan dos Santos Costa4,\\nMaryam Fazel-Zarandi1, Tom Sercu1, Salvatore Candido1, Alexander Rives1,2*\\nRecent advances in machine learning have leveraged evolutionary information in multiple sequence\\nalignments to predict protein structure. We demonstrate direct inference of full atomic-level\\nprotein structure from primary sequence using a large language model. As language models of\\nprotein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein\\nstructure emerges in the learned representations. This results in an order-of-magnitude acceleration\\nof high-resolution structure prediction, which enables large-scale structural characterization of\\nmetagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by\\npredicting structures for >617 million metagenomic protein sequences, including >225 million\\nthat are predicted with high confidence, which gives a view into the vast breadth and diversity of\\nnatural proteins.\\nT\\nh es e q u e n c e so fp r o t e i n sa tt h es c a l eo f\\nevolution contain an image of biolog-\\nical structure and function. The biolog-\\nical properties of a protein constrain\\nthe mutations to its sequence that are\\nselected through evolution, recording biol-\\nogy into evolutionary patterns (1–3). Protein\\nstructure and function can therefore be in-\\nferred from the patterns in sequences (4, 5).\\nThis insight has been central to progress in\\ncomputational structure prediction starting\\nfrom classical methods (6, 7)t h r o u g ht h ei n t r o -\\nduction of deep learning (8–11)u pt op r e s e n t\\nhigh-accuracy structure prediction (12, 13).\\nLanguage models have the potential to learn\\npatterns in protein sequences across evolu-\\ntion. This idea motivates research on evolu-\\ntionary-scale language models (14), in which\\nbasic models (15–17) learn representations\\nthat reflect aspects of the underlying biology\\nand, with greater representational capacity,\\ncapture secondary structure (14, 18) and ter-\\ntiary structure (14, 19–21) at a low resolution.\\nBeginning with Shannon’sm o d e lf o rt h e\\nentropy of text (22), language models of in-\\ncreasing complexity have been developed,\\nwhich has culminated in modern large-scale\\nattention-based architectures (23–25). Despite\\nthe simplicity of their training objectives, such\\nas filling in missing words or predicting the\\nnext word, language models of text are shown\\nto exhibit emergent capabilities that develop\\nas a function of scale in increasing compu-\\ntational power, data, and number of param-\\neters. Modern language models containing\\ntens to hundreds of billions of parameters\\nshow abilities such as few-shot language trans-\\nlation, commonsense reasoning, and math-\\nematical problem solving, all without explicit\\nsupervision (26–29).\\nWe posit that the task of filling in missing\\namino acids in protein sequences across evo-\\nlution will require a language model to under-\\nstand the underlying structure that creates the\\npatterns in the sequences. As the representa-\\ntional capacity of the language model and\\nthe diversity of protein sequences seen in its\\ntraining increase, we expect deep information\\nabout the biological properties of the protein\\nsequences to emerge because those proper-\\nties give rise to the patterns that are observed\\nin the sequences. To study this kind of emer-\\ngence, we scale language models from 8 mil-\\nlion parameters up to 15 billion parameters.\\nWe discover that atomic-resolution structure\\nemerges and continues to improve in language\\nmodels over the four orders of magnitude in\\nparameter scale. Strong correlations between\\nthe language model’s understanding of the\\nprotein sequence (perplexity) and the accu-\\nracy of the structure prediction reveal a close\\nlink between language modeling and the learn-\\ning of structure.\\nWe show that language models enable fast\\nend-to-end atomic-resolution structure pre-\\ndiction directly from sequence. Our approach\\nleverages the evolutionary patterns that are\\ncaptured by the language model to produce\\naccurate atomic-level predictions. This removes\\ncostly aspects of the current state-of-the-art\\nstructure prediction pipeline, which eliminates\\nthe need for a multiple sequence alignment\\n(MSA) while greatly simplifying the neural\\narchitecture used for inference. This results\\nin an improvement in speed of up to 60× on\\nthe inference forward pass alone while also\\nremoving the search process for related pro-\\nteins entirely, which can take >10 min with\\nthe high-sensitivity pipelines used by Alpha-\\nFold (12)a n dR o s e T T A F o l d(13)a n di sam e a n -\\ningful part of the computational cost even\\nwith recent lower-sensitivity fast pipelines\\n(30). In practice, this means the speedup over\\nthe state-of-the-art prediction pipelines is up\\nto one to two orders of magnitude.\\nThis speed advantage makes it possible to\\nexpand structure prediction to metagenomic\\nscale datasets. The past decade has seen efforts\\nto expand knowledge of protein sequences to\\nthe immense microbial natural diversity of\\nEarth through metagenomic sampling. These\\nefforts have contributed to an exponential\\ngrowth in the size of protein sequence data-\\nbases, which now contain billions of proteins\\n(31–33). Computational structural character-\\nizations have recently been completed for\\n∼20,000 proteins in the human proteome (34)\\nand the ∼200 million cataloged proteins of\\nUniprot (35), but the vast scale of metagenomic\\nproteins represents a far greater challenge\\nfor structural characterization. The extent and\\ndiversity of metagenomic structures is un-\\nknown and is a frontier for biological knowl-\\nedge, as well as a potential source of discoveries\\nfor medicine and biotechnology (36–38).\\nWe present an evolutionary-scale structural\\ncharacterization of metagenomic proteins that\\nfolds practically all sequences in MGnify90 (32),\\n>617 million proteins. We were able to complete\\nthis characterization in 2 weeks on a hetero-\\ngeneous cluster of 2000 graphics processing\\nunits (GPUs), which demonstrates scalability\\nto far larger databases. High-confidence pre-\\ndictions are made for >225 million structures,\\nwhich reveals and characterizes regions of\\nmetagenomic space distant from existing\\nknowledge. Most (76.8%) high-confidence pre-\\ndictions are separate from UniRef90 (39)b ya t\\nleast 90% sequence identity, and tens of mil-\\nlions of predictions (12.6%) do not have any\\nmatch to experimentally determined struc-\\ntures. These results give a large-scale view into\\nthe vast extent and diversity of metagenomic\\nprotein structures. These predicted structures\\ncan be accessed in the ESM Metagenomic Atlas\\n(https://esmatlas.com) open science resource.\\nAtomic-resolution structure emerges in\\nlanguage models trained on protein sequences\\nWe begin with a study of the emergence of\\nhigh-resolution protein structure. We trained a\\nfamily of transformer protein language models,\\nESM-2, at scales from 8 million parameters\\nup to 15 billion parameters. Relative to our\\nprevious generation model ESM-1b, ESM-2\\nintroduces improvements in architecture, train-\\ning parameters, and increases computational\\nresources and data [supplementary mate-\\nrial (SM) sections A.1.1 and A.2]. The resulting\\nESM-2 model family outperforms previously\\nstate-of-the-art ESM-1b (a∼650 million pa-\\nrameter model) at a comparable number of\\nRESEARCH\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 1o f8\\n1FAIR, Meta AI, New York, NY, USA.2New York University, New\\nYork, NY, USA.3Stanford University, Palo Alto, CA, USA.\\n4Massachusetts Institute of Technology, Cambridge, MA, USA.\\n*Corresponding author. Email: arives@meta.com\\n†These authors contributed equally to this work.\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nparameters, and on structure prediction bench-\\nmarks it also outperforms other recent pro-\\ntein language models (table S1).\\nESM-2 is trained to predict the identity of\\namino acids that have been randomly masked\\nout of protein sequences:\\nLMLM ¼/C0\\nX\\ni∈M\\nlog px ijxnM Þð ð1Þ\\nwhere for a randomly generated maskM that\\nincludes 15% of positionsi in the sequencex,\\nthe model is tasked with predicting the iden-\\ntity of the amino acidsxi in the mask from\\nthe surrounding contextxnM , excluding the\\nmasked positions. This masked language mod-\\neling objective (25)c a u s e st h em o d e lt ol e a r n\\ndependencies between the amino acids. Al-\\nthough the training objective itself is simple\\nand unsupervised, solving it over millions\\nof evolutionarily diverse protein sequences\\nrequires the model to internalize sequence\\npatterns across evolution. We expect that this\\ntraining will cause biological structure to\\nmaterialize in the language model because it\\nis linked to the sequence patterns. ESM-2 is\\ntrained over sequences in the UniRef (39)\\nprotein sequence database. During training,\\nsequences are sampled with even weighting\\nacross ∼43 million UniRef50 training clusters\\nfrom ∼138 million UniRef90 sequences, so\\nthat over the course of training, the model sees\\n∼65 million unique sequences.\\nAs we increase the scale of ESM-2 from 8 mil-\\nlion to 15 billion parameters, we observe large\\nimprovements in the fidelity of its modeling\\nof protein sequences. This fidelity can be\\nmeasured by using perplexity, which ranges\\nfrom 1 for a perfect model to 20 for a model\\nthat makes predictions at random. Intuitively,\\nthe perplexity describes the average number\\nof amino acids that the model is choosing\\namong for each position in the sequence. Math-\\nematically, perplexity is defined as the ex-\\nponential of the negative log-likelihood of the\\nsequence (SM A.2.2). Figure S1 shows perplex-\\nity for the ESM-2 family as a function of the\\nnumber of training updates, evaluated on a\\nset of∼500,000 UniRef50 clusters that have\\nbeen held out from training. Comparisons\\nare performed at 270,000 training steps for all\\nmodels in this section. The fidelity continues\\nto improve as the parameters increase up to the\\nlargest model. The 8-million-parameter mod-\\nel has a perplexity of 10.45, and the 15 billion\\nmodel reaches a perplexity of 6.37, which in-\\ndicates a large improvement in the under-\\nstanding of protein sequences with scale.\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 2o f8\\nFig. 1. Emergence of\\nstructure when scaling\\nlanguage models to\\n15 billionparameters.\\n(A) Predicted contact\\nprobabilities (bottom\\nright) and actual contact\\nprecision (top left) for\\nPDB 3LYW. A contact is\\na positive prediction if\\nit is within the top L\\nmost likely contacts for a\\nsequence of length L.\\n(B to D) Unsupervised\\ncontact prediction per-\\nformance [long-range\\nprecision at L (P@L)]\\n(SM A.2.1) for all scales\\nof the ESM-2 model. (B)\\nPerformance binned by\\nthe number of MMseqs\\nhits when searching the\\ntraining set. Larger ESM-2\\nmodels perform better\\nat all levels; the 150-\\nmillion-parameter ESM-2\\nmodel is comparable to\\nthe 650-million-\\nparameter ESM-1b\\nmodel. (C) Trajectory of\\nimprovement as model\\nscale increases for\\nsequences with differ-\\nent numbers of MMseqs\\nhits. (D) Left-to-right\\nshows models from\\n8 million to 15 billion\\nparameters, comparing\\nthe smaller model (x axis) against the next larger model (y axis) through\\nunsupervised contact precision. Points are PDB proteins colored by change\\nin perplexity for the sequence between the smaller and larger model. Sequences\\nwith large changes in contact prediction performance also exhibit large\\nchanges in language model understanding measured by perplexity. (E) TM-score\\non combined CASP14 and CAMEO test sets. Predictions are made by using\\nstructure module–only head on top of language models. Points are colored\\nby the change in perplexity between the models. (F) Structure predictions\\non CAMEO structure 7QQA and CASP target 1056 at all ESM-2 model scales,\\ncolored by pLDDT (pink, low; teal, high). For 7QQA, prediction accuracy\\nimproves at the 150-million-parameter threshold. For T1056, prediction\\naccuracy improves at the 15-billion-parameter threshold.\\nRMSD\\nplDDT\\nPerplexity\\nRMSD\\nplDDT\\nPerplexity\\n7.7\\n59.7\\n12.5\\n7.0\\n65.6\\n11.3\\n3.2\\n69.9\\n6.8\\n2.9\\n75.0\\n5.4\\n2.8\\n75.8\\n4.3\\n2.6\\n75.6\\n4.1\\n4.4\\n57.2\\n11.5\\n3.9\\n61.8\\n10.9\\n4.5\\n59.7\\n9.6\\n4.3\\n67.1\\n8.0\\n4.0\\n66.5\\n6.7\\n2.7\\n75.8\\n2.1\\nESM-2 (8M) ESM-2 (35M) ESM-2 (150M) ESM-2 (650M) ESM-2 (3B) ESM-2 (15B)\\nTarget:\\n7QQA\\nTarget:\\nT1056\\nAB C\\nD E\\nF\\npLDDT\\n≤ 50\\n≥ 90\\nUnsupervised Contact Map (3LYW)\\np(d(i, j) ≤ 8Å)\\n True Positive  False Positive  Other Contact\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nThis training also results in the emergence\\nof structure in the models. Because ESM-2’s\\ntraining is only on sequences, any informa-\\ntion about structure that develops must be\\nthe result of representing the patterns in se-\\nquences. Transformer models that are trained\\nwith masked language modeling are known to\\ndevelop attention patterns that correspond to\\nthe residue–residue contact map of the protein\\n(19, 20). We examine how this low-resolution\\npicture of protein structure emerges as a\\nfunction of scale. We use a linear projection\\nto extract the contact map from the atten-\\ntion patterns of the language model (SM\\nA.2.1). The precision of the top L (length of the\\nprotein) predicted contacts (long-range con-\\ntact precision) measures the correspondence\\nof the attention pattern with the structure\\nof the protein. Attention patterns develop in\\nESM-2 that correspond to tertiary structure\\n(Fig. 1A), and scaling leads to large improve-\\nments in the understanding of structure (Fig.\\n1B). The accuracy of the predicted contacts\\nvaries as a function of the number of evolu-\\ntionarily related sequences in the training set.\\nProteins with more related sequences in the\\ntraining set have steeper learning trajectories\\nwith respect to model scale (Fig. 1C). Improve-\\nment on sequences with high evolutionary\\ndepth thus saturates at lower model scales,\\nand improvement on sequences with low evo-\\nlutionary depth continues as models increase\\nin size.\\nFor individual proteins, we often observe\\nnonlinear improvements in the accuracy of\\nthe contact prediction as a function of scale.\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 3o f8\\nESM-2\\nFolding\\nTrunk\\n48 blocks\\nPair Rep\\nSingle Sequence\\nSeq Rep\\nFolding Block\\nFolding\\nTrunk\\nStructure\\nModule\\n8 blocks\\nEGRLTVYCTVQ...\\nPretrained via Masked LM\\nRecycling\\nA\\nB\\nC\\nPredicted\\nstructure and\\nconfidence\\nCASP14 T1057 (7M6B)\\nTM-score ESMFold: 0.98, Perplexity ESM-2: 4.4\\nTM-score Alphafold: 0.97\\nGlucosamine-6-phosphate deaminase (7LQM)\\nDockQ Score ESMFold: 0.91, Perplexity ESM-2: 2.3\\nL-asparaginase (7QYM)\\nDockQ Score ESMFold: 0.97, Perplexity ESM-2: 3.2\\nD\\nE\\nCASP14 T1074 (7OC9)\\nTM-score ESMFold: 0.64, Perplexity ESM-2: 16.6\\nTM-score Alphafold: 0.93\\nFig. 2. Single sequence structure prediction with ESMFold.(A) ESMFold\\nmodel architecture. Arrows show the information flow in the network from the\\nlanguage model to the folding trunk to the structure module that outputs 3D\\ncoordinates and confidences. LM, language model. (B) ESMFold produces accurate\\natomic resolution predictions, with similar accuracy to RoseTTAFold on CAMEO. When\\nMSAs are ablated for AlphaFold and RoseTTAFold, performance of the models\\ndegrades. Scatterplots compare ESMFold (x axis) predictions with AlphaFold2 (y axis),\\ncolored by language model perplexity. Proteins with low perplexity score similarly\\nto AlphaFold2. AF, AlphaFold2. (C) Model pLDDT versus true LDDT (left) and relative\\nperformance against AlphaFold (right) on CAMEO. pLDDT is a well-calibrated\\nestimate of prediction accuracy. (D) Successful examples: Top shows test-set\\npredictions of T1057, with ESMFold (left) and AlphaFold2 (right). Coloring shows\\npredicted LDDT for both models (ESMFold high confidence, teal; AlphaFold2 high\\nconfidence, green; both low confidence, pink). Ground truth is shown in gray.\\nThe bottom two show complex predictions on a dimer (PDB: 7LQM) and a tetramer\\n(PDB: 7QYM); ESMFold predictions are colored by chain ID and overlaid on ground\\ntruth (gray). DockQ (50) scores are reported for the interactions; in the case of\\nthe tetramer 7QYM, the score is the average of scores over interacting chain pairs.\\n(E) Unsuccessful example: test-set predictions of T1074, with ESMFold (left)\\nand AlphaFold2 (right). Coloring shows predicted LDDT for both models (ESMFold\\nhigh confidence, teal; AlphaFold2 high confidence, green; both low confidence,\\npink). Ground truth is shown in gray. ESMFold TM-score is substantially below\\nAlphaFold2 TM-score. The perplexity of the unsuccessful sequence is 16.6,\\nmeaning the language model does not understand the input sequence.\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nPlotting the change in the distribution of long-\\nrange contact precision at each transition\\nto a higher level of scale reveals an overall shift\\nin the distribution toward better performance\\n(Fig. 1D), as well as a subset of proteins that\\nundergo greater improvement. The accuracy\\nof the contact map prediction and perplex-\\nity are linked, with proteins undergoing large\\nchanges in contact map accuracy also under-\\ngoing large changes in perplexity [normalized\\ndiscounted cumulative gain (NDCG) = 0.87]\\n(SM A.2.6). This link indicates that the lan-\\nguage modeling objective is directly correlated\\nwith the materialization of the folded struc-\\nture in the attention maps.\\nTo identify atomic-resolution information\\nin the model, we project out spatial coordi-\\nnates for each of the atoms from the internal\\nrepresentations of the language model using\\nan equivariant transformer (SM A.3.3). This\\nprojection is fitted by using experimentally\\ndetermined protein structures from the Pro-\\ntein Data Bank (PDB) (40) and evaluated on\\n194 CAMEO proteins (41) and 51 CASP14 pro-\\nteins (42). TM-score, which ranges from 0 to 1,\\nmeasures the accuracy of the projection in\\ncomparison to the ground truth structure,\\nwith a value of 0.5 corresponding to the thresh-\\nold for correctly predicting the fold (43). The\\nevaluation uses a temporal cutoff, which en-\\nsures that the proteins used for testing are held\\nout from those used in fitting the projection.\\nThis makes it possible to measure how atomic-\\nlevel information emerges in the representations\\nas a function of the parameter scale.\\nWe discover that an atomic-resolution struc-\\nture prediction can be projected from the rep-\\nresentations of the ESM-2 language models.\\nThe accuracy of this projection improves with\\nt h es c a l eo ft h el a n g u a g em o d e l .T h e1 5b i l l i o n\\nparameter model reaches a TM-score of 0.72\\non the CAMEO test set and 0.55 on the CASP14\\ntest set, a gain of 14 and 17% respectively rel-\\native to the 150 million parameter ESM-2\\nmodel (Fig. 1E). At each increase in scale a\\nsubset of proteins undergoes large changes\\nin accuracy. For example, the protein 7QQA\\nimproves in root mean square deviation\\n(RMSD) from 7.0 to 3.2 Å when the scale is\\nincreased from 35 million to 150 million pa-\\nrameters, and the CASP target T1056 im-\\nproves in RMSD from 4.0 to 2.6 Å when the\\nscale is increased from 3 billion to 15 billion\\nparameters (Fig. 1F). Before and after these\\njumps, changes in RMSD are much smaller.\\nAcross all models (table S1), there is a cor-\\nrelation of−0.99 between validation perplex-\\nity and CASP14 TM-score and−1.00 between\\nvalidation perplexity and CAMEO TM-score,\\nwhich indicates a strong connection between\\nthe understanding of the sequence measured\\nby perplexity and the atomic-resolution struc-\\nture prediction. Additionally, there are strong\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 4o f8\\nFig. 3. Mapping metagenomic structural space.(A) ESMFold calibration with\\nAlphaFold2 for metagenomic sequences. Mean pLDDT is shown on thex axis, and\\nLDDT to the corresponding AlphaFold2 prediction is shown on they axis.\\nDistribution is shown as a density estimate across a subsample of∼4000\\nsequences from the MGnify database. (B) Distribution of mean pLDDT values\\ncomputed for each of∼617 million ESMFold-predicted structures from the MGnify\\ndatabase. (C) The distribution of the TM-score to the most similar PDB structure\\nfor each of 1 million randomly sampled high-confidence (mean pLDDT > 0.7 and\\npTM > 0.7) structures. Values were obtained by a Foldseek search, which does\\nnot report values under 0.5 TM-score (53). (D) Sample of 1 million high-\\nconfidence protein structures is visualized in two dimensions by using the UMAP\\nalgorithm and colored according to distance from the nearest PDB structure, in\\nwhich regions with low similarity to known structures are colored in dark blue.\\nExample protein structures and their locations within the sequence landscape are\\nprovided; see also Fig. 4 and table S2. (E) Additional UMAP plot in which the\\n1 million sequences are plotted according to the same coordinates as in (D) but\\ncolored by the sequence identity to the most similar entry in UniRef90 according\\nto a blastp (60) search.\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\ncorrelations between the low-resolution pic-\\nture of the structure that can be extracted from\\nthe attention maps and the atomic-resolution\\nprediction (0.96 between long-range contact\\nprecision and CASP14 TM-score and 0.99 be-\\ntween long-range contactp r e c i s i o na n dC A M E O\\nTM-score). These findings connect improve-\\nments in language modeling with the increases\\nin low-resolution (contact map) and high-\\nresolution (atomic-level) structural information.\\nAccelerating accurate atomic-resolution\\nstructure prediction with a language model\\nLanguage models greatly accelerate state-of-\\nthe-art high-resolution structure prediction.\\nThe language model internalizes evolutionary\\npatterns linked to structure, which eliminates\\nthe need for external evolutionary databases,\\nMSAs, and templates. We find that the ESM-2\\nlanguage model generates state-of-the-art\\nthree-dimensional (3D) structure predictions\\ndirectly from the primary protein sequence,\\nwhich results in a speed improvement for\\ns t r u c t u r ep r e d i c t i o no fm o r et h a na no r d e ro f\\nmagnitude while maintaining high-resolution\\naccuracy.\\nWe developed ESMFold, a fully end-to-end\\nsingle-sequence structure predictor, by train-\\ning a folding head for ESM-2 (Fig. 2A). At\\nprediction time, the sequence of a protein is\\ninputted to ESM-2. The sequence is processed\\nthrough the feedforward layers of the lan-\\nguage model, and the model’s internal states\\n(representations) are passed to the folding\\nhead. The head begins with a series of fold-\\ning blocks. Each folding block alternates be-\\ntween updating a sequence representation\\nand a pairwise representation. The output of\\nthese blocks is passed to an equivariant trans-\\nformer structure module, and three steps of\\nrecycling are performed before outputting\\na final atomic-level structure and predicted\\nconfidences (SM A.3.1). This architecture rep-\\nresents a major simplification in comparison\\nwith current state-of-the-art structure predic-\\ntion models, which deeply integrate the MSA\\ninto the neural network architecture through\\nan attention mechanism that operates across\\nt h er o w sa n dc o l u m n so ft h eM S A(12, 44).\\nOur approach results in a considerable im-\\nprovement in prediction speed. On a single\\nNVIDIA V100 GPU, ESMFold makes a predic-\\ntion on a protein with 384 residues in 14.2 s,\\nsix times faster than a single AlphaFold2 model.\\nOn shorter sequences, the improvement in-\\ncreases up to∼60× (fig. S2). The search process\\nfor related sequences, which is required to con-\\nstruct the MSA, can take >10 min with the\\nhigh-sensitivity protocols used by the published\\nversions of AlphaFold and RoseTTAFold; this\\ntime can be reduced to <1 min, although with\\nreduced sensitivity (30).\\nWe train the folding head on∼25,000 clus-\\nters covering a total of∼325,000 experimen-\\ntally determined structures from the PDB,\\nwhich is further augmented with a dataset\\nof ∼12 million structures that we predicted\\nwith AlphaFold2 (SM A.1.2). The model is\\ntrained with the same losses that are used for\\nAlphaFold (45). To evaluate the accuracy of\\nstructure predictions, we use test sets that\\nare held out from the training data by a May\\n2020 cutoff date; as a result, all structures that\\nare used in evaluation are held out from the\\ntraining, and the evaluation is representative\\nof the performance that would be expected\\nin regular usage as a predictive model on the\\nkinds of structures that are selected by ex-\\nperimentalists for characterization. This also\\nmakes it possible to compare with AlphaFold\\nand RoseTTAFold because these models also\\nhave not been trained on structures depos-\\nited after May 2020. We use two test sets: The\\nCAMEO test set consists of 194 structures that\\nare used in the ongoing CAMEO assessment\\n(between April 2022 and June 2022); the CASP14\\ntest set consists of 51 publicly released struc-\\ntures that have been selected for their dif-\\nficulty for the biannual structure prediction\\ncompetition.\\nWe compare the results of ESMFold on these\\nevaluation sets to AlphaFold2 and RoseTTAFold\\n(Fig. 2B). ESMFold achieves an average TM-score\\nof 0.83 on CAMEO and 0.68 on CASP14. Using\\nthe search protocols released with AlphaFold2,\\nincluding MSAs and templates, AlphaFold2\\nachieves 0.88 and 0.85 on CAMEO and CASP14,\\nrespectively. ESMFold achieves competitive\\naccuracy with RoseTTAFold on CAMEO, which\\naverages a TM-score of 0.82. When evaluat-\\ning AlphaFold2 and RoseTTAFold on single\\nsequences by ablating the MSA, their per-\\nformance degrades substantially and falls\\nwell below that of ESMFold. This is an arti-\\nficial setting because AlphaFold2 has not been\\nexplicitly trained for single sequences; how-\\never, it has recently emerged as important in\\nprotein design, in which these models have\\nbeen used with single-sequence inputs for de\\nnovo protein design (46–48).\\nAlthough the average performance on the\\ntest sets is below AlphaFold2, the performance\\ng a p sa r ee x p l a i n e db yt h el a n g u a g em o d e lp e r -\\nplexity. On proteins for which perplexity is\\nlow, ESMFold results match AlphaFold2. On\\nthe CAMEO test set, the 3-billion-parameter\\nESM-2 model used in ESMFold achieves an\\naverage perplexity of 5.7. On the CASP14 test\\nset, the same model only has an average per-\\nplexity of 10.0. Performance within each set\\nis also well correlated with perplexity. On the\\nCAMEO test set, language model perplexity\\nhas a Pearson correlation of−0.52 with the TM-\\nscore between the predicted and experimental\\nstructures; on CASP14, the correlation is−0.71\\n(Fig. 2B). On the subset of 18 CASP14 pro-\\nteins for which ESM-2 achieves perplexity <7,\\nESMFold matches AlphaFold in performance\\n(average TM-score difference <0.03 and no\\nTM-score differences >0.1). The relationship\\nbetween perplexity and structure prediction\\nsuggests that improvements in the language\\nmodel will translate into improvements in\\nsingle-sequence structure prediction accuracy,\\nw h i c hi sc o n s i s t e n tw i t ho b s e r v a t i o n sf r o mt h e\\nscaling analysis (Fig. 1, D and E). Additionally,\\nthis means that the language model’s perplex-\\nity for a sequence can be used to predict the\\nquality of the ESMFold structure prediction.\\nAblation studies indicate that the language\\nm o d e lr e p r e s e n t a t i o n sa r ec r i t i c a lt oE S M F o l d\\nperformance (fig. S3). With a folding trunk of\\neight blocks, performance on the CAMEO test\\nset is 0.74 local distance difference test (LDDT)\\n(baseline). Without the language model, this\\ndegrades substantially, to 0.58 LDDT. When\\nremoving the folding trunk entirely (i.e., only\\nusing the language model and the structure\\nmodule), the performance degrades to 0.66\\nLDDT. Other ablations, such as only one block\\nof a structure module, turning off recycling,\\nnot using AlphaFold2 predicted structures as\\ndistillation targets, or not using triangular up-\\ndates, result in small performance degradations\\n(change in LDDT of−0.01 to−0.04).\\nESMFold provides state-of-the-art structure\\nprediction accuracy, matching AlphaFold2 per-\\nformance (<0.05 LDDT difference) on more\\nthan half the proteins (Fig. 2B). We find that\\nthis is true even on some large proteins— T1076\\nis an example with 0.98 TM-score and 540 res-\\nidues (Fig. 2D). Parts of the structure with low\\naccuracy do not differ notably between ESM-\\nFold and AlphaFold, which suggests that lan-\\nguage models are learning information similar\\nto that contained in MSAs. We also observe\\nthat ESMFold is able to make good predic-\\ntions for components of homo- and hetero-\\ndimeric protein-protein complexes (Fig. 2D).\\nIn a comparison with AlphaFold-Multimer\\n(49) on a dataset of 2,978 recent multimeric\\ncomplexes deposited in the PDB, ESMFold\\nachieves the same qualitative DockQ (50)c a t -\\negorization for 53.2% of chain pairs, despite not\\nbeing trained on protein complexes (fig. S4).\\nConfidence is well calibrated with accuracy.\\nESMFold reports confidence in the form of pre-\\ndicted LDDT (pLDDT) and predicted TM (pTM).\\nThis confidence correlates well with the accuracy\\nof the prediction, and for high-confidence pre-\\ndictions (pLDDT > 0.7), the accuracy is compa-\\nrable to AlphaFold2 (ESMFold LDDT = 0.83,\\nAlphaFold2 LDDT = 0.85 on CAMEO) (Fig. 2C\\nand fig. S5). High-confidence predictions ap-\\nproach experimental-level accuracy. On the\\nCAMEO test set, ESMFold predictions have a\\nmedian all-atom RMSD95 (RMSD at 95% resi-\\ndue coverage) of 1.91 Å and backbone RMSD95\\nof 1.33 Å. When confidence is very high (pLDDT\\n> 0.9), predictions have median all-atom RMSD95\\nof 1.42 Å and backbone RMSD95 of 0.94 Å. The\\nconfidence can thus be used to predict how\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 5o f8\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nlikely it is that a given structure prediction\\nwill match the true structure if it were to be\\nexperimentally determined.\\nRecent work has investigated the use of\\nlanguage models for the direct prediction of\\nprotein structure from sequence, without a\\nlearned full atomic-level structure projection,\\nbut the accuracy has not been competitive\\nwith the use of MSAs (21, 51). An approach\\ndeveloped concurrently with ours that uses\\na similar attention-based processing of lan-\\nguage model representations to output atomic\\ncoordinates also appears to show results that\\nare MSAs (52).\\nEvolutionary-scale structural characterization\\nof metagenomics\\nThis fast and high-resolution structure predic-\\ntion capability enables the large-scale structural\\ncharacterization of metagenomic proteins.\\nWe fold >617 million sequences from the\\nMGnify90 database (32). This is the entirety\\nof the sequences of length 20 to 1024 and\\ncovers 99% of all the sequences in MGnify90.\\nOverall, the characterization produces∼365 mil-\\nlion predictions with good confidence (mean\\npLDDT > 0.5 and pTM > 0.5), which corresponds\\nto ∼59% of the database, and∼225 million pre-\\ndictions with high confidence (mean pLDDT >\\n0.7 and pTM > 0.7), which corresponds to∼36%\\nof total structures folded (Fig. 3). We were able\\nto complete the predictions in 2 weeks on a\\ncluster of ~2000 GPUs (SM A.4.1).\\nFor structure prediction at scale, it is crit-\\nical to distinguish well-predicted proteins from\\nthose that are poorly predicted. In the previous\\nsection, we evaluated calibration against ex-\\nperimentally determinedstructures on held-out\\ntest sets and found that the model confidence\\nis predictive of the agreement with experimen-\\ntally determined structures. We also assess\\ncalibration against AlphaFold predictions on\\nmetagenomic proteins. On a random subset\\nof ∼4000 metagenomic sequences, there is a\\nhigh correlation (Pearsonr =0 . 7 9 )b e t w e e n\\nE S M F o l dp L D D Ta n dt h eL D D Tt oA l p h a F o l d 2\\npredictions (Fig. 3A). When combined with\\nresults on CAMEO showing that when con-\\nfidence is very high (pLDDT > 0.9), ESMFold\\npredictions often approach experimental ac-\\ncuracy, these findings mean that ESMFold’s\\nconfidence scores provide a good indication\\nof the agreement with experimental struc-\\ntures and with the predictions that can be\\nobtained from AlphaFold2. Across the∼617 mil-\\nlion predicted structures,∼113 million structures\\nmeet the very high-confidence threshold.\\nMany of the metagenomic structure pre-\\ndictions have high confidence (Fig. 3B) and\\nare not represented in existing structure data-\\nbases (Figs. 3, C to E). On a random sample\\nof 1 million high-confidence structures, 76.8%\\n(767,580) of the proteins have a sequence iden-\\ntity below 90% to any sequence in UniRef90,\\nwhich indicates that these proteins are dis-\\ntinct from existing UniRef90 sequences (Fig.\\n3E). For 3.4% (33,521 proteins), no match is\\nfound in UniRef90 at all (SM A.4.2). We use\\nFoldseek (53) to compare the predicted struc-\\ntures with known structures in the PDB. At\\nthresholds of 0.7 and 0.5 TM-score, Foldseek\\nreports 25.4% (253,905 proteins) and 12.6%\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 6o f8\\nA\\nBC\\nFig. 4. Example ESMFold structure predictions of metagenomic sequences.\\n(A) Example predicted structures from six different metagenomic sequences;\\nalso see table S2. Left of each subfigure: The prediction is displayed with\\nthe AlphaFold2 prediction (light green). Right of each subfigure: The prediction\\nis displayed with the Foldseek-determined nearest PDB structure according\\nto TM-score. (B and C) Examples of two ESMFold-predicted structures that have\\ngood agreement with experimental structures in the PDB but that have low\\nsequence identity to any sequence in UniRef90. (B) Predicted structure\\nof MGYP000936678158 aligns to an experimental structure from a bacterial\\nnuclease (light brown, PDB: 3H4R), whereas (C) the predicted structure of\\nMGYP004000959047 aligns to an experimental structure from a bacterial sterol\\nbinding domain (light brown, PDB: 6BYM).\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\n(125,765 proteins) without a match, respec-\\ntively (Fig. 3, C and D). For 2.6% (25,664) there\\nis both low structural similarity (TM-score\\n≤0.5) and no close sequence homolog (>30%\\nidentity). On the basis of these subsampled es-\\ntimates, there are ~28 million proteins (12.6%\\nof 225 million) with both high-confidence\\npredictions and TM-score < 0.5 to known\\nprotein structures (examples in Fig. 4A and\\ntable S2). These results demonstrate that\\nESMFold can effectively characterize regions\\nof protein space that are distant from existing\\nknowledge.\\nLarge-scale structural characterization also\\nm a k e si tp o s s i b l et oi d e n t i f ys t r u c t u r a ls i m -\\nilarities in the absence of sequence similarity.\\nMany high-confidence structures with low sim-\\nilarity to UniRef90 sequences do have similar\\nstructures in the PDB. This remote homology\\noften extends beyond the limit detectable by\\nsequence similarity. For example, MGnify se-\\nquence MGYP000936678158 has no matches\\nto any entry in UniRef90 or through a jackhm-\\nmer (54) reference proteome search, but it has\\na predicted structure conserved across many\\nnucleases (PDB: 5YET_B, TM-score 0.68; PDB:\\n3HR4_A, TM-score 0.67) (Fig. 4B and table S2);\\nsimilarly, MGnify sequence MGYP004000959047\\nhas no UniRef90 or jackhmmer reference pro-\\nteome matches, but its predicted structure has\\na high similarity to the experimental structures\\nof lipid binding domains (PDB: 6BYM_A, TM-\\nscore 0.80; PDB: 5YQP_B, TM-score 0.78) (Fig.\\n4C and table S2). The ability to detect remote\\nsimilarities in structure enables insight into\\nfunction that cannot be obtained from the\\nsequence.\\nAll predicted structures are available in the\\nESM Metagenomic Atlas (https://esmatlas.com)\\nas an open science resource. Structures are\\navailable for bulk download, by means of an\\napplication programming interface (API), and\\nthrough a web resource that provides search\\nby structure and by sequence (53, 55). These\\ntools facilitate both large-scale and focused\\nanalysis of the full scope of the hundreds of\\nmillions of predicted structures.\\nConclusions\\nFast and accurate computational structure\\nprediction has the potential to accelerate\\nprogress toward an era in which it is possible\\nto understand the structure of all proteins\\ndiscovered in gene sequencing experiments.\\nSuch tools promise insights into the vast\\nnatural diversity of proteins, most of which\\nare discovered in metagenomic sequencing.\\nTo this end, we have completed a large-scale\\nstructural characterization of metagenomic\\nproteins that reveals the predicted structures\\nof hundreds of millions of proteins, mil-\\nlions of which are expected to be distinct in\\ncomparison to experimentally determined\\nstructures.\\nAs structure prediction continues to scale to\\nlarger numbers of proteins, calibration be-\\ncomes critical because, when the throughput\\nof prediction is limiting, the accuracy and\\nspeed of the prediction form a joint frontier\\nin the number of accurate predictions that can\\nbe generated. Very high-confidence predic-\\ntions in the metagenomic atlas are expected\\nt oo f t e nb er e l i a b l ea tar e s o l u t i o ns u f f i c i e n tf o r\\ninsight similar to experimentally determined\\nstructures, such as into the biochemistry of\\nactive sites (56). For many more proteins for\\nwhich the topology is predicted reliably, in-\\nsight can be obtained into function through\\nremote structural relationships that could not\\nbe otherwise detected with sequence.\\nThe emergence of atomic-level structure in\\nlanguage models shows a high-resolution pic-\\nture of protein structure encoded by evolution\\ninto protein sequences that can be captured\\nwith unsupervised learning. Our current mod-\\nels are very far from the limit of scale in pa-\\nrameters, sequence data, and computing power\\nthat can in principle be applied. We are op-\\ntimistic that as we continue to scale, there will\\nbe further emergence. Our results showing the\\nimprovement in the modeling of low depth\\nproteins point in this direction.\\nESM-2 results in an advance in speed that\\nin practical terms is up to one to two orders\\nof magnitude, which puts far larger numbers\\nof sequences within reach of accurate atomic-\\nlevel prediction. Structure prediction at the\\nscale of evolution can open a deep view into\\nthe natural diversity of proteins and accel-\\nerate the discovery of protein structures and\\nfunctions.\\nREFERENCES AND NOTES\\n1. C. Yanofsky, V. Horn, D. Thorpe,Science 146, 1593–1594\\n(1964).\\n2. D. Altschuh, T. Vernet, P. Berti, D. Moras, K. Nagai,Protein Eng.\\n2, 193–199 (1988).\\n3. U. Göbel, C. Sander, R. Schneider, A. Valencia,Proteins 18,\\n309–317 (1994).\\n4. A. S. Lapedes, B. G. Giraud, L. Liu, G. D. Stormo,Lect. Notes\\nMonogr. Ser.33, 236–256 (1999).\\n5. J. Thomas, N. Ramakrishnan, C. Bailey-Kellogg,IEEE/ACM\\nTrans. Comput. Biol. Bioinform.5, 183–197 (2008).\\n6. M. Weigt, R. A. White, H. Szurmant, J. A. Hoch, T. Hwa,Proc.\\nNatl. Acad. Sci. U.S.A.106,6 7–72 (2009).\\n7. F. Morcos et al., Proc. Natl. Acad. Sci. U.S.A.108, E1293–E1301\\n(2011).\\n8. S. Wang, S. Sun, Z. Li, R. Zhang, J. Xu,PLOS Comput. Biol.13,\\ne1005324 (2017).\\n9. Y. Liu, P. Palmedo, Q. Ye, B. Berger, J. Peng,Cell Syst. 6,\\n65–74.e3 (2018).\\n10. A. W. Senioret al., Nature 577, 706–710 (2020).\\n11. J. Yang et al., Proc. Natl. Acad. Sci. U.S.A.117, 1496–1503\\n(2020).\\n12. J. Jumper et al., Nature 596, 583–589 (2021).\\n13. M. Baek et al., Science 373, 871–876 (2021).\\n14. A. Rives et al., Proc. Natl. Acad. Sci. U.S.A.118, e2016239118\\n(2021).\\n15. T. Bepler, B. Berger,Cell Syst. 12, 654–669.e3 (2021).\\n16. E. C. Alley, G. Khimulya, S. Biswas, M. AlQuraishi, G. M. Church,\\nNat. Methods 16, 1315–1322 (2019).\\n17. M. Heinzinger et al., BMC Bioinformatics 20, 723 (2019).\\n18. A. Elnaggar et al., IEEE Trans. Pattern Anal. Mach. Intell.14,\\n1 (2021).\\n19. J. Vig et al., arXiv:2006.15222 [cs, q-bio] (2021).\\n20. R. Rao, J. Meier, T. Sercu, S. Ovchinnikov, A. Rives,\\nbioRxiv 422761 [Preprint] (2021); https://doi.org/10.1101/\\n2020.12.15.422761.\\n21. R. Chowdhury et al., Nat. Biotechnol. 40, 1617–1623 (2022).\\n22. C. E. Shannon,Bell Syst. Tech. J.30,5 0–64 (1951).\\n23. A. Vaswaniet al., in Advances in Neural Information Processing\\nSystems (Curran Associates, 2017), pp. 5998–6008.\\n24. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving\\nlanguage understanding by generative pre-training (2018); https://\\ncdn.openai.com/research-covers/language-unsupervised/\\nlanguage_understanding_paper.pdf.\\n25. J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, inProceedings\\nof the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers)(Association for\\nComputational Linguistics, 2019), pp. 4171–4186.\\n26. T. B. Brownet al., inAdvances in Neural Information Processing\\nSystems (Curran Associates, 2020), pp. 1877–1901.\\n27. J. Wei et al., arXiv:2109.01652 [cs.CL] (2021).\\n28. J. Weiet al., arXiv:2201.11903 [cs] (2022).\\n29. A. Chowdheryet al., arXiv:2204.02311 [cs] (2022).\\n30. M. Mirditaet al., Nat. Methods 19, 679–682 (2022).\\n31. M. Steinegger, M. Mirdita, J. Söding,Nat. Methods16, 603–606\\n(2019).\\n32. A. L. Mitchellet al., Nucleic Acids Res.48, D570–D578 (2020).\\n33. S. Mukherjeeet al., Nucleic Acids Res.49 (D1), D723–D733 (2021).\\n34. K. Tunyasuvunakoolet al., Nature 596, 590–596 (2021).\\n35. M. Varadiet al., Nucleic Acids Res.50, D439–D444 (2022).\\n36. O. Shimomura, F. H. Johnson, Y. Saiga,J. Cell. Comp. Physiol.\\n59, 223–239 (1962).\\n37. K. Mulliset al., Cold Spring Harb. Symp. Quant. Biol.51,\\n263–273 (1986).\\n38. M. Jineket al., Science 337, 816–821 (2012).\\n39. B. E. Suzek, Y. Wang, H. Huang, P. B. McGarvey, C. H. Wu;\\nUniProt Consortium,Bioinformatics31,9 2 6–932 (2015).\\n40. S. K. Burleyet al., Nucleic Acids Res.47 (D1), D464–D474 (2019).\\n41. J. Haas et al., Proteins 86 (suppl. 1), 387–398 (2018).\\n42. A. Kryshtafovych, T. Schwede, M. Topf, K. Fidelis, J. Moult,\\nProteins 89, 1607–1617 (2021).\\n43. Y. Zhang, J. Skolnick,Proteins 57, 702–710 (2004).\\n44. R. M. Raoet al., in Proceedings of the 38th International\\nConference on Machine Learning(PMLR, 2021), pp. 8844–8856.\\n45. G. Ahdritzet al., bioRxiv 517210 [Preprint] (2022).\\nhttps://doi.org/10.1101/2022.11.20.517210.\\n46. J. Dauparaset al., Science 378,4 9–56 (2022).\\n47. J. Wanget al., Science 377, 387–394 (2022).\\n48. B. I. M. Wickyet al., Science 378,5 6–61 (2022).\\n49. R. Evanset al., bioRxiv 463034 [Preprint] (2021).\\nhttps://doi.org/10.1101/2021.10.04.463034.\\n50. S. Basu, B. Wallner,PLOS ONE 11, e0161879 (2016).\\n51. K. Weissenow, M. Heinzinger, B. Rost,Structure30, 1169–1177.e4\\n(2022).\\n52. R. Wuet al., bioRxiv 500999 [Preprint] (2022).\\n53. M. van Kempenet al., bioRxiv 479398 [Preprint] (2022).\\nhttps://doi.org/10.1101/2022.02.07.479398.\\n54. S. C. Potteret al., Nucleic Acids Res.46, W200–W204 (2018).\\n55. M. Steinegger, J. Söding,Nat. Biotechnol.35, 1026–1028 (2017).\\n56. Y. Zhang,Curr. Opin. Struct. Biol.19,1 4 5–155 (2009).\\n57. Z. Linet al., ESM-2 and ESMFold-v0 Model Code and Weights,\\nZenodo (2023). https://doi.org/10.5281/zenodo.7566741.\\n58. Z. Linet al., ESM Atlas v0 representative random sample of\\npredicted protein structures, Zenodo (2022). https://doi.org/\\n10.5281/zenodo.7623482.\\n59. Z. Linet al., ESM Atlas v0 random sample of high confidence\\npredicted protein structures, Zenodo (2022). https://doi.org/\\n10.5281/zenodo.7623627.\\n60. S. F. Altschul, W. Gish, W. Miller, E. W. Myers, D. J. Lipman,\\nJ. Mol. Biol.215, 403–410 (1990).\\nACKNOWLEDGMENTS\\nWe thank FAIR team members N. Goyal, Y. LeCun, A. Lerer,\\nJ. Liu, L. van der Maaten, and S. Sukhbaatar and collaborators\\nJ. Dauparas and S. Ovchinnikov for technical help, feedback, and\\ndiscussions that helped shape this project. We thank E. Koonin\\nand F. Zhang for feedback on the metagenomic dataset. We thank\\nA. Rizvi, J. Shepard, and J. Spisak for program support. We thank\\nS. Gomez, S. Jain, W. Ngan, and N. Seejoor for their work on the\\nESM Metagenomic Atlas website. We also thank the developers\\nof the OpenFold project, fairseq, PyTorch, Foldseek, MMseqs2,\\nPyMol, Biotite, and others for building invaluable open-source tools\\nand the creators and maintainers of MGnify, PDB, UniProt, and\\nUniRef, as well as the researchers whose experimental efforts are\\nincluded in these resources.Funding: There were no external\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 7o f8\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\nsources of funding for the project.Author contributions:\\nConceptualized and initiated the project: Z.L., S.C., and A.R.;\\ndeveloped and trained ESM-2: H.A., Z.Z., W.L., and R.V.; developed\\nand trained ESMFold: Z.L., R.R., Z.Z., N.S., A.S.C., M.F.Z., and\\nS.C.; produced metagenomic structure predictions: Z.L., H.A., and\\nW.L.; analyzed ESM-2 and ESMFold: Z.L., R.R., Z.Z., and W.L.;\\nanalyzed predicted metagenomic structures: B.H. and T.S.;\\ndeveloped ESM Atlas: B.H., N.S., O.K., Y.S., and T.S.; wrote the\\nmanuscript: Z.L., H.A., R.R., B.H., and A.R.; engineering and science\\nleadership: T.S., S.C., and A.R.Competing interests:The authors\\ndeclare no competing financial interests. No patent applications\\nhave been filed on this work.Data and materials availability:\\nAll predicted structures in the ESM Metagenomic Atlas are\\navailable at https://esmatlas.com. ESM-2 and ESMFold model\\nsource code and parameters are available at https://github.\\ncom/facebookresearch/esm and archived on Zenodo (57). A\\nrepresentative random sample of ~1 million predicted structures is\\narchived on Zenodo (58), and the random sample of ~1 million\\nhigh-confidence predictions used for analysis in this work is also\\narchived on Zenodo (59). Models and data have been released\\nunder permissive licenses (MIT license for model source code and\\nparameters and CC-BY for predicted structures).License\\ninformation: Copyright © 2023 the authors, some rights reserved;\\nexclusive licensee American Association for the Advancement of\\nScience. No claim to original US government works. https://www.\\nsciencemag.org/about/science-licenses-journal-article-reuse\\nSUPPLEMENTARY MATERIALS\\nscience.org/doi/10.1126/science.ade2574\\nMaterials and Methods\\nSupplementary Text\\nFigs. S1 to S8\\nTables S1 to S5\\nReferences (61–74)\\nView/request a protocol for this paper fromBio-protocol.\\nSubmitted 3 August 2022; resubmitted 10 November 2022\\nAccepted 16 February 2023\\n10.1126/science.ade2574\\nLin et al., Science 379, 1123–1130 (2023) 17 March 2023 8o f8\\nRESEARCH | RESEARCH ARTICLE\\nDownloaded from https://www.science.org at University of Chicago on January 11, 2025\\n\\n'}, {'file_name': 'conch', 'text': \"Nature Medicine | Volume 30 | March 2024 | 863–874\\n 863\\nnature medicine\\nhttps://doi.org/10.1038/s41591-024-02856-4\\nArticle\\nA visual-language foundation model for \\ncomputational pathology\\nMing Y . Lu\\u2009  \\u20091,2,3,4,5,11, Bowen Chen1,2,11, Drew F. K. Williamson\\u2009  \\u20091,2,3,11, \\nRichard J. Chen\\u2009  \\u20091,2,3,4,6, Ivy Liang1,7, Tong Ding1,7, Guillaume Jaume1,2,3,4, \\nIgor Odintsov1, Long Phi Le2, Georg Gerber\\u2009  \\u20091, Anil V . Parwani8, \\nAndrew Zhang\\u2009  \\u20091,2,3,4,9 & Faisal Mahmood\\u2009  \\u20091,2,3,4,10 \\nThe accelerated adoption of digital pathology and advances in deep learning \\nhave enabled the development of robust models for various pathology \\ntasks across a diverse array of diseases and patient cohorts. However, \\nmodel training is often difficult due to label scarcity in the medical domain, \\nand a model’s usage is limited by the specific task and disease for which \\nit is trained. Additionally, most models in histopathology leverage only \\nimage data, a stark contrast to how humans teach each other and reason \\nabout histopathologic entities. We introduce CONtrastive learning from \\nCaptions for Histopathology (CONCH), a visual-language foundation model \\ndeveloped using diverse sources of histopathology images, biomedical text \\nand, notably, over 1.17 million image–caption pairs through task-agnostic \\npretraining. Evaluated on a suite of 14 diverse benchmarks, CONCH can be \\ntransferred to a wide range of downstream tasks involving histopathology \\nimages and/or text, achieving state-of-the-art performance on histology \\nimage classification, segmentation, captioning, and text-to-image \\nand image-to-text retrieval. CONCH represents a substantial leap over \\nconcurrent visual-language pretrained systems for histopathology, with \\nthe potential to directly facilitate a wide array of machine learning-based \\nworkflows requiring minimal or no further supervised fine-tuning.\\nThe gold standard for the diagnosis of many diseases remains the exami-\\nnation of tissue by a pathologist. The recent rise of computational \\npathology1–4, which leverages artificial intelligence (AI) to solve prob-\\nlems in pathology, has demonstrated considerable advances across \\nmany tasks, including metastasis detection5, cancer subtyping6,7, sur-\\nvival prediction8–10, unknown primary origin site prediction11,12, image \\nsearch13–16 and prediction of molecular alterations17,18, among other \\ntasks19. Additionally, current strides in the field are made under the \\nparadigm of developing models targeting specific tasks using large \\ncohorts of labeled training examples, such as in lymph node metastasis \\ndetection20 and prostate cancer grading21,22. However, the process of \\ndata collection and annotation of whole-slide images (WSIs) is labor \\nintensive and is not scalable to open-set recognition problems or rare \\ndiseases, both of which are common to the practice of pathology. \\nWith thousands of possible diagnoses and many other tasks, training \\nseparate models for every step of the pathology workflow is untenable. \\nReceived: 2 August 2023\\nAccepted: 5 February 2024\\nPublished online: 19 March 2024\\n Check for updates\\n1Department of Pathology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA. 2Department of Pathology, Massachusetts General \\nHospital, Harvard Medical School, Boston, MA, USA. 3Cancer Program, Broad Institute of Harvard and MIT, Cambridge, MA, USA. 4Cancer Data Science \\nProgram, Dana-Farber Cancer Institute, Boston, MA, USA. 5Electrical Engineering and Computer Science, Massachusetts Institute of Technology (MIT), \\nCambridge, MA, USA. 6Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA. 7Harvard John A. Paulson School of Engineering \\nand Applied Sciences, Harvard University, Cambridge, MA, USA. 8Department of Pathology, Wexner Medical Center, Ohio State University, Columbus, \\nOH, USA. 9Health Sciences and Technology, Harvard-MIT, Cambridge, MA, USA. 10Harvard Data Science Initiative, Harvard University, Cambridge, MA, USA. \\n11These authors contributed equally: Ming Y. Lu, Bowen Chen, Drew F. K. Williamson. \\u2009e-mail: faisalmahmood@bwh.harvard.edu\\nNature Medicine | Volume 30 | March 2024 | 863–874 864\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nstate-of-the-art performance across all benchmarks relative to other \\nvisual-language foundation models (Fig. 1d), including PLIP54, Biomed-\\nCLIP44 and OpenAICLIP 30, and it outperforms concurrent baselines, \\noften by a large margin (Figs. 2–5).\\nResults\\nZero-shot classification of diverse tissues and diseases\\nContrastively aligned visual-language pretraining allows the model to \\nbe directly applied to downstream classification tasks without requiring \\nfurther labeled examples for supervised learning or fine-tuning. This \\nzero-shot transfer capability allows a single pretrained foundation \\nmodel to be applied off the shelf to different downstream datasets with \\nan arbitrary number of classes compared with the current paradigm \\nof training a new model for every new task. While we do not expect \\nzero-shot classification to currently be sufficiently accurate for most \\nclinical use cases, in some tasks, we found CONCH to perform sur -\\nprisingly well, and it may serve as a strong baseline for conventional \\nsupervised learning, especially when training labels are scarce.\\nGiven a task, we first represented the set of class or category names \\nusing a set of predetermined text prompts, where each prompt corre-\\nsponded to a class. An image was then classified by matching it with the \\nmost similar text prompt in the model’s shared image–text representa-\\ntion space (Fig. 2a; see Methods for details). In practice, there are often \\nmultiple ways to phrase the same concept in text (for example, ‘invasive \\nlobular carcinoma (ILC) of the breast’ and ‘breast ILC’); therefore, we \\ncreated an ensemble of multiple text prompts for each class during \\nprediction, which was found to generally boost predictive perfor -\\nmance compared to using a single text prompt (Extended Data Fig. 2). \\nAdditionally, while previous studies44,54 primarily focused on classifi-\\ncation tasks at the region-of-interest (ROI) level, we also investigated \\nthe zero-shot capability of our model on gigapixel WSIs by leveraging \\nMI-Zero56, which divides a WSI into smaller tiles and subsequently aggre-\\ngates individual tile-level scores into a slide-level prediction (Fig. 2b).\\nIn total, we evaluated CONCH on four slide-level classification \\ntasks: The Cancer Genome Atlas (TCGA) BRCA (invasive breast carci-\\nnoma subtyping), TCGA NSCLC (non-small-cell lung cancer subtyping), \\nTCGA RCC (renal cell carcinoma subtyping) and Dartmouth Hitchcock \\nMedical Center (DHMC) LUAD (lung adenocarcinoma histologic pat-\\ntern classification) and three ROI-level tasks: CRC100k (colorectal \\ncancer tissue classification), WSSS4LUAD (LUAD tissue classifica -\\ntion) and SICAP (Gleason pattern classification). We used balanced \\naccuracy as the primary evaluation metric for TCGA NSCLC, TCGA \\nRCC, TCGA LUAD, CRC100k and WSSS4LUAD, which accounted for \\nclass imbalance by weighing the accuracy score of each class equally. \\nFollowing the community standard, we used Cohen’s κ and quadratic \\nweighted Cohen’s κ as primary metrics for LUAD pattern classification \\nand Gleason pattern classification, respectively, as they are regarded as \\nmore subjective tasks, which typically translates to higher inter-rater \\nvariability. We refer readers to Supplementary Tables 1–14 for more \\ndetailed reporting of model performance and Methods for detailed \\ndescriptions of evaluation datasets.\\nAdditionally, as diverse as these tasks are, they are all analyses of \\nvisual data or include other structured information such as ‘omics’  \\n(refs. 23–26) and other multimodal data sources 27–29. However, the \\npractice of pathology and the communication of pathological findings \\nmake extensive use of natural language, be it in the form of the report \\nthat the pathologist prepares for the patient and their treating clini-\\ncian, the journal article that details a new histopathologic entity or the \\ntextbook chapter that teaches residents how to practice pathology.\\nThe general machine learning community has made immense \\nstrides in foundation models that use both visual and language infor-\\nmation. Representative tools such as CLIP 30, ALIGN 31 and CoCa 32, \\namong others33–38, use large-scale image–caption pairs 39 to pretrain \\nvisual-language foundation models—task-agnostic pretrained mod -\\nels that demonstrate robust performance in downstream vision and \\nvisual-language tasks. In the broader biomedical imaging domain, \\nvisual-language data have been leveraged for a variety of tasks, \\nincluding X-ray report generation40,41, zero-shot classification42–45 and \\nretrieval45–48, among others49–53. However, the number of studies inte-\\ngrating vision and language data for representation learning in com-\\nputational pathology is small, with recent studies44,54–58 demonstrating \\nthe potential of using paired image–caption data to learn meaningful \\nvisual representations and to develop foundation models for histo -\\npathology that can be transferred to multiple downstream tasks in a \\nzero-shot setting, that is, using no task-specific training data. However, \\nthese studies44,54,56 were limited in the scale of histopathology-specific \\npretraining data due to the lack of readily available image–caption pairs \\nin this domain, leading to limited practical utility from relatively poor \\nperformance. Additionally, the broader capabilities of these models \\nremain underexplored.\\nGiven the diversity of tasks, the difficulty in acquiring large data-\\nsets of rare diseases or combinations of findings, and the central nature \\nof language to the practice of pathology, there is a need for (1) high-  \\nperforming visual-language foundation models that leverage \\nlarge-scale pretraining and generalize well across tasks; and (2) exten-\\nsive studies on the wide range of potential applications of these models \\nto understand their utility and limitations. We introduce CONtrastive \\nlearning from Captions for Histopathology (CONCH), a visual-language \\nfoundation model developed using diverse sources of histopathology \\nimages, biomedical text and over 1.17 million image–caption pairs \\n(Fig. 1a–b and Extended Data Fig. 1) through task-agnostic pretraining \\nto address these unfilled needs. Based on CoCa 32, a state-of-the-art \\nvisual-language foundation pretraining framework, CONCH uses an \\nimage encoder, a text encoder and a multimodal fusion decoder, and it \\nis trained using a combination of contrastive alignment objectives that \\nseek to align the image and text modalities in the model’s representa-\\ntion space and a captioning objective that learns to predict the caption \\ncorresponding to an image (Fig. 1c ). We investigate the capabilities \\nof CONCH on a wide array of tasks, including classification of image \\ntiles and gigapixel WSIs, cross-modal image-to-text and text-to-image \\nretrieval, image segmentation and image captioning, using a total \\nof 14 diverse benchmarks. We demonstrate that our model achieves \\nFig. 1 | Data curation and model schematic. a, Automated data cleaning \\npipeline. Educational sources (EDU) and parts of the PubMed Central Open \\nAccess Dataset (PMC OA) were manually cleaned and used to train an object \\ndetector to detect histopathology images, a language model to split captions \\nreferring to multiple images and a matching model to match detected images \\nto their corresponding captions. The cleaning process yielded a dataset of 1.79 \\nmillion image–text pairs, and we then filtered out pairs referring to nonhumans \\nto create our CONCH (human-only) pretraining dataset of 1.17 million (see \\nMethods for details on data cleaning and Discussion on ablation experiments \\ninvestigating data filtering). b, Estimated distribution of image–text pairs in \\nthe human-only pretraining dataset by topic. Note that pretraining data cover \\na diverse range of pathology topics. Inset, comparison of the distribution \\nof caption lengths between PMC-Path and EDU (see Extended Data Fig. 1 for \\nwordclouds of captions from each category). c, Visual-language pretraining \\nsetup. CONCH consists of an image encoder, a text encoder and a multimodal \\ntext decoder. The pretraining process uses both contrastive and captioning \\nobjectives. The contrastive objectives align the image and text encoders \\nby maximizing the cosine-similarity scores between paired image and text \\nembeddings, while the captioning objective maximizes the likelihood of \\ngenerating the correct text conditioned on the image and previously generated \\ntext (see Methods for details). <bos>, beginning of sentence; attn, attention; \\n<eos>, end of sentence. d, Radar plot comparing the performance of CONCH \\nand baselines on various downstream tasks. CONCH outperforms baselines by \\na significant margin on a diverse set of tasks spanning zero-shot classification, \\nretrieval and zero-shot segmentation (see Results for detailed descriptions of \\neach task and metric).\\nNature Medicine | Volume 30 | March 2024 | 863–874\\n 865\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nOn slide-level benchmarks, CONCH outperformed state-of-the-art \\nvisual-language foundation models (PLIP, BiomedCLIP and OpenAI -\\nCLIP) on all tasks, often by a wide margin (Fig. 2c ). For instance, for \\nNSCLC subtyping and RCC subtyping, CONCH achieved a zero-shot \\naccuracy of 90.7% and 90.2%, respectively, and it outperformed \\nthe next-best-performing model, PLIP, by 12.0% and 9.8% on each \\n23 46 69 924623 4646\\n23\\n46\\n69\\n92\\n23\\n46\\n69\\n92\\n5\\n10\\n15\\n20\\n10\\n20\\n30\\n40\\n17\\n34\\n51\\n68\\n20\\n40\\n60\\n80\\n18\\n36\\n54\\n72\\n17345168\\n6\\n12\\n18\\n24\\n10\\n20\\n30\\n40\\n17\\n34\\n51\\n68\\n5\\n10\\n15\\n20\\n10\\n20\\n30\\n40\\n16\\n32\\n48\\n64\\n16\\n32\\n48\\n64\\n0 20000 40000 60000 80000 100000 120000 140000\\nEye\\nLower urinary tract\\nPancreas\\nEndocrine system\\nBlood vessels\\nHeart\\nMale genital tract\\nHead\\n& neck\\nPeripheral nerve\\n& skeletal muscle\\nKidney\\nBreast\\nFemale genital tract\\nCentral nervous system\\nHematopathology\\nLiver\\n& biliary tract\\nSkin\\nLung\\nBones, joints\\n& soft-tissue tumors\\nGastrointestinal tract\\n11,569\\n12,916\\n21,963\\n26,358\\n27,709\\n29,049\\n33,668\\n53,438\\n55,665\\n61,341\\n64,992\\n83,311\\n86,163\\n87,388\\n89,494\\n90,585\\n102,751\\n111,078\\n121,209\\nObject\\ndetector\\nCaption\\nsplitter\\nPleuropulmonary blastoma…\\nSeptal markings…,\\nPleuropulmonary blastoma…\\nThe peripheral…,\\nPleuropulmonary blastoma…\\nUniformly expanded septa\\nby…, …\\nMatcher\\nFig. 2.\\nPleuropulmonary\\nblastoma... A Septal\\nmarkings and cysts\\nare appreciated... B\\nThe peripheral\\nmulticystic lesion is...\\nC Uniformly expanded\\nsepta by...\\nPleuropulmonary\\nblastoma… The\\nperipheral…\\nPleuropulmonary\\nblastoma…\\nUniformly\\nexpanded septa\\nby…\\n0.98\\n 0.99 0.98\\nTrain\\nTrain\\nTrain\\nPleuropulmonary blastoma… Septal\\nmarkings…,\\nPleuropulmonary blastoma… The\\nperipheral…,\\nPleuropulmonary blastoma… Uniformly\\nexpanded septa by…, …\\na b\\n<bos>\\na\\nFocus\\nlymphatic\\n“A focus of lung adenocarcinoma\\nwithin a lymphatic channel.” channel\\nCLS\\nContrastive\\nalignment\\na\\nFocus\\nof\\nchannel\\n<eos>\\nText encoder Fusion decoder\\nImage encoder\\nCLS\\nFusion\\nN = 1.17 million pairs\\nA focus of lung \\nadenocarcinoma within \\na lymphatic channel.\\nText tokens\\nImage\\nCaption\\nTransformer block\\nc d\\nTokenizer\\n(~1.79 million)\\n(~1.17 million)\\nUnfiltered \\ndataset\\nHuman-only \\ndataset\\nTransformer block\\nTransformer block\\nTransformer block\\nTransformer block\\nTransformer block\\nTransformer block\\nCross-attn block\\nTransformer block\\nAttention Pooler\\nImage tokens\\nAttention pooler Pooled image tokens\\n0 50 100 150 200\\nWord count\\nDensity\\nPMC-Path\\nEDU\\nDataset curation Dataset characterization\\nPretraining algorithm\\nCRC100k\\nSICAP DHMC LUAD\\nWSSS4LUAD\\nTCGA NSCLC\\nTCGA RCC\\nTCGA BRCA\\nSICAP\\nSource B (i2t)TCGA LUAD (i2t)Source A (i2t)\\nSource B (t2i)\\nTCGA \\nLUAD (t2i)\\nSource A (t2i)\\nDigestPath\\nEBRAINS\\nClassification\\nRetrieval\\nSegmentation\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nt2i: text-to-image\\ni2t: image-to-text\\nDownstream performance\\n46\\nH&E\\n(457,373)\\nIHC + special\\n(713,595)\\nPleuropulmonary blastoma, \\ntype I presenting as a cystic \\nlesion. Cambium layer-like \\nconcentration of primitive \\nsmall cells with adjacent \\nfibrous stroma.\\nProteinase-3 (PR-3) is also \\nexpressed on the protein \\nlevel (brown cells). The \\nPR-3 protein was detected \\nby WGM2 antibodies, \\nspecific for PR-3 \\n(magnification, 60 × 2.5).\\nNature Medicine | Volume 30 | March 2024 | 863–874 866\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\na\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nSupervised performance\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nResNet50 (tr)\\n0\\n0.25\\n0.50\\n0.75\\n1.00Supervised performance\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nCTransPath\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nZero\\xadshot performance\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nTiles\\nWSI\\nTop\\xad\\nK pooling\\nSlide prediction\\nb\\nc d\\n80 µm\\n96 µm\\ne\\nHeatmap\\nLow sim. High sim.\\nTiled WSI\\nClasses as prompts\\n“An image of {invasive\\nductal carcinoma} .”\\n“An image of {invasive\\nlobular carcinoma} .”\\nImage/tile\\nImage \\nencoder      Text encoder\\nDuctal\\nEmbedding \\nspace Lobular\\nROI\\xadlevel zero\\xadshot classification Slide\\xadlevel zero\\xadshot classification\\nZero\\xadshot performance on downstream tasks Supervised evaluation of embeddings\\nHeatmap and example ROIs\\nWSSS4LUAD\\nCRC100k\\nSICAP\\nDHMC LUADTCGA NSCLC\\nTCGA RCCTCGA BRCA CRC100k\\nSICAP\\nTCGA NSCLC\\nTCGA RCCTCGA BRCA\\n3 mm\\nFig. 2 | Zero-shot and supervised classification. a, Schematic of zero-shot \\nclassification using contrastively aligned image and text encoders. A prompt is \\nconstructed for each class, and the image is classified according to the prompt \\nwhose embedding is closest to that of the image in the shared embedding space. \\nb, Zero-shot classification of WSIs. Each WSI is divided into tiles and processed \\nas in a. The similarity scores for tiles are aggregated using top-K pooling to \\nform slide-level similarity scores, the highest of which corresponds to the \\nslide-level prediction. In c,d, dashed lines represent the average over tasks. \\nError bars represent 95% confidence intervals, and the centers correspond to \\ncomputed values of each metric, as specified below. c, Zero-shot performance on \\ndownstream subtyping (TCGA BRCA, n\\u2009=\\u2009150; TCGA RCC, n\\u2009=\\u2009225; TCGA NSCLC, \\nn\\u2009=\\u2009150; DHMC LUAD, n\\u2009=\\u2009143; CRC100k, n\\u2009=\\u20097,180; WSSS4LUAD, n\\u2009=\\u20094,693) and \\ngrading (SICAP, n\\u2009=\\u20092,122) tasks. Cohen’s κ is reported for DHMC LUAD and \\nquadratically weighted Cohen’s κ is reported for SICAP, while balanced accuracy \\nis reported for all other tasks. Additional metrics are reported in Supplementary \\nTables 1–7. d, Supervised evaluation of embeddings of each model. Linear \\nprobing is used for ROI-level tasks (CRC100k and SICAP), while ABMIL is used \\nfor slide-level tasks, with the same metrics reported as in c (see Supplementary \\nTables 15–19 for more detailed results). e, From left to right: pathologist-\\nannotated IDC, corresponding heatmap and selected tiles at higher power. The \\nheatmap is colored on the basis of the cosine-similarity score between each tile \\nwithin the slide and the text prompt corresponding to the predicted class label. \\nWe find excellent agreement between the annotated image and high-similarity \\nregions, with the tiles demonstrating classic IDC morphology within the high-\\nsimilarity (high sim.) regions and stroma or other normal constituents of the \\nbreast in the low-similarity (low sim.) regions.\\nNature Medicine | Volume 30 | March 2024 | 863–874\\n 867\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\ntask with P\\u2009<\\u20090.01 according to a two-sided paired permutation test \\n(Methods, ‘Statistical analysis’). On the more difficult BRCA subtyp -\\ning task, CONCH achieved a zero-shot accuracy of 91.3%, while other \\nmodels performed at near-random chance, with accuracies ranging \\nfrom 50.7% (PLIP) to 55.3% (BiomedCLIP), nearly 35% (P\\u2009<\\u20090.01) lower \\nthan CONCH. Lastly, on the LUAD pattern classification task, CONCH \\nachieved a κ score of 0.200, which was 0.12 higher than that for the \\nnext-best-performing model, PLIP, although no significance was noted \\n(P\\u2009=\\u20090.055). On ROI-level benchmarks, we observed similar findings, \\nwhere CONCH achieved a zero-shot quadratic κ  of 0.690 on SICAP \\n(outperforming BiomedCLIP by 0.140, P\\u2009<\\u20090.01), a zero-shot accuracy \\nof 79.1% on CRC100k (outperforming PLIP by 11.7%, P \\u2009<\\u20090.01) and a \\nzero-shot accuracy of 71.9% on WSSS4LUAD (outperforming PLIP by \\n9.5%, P\\u2009<\\u20090.01). These results demonstrate that, in addition to achieving \\nmore accurate predictions on relatively easy tasks, CONCH was still \\nable to achieve meaningful predictions on some more challenging \\ntasks where other models may especially struggle.\\nWhen classifying a WSI using zero-shot transfer, in addition to \\ncomputing an aggregated, slide-level prediction, we can create a heat-\\nmap to visualize the cosine-similarity score between each tile in the \\nslide and the text prompt corresponding to the predicted class label. \\nRegions with high similarity scores are deemed by the model to be close \\nmatches with the diagnosis (for example, invasive ductal carcinoma \\n(IDC)), while regions with low similarity scores do not match the diag-\\nnosis (Fig. 2e). In an example of a breast IDC slide, we found that regions \\nhighlighted in the heatmap closely resembled the tumor regions as \\ndelineated by pathologist annotation (Fig. 2e, left and middle). Because \\nthe slide-level prediction score is a simple average of the similarity \\nscores of the top-K tiles for a given class, the heatmap enables human \\ninterpretability by directly highlighting regions involved in the model’s \\ndecision-making process, which can be displayed in high resolution \\nto the human user for inspection (Fig. 2e, right). Additional examples \\nare visualized in Extended Data Figs. 3–5. These findings suggest the \\npossibility of using the zero-shot recognition ability of our model for \\ncoarse-grained tissue segmentation on WSIs, which we quantitatively \\nevaluated in Results (‘Zero-shot segmentation’).\\nFew-shot classification with task-specific \\nsupervised learning\\nThe zero-shot recognition capability of contrastive pretrained \\nvisual-language models for histopathology enables efficient and \\nexpedited application of a single foundation model to a potentially \\nwide range of tasks without going through the laborious processes of \\ntraining data collection, annotation and supervised model training \\nfor each new task. Sometimes, however, it may still be desirable to \\nspecialize the model with labeled training examples to maximize per-\\nformance for a given task, ideally using as few labels as possible. In this \\nsection, we investigate the label efficiency when using the pretrained \\nrepresentation of the image encoder backbone of the visual-language \\nfoundation models for task-specific supervised classification. For \\neach benchmark using supervised training, we used either the official \\ntraining set (if provided) or the remaining cases from the dataset after \\nholding out the set of cases used for zero-shot evaluation (Methods, \\n‘Downstream evaluation datasets’). For slide-level tasks, we trained \\nweakly supervised classification models using slide-level labels based \\non the widely used attention-based multiple-instance learning (ABMIL) \\nalgorithm59. For ROI-level tasks, we used logistic regression on top of \\nthe global (for example, classification (<CLS>) token) representation \\nof each encoder, a practice commonly known as linear probing. In addi-\\ntion to PLIP, BiomedCLIP and OpenAICLIP encoders, we introduced \\nsupplementary baselines for comparison: for slide-level tasks, given \\nits popularity, we used ResNet50 (ref. 60) (truncated after the third \\nresidual block) pretrained on ImageNet61, while, for ROI-level tasks, we \\nincluded CTransPath62—a state-of-the-art self-supervised pretrained \\nhistopathology image encoder (see Methods for details).\\nOn the slide-level tasks (Fig. 2d, left), CONCH achieved a balanced \\naccuracy score of 86.7%, 94.2% and 93.3% on BRCA subtyping, RCC \\nsubtyping and NSCLC subtyping, respectively, outperforming the com-\\nmonly used ResNet50 ImageNet baseline by 10.0%, 2.6% and 10.7%, \\nrespectively (P\\u2009<\\u20090.01, P\\u2009=\\u20090.223 and P\\u2009=\\u20090.033). Overall, CONCH obtained \\nan average accuracy of 91.4% across the three tasks, whereas PLIP and \\nBiomedCLIP had an average accuracy of 87.3% and 89.4%, respectively, \\nbut no statistical significance was detected other than for BRCA subtyp-\\ning in the comparison with PLIP (P\\u2009=\\u20090.04). In the ROI-level tasks (Fig. 2d, \\nright), CONCH performed nearly identically to the state-of-the-art \\nCTransPath encoder (93.8% versus 93.8% balanced accuracy on CRC100k \\nand 0.833 versus 0.835 quadratically weighted κ on SICAP), while out-\\nperforming PLIP, BiomedCLIP and OpenAICLIP by 4.0–5.8% in balanced \\naccuracy on CRC100k and by 0.071–0.128 in quadratically weighted κ \\non SICAP (P\\u2009<\\u20090.01 for all comparisons). These results demonstrated \\nthat, overall, CONCH provides a strong image encoder that performed \\neither comparably to or better than all visual encoders tested, includ-\\ning a strong, vision-only self-supervised baseline (see Supplementary \\nTables 15–19 for detailed reporting of model performance).\\nNext, we investigated the label efficiency of different visual-  \\nlanguage pretrained encoders in the few-shot setting, where we varied \\nthe number of training labels per class (nc), for nc\\u2009=\\u20091,\\u20092,\\u20094,\\u20098, up to 512 \\nper class or until we reached the maximum number of available labels \\nin the training set. In the few-shot setting, for each experiment, we \\nsampled five different sets of training examples and showed their \\nindividual performance by boxplot to account for the high variance \\nin model performance when performing supervised learning with \\nvery few training examples (Fig. 3 and Extended Data Fig. 6). We first \\nobserved that CONCH achieved better performance (in terms of the \\nmedian accuracy of five runs) than other encoders for all sizes of train-\\ning set and for all tasks, which translated to requiring fewer labels to \\nachieve the same performance. For instance, in BRCA subtyping, using \\nthe CONCH encoder and 8 training labels per class outperformed using \\nPLIP, BiomedCLIP or OpenAICLIP with 64 labels per class, representing \\na nontrivial reduction in training set size—a trend we also observed for \\nmost tasks tested. Additionally, we noted that the zero-shot perfor -\\nmance of CONCH was highly competitive when compared to few-shot \\nsupervised learning. Aside from relatively easy tasks such as RCC sub-\\ntyping and CRC tissue classification, CONCH zero-shot outperformed \\nPLIP-based and BiomedCLIP-based supervised learning in BRCA sub-\\ntyping (up to 64 labels per class), NSCLC subtyping (up to 128 labels per \\nclass) and Gleason grading (up to 8 labels per class for PLIP and 64 labels \\nper class for BiomedCLIP). These findings suggest that the zero-shot \\ncapability of a good visual-language foundation model should not be \\ntrivialized and, in fact, can serve as a very good baseline when evaluat-\\ning the performance of task-specific diagnostic models trained with \\nsupervised learning. On the other hand, we found that the zero-shot \\ncapability of previous visual-language foundation models (that is, \\nPLIP and BiomedCLIP) could be relatively easily surpassed by using \\nsupervised learning on top of the CONCH vision encoder with just a \\nfew labeled examples.\\nApplication to classification of rare diseases\\nWhile previous investigations have focused on evaluating zero-shot \\nand few-shot performance of visual-language pretrained models on \\nrelatively narrow tasks corresponding to a small set of possible classes \\n(2–5 classes), to our best knowledge, the effectiveness of such models \\nin large-scale, potentially fine-grained disease classification involving \\nrare diseases has yet to be studied. Here, we investigated the utility of \\nCONCH in recognizing up to 30 categories of brain tumors, all of which \\nare classified as rare cancers following the definition of the RARECARE \\nproject63 as having an annual crude incidence rate smaller than 6 per \\n100,000, the definition adopted by the National Cancer Institute’s \\nSurveillance, Epidemiology and End Results (SEER) program. We con-\\nstructed a large-scale subtyping benchmark using the EBRAINS dataset \\nNature Medicine | Volume 30 | March 2024 | 863–874 868\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nand evaluated the effectiveness of both zero-shot and supervised learn-\\ning of various models.\\nIn zero-shot classification, CONCH achieved a balanced accuracy \\nscore of 37.1% on the 30-class subtyping problem (Extended Data Fig. 7 \\nand Supplementary Table 20), far surpassing the random chance base-\\nline of 3.3%, as well as the second-best-performing visual-language pre-\\ntrained zero-shot classifier, BiomedCLIP (+17.0%, P\\u2009<\\u20090.01). However, the \\ngenerally low zero-shot performance of these models suggests that the  \\n current generation of visual-language foundation models may not yet \\nbe capable of directly performing ‘in the wild’ , that is, open-set recog-\\nnition of diverse diseases in pathology, and they are likely to achieve \\nlimited performance when evaluated on more challenging benchmarks \\ninvolving many classes and rare entities.\\nNext, we studied the quality of pretrained representations of our \\nvision encoder for training weakly supervised ABMIL classification \\nmodels. Similar to the previous section, we also included additional \\nbaselines for pretrained vision encoders, including CTransPath, Kimi-\\naNet64 and truncated ResNet50 (ImageNet initialized weights). We found \\nthat, while the zero-shot performance of CONCH was limited due to the \\nchallenging nature of the task, image embeddings of the frozen CONCH \\nencoder could be used to develop strong-performing classification \\nmodels when combined with weakly supervised learning. Specifically, \\nCONCH combined with ABMIL achieved a balanced accuracy of 68.2% \\n(Extended Data Fig. 7a and Supplementary Table 21), surpassing the \\nvision-only self-supervised learning (SSL) pretrained CTransPath model \\n(+6.8%, P\\u2009<\\u20090.01), as well as all other visual-language pretrained models \\ntested by a substantial margin (+10.7%, P\\u2009<\\u20090.01 for PLIP, +14.4%, P\\u2009<\\u20090.01 \\nfor BiomedCLIP and +17.8%, P\\u2009<\\u20090.01 for OpenAICLIP). These results \\ndemonstrate the potential utility of a strong pretrained visual-language \\nmodel as an effective image-only encoder for standard weakly super-\\nvised learning of computational pathology workflows, even when the \\ntask predominantly involves rare diseases. Lastly, we also investigated \\nthe few-shot learning performance of various models, motivated by the \\nneed for high label efficiency when training diagnostic models for rare \\ndiseases due to limited data availability. We observed a similar trend \\nof superior label efficiency for CONCH compared to all other models \\ntested, with other models generally requiring around four times as many \\nlabels to achieve comparable performance (Extended Data Fig. 7b).\\nZero-shot cross-modal retrieval\\nBy learning an aligned latent space for visual and language embeddings, \\nour model is capable of cross-modal retrieval in a zero-shot setting, that \\n1 2 4 8 16 32\\nTraining labels per class\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nBalanced accuracy\\n1 2 4 8 16 32 64\\nTraining labels per class\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nBalanced accuracy\\n1 2 4 8 16 32 64 128 256\\nTraining labels per class\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nBalanced accuracy\\n1 2 4 8 16 32 64\\nTraining Labels Per Class\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00Supervised Performance\\nCONCH\\nBiomedCLIP\\nOpenAICLIP\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nCONCH (Sup.)\\nPLIP (Sup.)\\nBiomedCLIP (Sup.)\\nOpenAICLIP (Sup.)\\nCONCH (zero shot)\\nPLIP (zero shot)\\nBiomedCLIP (zero shot)\\nOpenAICLIP (zero shot)\\nCONCH (Sup.)\\nPLIP\\nBiomedCLIP\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nPLIP (Sup.)\\nBiomedCLIP (Sup.)\\nOpenAICLIP (Sup.)\\nCONCH (zero shot)\\nPLIP (zero shot)\\nBiomedCLIP (zero shot)\\nOpenAICLIP (zero shot)\\nc\\na b\\nTCGA NSCLC (NSCLC subtyping)\\nTCGA BRCA (BRCA subtyping) TCGA RCC (RCC subtyping)\\nFig. 3 | Slide-level few-shot classification experiments. a–c, We investigated \\nthe label efficiency of different visual-language pretrained encoders in the \\nfew-shot setting where we varied the number of training labels per class (nc), for \\nnc\\u2009=\\u20091,\\u20092,\\u20094,\\u20098,\\u200916… until we reached the maximum number of available labels in \\nthe training set. For each nc, we sampled five different sets of training examples \\nand trained a weakly supervised ABMIL model on each training set using slide-\\nlevel labels (see Methods, ‘Supervised and weakly supervised classification \\nexperiments’ for details). We show their individual model performance for BRCA \\nsubtyping (a), RCC subtyping (b) and NSCLC subtyping (c) by boxplot (n\\u2009=\\u20095 \\nfor each box) to study the variance in model performance when performing \\nsupervised learning with very few training examples. Boxes indicate quartile \\nvalues and whiskers extend to data points within 1.5× the interquartile range. For \\nreference, the zero-shot performance of each model is shown as a dashed line on \\nthe same plot. In terms of few-shot supervised learning, CONCH achieves better \\nperformance (in terms of the median accuracy of five runs) than other encoders \\nfor different sizes of training set and for all tasks. Additionally, the zero-shot \\nperformance of CONCH is surprisingly competitive, exceeding the few-shot \\nperformance of PLIP, BiomedCLIP and OpenAICLIP with up to 64 labels per class \\nin the case of BRCA and NSCLC subtyping. Sup., supervised learning.\\nNature Medicine | Volume 30 | March 2024 | 863–874\\n 869\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nis, retrieving the corresponding text entry on the basis of an image query \\n(image-to-text, abbreviated as ‘i2t’) or vice versa (text-to-image, abbrevi-\\nated as ‘t2i’). This task naturally lends itself to image search applications,  \\nwhich are useful in the biomedical domain for applications such as identify-\\ning cases for inclusion in research cohorts or clinical trials, assistance with \\nrare disease presentations or morphologies, and collecting cases for or  \\nhelping to create educational resources. T o perform text-to-image retrieval \\n(the image-to-text direction was analogous), we used the text encoder to \\nembed a text input that served as a query. We then used the query text  \\nembedding to retrieve similar images in the latent space (Fig. 4b).\\nWe evaluated our model on three image–caption datasets, source \\nA and source B (both are held-out sources from model pretraining that \\ncover a diverse range of general pathology concepts) and TCGA LUAD \\n(a much more specific dataset of tiles extracted from LUAD slides in \\nTCGA and annotated with captions in house). Following previous stud-\\nies31,44,54, we used Recall@K as the metric for cross-modal retrieval (see \\nMethods for more detailed descriptions of retrieval datasets).\\nOn average, over the three datasets, CONCH significantly out -\\nperformed baselines by a large margin, achieving mean recall for \\ntext-to-image retrieval of 44.0%, and it outperformed the next-best \\nmodel, BiomedCLIP, by 17.3% with P\\u2009<\\u20090.01 according to a two-sided \\npaired permutation test (Fig. 4a). For source A and source B, CONCH \\nachieved mean recall for text-to-image retrieval of 68.8% and 39.0%, \\nrespectively, outperforming the second-best model, BiomedCLIP, by \\n31.5% and 15.1% (P\\u2009<\\u20090.01 for both). For TCGA LUAD, CONCH achieved \\ntext-to-image mean recall of 24.0%, outperforming the next-best \\nmodel, BiomedCLIP, by 5.3% but with no statistical significance \\n(P\\u2009=\\u20090.22). However, CONCH significantly outperformed PLIP and \\nOpenAICLIP (P\\u2009<\\u20090.01). Image-to-text retrieval for all three datasets  \\nfollowed the same trend as text-to-image retrieval in terms of per-\\nformance and statistical significance, except for TCGA LUAD where \\nthe gap for CONCH and BiomedCLIP was slightly smaller (1.6%). \\nWe refer readers to Supplementary Tables 22–27 for more detailed \\nreporting of model performance. On the basis of these results,  \\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\nSource A Average\\na\\nb c\\nMicropapillary Acinar Papillary Leipidic Solid\\nInflammation Bronchus Vessel Necrosis\\nCrowded gland lined by \\natypical cells with pale \\neosinophilic intraluminal \\nsecretions\\nDesmoplastic stromal \\nreaction to tumor invasion\\nDetached tumor cluster \\nfloating in the alveolar spaces\\nImage\\nencoder\\n \\n Text\\nencoder\\n \\nEmbedding\\nspace\\n400 µm\\nTop-1 \\nretrieved\\n150 µm 400 µm\\n700 µm\\n1,000 µm 200 µm\\n200 µm 300 µm 700 µm\\nText-to-image recallImage-to-text recall\\nSource B\\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\nTCGA LUAD\\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\n@1 @5 @10 Mean0\\n0.25\\n0.50\\n0.75\\n1.00\\n@1 @5 @10 Mean\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\nCONCH PLIP BiomedCLIP OpenAICLIP\\n@1 @5 @10 Mean0.0\\n0.25\\n0.5\\n0.75\\n1.0\\nRecall\\n0.6139 0.4567 0.6173 0.5532 0.4706\\n0.30260.19930.2038 0.3715\\nCross-modal retrieval performance\\nCross-modal retrieval Retrieved images for example prompts\\n… pattern lung adenocarcinoma\\nFig. 4 | Zero-shot cross-modal retrieval. a, Model performance in cross-modal \\nretrieval was evaluated on three datasets of image–text pairs (source A, n\\u2009=\\u2009797; \\nsource B, n\\u2009=\\u20091,755; TCGA LUAD, n\\u2009=\\u2009165). Similarity in the embedding space was \\ncomputed between the query image and all text samples in the database. The \\ntop-K most similar texts were retrieved. We report Recall@K for K\\u2009∈\\u2009{1,\\u20095,\\u200910} and \\nthe mean recall, which averages over K. We show both text-to-image (top row) \\nand image-to-text (bottom row) retrieval for each retrieval task (columns). The \\nrightmost column reports the average across tasks for each metric. CONCH \\noutperforms other baselines on all retrieval tasks. Error bars indicate 95% \\nconfidence intervals. b, Schematic for zero-shot image-to-text retrieval (the text-\\nto-image direction is analogous). c, Examples of images in the top five retrieved \\nresults from TCGA LUAD using LUAD-relevant queries with cosine-similarity \\nscores shown in the top-right corner. Examples of other datasets using more \\ndiverse queries are shown in Extended Data Fig. 7. In general, we found that the \\nimages retrieved by the model matched what was described in the text prompt.\\nNature Medicine | Volume 30 | March 2024 | 863–874 870\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nCONCH was able to perform more accurate cross-modal retrieval  \\nthan baselines.\\nIn addition to using the paired captions as queries, we show exam-\\nples of retrieved results using CONCH with simple text prompts of \\nconcepts related to LUAD (for example, ‘solid-pattern LUAD’) on the \\nTCGA LUAD dataset (Fig. 4c). T o provide examples from more complex \\ntext queries, such as ‘cribriform prostatic adenocarcinoma’ , we used a  \\nhighly diverse dataset of 321,261 tiles sampled from 1,620 cases held \\nout during pretraining, spanning 108 OncoTree 65 codes (Extended \\nData Fig. 8). However, as this dataset did not have paired text data, we \\nwere not able to quantify the retrieval performance. The presented \\nexamples were confirmed by a pathologist to represent the text query  \\nclosely.\\nZero-shot segmentation\\nWhile WSIs can be gigapixels in size, they are generally heterogene-\\nous, with diverse cell types, morphologies and tissue architectures \\nrepresented, each often making up a small share of the slide. Conse-\\nquently, segmentation on the slide level is a difficult and useful task to \\nidentify distinct regions of a WSI on the basis of the characteristics of \\ninterest, and it can reduce the number of tiles needed for downstream \\napplications. However, because annotated data at the sub-slide level \\nare expensive and laborious to collect, a general model capable of \\nperforming slide-level segmentation in a zero-shot setting is valuable. \\nIn this work, we explored the possibility of performing coarse-grained \\ntissue segmentation on WSIs without labeled examples, instead directly \\nusing the demonstrated zero-shot retrieval and classification capabili-\\nties of our model.\\nGiven a WSI, we divided the tissue regions into smaller image \\ntiles and posed a given segmentation task as classifying each tile \\nusing zero-shot classification and assigning the predicted class label \\nto all pixels in the tile, performed for all tiles (Fig. 5a). T o minimize \\nsharp transition in predicted values for pixels at the boundary of \\nneighboring tiles, we tiled the WSIs with a 75% overlap and averaged \\nthe prediction scores in overlapped regions to achieve a smoother \\nappearance in the predicted segmentation map. We evaluated our \\nmodel on SICAP for prostate tumor versus normal tissue segmenta -\\ntion and on DigestPath for malignant versus benign tissue segmen -\\ntation in CRC specimens. We report the widely used Dice score, in \\naddition to precision and recall, for each task against ground-truth \\npixel-level annotations, with scores macro-averaged over all images \\nin each dataset (see Methods for more details). We refer the reader to \\nSupplementary Tables 28 and 29 for more detailed results of model  \\nperformance.\\nCONCH outperformed other models in both tasks (Fig. 5b,c). In \\nSICAP, CONCH achieved an average Dice score of 0.601 (0.549, P\\u2009=\\u20090.08 \\nfor PLIP and 0.484, P\\u2009<\\u20090.01 for BiomedCLIP), an average recall score \\nof 0.751 (0.644, P\\u2009<\\u20090.01 for PLIP and 0.557, P\\u2009<\\u20090.01 for BiomedCLIP) \\nand an average precision core of 0.672 (0.605, P\\u2009=\\u20090.024 for PLIP and \\n0.536, P\\u2009<\\u20090.01 for BiomedCLIP). In DigestPath, CONCH achieved an \\naverage Dice score of 0.615 (0.426, P\\u2009<\\u20090.01 for PLIP and 0.446, P\\u2009<\\u20090.01 \\nfor BiomedCLIP), an average recall score of 0.709 (0.541, P\\u2009<\\u20090.01 for \\nPLIP and 0.601, P\\u2009<\\u20090.01 for BiomedCLIP) and an average precision core \\nof 0.663 (0.526, P\\u2009=\\u20090.024 for PLIP and 0.581, P\\u2009<\\u20090.01 for BiomedCLIP). \\nAdditionally, we found that, despite the coarse-grained and zero-shot \\nnature of the approach, the model was able to produce reasonably \\naccurate pixel-level segmentation masks in some instances, as visual-\\nized in Fig. 5d,e.\\nDiscussion\\nMost previous tools in computational pathology have attempted to \\nextract meaningful patterns and discriminative signals from image \\ndata and/or structured patient data such as genomics and have ignored \\nthe textual aspect of pathology. However, these approaches leave \\non the table a huge amount of information present in descriptions \\nof images, information that allows pathology trainees to generalize \\nfrom a few exemplar images of an entity to images in the real world that \\nare often substantially more diverse. While several recent studies44,54 \\nattempted to leverage image and caption data from social media or \\nbiomedical research articles to build visual-language foundation mod-\\nels applicable to the domain of histopathology, we found that, across a \\nnumber of tasks, both their zero-shot and their supervised classifica-\\ntion performance remain limited, hindering their practical value as \\ngeneral-purpose recognition or retrieval systems for histopathology. \\nAdditionally, beyond working on small ROIs, the models’ abilities to \\nperform in more complex settings (for example, classification of rare \\ndiseases or tumor segmentation on heterogeneous gigapixel WSIs) \\nremain underexplored.\\nIn this study, we demonstrated that, by using the currently larg-\\nest histopathology-specific, paired image–text dataset of over 1.17 \\nmillion examples for task-agnostic pretraining, we could build a \\nhigh-performance visual-language foundation model that could then \\ndemonstrate utility in a wide range of clinically relevant downstream \\ntasks such as classification, retrieval and tissue segmentation. Our \\nmodel is equipped with strong zero-shot recognition capabilities out of \\nthe box, which can potentially relieve the burden of annotating training \\nexamples for many specific classification tasks, and we demonstrated \\nthat its zero-shot performance often rivaled or even outperformed con-\\nventional supervised learning baselines in these tasks under few-shot \\nsettings. Additionally, the much-improved zero-shot image-to-text \\nand text-to-image retrieval capabilities of our model will potentially \\nempower trainees, physicians and researchers to more accurately \\nand flexibly retrieve relevant patient cases or educational examples \\nbased on image or natural language queries once it can be efficiently \\nimplemented into healthcare systems or databases. Equipped with \\na multimodal decoder, our visual-language foundation model also \\nprovides the flexibility to be further fine-tuned in downstream tasks \\nthat involve language generation (for example, image captioning; see \\nMethods, ‘Captioning with fine-tuning’ for details and Extended Data \\nFig. 9 and Supplementary Table 30 for exploratory results) and/or \\nmultimodal reasoning based on both visual and textual inputs. How-\\never, beyond promising results in select tasks, we also found and noted \\nthat current visual-language pretrained models, including CONCH, \\nstill perform poorly on challenging zero-shot problems (relative to \\ntheir supervised learning counterparts) that involve a large number \\nof classes and rare diseases. These observations suggest that we still \\npotentially have a long way to go before achieving the goal of building \\na foundation model capable of truly universal zero-shot recognition \\nor retrieval for histopathology.\\nWe additionally performed ablation experiments to investigate the \\neffect of data filtering, different pretraining algorithms and unimodal \\npretraining on the performance of our model. Most notably, we found \\nthat performing unimodal pretraining (especially vision encoder SSL \\npretraining) could improve model performance in zero-shot clas -\\nsification and retrieval across most tasks (see Extended Data Fig. 10 \\nfor more details).\\nAnother relatively underexplored aspect is the compatibility of \\nvisual-language pretrained foundation models with conventional \\nend-to-end supervised learning aimed at targeting specific tasks. \\nFor some widely studied, single-disease model tasks such as prostate \\nadenocarcinoma Gleason grading, there have been substantial efforts \\nby various groups around the world to build large and diverse datasets \\nwith detailed ROI or pixel-level annotations suitable for end-to-end \\nsupervised machine learning. A natural question is, given the abun -\\ndance of annotated data, does pretraining a foundation model on \\nimages and captions from diverse tissue types and diseases still lead \\nto tangible benefits for these specific tasks? We attempted to provide \\nsome insight into this question by assembling a large and diverse data-\\nset of more than 200,000 labeled ROIs for the task of prostate can -\\ncer Gleason grading from multiple publicly available sources, before \\nNature Medicine | Volume 30 | March 2024 | 863–874\\n 871\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nperforming end-to-end fine-tuning of our vision encoder, as well as a \\nhandful of other pretrained standard convolutional neural network \\n(CNN)-based and vision transformer (ViT)-based models including \\ndomain-specific encoders such as KimiaNet 64 and CTransPath62. In \\nour experiments, we found that, even with hundreds of thousands of \\nlabeled ROIs paired with transfer learning from ImageNet weights or \\nSSL pretraining, a fine-tuned CONCH model can still provide a sizeable \\nimprovement, even when compared to a much larger ViT-Large model \\n(Supplementary Table 31).\\nWhile a recent investigation found that current visual-language \\npretrained foundational models may perform worse than smaller \\nencoders in the specific scenario of WSI-to-WSI matching using one \\nspecific algorithm66, our experiments in both rare disease few-shot \\nand weakly supervised classification, as well as end-to-end fine-tuning, \\nshowed that CONCH can serve as a state-of-the-art visual encoder for \\nhistopathology images, in addition to providing a shared image–text \\nlatent space that unlocks additional multimodal capabilities. Neverthe-\\nless, these findings highlight the importance of continuous research \\nand evaluation to better understand the strengths and limitations of \\nfoundational models for computational pathology.\\nA key limitation of our study is the scale of data pretraining, which \\nstill pales in comparison to billion-scale datasets used in developing \\nlarge-scale visual-language foundation models in the general machine \\nlearning community; therefore, we are likely to see further potential \\nimprovement in zero-shot recognition capabilities, representation \\nquality and robustness by increasing both the quantity and the quality \\nof histopathology image–caption datasets. However, given the increas-\\ning data scale used in pretraining, the potential for unintentional data \\noverlap between pretraining data and downstream test data becomes \\nincreasingly high, a limitation also shared by previous vision-language \\npretraining approaches in the biomedical domain44,54. Detecting and \\nremoving duplicates and near-duplicates typically relies on a combina-\\ntion of heuristics and manual assessment, and this has not been suffi-\\nciently explored in the biomedical domain, serving as an open research \\nquestion for future work. In this study, we minimized the potential for \\ndata overlap by ensuring that no publicly available test dataset was \\ndirectly derived from any training sources and by only holding out data \\nat the source level. Another limitation of the study is that we did not \\ninvestigate the robustness of zero-shot classification (for both image \\nROIs and WSIs) across different data cohorts with potentially different \\nstaining variations, tissue preparation protocols and scanner-specific \\nimaging profiles, compared to using conventional supervised learning \\nor parameter-efficient fine-tuning techniques67,68. Additionally, while \\nwe showed that simply ensembling a small number of templates and \\nclass names written by a pathologist can already work well for several \\ntasks, we did not attempt to explicitly engineer the prompts on the \\nGround truth Predicted\\n2 mm\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nZero-shot performance\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nd\\nSICAP\\nGround truth Predicted\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n0.4 mm\\ne\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\nWSI\\nPredicted \\nsegmentation\\nZero shot \\nTumor\\nBenign\\na\\nTiles\\nb c\\nStitch\\n0.1 mm\\n0.1 mm\\nDice score Precision Recall Dice score Precision Recall\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nCONCH\\nPLIP\\nBiomedCLIP\\nOpenAICLIP\\nDigestPath\\nFig. 5 | Zero-shot segmentation. a, Schematic illustrating zero-shot \\nsegmentation on WSIs (or large tissue sections). T o perform segmentation, we \\ndivided each WSI into tiles and used zero-shot classification to predict the label of \\neach tile. The tile-level predictions were stitched together to form the predicted \\nsegmentation mask. b,c, Zero-shot segmentation performance of CONCH and \\nbaselines on SICAP (n\\u2009=\\u200931) (b) and DigestPath (n\\u2009=\\u2009250) (c) datasets. The macro-\\naveraged Dice score, precision and recall are reported. Error bars represent 95% \\nconfidence intervals. d,e, Examples of CONCH segmentation prediction on WSIs \\nfor SICAP (d) and DigestPath (e). The left panel shows the ground truth, and \\nthe right panel shows the predicted segmentation mask, with example regions \\nenlarged. Red and blue indicate tumor and normal tissue, respectively. In general, \\nin these examples, CONCH displays excellent sensitivity to tumor regions with \\nslightly lower specificity, although most of the regions that CONCH segments \\nas tumor that are in fact nontumor are adjacent to cancerous glands or contain \\ncancer-associated stroma for both SICAP and DigestPath.\\nNature Medicine | Volume 30 | March 2024 | 863–874 872\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nbasis of the model’s performance (for example, by using a validation \\nset). We note that doing an explicit search for ‘good’ prompts on a \\nsmall validation set (if it is available) may be much more effective in \\npractice while still retaining the benefit of not needing to fine-tune the \\nmodel, although it would no longer be strictly considered zero-shot \\ntransfer69,70. Moreover, as a zero-shot classification algorithm for WSIs, \\nMI-Zero is only best suited for tasks where the defining morphological \\npatterns of each class are mutually exclusive, and it may not work on \\ntasks with specific assumptions or guidelines. This includes tasks such \\nas Gleason scoring where both the primary and the secondary pattern \\nmay need to be considered to inform the classification or tumor versus \\nnormal classification, in which a slide may be appropriately labeled as \\n‘positive’ as soon as a single tumor-containing region is identified. We \\nnote that, for these types of tasks, the pooling function of MI-Zero can \\nbe adjusted to better suit the nature of the task, and we leave its imple-\\nmentation and evaluation to future studies. Lastly, while the current \\nlandscape of visual-language foundation models for histopathology \\nfocuses primarily on image-level tasks, the ability of these models to \\nrecognize fine-grained visual concepts at the region level (that is, cel-\\nlular or even subcellular level) has not yet been studied, meaning that \\nother important tasks such as mitosis detection, fine-grained tissue \\nsegmentation or cell counting currently remain outside the scope of \\ntheir downstream capabilities.\\nOnline content\\nAny methods, additional references, Nature Portfolio reporting sum-\\nmaries, source data, extended data, supplementary information, \\nacknowledgements, peer review information; details of author contri-\\nbutions and competing interests; and statements of data and code avail-\\nability are available at https://doi.org/10.1038/s41591-024-02856-4.\\nReferences\\n1. Song, A. H. et al. Artificial intelligence for digital and \\ncomputational pathology. Nat. Rev. Bioeng. 1, 930–949 (2023).\\n2. Bera, K., Schalper, K. A., Rimm, D. L., Velcheti, V. & Madabhushi, A. \\nArtificial intelligence in digital pathology—new tools for diagnosis \\nand precision oncology. Nat. Rev. Clin. Oncol. 16, 703–715 (2019).\\n3. Shmatko, A., Ghaffari Laleh, N., Gerstung, M. & Kather, J. N. \\nArtificial intelligence in histopathology: enhancing cancer \\nresearch and clinical oncology. Nat. Cancer 3, 1026–1038 (2022).\\n4. Lipkova, J. et al. Artificial intelligence for multimodal data \\nintegration in oncology. Cancer Cell 40, 1095–1110 (2022).\\n5. Bejnordi, B. E. et al. Diagnostic assessment of deep learning \\nalgorithms for detection of lymph node metastases in women \\nwith breast cancer. JAMA 318, 2199–2210 (2017).\\n6. Coudray, N. et al. Classification and mutation prediction from \\nnon-small cell lung cancer histopathology images using deep \\nlearning. Nat. Med. 24, 1559–1567 (2018).\\n7. Lu, M. Y. et al. Data-efficient and weakly supervised computational \\npathology on whole-slide images. Nat. Biomed. Eng. 5, 555–570 \\n(2021).\\n8. Skrede, O.-J. et al. Deep learning for prediction of colorectal \\ncancer outcome: a discovery and validation study. Lancet 395, \\n350–360 (2020).\\n9. Chen, R. J. et al. Pan-cancer integrative histology–genomic \\nanalysis via multimodal deep learning. Cancer Cell 40, 865–878 \\n(2022).\\n10. Courtiol, P. et al. Deep learning-based classification of \\nmesothelioma improves prediction of patient outcome. Nat. Med. \\n25, 1519–1525 (2019).\\n11. Lu, M. Y. et al. AI-based pathology predicts origins for cancers of \\nunknown primary. Nature 594, 106–110 (2021).\\n12. Zhu, L. et al. An accurate prediction of the origin for bone \\nmetastatic cancer using deep learning on digital pathological \\nimages. EBioMedicine 87, 104426 (2023).\\n13. Kalra, S. et al. Yottixel—an image search engine for large archives \\nof histopathology whole slide images. Med. Image Anal. 65, \\n101757 (2020).\\n14. Hegde, N. et al. Similar image search for histopathology: SMILY. \\nNPJ Digit. Med. 2, 56 (2019).\\n15. Wang, X. et al. RetCCL: clustering-guided contrastive learning  \\n for whole-slide image retrieval. Med. Image Anal. 83, 102645 \\n(2023).\\n16. Chen, C. et al. Fast and scalable search of whole-slide images via \\nself-supervised deep learning. Nat. Biomed. Eng. 6, 1420–1434 \\n(2022).\\n17. Kather, J. N. et al. Pan-cancer image-based detection of clinically \\nactionable genetic alterations. Nat. Cancer 1, 789–799 (2020).\\n18. Saldanha, O. L. et al. Self-supervised attention-based deep \\nlearning for pan-cancer mutation prediction from histopathology. \\nNPJ Precis. Oncol. 7, 35 (2023).\\n19. Graham, S. et al. Hover-Net: simultaneous segmentation  \\nand classification of nuclei in multi-tissue histology images.  \\nMed. Image Anal. 58, 101563 (2019).\\n20. Campanella, G. et al. Clinical-grade computational pathology \\nusing weakly supervised deep learning on whole slide images. \\nNat. Med. 25, 1301–1309 (2019).\\n21. Bulten, W. et al. Automated deep-learning system for Gleason \\ngrading of prostate cancer using biopsies: a diagnostic study. \\nLancet Oncol. 21, 233–241 (2020).\\n22. Nagpal, K. et al. Development and validation of a deep learning \\nalgorithm for improving Gleason scoring of prostate cancer.  \\nNPJ Digit. Med. 2, 48 (2019).\\n23. Mobadersany, P. et al. Predicting cancer outcomes from histology \\nand genomics using convolutional networks. Proc. Natl Acad.  \\nSci. USA 115, E2970–E2979 (2018).\\n24. Chen, R. J. et al. Multimodal co-attention transformer for survival \\nprediction in gigapixel whole slide images. In Proc. IEEE/CVF \\nInternational Conference on Computer Vision 4015–4025 (IEEE, \\n2021).\\n25. Fu, Y. et al. Pan-cancer computational histopathology reveals \\nmutations, tumor composition and prognosis. Nat. Cancer 1, \\n800–810 (2020).\\n26. Sammut, S.-J. et al. Multi-omic machine learning predictor of \\nbreast cancer therapy response. Nature 601, 623–629 (2022).\\n27. Huang, Z. et al. Artificial intelligence reveals features associated \\nwith breast cancer neoadjuvant chemotherapy responses from \\nmulti-stain histopathologic images. NPJ Precis. Oncol. 7, 14 \\n(2023).\\n28. Foersch, S. et al. Multistain deep learning for prediction of \\nprognosis and therapy response in colorectal cancer. Nat. Med. \\n29, 430–439 (2023).\\n29. Vanguri, R. S. et al. Multimodal integration of radiology, pathology \\nand genomics for prediction of response to PD-(L)1 blockade in \\npatients with non-small cell lung cancer. Nat. Cancer 3, 1151–1164 \\n(2022).\\n30. Radford, A. et al. Learning transferable visual models from natural \\nlanguage supervision. In International Conference on Machine \\nLearning (eds Meila, M. & Zhang, T.) 8748–8763 (PMLR, 2021).\\n31. Jia, C. et al. Scaling up visual and vision-language representation \\nlearning with noisy text supervision. In International Conference \\non Machine Learning (eds Meila, M. & Zhang, T.) 4904–4916 \\n(PMLR, 2021).\\n32. Yu, J. et al. CoCa: contrastive captioners are image–text foundation \\nmodels. Trans. Mach. Learn. Artif. Intell. https://openreview.net/\\nforum?id=Ee277P3AYC (2022).\\n33. Li, J., Li, D., Xiong, C. & Hoi, S. BLIP: bootstrapping language–\\nimage pre-training for unified vision-language understanding  \\nand generation. In International Conference on Machine Learning \\n(eds Chaudhur, K. et al.) 12888–12900 (PMLR, 2022).\\nNature Medicine | Volume 30 | March 2024 | 863–874\\n 873\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\n34. Singh, A. et al. FLAVA: a foundational language and vision \\nalignment model. In Proc. IEEE/CVF Conference on Computer \\nVision and Pattern Recognition 15638–15650 (IEEE, 2022).\\n35. Li, H. et al. Uni-Perceiver v2: a generalist model for large-scale \\nvision and vision-language tasks. In Proc. IEEE/CVF Conference  \\non Computer Vision and Pattern Recognition 2691–2700 (IEEE, \\n2023).\\n36. Alayrac, J.-B. et al. Flamingo: a visual language model for \\nfew-  shot learning. Adv. Neural Inf. Process. Syst. 35, 23716–23736 \\n(2022).\\n37. Li, Y., Fan, H., Hu, R., Feichtenhofer, C. & He, K. Scaling language–\\nimage pre-training via masking. In Proc. IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition 23390–23400 (IEEE, \\n2023).\\n38. Wang, W. et al. Image as a foreign language: BEiT pretraining for \\nvision and vision-language tasks. In Proc. IEEE/CVF Conference \\non Computer Vision and Pattern Recognition 19175–19186 (IEEE, \\n2023).\\n39. Schuhmann, C. et al. LAION-5B: an open large-scale dataset \\nfor training next generation image-text models. Adv. Neural Inf. \\nProcess. Syst. 35, 25278–25294 (2022).\\n40. Chen, Z., Song, Y., Chang, T.-H. & Wan, X. Generating radiology \\nreports via memory-driven transformer. In Proc. 2020 Conference \\non Empirical Methods in Natural Language Processing (EMNLP) \\n(eds Webber, B. et al.) 1439–1449 (Association for Computational \\nLinguistics, 2020); https://aclanthology.org/2020.emnlp-main.112\\n41. Liu, G. et al. Clinically accurate chest X-ray report generation.  \\nIn Proc. 4th Machine Learning for Healthcare Conference  \\n(eds Doshi-Velez, F. et al.), Vol. 106, 249–269 (PMLR, 2019).\\n42. Tiu, E. et al. Expert-level detection of pathologies from \\nunannotated chest X-ray images via self-supervised learning.  \\nNat. Biomed. Eng. 6, 1399–1406 (2022).\\n43. Huang, S.-C., Shen, L., Lungren, M. P. & Yeung, S. GLoRIA:  \\na multimodal global–local representation learning framework for  \\nlabel-efficient medical image recognition. In Proc. IEEE/CVF \\nInternational Conference on Computer Vision 3942–3951 (IEEE, \\n2021).\\n44. Zhang, S. et al. BiomedCLIP: a multimodal biomedical foundation \\nmodel pretrained from fifteen million scientific image–text  \\npairs. Preprint at https://doi.org/10.48550/arXiv.2303.00915 \\n(2023).\\n45. Wang, Z., Wu, Z., Agarwal, D. & Sun, J. MedCLIP: contrastive \\nlearning from unpaired medical images and text. In Proc. \\n2022 Conference on Empirical Methods in Natural Language \\nProcessing (eds Che, W. & Shutova, E.) 3876–3887 (Association for \\nComputational Linguistics, 2022).\\n46. Schaumberg, A. J. et al. Interpretable multimodal deep learning \\nfor real-time pan-tissue pan-disease pathology search on social \\nmedia. Mod. Pathol. 33, 2169–2185 (2020).\\n47. Maleki, D. & Tizhoosh, H. R. LILE: look in-depth before looking \\nelsewhere—a dual attention network using transformers for \\ncross-modal information retrieval in histopathology archives. In \\nInternational Conference on Medical Imaging with Deep Learning \\n(eds Konukoglu, E. et al.) 879–894 (PMLR, 2022).\\n48. Zhang, Y., Jiang, H., Miura, Y., Manning, C. D. & Langlotz, C. P. \\nContrastive learning of medical visual representations from \\npaired images and text. In Machine Learning for Healthcare \\nConference (eds Lipton, Z. et al.) 2–25 (PMLR, 2022).\\n49. Zhang, H. et al. PathNarratives: data annotation for pathological \\nhuman–AI collaborative diagnosis. Front. Med. 9, 1070072  \\n (2023).\\n50. Tsuneki, M. & Kanavati, F. Inference of captions from \\nhistopathological patches. In International Conference on Medical \\nImaging with Deep Learning (eds Konukoglu, E. et al.) 1235–1250 \\n(PMLR, 2022).\\n51. Zhang, R., Weber, C., Grossman, R. & Khan, A. A. Evaluating and \\ninterpreting caption prediction for histopathology images. In \\nMachine Learning for Healthcare Conference (eds Doshi-Velez, F. \\net al.) 418–435 (PMLR, 2020).\\n52. Naseem, U., Khushi, M. & Kim, J. Vision-language transformer \\nfor interpretable pathology visual question answering. IEEE J. \\nBiomed. Health Inform. 27, 1681–1690 (2022).\\n53. He, X. Towards visual question answering on pathology images. \\nIn Proc. 59th Annual Meeting of the Association for Computational \\nLinguistics and the 11th International Joint Conference on Natural \\nLanguage Processing (Volume 2: Short Papers) (eds Zong, C. et al.) \\n708–718 (Association for Computational Linguistics, 2021).\\n54. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J.  \\nA visual-language foundation model for pathology image analysis \\nusing medical Twitter. Nat. Med. 29, 2307–2316 (2023).\\n55. Gamper, J. & Rajpoot, N. Multiple instance captioning: learning \\nrepresentations from histopathology textbooks and articles. \\nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition 16549–16559 (IEEE, 2021).\\n56. Lu, M. Y. et al. Visual language pretrained multiple instance \\nzero-shot transfer for histopathology images. In Proc. of the IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition \\n19764–19775 (IEEE, 2023).\\n57. Lin, W. et al. PMC-CLIP: contrastive language–image pre-training \\nusing biomedical documents. In Medical Image Computing and \\nComputer Assisted Intervention—MICCAI 2023 (ed. Greenspan, H. \\net al.) 525–536 (Springer Nature, 2023).\\n58. Ikezogwo, W. O. et al. Quilt-1M: one million image–text pairs for \\nhistopathology. In Advances in Neural Information Processing \\nSystems (eds Oh, A. et al.) 37995–38017 (Curran Associates, Inc., \\n2023).\\n59. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple \\ninstance learning. In International Conference on Machine \\nLearning (eds Dy, J. & Krause, A.) 2127–2136 (PMLR, 2018).\\n60. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for \\nimage recognition. In Proc. IEEE Conference on Computer Vision \\nand Pattern Recognition 770–778 (IEEE, 2016).\\n61. Deng, J. et al. ImageNet: a large-scale hierarchical image \\ndatabase. In 2009 IEEE Conference on Computer Vision and \\nPattern Recognition 248–255 (IEEE, 2009).\\n62. Wang, X. et al. Transformer-based unsupervised contrastive \\nlearning for histopathological image classification. Med. Image \\nAnal. 81, 102559 (2022).\\n63. Gatta, G. et al. Burden and centralised treatment in Europe of rare \\ntumours: results of RARECAREnet—a population-based study. \\nLancet Oncol. 18, 1022–1039 (2017).\\n64. Riasatian, A. et al. Fine-tuning and training of densenet for \\nhistopathology image representation using TCGA diagnostic \\nslides. Med. Image Anal. 70, 102032 (2021).\\n65. Kundra, R. et al. OncoTree: a cancer classification system for \\nprecision oncology. JCO Clin. Cancer Inform. 5, 221–230  \\n(2021).\\n66. Alfasly, S. et al. When is a foundation model a foundation model. \\nPreprint at https://doi.org/10.48550/arXiv.2309.11510 (2023).\\n67. Zhou, K., Yang, J., Loy, C. C. & Liu, Z. Learning to prompt for \\nvision-language models. Int. J. Comput. Vis. 130, 2337–2348 \\n(2022).\\n68. Gao, P. et al. CLIP-Adapter: better vision-language models with \\nfeature adapters. Int. J. Comput. Vis. 132, 581–595 (2024).\\n69. Perez, E., Kiela, D. & Cho, K. True few-shot learning with language \\nmodels. Adv. Neural Inf. Process. Syst. 34, 11054–11070 (2021).\\n70. Sanh, V. et al. Multitask prompted training enables zero-shot \\ntask generalization. In 10th International Conference on Learning \\nRepresentations https://openreview.net/forum?id=9Vrb9D0WI4 \\n(OpenReview.net 2021).\\nNature Medicine | Volume 30 | March 2024 | 863–874 874\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nPublisher’s note Springer Nature remains neutral with regard  \\nto jurisdictional claims in published maps and institutional  \\naffiliations.\\nSpringer Nature or its licensor (e.g. a society or other partner) holds \\nexclusive rights to this article under a publishing agreement with \\nthe author(s) or other rightsholder(s); author self-archiving of the \\naccepted manuscript version of this article is solely governed by the \\nterms of such publishing agreement and applicable law.\\n© The Author(s), under exclusive licence to Springer Nature America, \\nInc. 2024\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nMethods\\nDataset curation\\nMost data used for this study were obtained from publicly available \\nresearch articles. For internal data, the Mass General Brigham insti -\\ntutional review board approved the retrospective analysis of internal \\npathology images, corresponding reports and electronic records. \\nAll internal digital data, including WSIs, pathology reports and EMRs \\nwere deidentified before computational analysis and model develop-\\nment. Patients were not directly involved or recruited for the study. \\nInformed consent was waived for analyzing archival pathology slides \\nretrospectively. We used publicly available articles from PubMed to \\ncurate the largest-to-date dataset of histopathology image–caption \\npairs. We used deep learning to automate data cleaning iteratively.  For \\ncuration, we divided the data sources into two categories: EDU, which \\nconsists of data extracted from educational notes, and PMC OA, which \\nconsists of data downloaded from the PubMed Central Open Access \\nDataset (https://ncbi.nlm.nih.gov/pmc/tools/openftlist/).\\nThe data curation process poses two main challenges: filtering for \\nhistopathology data and handling image panels. The first challenge \\nis that the raw downloaded data comprised both histopathology and \\nnon-histopathology examples. The second challenge is that a sub -\\nstantial portion of the data were in the form of figure panels, where \\nthe images consisted of multiple subimages arranged in a panel with \\nparts of the caption addressing all or some of the subimages. In light \\nof these challenges, manually cleaning the data was infeasible. We \\ncleaned the data in three steps: (1) detecting histopathology images \\n(as single images or subimages); (2) splitting captions that referred to \\nimage panels into separate captions into subcaptions; and (3) aligning \\nsubimages with subcaptions within each image panel.\\nT o detect histopathology images, we used an object detection \\nmodel (YOLOv5)71 to generate bounding boxes for extracting detected \\nimages. T o avoid the laborious task of manually labeling ground-truth \\nbounding boxes, we generated synthetic data by randomly selecting \\nsingle-panel images and arranging them in an image panel. We itera-\\ntively refined the detection model by validating it on a small subset \\n(<0.5%) of PMC OA and adding incorrectly labeled samples to the \\ntraining set.\\nFor caption splitting, we collected a dataset of original and split \\ncaptions (while cleaning the EDU dataset) to fine-tune a generative \\npretrained transformer (GPT)-style model pretrained on PubMed and \\nother medical text72. We posed the problem of splitting captions as \\ncausal language modeling, where we fine-tuned the language model \\nto take the original full caption as input and predicted the subcaptions \\nseparated by the keyword ‘next caption’ . We used the fine-tuned model \\nto perform caption splitting.\\nT o align the detected histopathology images with split captions, \\nwe first trained a CLIP model30 on the cleaned EDU dataset, along with \\nPMC OA single figures that did not require splitting and alignment. \\nUsing the trained model, given a set of m detected images and n split \\ncaptions from an image panel, we computed the image embeddings \\n{u0,\\u2009u1,\\u2009…,\\u2009um} and text embeddings {v0,\\u2009v1,\\u2009…,\\u2009vn} in the aligned latent \\nspace. For each image embedding ui, we computed the cosine-similarity \\nscore with each text embedding vj. We retrieved the text with the high-\\nest cosine-similarity score si,j ∶= uT\\ni vj and considered {u i,\\u2009vj} to be an \\nimage–caption pair for our cleaned dataset.\\nBy applying the three steps above to PMC OA, we created \\nPMC-Path, a pathology-specific image–caption dataset derived from \\nPubMed figures. We then combined it with EDU to form our full, unfil-\\ntered pretraining dataset of 1,786,362 image–caption pairs. However, \\nPMC-Path also contained a substantial number of pairs referring to \\nanimal histopathology, as well as non-hematoxylin and eosin (H&E) \\nstains (immunohistochemistry (IHC), Masson’s trichrome, Congo \\nred, etc.). Because our downstream evaluation concerned only human \\nhistopathology and H&E tasks, we wanted to assess how the animal and \\nspecial staining data would affect performance. We first parsed the \\ncaptions to exclude samples referencing nonhuman animals, forming \\na dataset of 1,170,647 human pairs. Additionally, we trained a classifier \\nthat identified H&E stains to further filter the human-only dataset and \\ncreate a dataset of 457,372 pairs. We found that CONCH pretrained on \\nthe human-only dataset performed the best on downstream tasks in \\ngeneral (Extended Data Fig. 10a).\\nVisual-language pretraining\\nFor visual-language pretraining, we used an equal-weighted combina-\\ntion of the image–text contrastive loss and the captioning loss following \\nCoCa32, a state-of-the-art visual-language foundation model pretrained \\non general-domain image–caption pairs. The model consisted of an \\nimage encoder, f(\\u2009⋅\\u2009;\\u2009θ), a text encoder, g(\\u2009⋅\\u2009;\\u2009ϕ), and a multimodal text \\ndecoder, h(\\u2009⋅\\u2009;\\u2009ψ). The image encoder included the backbone and two \\nattentional pooler modules, parameterized by θ backbone, θcontrast and \\nθcaption, respectively. The backbone was a ViT73 following the standard \\nViT-base architecture with 12 transformer layers, 12 attention heads, \\nan embedding dimension of 768 and a hidden dimension of 3,072. The \\ntoken size was 16\\u2009×\\u200916, and learned absolute positional embeddings \\nwere added to each token. The backbone transformed images in the \\nform of raw red–green–blue (RGB) pixel values to dense feature maps \\nin a more semantically rich representation space learned from data. \\nEach attentional pooler was responsible for computing a fixed number \\n(denoted by n) of image tokens from the last layer representation of the \\nViT backbone using multiheaded attention and n learned queries. For \\nenabling cross-modal retrieval through contrastive learning, the first \\nattentional pooler fcontrast(\\u2009⋅\\u2009;\\u2009θcontrast) used a single query (ncontrast\\u2009=\\u20091) to \\ncompute a single image token designed to capture the global repre -\\nsentation of the image. The second attentional pooler fcaption(\\u2009⋅\\u2009;\\u2009θcaption) \\nused ncaption\\u2009=\\u2009256 queries to generate a set of 256 image tokens designed \\nto capture more local and fine-grained details of the image, which are \\ntypically required for captioning. The text encoder and multimodal \\ndecoder were both GPT-style models that used causal attention masks \\nfor left-to-right autoregressive language modeling. Similar to the image \\nencoder, the text encoder and multimodal decoder consisted of 12 \\ntransformer layers with an embedding dimension of 768 and a hidden \\ndimension of 3,072. The text encoder included an embedding table for \\nmapping discrete word tokens to continuous embeddings and a set of \\nlearned absolute positional embeddings. Additionally, the text encoder \\nappended a learned <CLS> token to each tokenized caption, which \\nhad access to the full context during transformer attention to extract \\na global representation of a given caption. The multimodal decoder \\ninserted a cross-attention layer after each multiheaded self-attention \\nlayer to incorporate information from image tokens and included a \\nfinal language modeling head for predicting the distribution of the \\nnext token over the supported vocabulary.\\nDuring visual-language pretraining, a mini-batch consisted of M \\nimage–caption pairs (xi,wi)\\nM\\ni=1, where wi = (<BOS>, wi,1,\\u2009…,\\u2009wi,T, <EOS>) \\nis a sequence of T word tokens representing the ith caption. For a given \\npair (x i,\\u2009wi), we let (u i,\\u2009vi) be the output of f contrast(\\u2009⋅\\u2009;\\u2009θcontrast) and the \\noutput of g(\\u2009⋅\\u2009;\\u2009ϕ) at the position corresponding to the < CLS> token \\nafter ℓ2-normalization. The complete objective is given by:\\nℒ=−\\n1\\n2M\\nM\\n∑\\ni=1\\nlog\\nexp(τuT\\ni vi)\\n∑\\nM\\nj=1 exp(τuT\\ni vj)\\n−\\n1\\n2M\\nM\\n∑\\nj=1\\nlog\\nexp(τ vT\\nj uj)\\n∑\\nM\\ni=1 exp(τ vT\\nj ui)\\n−\\n1\\nM\\nM\\n∑\\ni=1\\nT+1\\n∑\\nt=1\\nlogp(wi,t|wi,0∶t−1,xi;θ,ϕ,ψ)\\nThe first and second terms represent image-to-text and text-to-image \\ncontrastive loss, respectively, to maximize the cosine-similarity scores \\nbetween paired image and text embeddings relative to remaining \\nnegative pairings in the mini-batch. The last term seeks to maximize \\nthe log-likelihood of each observed token under the multimodal \\nautoregressive language model ( jointly parameterized by the image \\nencoder, text encoder and multimodal decoder), conditioned on \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nprevious tokens in the caption, as well as the corresponding image. Each \\nvisual-language pretraining experiment was trained for 40 epochs, \\ndistributed across eight NVIDIA A100 80-GB graphics processing units \\n(GPUs) with a local batch size of 48 per GPU, and gradient accumulation \\nwas used to achieve an effective global batch size of 1,536. We set the \\nimage size to 448\\u2009×\\u2009448\\u2009pixels, where larger images were first resized \\nalong the shorter edge and center-cropped, and smaller images were \\nzero-padded as needed. For all optimization hyperparameters, refer \\nto Supplementary Table 32.\\nPretraining unimodal encoders\\nPrior work56 showed that performing self-supervised pretraining of \\nunimodal modules using unpaired data before joint visual-language \\npretraining using paired image–caption data can substantially improve \\ndownstream zero-shot transfer performance. We pretrained our \\nimage encoder using iBOT74, a state-of-the-art, self-supervised pre-\\ntraining algorithm for unlabeled image data. An in-house dataset of \\n16 million 256\\u2009×\\u2009256-sized image tiles were sampled and extracted at \\n×20-equivalent magnification from the tissue regions of 21,442 WSIs \\nspanning over 350 cancer subtypes under the OncoTree classification \\nsystem65. Detailed hyperparameters for image-only pretraining are pro-\\nvided in Supplementary Table 33. For pretraining the language model, \\nwe built a diverse corpus of pathology-relevant texts ranging from \\npathology educational texts to final diagnosis sections of over 550,000 \\nsurgical pathology reports from Massachusetts General Hospital and \\nover 400,000 select histopathology-relevant PubMed abstracts. We \\nused regex to deidentify in-house diagnostic reports, notably replac-\\ning patient and physician names, specimen identifiers, medical record \\nnumbers and dates with a corresponding special token in the vocabu-\\nlary. We pretrained a 24-layer GPT-style autoregressive model using the \\nnext-word prediction loss. Specifically, given a sequence of word tokens \\nw = (<BOS>, w1,\\u2009…,\\u2009wT, <EOS>), we maximized the log-likelihood of each \\ntoken under an autoregressive generative model parameterized by ξ:\\nℒclm(ξ)=−\\nT+1\\n∑\\nt=1\\nlogp(wt|w0∶t−1;ξ)\\nDetailed hyperparameters for text-only pretraining are provided in \\nSupplementary Table 34. After pretraining, the first 12 layers of the \\ntransformer-based language models and the embedding table were \\nused to initialize the unimodal text encoder, while the last 12 layers \\nand the language modeling classifier head were used to initialize the \\ncorresponding parameters in the multimodal decoder.\\nWe assessed the benefit of unimodal pretraining by comparing \\ndownstream performance between the unimodal domain-specific \\npretraining scheme above versus CONCH with the image encoder \\npretrained on ImageNet versus CONCH with the language model ran-\\ndomly initialized (Extended Data Fig. 10). We found that CONCH with \\ndomain-specific pretraining outperformed CONCH with ImageNet \\npretraining on both zero-shot transfer and retrieval tasks. CONCH with \\nthe pretrained language model performed similarly to CONCH with \\na randomly initialized language model on classification and grading \\ntasks but outperformed it in retrieval tasks.\\nZero-shot transfer on ROIs and tiles\\nFor zero-shot transfer, we used the method described in CLIP30. Each \\nclass was associated with a text prompt consisting of a class name \\n(for example, ‘adenocarcinoma’) and a template (for example, ‘this \\nis {}. ’; see Supplementary Table 35 for templates used across all \\ntasks). For a prompt associated with class j\\u2009∈\\u2009{1,\\u20092,\\u2009…,\\u2009C}, we computed \\nthe ℓ2-normalized embedding vj using a text encoder trained on our \\npaired dataset to form the linear classifier weights. Because model \\nperformance can vary considerably depending on the choice of \\nprompts, we measured the performance spread by sampling subsets \\nfrom a pathologist-curated set of prompts and reporting the median. \\nAlternatively, we could also ensemble all the prompts within a class \\nby using the mean embedding over the prompts as the text embed-\\nding associated with that class (see Extended Data Fig. 2 for a com-\\nparison with and without ensembling). Analogously, for each image, \\nwe computed the ℓ2-normalized embedding ui. We then computed \\ncosine-similarity scores between the image and each text embedding, \\nand the predicted class was consequently the class with the highest \\nsimilarity score:\\n̂yi = argmax\\nj\\nui\\nTvj\\nBecause some evaluation sets were imbalanced, we report the balanced \\naccuracy (that is, the macro average over the accuracy obtained on each \\nclass) and the average F1 score weighted by the support of each class. \\nFor SICAP, we also report the quadratic Cohen’s κ score, which is often \\nused for prostate Gleason grading75, where errors between adjacent \\ngrading classes are penalized less.\\nSimilarly, for cross-modal retrieval, we used the same method as \\nzero-shot classification above to retrieve the top-K images that were \\nclosest in the aligned latent space to a specific text query (text-to-image \\nretrieval). Image-to-text retrieval was performed analogously. T o evalu-\\nate retrieval, we followed ALIGN31 and used Recall@K, that is, for what \\npercentage of the test set is the correct result in the top-K retrieved \\nsamples. We chose K\\u2009∈\\u2009{1,\\u20095,\\u200910}, and we also report mean recall by \\naveraging the scores over the three Recall@K values.\\nUnless otherwise specified, we enforced the maximum image \\nsize to be 448\\u2009×\\u2009448 for CONCH through image resizing and center \\ncropping, similar to its pretraining configuration. For all models that \\nwere not ours, we used their provided processor function and default \\nconfiguration for image and text processing in downstream evaluation.\\nExtending zero-shot transfer to WSIs\\nT o extend zero-shot transfer to gigapixel images, we followed the \\nmethod introduced by MI-Zero56. Specifically, for classification over \\nC classes, the WSI was first divided into N tiles, and the ℓ2-normalized \\nembeddings were computed independently using the image encoder. \\nFor each tile embedding, we computed similarity scores with each text \\nembedding following the method for tiles described above, obtaining \\na set of C similarity scores for each tile. T o aggregate similarity scores \\nacross tiles, we used the top-K pooling operator by averaging over \\nthe highest K similarity scores for each class to obtain the slide-level \\nsimilarity score. Consequently, the class with the highest slide-level \\nscore was the predicted class. We chose K\\u2009∈\\u2009{1,\\u20095,\\u200910,\\u200950,\\u2009100}, and we \\nreport metrics for the K value with the highest balanced accuracy for \\nclassification tasks and Cohen’s κ for DHMC LUAD. Similarly to the \\nclassification of tiles, we report the slide-level balanced accuracy and \\nweighted F1 score for classification tasks. For DHMC LUAD, because the \\ntask of LUAD subtyping can be subjective, we report Cohen’s κ score.\\nWe performed zero-shot slide-level segmentation using a similar \\napproach to that used for classification. We divided the WSI into tiles \\nand computed similarity scores for each tile independently. However, \\ninstead of aggregating the scores across tiles into a single slide-level \\nprediction, we mapped the tile-level scores to their corresponding \\nspatial locations in the WSI, averaging the scores in overlapped regions. \\nFinally, for each pixel, we assigned the class with the highest score as the \\nprediction, producing a pixel-level segmentation mask. We computed \\nthe Dice score76 to quantify the quality of the predicted segmentation \\nmask relative to the ground truth.\\nDetails of WSI preprocessing for both classification and segmenta-\\ntion tasks are described in Methods, ‘WSI processing’ .\\nSupervised and weakly supervised classification experiments\\nWe performed supervised classification experiments on all tasks with \\na labeled set of training examples available, including TCGA BRCA for \\nBRCA subtyping, TCGA NSCLC for NSCLC subtyping, TCGA RCC for \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nRCC subtyping, CRC100k for CRC tissue classification and SICAP for \\nGleason grading. For each dataset, we used the official training and \\ntesting split if it was available or we used the remaining labeled cases \\nfor training after holding out the cases used for zero-shot classifica-\\ntion evaluation (see Methods, ‘Downstream evaluation datasets’ for a \\nmore detailed breakdown). For slide-level experiments, we considered \\nfour visual-language pretrained image encoders, namely, CONCH, \\nPLIP, BiomedCLIP and OpenAICLIP. All four encoders followed the \\nViT-base architecture with a patch size of 16 except PLIP, which used \\na patch size of 32. For slide-level tasks, we additionally considered \\na ResNet50 encoder truncated after the third residual block, with \\nweights initialized from supervised classification on ImageNet, as it \\nhas been a common choice in the weakly supervised classification of \\nWSIs. For ROI-level tasks, we added CTransPath62 as a baseline, which \\nis a state-of-the-art general-purpose vision encoder trained with SSL \\non a large dataset of unlabeled histopathology images. We did not use \\nCTransPath for TCGA slide-level tasks because TCGA slides (including \\nthose used in our test sets) made up a large portion of the data used to \\ntrain CTransPath; therefore, this could have resulted in information \\nleakage that unfairly inflated the performance of CTransPath on TCGA \\nbenchmarks.\\nFor all experiments, we standardized the image input size to \\n224\\u2009×\\u2009224. We used each image encoder to extract a low-dimensional \\nfeature embedding from each image (tiles in the case of WSIs). For \\nCONCH, we used the output of the attentional pooler that corre-\\nsponded to image–text alignment, with an embedding dimension \\nof 512. For CLIP-based models, including PLIP, BiomedCLIP and Ope-\\nnAICLIP, we used the <CLS> token, which was also used for image–text \\nalignment during pretraining and similarly had a dimension of 512. \\nFor ResNet50, we used global average pooling after the third residual \\nblock to obtain a 1,024-dimensional embedding. For CTransPath, we \\nalso used the <CLS> token representation, which had an embedding \\ndimension of 768.\\nFor WSI classification, we used the same preprocessing setup as \\nzero-shot classification with MI-Zero. We used the widely used ABMIL59 \\nfor weakly supervised classification of WSIs using slide-level labels. \\nThe ABMIL model architecture consists of a fully connected layer and \\na rectified linear unit (ReLU) nonlinearity that first maps the inputs to \\nan embedding dimension of 512, followed by a two-layer, gated vari-\\nant (as described in the original paper) of the attention network, with \\na hidden dimension of 384. Lastly, a fully connected classifier head \\nmaps the attention-pooled slide-level representation to logits, which \\nare interpreted as class probabilities after softmax normalization. We \\nused dropout with P\\u2009=\\u20090.25 after each intermediate layer in the network \\nfor regularization. We trained each model for 20 epochs on the train-\\ning set, using an AdamW optimizer, a cosine learning rate scheduler \\nand a learning rate of 1\\u2009×\\u200910−4. We used a weighted data sampler that \\nincreased the sampling probability of slides from minority classes \\nsuch that, on average, the model saw the same number of slides from \\neach class each epoch. The full set of hyperparameters is summarized \\nin Supplementary Table 36.\\nFor ROI-level classification, we conducted linear probing by train-\\ning a logistic regression model on top of the pretrained image embed-\\ndings of each encoder. We followed a practice recommended by the \\nlarge-scale self-supervised representation learning community77 and \\nset the ℓ2 regularization coefficient λ to \\n100\\nMC\\n, where M is the embedding \\ndimension and C is the number of classes. We used the limited-memory \\nBroyden–Fletcher–Goldfarb–Shanno (L-BFGS) solver and set the maxi-\\nmum number of iterations to 800.\\nFor few-shot classification, we kept the test set the same, and we \\nvaried the number of labeled examples per class for training (known \\nas ‘shot’) from nc\\u2009=\\u20091,\\u20092,\\u20094,\\u20098,\\u200916,\\u200932, up to either nc\\u2009=\\u2009512 or the maximum \\nnumber of labeled examples available for a given class. Otherwise, the \\nhyperparameters and training setup remained the same as described \\nabove.\\nEnd-to-end fine-tuning for classification experiments\\nWe evaluated the utility of CONCH in image ROI classification using \\nstandard end-to-end fine-tuning on a four-class Gleason grading bench-\\nmark with a total of 228,482 (training, 189,484; validation, 9,959; test-\\ning, 29,039) image ROIs individually labeled as NC, G3, G4 or G5 (see \\nMethods, ‘Downstream evaluation datasets’ for more details). We \\ncompared its performance against that of five other models covering a \\nvariety of model architectures, pretraining strategies and sizes, includ-\\ning ViT-B/16 (ViT of the same architecture as the CONCH vision encoder \\nbackbone), ViT-L/16 (larger ViT with ~3.5 times the number of param-\\neters as ViT-B), ResNet50 (popular, widely used standard CNN architec-\\nture), CTransPath (a histopathology-specific image encoder based on \\nthe Swin transformer architecture, pretrained using large-scale vision \\nSSL, which has achieved state-of-the-art performance on many compu-\\ntational pathology tasks) and KimiaNet64 (a lightweight CNN based on \\nthe DenseNet121 architecture, pretrained on a histopathology image \\nclassification task using supervised learning). For ViT-B/16, ViT-L/16 \\nand ResNet50, we initialized the models using weights pretrained \\non ImageNet; for CTransPath and KimiaNet, we used the pretrained \\nweights provided by their respective authors. We also investigated \\nthe label efficiency of each model by further subsampling 10% and \\n1% of labels from the full training set (189,484 ROIs from 4,622 slides) \\nat the slide level, corresponding to 19,304 ROIs from 462 slides and \\n1,864 ROIs from 46 slides, respectively. The results are summarized in \\nSupplementary Table 31.\\nWe used eight 80-GB NVDIA A100 GPUs for each experiment using \\na batch size per GPU of 32 for ViT-L/16 (due to GPU memory constraints) \\nand a batch size of 128 for all other models. All images were resized to \\n448\\u2009×\\u2009448 for both training and inference. We warmed up the learning \\nrate over 250 steps and used the AdamW optimizer with β\\u2009=\\u2009(0.9,\\u20090.999) \\nwith fp16 automatic mixed precision training. For each model, we swept \\nthe learning rate over {1\\u2009×\\u200910−6, 1\\u2009×\\u200910−5, 1\\u2009×\\u200910−4, 1\\u2009×\\u200910−3, 1\\u2009×\\u200910−2} using \\nthe validation set. We trained for a maximum of 20 epochs and moni-\\ntored the validation performance for early stopping with a patience \\nof five epochs, using the best-performing model on the validation set \\nfor evaluation on the test set. We increased the maximum number of \\nepochs to 40 and 80 for training with 10% labels and 1% labels, respec-\\ntively, to account for the fewer training iterations per epoch, and we \\nsimilarly increased the early-stopping patience to 10 and 20 epochs, \\nrespectively. We used standard data augmentation techniques during \\ntraining, including random horizontal and vertical flips, discrete angle \\nrotation (θrot\\u2009∈\\u2009{0,\\u200990,\\u2009180,\\u2009270}) and color jittering (brightness, 16/255; \\ncontrast, 0.125; saturation, 0.075; hue, 0.01).\\nCaptioning with fine-tuning\\nImage captioning has been a widely explored task in the general visual- \\nlanguage domain36,78,79. In addition to distilling a top-level diagnosis of \\nthe image, image captioning can potentially provide morphological \\nand contextual details, as well as additional interpretability, offering \\na much richer set of information than discrete labels. While prior stud-\\nies44,54,56 in visual-language pretraining showed applications in classifi-\\ncation and retrieval, they are not equipped with generative capabilities. \\nBy adding a generative loss along with alignment and a text encoder \\nmodule using the CoCa framework, our model is augmented with the \\nability to generate text conditioned on image inputs. We explored the \\ncaptioning capabilities of CONCH on image–caption pairs extracted \\nfrom a held-out source, source A, where a board-certified pathologist \\nmanually reviewed and condensed each caption such that it retained \\nonly information that could be inferred from the image, including \\nthe top-level diagnosis and detailed morphological descriptions. \\nGiven that our pretraining data were far from the scale of high-quality \\nzero-shot captioning, we performed fine-tuning on the dataset. We \\npartitioned the dataset into training, validation and testing splits and \\nfine-tuned CONCH and baselines. Because PLIP and BiomedCLIP are not \\nreadily adaptable to captioning tasks, we compared the results against \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nGenerativeImage2T ext (GIT)78, a widely used family of open-source \\nvisual-language pretrained models for image captioning.\\nWe fine-tuned the entire model on a small training set of image–\\ncaption pairs. When fine-tuning CONCH, we simply set the contrastive \\nloss to zero and kept only the captioning loss in the training objec-\\ntive. T o evaluate performance, we report the commonly used metrics \\nMETEOR (metric for evaluation of translation with explicit ordering)80 \\nand ROUGE (recall-oriented understudy for gisting evaluation)81. For \\neach model, we trained for a maximum of 40 epochs and selected the \\ncheckpoint with the highest METEOR on the validation set using an \\nearly-stopping patience of 10 epochs. At inference time, we gener-\\nated captions using top-K sampling82 as the decoding strategy with \\nK\\u2009=\\u200950, where, at each timestep, the K most likely tokens were filtered \\nand the probability mass was redistributed before sampling. Similar \\nto zero-shot classification and retrieval, we set the maximum image \\nsize to 448\\u2009×\\u2009448. The full set of hyperparameters used to fine-tune \\ncaptioning is presented in Supplementary Table 37.\\nEvaluation metrics\\nFor classification tasks, we report balanced accuracy, weighted F1 score \\nand the area under the receiver operating characteristic curve (AUROC). \\nBalanced accuracy is defined as the macro average of the recall of each \\nclass. Weighted F1 score is computed by taking the average of the F1 score \\n(the harmonic mean of precision and recall) of each class, weighted by \\nthe support of each class. In the binary case, the AUROC is calculated \\nfrom a plot of the true positive rate against the false positive rate as the \\nclassification threshold is varied. The AUROC is generalized to the mul-\\nticlass case by averaging over the AUROC of all pairwise combinations of \\nclasses. For retrieval, we used the metric Recall@K, which is the propor-\\ntion of the data correctly retrieved among the top-K retrieved samples. \\nFollowing ALIGN31, we chose K\\u2009∈\\u2009{1,\\u20095,\\u200910}, and we also computed the \\nmean recall, which averages over the Recall@K values. For segmentation, \\nwe report the Dice score, which is the same as the F1 score, and the preci-\\nsion and recall score, macro-averaged across all images and classes. For \\ncaptioning, we report METEOR and ROUGE for comparing the predicted \\ncaption with the ground-truth caption. METEOR80 is a metric based on \\nunigram matching that considers both precision and recall between the \\noriginal and ground truth and takes into account synonyms and word \\nforms. ROUGE81 computes the overlap of n-grams between the predicted \\ncaption and ground truth. We used ROUGE-1, which considers unigrams.\\nDownstream evaluation datasets\\nSource A was a dataset of image–caption pairs extracted from a \\nheld-out source. We split multipanel figures and matched them with \\ncaptions manually. Because we also used this dataset for captioning, \\nand because the captions were generally noisy and often contained \\ninformation not present in the images, a board-certified pathologist \\ncleaned the text, and we used the cleaned version for all downstream \\ntasks. After filtering and cleaning, we obtained 797 images with an \\naverage width of 570\\u2009pixels and an average height of 428\\u2009pixels. We used \\nthis dataset in its entirety for cross-modal retrieval. We also used this \\ndataset for captioning after performing a 70–10–20 split for training, \\nvalidation and testing. T o avoid information leakage, the dataset split \\nwas performed at the figure level (taking into account multifigure \\npanels that were separated).\\nSource B was a dataset of image–caption pairs extracted from a \\nheld-out source. Similar to source A, we split multipanel figures and \\nmatched them with captions manually. After filtering and cleaning, \\nwe obtained 1,755 images with an average width of 512\\u2009pixels and an \\naverage height of 410\\u2009pixels. Because the dataset was much bigger than \\nsource A, we did not perform manual cleaning of the captions. We used \\nthis dataset for cross-modal retrieval.\\nTCGA LUAD consisted of 165 image–caption pairs extracted from \\n49 LUAD H&E histopathology slides from TCGA (https://portal.gdc.\\ncancer.gov/).\\nFor each slide, a board-certified pathologist chose up to five tiles \\nof interest from each slide and provided captions describing the tis-\\nsue pattern and any notable morphological features. This process \\nyielded a set of 165 image tiles with an average width of 656\\u2009pixels \\nand an average height of 642\\u2009pixels. We used this set of image tiles for \\ncross-modal retrieval.\\nTCGA BRCA consisted of BRCA H&E formalin-fixed paraffin- \\nembedded (FFPE) diagnostic histopathology WSIs from TCGA. This \\ndataset consisted of cases for primary IDC and ILC. After removing slides \\nwith missing metadata, we collected a total of 1,048 slides (837 IDC and \\n211 ILC). The zero-shot test set was a sampled subset of the full TCGA \\nRCC dataset consisting of 150 WSIs (75 for each class). For the supervised \\nlearning experiments, we held out the zero-shot test set as the test set and \\nused the remaining slides as the supervised training set after excluding \\nslides from patients who appeared in the test set. This process yielded \\na training set of 881 slides (754 IDC and 127 ILC; see Supplementary \\nTable 38 for prompts used for each class in zero-shot classification).\\nTCGA NSCLC consisted of NSCLC H&E FFPE diagnostic histopa-\\nthology WSIs from TCGA. This dataset consisted of cases of primary \\nLUAD and lung squamous cell carcinoma (LUSC). After removing slides \\nwith missing or incorrect metadata, we collected a total of 1,041 slides \\n(529 LUAD and 512 LUSC). The zero-shot test set was a sampled subset \\nof the full TCGA RCC dataset consisting of 150 WSIs (75 for each class). \\nFor the supervised learning experiments, we held out the zero-shot \\ntest set as the test set and used the remaining slides as the supervised \\ntraining set after excluding slides from patients who appeared in the \\ntest set. This process yielded a training set of 846 slides (432 LUAD and \\n414 LUSC; see Supplementary Table 38 for prompts used for each class \\nin zero-shot classification).\\nTCGA RCC consisted of RCC H&E FFPE diagnostic histopathology \\nWSIs from TCGA. This dataset consisted of cases of primary clear cell \\nRCC (CCRCC), papillary RCC (PRCC) and chromophobe RCC (CHRCC). \\nAfter removing slides missing low-resolution downsamples, we col-\\nlected a total of 922 WSIs (519 CCRCC, 294 PRCC and 109 CHRCC). The \\nzero-shot test set was a sampled subset of the full TCGA RCC dataset \\nconsisting of 225 WSIs (75 for each of the three classes). For the super-\\nvised learning experiments, we held out the zero-shot test set as the \\ntest set and used the remaining slides as the supervised training set \\nafter excluding slides from patients who appeared in the test set. This \\nprocess yielded a training set of 693 slides (444 CCRCC, 215 PRCC and \\n34 ChRCC; see Supplementary Table 38 for prompts used for each class \\nin zero-shot classification).\\nDHMC LUAD83 consisted of 143 H&E LUAD slides, each labeled with \\nthe primary histologic growth pattern (59 acinar, 51 solid, 19 lepidic, 9 \\nmicropapillary and 5 papillary). We only used this dataset for zero-shot \\nclassification (see Supplementary Table 39 for prompts used for each \\nclass in zero-shot classification).\\nCRC100k84 consisted of 224\\u2009×\\u2009224\\u2009pixel image tiles at 0.5\\u2009µm per \\npixel (mpp) extracted from 50 patients with colorectal adenocarci-\\nnoma. Each image belonged to one of nine classes: adipose, back-\\nground, debris, lymphocytes, mucus, smooth muscle, normal colon \\nmucosa, cancer-associated stroma or colorectal adenocarcinoma \\nepithelium. For the supervised dataset, we used the officially provided \\nsplits of 100,000 images in the training set and 7,180 images in the test \\nset. For the zero-shot test set, we used only the official test set (see \\nSupplementary Table 40 for prompts used for each class in zero-shot \\nclassification).\\nWSSS4LUAD85 consisted of LUAD image tiles of around 200–\\n500\\u2009pixels in dimension, each labeled as tumor, tumor-associated \\nstroma and/or normal. For our evaluation, we filtered for the samples \\nwith only one ground-truth label. We were left with 4,693 images from \\nthe official training split (see Supplementary Table 41 for prompts used \\nfor each class in zero-shot classification).\\nSICAP75 consisted of 512\\u2009×\\u2009512\\u2009pixel images extracted from 155 \\nWSIs of core-needle biopsies of prostate cancer, digitized at ×10 \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nmagnification. The official training and testing split partitioned the \\ndataset into 9,959 images from 124 WSIs for training and 2,122 images \\nfrom 31 WSIs for testing. Each tile was labeled with the primary Gleason \\npattern (G3, G4 or G5) or as noncancerous (NC). For zero-shot classifica-\\ntion, we used only the official test set for evaluation, while, for super-\\nvised classification, we used the official splits for training and testing. \\nFor zero-shot segmentation (tumor versus benign), we used the slides \\nfrom the official test split and corresponding pixel-level segmentation \\nmask for evaluation (combining Gleason patterns G3, G4 and G5 as the \\ntumor class; see Supplementary Table 41 for prompts used for each \\nclass in zero-shot classification and segmentation).\\nDigestPath86 consisted of 660 colonoscopy H&E tissue section \\nimages from 324 patients, acquired at ×20-equivalent magnification. \\nWe used the subset of 250 images from 93 patients for which pixel-level \\nlesion annotation for colorectal cancer tissue was provided, and we \\nperformed zero-shot segmentation evaluation (see Supplementary \\nTable 41 for prompts used for each class in zero-shot segmentation).\\nEBRAINS87,88 consisted of H&E histopathology WSIs of brain tis-\\nsue from the EBRAINS Digital Tumor Atlas. We used a subset of 2,319 \\nslides corresponding to a 30-way fine-grained brain tumor subtyping \\ntask, where only classes with at least 30 slides were kept to ensure that \\na reasonable number of slides were available for both model training \\nand evaluation. For the supervised dataset, we performed a 50–25–25 \\nsplit for training (1,151 slides), validation (595 slides) and testing (573 \\nslides). For the zero-shot test set, we used the testing split of 573 slides \\n(see Supplementary Tables 42–44 for prompts used for each class in \\nzero-shot classification). The WSI counts for each class in the dataset \\nwere as follows: (1) IDH1-wild-type glioblastoma (474 slides); (2) pilo-\\ncytic astrocytoma (173 slides); (3) meningothelial meningioma (104 \\nslides); (4) pituitary adenoma (99 slides); (5) IDH1-mutant and 1p/19q \\ncodeleted anaplastic oligodendroglioma (91 slides); (6) ganglioglioma \\n(88 slides); (7) hemangioblastoma (88 slides); (8) adamantinomatous \\ncraniopharyngioma (85 slides); (9) IDH1-mutant and 1p/19q codeleted \\noligodendroglioma (85 slides); (10) atypical meningioma (83 slides); \\n(11) schwannoma (81 slides); (12) IDH1-mutant diffuse astrocytoma \\n(70 slides); (13) transitional meningioma (68 slides); (14) diffuse large \\nB cell lymphoma of the central nervous system (59 slides); (15) gliosar-\\ncoma (59 slides); (16) fibrous meningioma (57 slides); (17) anaplastic \\nependymoma (50 slides); (18) IDH1-wild-type anaplastic astrocytoma \\n(47 slides); (19) metastatic tumors (47 slides); (20) IDH1-mutant ana-\\nplastic astrocytoma (47 slides); (21) ependymoma (46 slides); (22) \\nanaplastic meningioma (46 slides); (23) secretory meningioma (41 \\nslides); (24) lipoma (38 slides); (25) hemangiopericytoma (34 slides); \\n(26) IDH1-mutant glioblastoma (34 slides); (27) non-Wingless-related \\nintegration (Wnt)/non-Sonic hedgehog (Shh) medulloblastoma (32 \\nslides); (28) Langerhans cell histiocytosis (32 slides); (29) angiomatous \\nmeningioma (31 slides); and (30) hemangioma (30 slides).\\nProstate Gleason Grading consisted of 228,482 image ROIs of \\nH&E-stained prostate tissue curated from three publicly available \\ndatasets: AGGC89, PANDA90 and SICAP75. In the case of PANDA and AGGC, \\neach ROI was extracted at ×10-equivalent magnification with dimen-\\nsions 512\\u2009×\\u2009512\\u2009pixels and was labeled as NC, G3, G4 or G5, assigned using \\nthe pixel-level annotation masks provided by the respective dataset. \\nWe used this dataset to compare end-to-end fine-tuning performance \\nbetween our model and other vision encoders commonly used in com-\\nputational pathology. We partitioned the dataset at the slide level and \\nsplit the dataset into training (189,000 ROIs from 4,622 slides in PANDA \\nand the AGGC official training set), validation (10,000 ROIs from 124 \\nslides in the SICAP official training set), and testing (29,000 ROIs from \\n92 slides in the official test sets of AGGC and SICAP).\\nWSI processing\\nFor slide-level tasks, the processing pipeline for WSIs consisted of tissue \\nsegmentation, tiling and feature extraction. We used the CLAM library7 \\nfor tissue segmentation, which computes a binary mask for tissue using \\nbinary thresholding along the saturation channel after converting a \\ndownsample of the slide from the RGB to hue–saturation–value (HSV) \\ncolor space. Median blurring and morphological closing were used \\nto smooth tissue contours and remove artifacts. The contours were \\nfiltered by area to yield the segmentation mask. For zero-shot and \\nsupervised classification, we followed previous conventions7,62 and \\ndivided the segmented tissue regions into contiguous 256\\u2009×\\u2009256\\u2009pixel \\ntiles at ×10-equivalent magnification. For segmentation, we extracted \\ntiles using a smaller tile size (224\\u2009×\\u2009224\\u2009pixels) with 75% overlap at the \\nhighest magnification possible (that is, ×10 for SICAP and ×20 for \\nDigestPath) to achieve more fine-grained predictions. After tiling, for \\nfeature extraction, we resized all tiles to 224\\u2009×\\u2009224\\u2009pixels and computed \\nembeddings for each tile independently using a frozen pretrained \\nimage encoder, before caching them for downstream evaluation.\\nPretraining dataset characterization\\nWe estimated the distribution of topics covered by our pretrain-\\ning captions. We first created a list of 19 topics that covered major \\nanatomical sites relevant to the study of pathology. For each topic, \\na board-certified pathologist then curated a list of keywords associated \\nwith the topic. We then mapped a caption to a topic if it contained a \\nspecific word. Because it was impractical to curate an exhaustive set \\nof keywords to cover all captions, we used k-nearest neighbors (kNN) \\nwith k\\u2009=\\u20095 to categorize the remaining captions. The distribution of \\ncaptions on the topics is shown in Fig. 1b. Within each topic (as well \\nas the overall dataset), we qualitatively visualized the contents of the \\ncaptions using wordclouds (Extended Data Fig. 1).\\nStatistical analysis\\nNonparametric bootstrapping with 1,000 samples was used to con-\\nstruct 95% confidence intervals for model performance. For each evalu-\\nation metric, observed differences in model performance were tested \\nfor statistical significance using a two-sided paired permutation test \\nwith 1,000 permutations. In each permutation, independent predic-\\ntions of two models were randomly swapped to obtain a new difference \\nin model performance. The P value was the proportion of differences \\nin model performance greater than the observed difference in terms \\nof absolute value. The null hypothesis was that there was no difference \\nin model performance for the given test set and evaluation metric.\\nComputing hardware and software\\nWe used Python (version 3.8.13) for all experiments and analyses in the \\nstudy, which can be replicated using open-source libraries as outlined \\nbelow. For task-agnostic pretraining, we used eight 80-GB NVIDIA A100 \\nGPUs configured for multi-GPU training using DistributedDataParal-\\nlel (DDP) as implemented by the popular open-source deep learn-\\ning framework PyT orch (version 2.0.0, CUDA 11.7) (https://pytorch.\\norg). All downstream experiments were conducted on single 24-GB \\nNVIDIA 3090 GPUs. For unimodal pretraining of our visual encoder \\nusing iBOT, we modified the ViT implementation maintained by the \\nopen-source Timm library (version 0.9.2) from Hugging Face (https://\\nhuggingface.co) for the encoder backbone and used the original iBOT \\nimplementation (https://github.com/bytedance/ibot) for training. For \\nnatural language processing (NLP) workflows, we used open-source \\nlibraries provided by Hugging Face. Notably, we used Transformers \\n(version 4.27.3) and Accelerate (version 0.15.0) for tokenization of text \\ndata and unimodal pretraining of our language model, and we used \\nEvaluate (version 0.4.0) for accessing common machine translation \\nand image captioning metrics including ROUGE (from rouge-score \\nversion 0.1.2) and METEOR (from nltk version 3.6.7). We integrated \\nour pretrained unimodal visual encoder and language model into \\nthe open clip library (version 2.14.0) for visual-language pretrain-\\ning using the CoCa framework. All WSI processing was supported by \\nOpenSlide (version 4.3.1) and openslide-python (version 1.2.0). We \\nused Scikit-learn (version 1.2.1) for its implementation of common \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nmachine learning model evaluation metrics for image classification \\nand to train logistic regression models for linear probe experiments. \\nNumpy (version 1.20.3) and Pandas (version 1.5.3) were used data col-\\nlection and preparation. Implementations of other visual-language \\nmodels benchmarked in the study were found on the Hugging Face \\nmodel hub (https://huggingface.co/models): PLIP (https://hugging-\\nface.co/vinid/plip), BiomedCLIP (https://huggingface.co/microsoft/\\nBiomedCLIP-PubMedBERT_256-vit_base_patch16_224), OpenAICLIP \\n(https://huggingface.co/openai/clip-vit-base-patch16), GIT-base \\n(https://huggingface.co/microsoft/git-base) and GIT-large (https://\\nhuggingface.co/microsoft/git-large). Pillow (version 9.3.0) and \\nOpencv-python were used to perform basic image processing tasks. \\nMatplotlib (version 3.7.1) and Seaborn (version 0.12.2) were used to \\ncreate plots and figures. Usage of other miscellaneous Python libraries \\nis listed in the Nature Portfolio Reporting Summary.\\nReporting summary\\nFurther information on research design is available in the Nature \\nPortfolio Reporting Summary linked to this article.\\nData availability\\nTCGA whole-slide data and labels are available from the NIH genomic \\ndata commons (http://portal.gdc.cancer.gov). DHMC LUAD whole-slide \\ndata and labels can be accessed through the Dartmouth Biomedical \\nInformatics Research and Data Science website (http://bmirds.github.\\nio/LungCancer/). SICAP whole-slide and tile data with corresponding \\nlabels can be accessed through the data portal at http://data.mende-\\nley.com/datasets/9xxm58dvs3/1. CRC100k tile data and labels can be \\nfound at http://zenodo.org/record/1214456. WSSS4LUAD image tiles \\nand labels can be found at http://wsss4luad.grand-challenge.org/. \\nPretraining data were curated from image–caption pairs in educa-\\ntional resources and PubMed. EBRAINS WSIs can be found at http://\\nsearch.kg.ebrains.eu/instances/Dataset/8fc108ab-e2b4-406-8999-\\n60269dc1f994. AGGC and PANDA WSIs can be accessed through their \\nrespective Grand Challenge portals (http://aggc22.grand-challenge.\\norg/data/ and http://panda.grand-challenge.org/data/). The unpro-\\ncessed PubMed Central Open Access dataset is available from the NIH \\nPubMed Central website (http://ncbi.nlm.nih.gov/pmc/tools/open-\\nftlist/). Restrictions apply to the availability of anonymized patient \\ndata that were used retrospectively for this project with institutional \\npermission and are, thus, not publicly available. All requests for pro-\\ncessed or raw data collected or curated in house should be made to the \\ncorresponding author and will be evaluated according to institutional \\nand departmental policies to determine whether the data requested \\nare subject to intellectual property or patient privacy  obligations.\\nCode availability\\nModel weights for CONCH can be assessed for academic research pur-\\nposes at http://huggingface.co/MahmoodLab/conch. Code for using \\nthe pretrained model is provided at http://github.com/mahmoodlab/\\nCONCH. We have documented all technical deep learning methods and \\nsoftware libraries used in the study while ensuring the paper is acces-\\nsible to the broader clinical and scientific audience.\\nReferences\\n71. Redmon, J., Divvala, S., Girshick, R. & Farhadi, A. You Only Look \\nOnce: unified, real-time object detection. In Proc. IEEE Conference \\non Computer Vision and Pattern Recognition 779–788 (IEEE, 2016).\\n72. Luo, R. et al. BioGPT: generative pre-trained transformer for \\nbiomedical text generation and mining. Brief. Bioinform. 23, \\nbbac409 (2022).\\n73. Dosovitskiy, A. et al. An image is worth 16×16 words: transformers \\nfor image recognition at scale. In 9th International Conference \\non Learning Representations https://openreview.net/\\nforum?id=YicbFdNTTy (OpenReview.net, 2021).\\n74. Zhou, J. et al. Image BERT pre-training with online tokenizer.  \\nIn 10th International Conference on Learning Representations \\nhttps://openreview.net/forum?id=ydopy-e6Dg (OpenReview.net, \\n2022).\\n75. Silva-Rodriguez, J., Colomer, A., Dolz, J. & Naranjo, V. Self-learning \\nfor weakly supervised Gleason grading of local patterns. IEEE J. \\nBiomed. Health Inform. 25, 3094–3104 (2021).\\n76. Dice, L. R. Measures of the amount of ecologic association \\nbetween species. Ecology 26, 297–302 (1945).\\n77. Kolesnikov, A., Zhai, X. & Beyer, L. Revisiting self-supervised  \\nvisual representation learning. In Proc. IEEE/CVF Conference  \\non Computer Vision and Pattern Recognition 1920–1929 (IEEE, \\n2019).\\n78. Wang, J. et al. GIT: a generative image-to-text transformer for \\nvision and language. Trans. Mach. Learn. Res. https://openreview.\\nnet/forum?id=b4tMhpN0JC (2022).\\n79. Li, J., Li, D., Savarese, S. & Hoi, S. BLIP-2: bootstrapping \\nlanguage–image pre-training with frozen image encoders and \\nlarge language models. In Proc. 40th International Conference \\non Machine Learning (eds Krause, A. et al.) 19730–19742 (PMLR, \\n2023).\\n80. Banerjee, S. & Lavie, A. METEOR: an automatic metric for MT \\nevaluation with improved correlation with human judgments. In \\nProc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures \\nfor Machine Translation and/or Summarization 65–72 (Association \\nfor Computational Linguistics, 2005).\\n81. Lin, C.-Y. ROUGE: a package for automatic evaluation of \\nsummaries. In Text Summarization Branches Out 74–81 \\n(Association for Computational Linguistics, 2004).\\n82. Lewis, M., Dauphin, Y. & Fan, A. Hierarchical neural story \\ngeneration. In Proc. 56th Annual Meeting of the Association for \\nComputational Linguistics (eds Gurevych, I. & Miyao, Y.) 889–898 \\n(Association for Computational Linguistics, 2018).\\n83. Wei, J. W. et al. Pathologist-level classification of histologic \\npatterns on resected lung adenocarcinoma slides with deep \\nneural networks. Sci. Rep. 9, 3358 (2019).\\n84. Kather, J. N. et al. Predicting survival from colorectal cancer \\nhistology slides using deep learning: a retrospective multicenter \\nstudy. PLoS Med. 16, e1002730 (2019).\\n85. Han, C. et al. WSSS4LUAD: Grand Challenge on weakly-supervised \\ntissue semantic segmentation for lung adenocarcinoma.  \\nPreprint at https://doi.org/10.48550/arXiv.2204.06455  \\n(2022).\\n86. Da, Q. et al. DigestPath: a benchmark dataset with challenge \\nreview for the pathological detection and segmentation of \\ndigestive-system. Med. Image Anal. 80, 102485 (2022).\\n87. Roetzer-Pejrimovsky, T. et al. The Digital Brain Tumour Atlas, an \\nopen histopathology resource. Sci. Data 9, 55 (2022).\\n88. Roetzer-Pejrimovsky, T. et al. The Digital Brain Tumour Atlas, an \\nopen histopathology resource [Data set]. EBRAINS https://doi.org/ \\n10.25493/WQ48-ZGX (2022).\\n89. Huo, X. et al. Comprehensive AI model development for Gleason \\ngrading: from scanning, cloud-based annotation to pathologist–\\nAI interaction. Preprint at SSRN https://doi.org/10.2139/\\nssrn.4172090 (2022).\\n90. Bulten, W. et al. Artificial intelligence for diagnosis and Gleason \\ngrading of prostate cancer: the PANDA challenge. Nat. Med. 28, \\n154–163 (2022).\\nAcknowledgements\\nWe thank Jinghao Zhou for providing insights into the training \\ndynamics for iBOT. We thank A. Song for his feedback. This work \\nwas supported in part by the BWH president’s fund, BWH and MGH \\nPathology, and NIH NIGMS R35GM138216 (F.M.). M.Y.L. was also \\nsupported by the Siebel Scholars program. D.F.K.W. was also funded by \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nthe NIH NCI Ruth L. Kirschstein National Service Award, T32CA251062. \\nR.J.C. was also supported by the NSF Graduate Fellowship. T.D. was \\nalso supported by the Harvard SEAS Fellowship. G.G. was supported \\nby the BWH president’s scholar award, NIGMS R35GM149270, NIDDK \\nP30DK034854 and the Massachusetts Life Sciences Center. We \\nthank T. Janicki, R. Kenny and the system administration staff at the \\nMGB Enterprise Research Infrastructure and Services (ERIS) research \\ncomputing core for maintaining the GPU computing resources that \\nwere instrumental in this study. We also thank T. Mages and T. Ramsey \\nfor their administrative support. The content is solely the responsibility \\nof the authors and does not reflect the official views of the National \\nInstitutes of Health or the National Science Foundation.\\nAuthor contributions\\nF.M., M.Y.L., B.C. and D.F.K.W. conceptualized the study and designed \\nthe experiments. M.Y.L., B.C., R.J.C., T.D., I.L., D.F.K.W., I.O. and L.P.L. \\nperformed collection and cleaning of data used for unimodal and \\nvisual-language pretraining. M.Y.L., B.C. and R.J.C. performed model \\ndevelopment. M.Y.L., B.C., D.F.K.W. and G.J. performed experimental \\nanalysis. M.Y.L., B.C., D.F.K.W., A.Z., R.J.C., I.L., T.D., G.J., F.M., G.G., L.P.L. \\nand A.V.P. interpreted experimental results and provided feedback on \\nthe study. M.Y.L., B.C., D.F.K.W. and F.M. prepared the paper with input \\nfrom all co-authors. F.M. supervised the research.\\nCompeting interests\\nM.Y.L., B.C., R.J.C. and F.M. are inventors on a provisional US patent \\n(application number 63/610,645) filed corresponding to the \\nmethodological aspects of this work.\\nAdditional information\\nExtended data is available for this paper at  \\nhttps://doi.org/10.1038/s41591-024-02856-4.\\nSupplementary information The online version  \\ncontains supplementary material available at  \\nhttps://doi.org/10.1038/s41591-024-02856-4.\\nCorrespondence and requests for materials should be addressed to \\nFaisal Mahmood.\\nPeer review information Nature Medicine thanks Andrew Beck,  \\nLee Cooper and Geert Litjens for their contribution to the peer review \\nof this work. Primary Handling Editor: Lorenzo Righetto,  \\nin collaboration with the Nature Medicine team.\\nReprints and permissions information is available at  \\nwww.nature.com/reprints.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 1 | Caption content of pre-training dataset. Wordclouds of captions to qualitatively visualize the caption content of each category in the pre-\\ntraining dataset. Larger words are more represented in the captions. Common articles, nouns, and verbs are ignored.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 2 | Zero-shot classification: single prompt vs. ensembling. \\na-d, slide-level tasks. e, ROI-level tasks. We compare using a single text prompt \\nper class vs. ensembling over multiple class names and templates. Since zero-\\nshot performance of a visual-language pretrained model can be sensitive to \\nthe prompts used52 when using a single prompt per class, for each class, we \\nindependently randomly sample a prompt from the pool of candidate templates \\nand class names (see Supplementary Data Tables 34-38 for the prompt pools). \\nWe randomly sample 50 sets of prompts for each task, and plot the resulting \\ndistribution of zero-shot performance for each model using boxplot. Each dot \\ncorresponds to a single set of prompts (n\\u2009=\\u200950 for each box). Boxes indicate \\nquartile values, and whiskers extend to data points within 1.5\\u2009×\\u2009the interquartile \\nrange. Triangles indicate the performance of prompt ensembling. For slide-\\nlevel tasks, we show performance for all Ks used in top-K pooling. We observe \\nprompt ensembling can substantially boost performance (relative to the \\nmedian performance of randomly sampled single prompts) for most models in \\nmost tasks, except when the median performance is near random chance, such \\nas for OpenAICLIP on most tasks and PLIP on TCGA BRCA. The poor median \\nperformance in these scenarios indicates that the model fails to perform under \\nthe majority of prompts sampled and therefore it is unsurprising that the \\nensembled prompt performs equally bad or worse. See Supplementary Data \\nTables 1-14 for more results.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 3 | CONCH heatmaps, renal cell carcinomas. Pathologist-\\nannotated H&E images, corresponding cosine-similarity heatmaps of, from top \\nto bottom, papillary, chromophobe, and clear cell renal cell carcinomas. Tiles of \\nhigh similarity (red border) and low similarity (black border) with the predicted \\nclass label are randomly sampled and displayed next to each heatmap. We find \\nexcellent agreement between the annotated image and the regions of the slide \\nwith high similarity, with the tiles demonstrating stereotypical morphology \\nof the tumors within the high-similarity regions and stroma or other normal \\nconstituents of the kidney in the low similarity regions.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 4 | CONCH heatmaps, non-small cell lung carcinomas. \\nPathologist-annotated H&E images, corresponding cosine-similarity heatmaps \\nof adenocarcinoma (top) and squamous cell carcinoma (bottom) of the lung. \\nTiles of high similarity (red border) and low similarity (black border) with the \\npredicted class label are randomly sampled and displayed next to each heatmap. \\nWe find excellent agreement between the annotated image and the regions of the \\nslide with high similarity, with the tiles demonstrating stereotypical morphology \\nof the tumors within the high-similarity regions and stroma or other normal \\nconstituents of the lung in the low similarity regions.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 5 | CONCH heatmap, lobular carcinoma of the breast. \\nPathologist-annotated H&E image, corresponding cosine-similarity heatmap \\nof lobular carcinoma of the breast. Tiles of high similarity (red border) and low \\nsimilarity (black border) with the predicted class label are randomly sampled and \\ndisplayed next to the heatmap. As with the ductal carcinoma heatmap in Fig. 2e, \\nwe find excellent agreement between the annotated image and the regions of the \\nslide with high similarity, with the tiles demonstrating stereotypical morphology \\nof lobular caricnoma within the high-similarity regions and stroma or other \\nnormal constituents of the breast in the low similarity regions.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 6 | ROI-level few-shot classification experiments. a, b. We \\ninvestigate the label efficiency of different visual-language pretrained encoders \\nin the few-shot setting where we vary the number of training labels per class (nc), \\nfor nc= 1,2,4,8,16,… up to 512. For each nc, we sample 5 different sets of training \\nexamples and perform linear probing on each training set using associated \\nimage labels (see Supervised classification experiments for details). We show \\ntheir individual model performance via boxplot (i.e., n\\u2009=\\u20095 for each box) to study \\nthe variance in model performance when performing supervised learning with \\nvery few training examples. Boxes indicate quartile values and whiskers extend \\nto data points within 1.5\\u2009×\\u2009the interquartile range. For reference, the zero-shot \\nperformance of each model is shown as a dotted line on the same plot. In terms \\nof few-shot supervised learning, CONCH achieves better performance (i.e. in \\nterms of the median accuracy of 5 runs) than other encoders for different sizes \\nof training set and for all tasks. Additionally, in SICAP, we find CONCH zero-shot \\nperformance to be competitive with PLIP and BiomedCLIP few-shot up to 64 \\nlabels per class.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 7 | Rare disease classification results on EBRAINS.  \\na. Weakly-supervised ABMIL performance for CONCH and other pretrained \\nencoder models on the EBRAINS 30-class brain tumor subtyping task (n\\u2009=\\u2009573). \\nError bars represent 95% confidence intervals; the center is the computed \\nvalue of balanced accuracy. b. We investigate the label efficiency of different \\npretrained encoders in the few-shot setting where we vary the number of \\ntraining labels per class (nc), for ncϵ{1,\\u20092,\\u20094,\\u20098,\\u200916}. For each nc, we sample 5 \\ndifferent sets of training examples and follow the experimental protocol in a \\nto train an ABMIL model on each training set using associated slide labels (see \\nSupervised classification experiments for details). We show their individual \\nmodel performance via boxplot (i.e., n\\u2009=\\u20095 for each box) to study the variance in \\nmodel performance when performing supervised learning with very few training \\nexamples. Boxes indicate quartile values and whiskers extend to data points \\nwithin 1.5\\u2009×\\u2009the interquartile range. For reference, the zero-shot performance \\nof each model is shown as a dotted line on the same plot. Additional metrics are \\nreported in Supplementary Data Table 20 - 21. We find that CONCH consistently \\noutperform all other visual language pretrained models in zeroshot classification \\nand all pretrained encoders in weakly-supervised learning in terms of both \\nperformance and label efficiency.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 8 | Additional Retrieval Examples. Retrieved examples \\n(among top 10) using complex prompts with detailed morphological \\ninformation. Images are from an in-house dataset of tiles sampled from 1,620 \\ncases held-out during pretraining, spanning 108 OncoTree codes (5 for each \\ncode). Similarity scores between each image and prompt are shown in the  \\ntop-right corner of each image.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 9 | Image captioning results. a. Captioning performance \\nof CONCH and baselines fine-tuned on Source A (train n=558, validation n=77, \\ntest n=162). The METEOR and ROUGE metrics are both calculated to evaluate the \\nquality of generated captions. Captions were generated using top-K sampling \\nwith K=50 as the decoding strategy. Error bars representing 95% confidence \\nintervals; the center is the computed value of each metric indicated by the \\nx-axis label. CONCH outperforms both GIT baselines with p\\u2009<\\u20090.01. Although \\nour absolute performance on these metrics is not ideal, image captioning is a \\nconsiderably more difficult task than classification and retrieval, and we show \\nthat our pretraining data and approach can significantly improve performance \\nover general visual-language models. b. Examples of captions generated by \\nCONCH considered by a pathologist to be high quality. The green text boxes \\nshow generated captions and gray text boxes show captions corrected by a \\npathologist. c. Examples of partially correct captions generated by CONCH. \\nReasonably correct portions of the generated caption are highlighted in blue. \\nIn general, we noticed that some of the generated captions are regurgitated \\nverbatim from the training dataset, likely due to the limited scale of fine-tuning \\n(training split: n=558). Given that our current pretraining scale is still relatively \\nsmall compared to works in the general visual-language domain, we expect the \\nfine-tuned captioning performance to potentially improve substantially with \\nmore high-quality training data.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02856-4\\nExtended Data Fig. 10 | CONCH pretraining ablations. In a, b, error bars \\nrepresent 95% confidence intervals and the centres correspond to computed \\nvalues of each metric as specified by the legend (left) or the y-axis label \\n(middle, right). a. Comparison between CONCH pretrained on human-only \\ndata (n\\u2009=\\u20091,170,647) using CoCa vs. human-only data using CLIP vs. H&E only \\ndata (n\\u2009=\\u2009457,372) vs. the full unfiltered dataset (n\\u2009=\\u20091,786,362). Left. Zero-shot \\nperformance on downstream subtyping (TCGA BRCA, n\\u2009=\\u2009150; TCGA RCC, n\\u2009=\\u2009225; \\nTCGA NSCLC, n\\u2009=\\u2009150; DHMC LUAD, n\\u2009=\\u2009143; CRC100k, n\\u2009=\\u20097,\\u2009180; WSSS4LUAD, \\nn\\u2009=\\u20094,\\u2009693) and grading (SICAP, n\\u2009=\\u20092,\\u2009122) tasks. Following pre-established \\nconventions, quadratically weighted Cohen’s κ is reported for SICAP and \\nCohen’s κ is reported for DHMC LUAD, while balanced accuracy is reported for \\nall other tasks. CONCH performs the best on average. Middle and right. Model \\nperformance in cross-modal retrieval on 3 datasets of image-text pairs (Source A,  \\nn\\u2009=\\u2009797; Source B, n\\u2009=\\u20091,755; TCGA LUAD, n\\u2009=\\u2009165). CONCH (CLIP) performs the \\nbest on average. b. Comparison between CONCH and no domain-specific \\nunimodal pretraining. CONCH (No vision pretraining) replaces the image \\nencoder pretrained on histopathology image patches with an analogous encoder \\npretrained on ImageNet. CONCH (No language pretraining) initializes the text \\nencoder randomly instead of pretraining on pathology-related text. Left. Zero-\\nshot performance on subtyping and grading tasks. Middle and right. Cross-\\nmodal retrieval performance.\\n1 nature portfolio  |  reporting summaryApril 2023\\nCorresponding author(s): Faisal Mahmood\\nLast updated by author(s): Mar 3, 2024\\nReporting Summary\\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \\nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\\nStatistics\\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\\nn/a Confirmed\\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\\nThe statistical test(s) used AND whether they are one- or two-sided \\nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\\nA description of all covariates tested\\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \\nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \\nGive P values as exact values whenever suitable.\\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\\nOur web collection on statistics for biologists contains articles on many of the points above.\\nSoftware and code\\nPolicy information about availability of computer code\\nData collection Scripts for data collection and preparation was written in Python (version 3.8.13) using the libraries numpy (version 1.20.3), pandas (version \\n1.5.3), and Pillow (version 9.3.0).\\nData analysis All WSI processing was supported by OpenSlide (version 4.3.1) and openslide-python (version 1.2.0). The CLAM package was used for feature \\nextraction (http://github.com/mahmoodlab/CLAM). Code for data analysis was written in Python (version 3.8.13). The following Python \\nlibraries were used for analysis: HuggingFace Evaluate (version 0.4.0), nltk (version 3.6.7), rouge-score (version 0.1.2), scikit-learn (version \\n1.2.1), Pillow (version 9.3.0), opencv-python (version 4.7.0.72), matplotlib (version 3.7.1), seaborn (version 0.12.2), numpy (1.20.3), pandas \\n(1.5.3), PyTorch (2.0.0, CUDA 11.7).\\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \\nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\\n2 nature portfolio  |  reporting summaryApril 2023\\nData\\nPolicy information about availability of data\\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \\n- Accession codes, unique identifiers, or web links for publicly available datasets \\n- A description of any restrictions on data availability \\n- For clinical datasets or third party data, please ensure that the statement adheres to our policy \\n \\nTCGA whole slide data and labels are available from the NIH genomic data commons (https://portal.gdc.cancer.gov). DHMC LUAD whole slide data and labels can be \\naccessed through the Dartmouth Biomedical Informatics Research and Data Science website (https://bmirds.github.io/LungCancer/). SICAP whole slide and tile data \\nwith corresponding labels can be accessed through a data portal at https://data.mendeley.com/datasets/9xxm58dvs3/1. CRC100k tile data and labels can be found \\nat (https://zenodo.org/record/1214456). WSSS4LUAD image tiles and labels can be found at (https://wsss4luad.grand-challenge.org/). EBRAINS whole slide images \\ncan be found at (https://search.kg.ebrains.eu/instances/Dataset/8fc108ab-e2b4-406f-8999-60269dc1f994). Pretraining data was curated from image-caption pairs \\nin educational resources and PubMed. AGGC and PANDA whole slide images can be accessed through their respective Grand Challenge portal (https://\\naggc22.grand-challenge.org/Data/, https://panda.grand-challenge.org/data/). Educational resources are subject to copyright terms of publishers and will not be \\nmade available. The unprocessed PubMed Central Open Access dataset are available from the NIH PubMed Central website (https://www.ncbi.nlm.nih.gov/pmc/\\ntools/openftlist/). In house data are not publicly available. All requests for data collected or curated in-house will be evaluated based on institutional and \\ndepartmental policies to determine whether the data requested is subject to intellectual property or patient privacy obligations. \\nResearch involving human participants, their data, or biological material\\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), \\nand sexual orientation and race, ethnicity and racism.\\nReporting on sex and gender Gender or sex was not included as a covariate in our experimental analysis at any stage of the study. Data pertaining to sex \\nand gender may have been collected for data used in downstream tasks, which were curated by their original investigators, \\nwe refer readers to their original source for more detailed descriptions. For pretraining data sourced from the web and \\neducational resources, no gender or sex data are made available in a structured format.  For in-house BWH data used for \\npretraining, we provide the aggregate distribution of self-reported sex as follows: 11855 Female, 9575 Male, 12 Unspecified. \\nFor in-house BWH data used for in retrieval analysis (ED figure 8), the distribution is as follows: 908 Female, 711 Male, 1 \\nUnspecified. \\n \\nReporting on race, ethnicity, or \\nother socially relevant \\ngroupings\\nNo covariates regarding race, ethnicity, and other social groupings were collected, used or analyzed in the study. \\nPopulation characteristics No covariates relating to population characteristics were collected, used or analyzed in the study.\\nRecruitment No patient recruitment was necessary for using histology whole slide images retrospectively.  \\nEthics oversight Brigham and Women's Hospital IRB committee approved the retrospective analysis of pathology data. \\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\nField-specific reporting\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\\nLife sciences study design\\nAll studies must disclose on these points even when the disclosure is negative.\\nSample size No sample size calculations were performed. For pretraining data, we internally curated as much paired histology data as possible from  \\neducational resources and the PubMed Central Open Access data portal. After cleaning and curation, we yielded an unfiltered dataset of \\n1,786,362 cases, a human-only subset of 1,170,647 cases (on which we base most of analysis), and a human-only subset of 457,372 cases. For \\ndownstream datasets, see the Downstream evaluation datasets subsection of the Methods section in the manuscript for more details.\\nData exclusions For pretraining data, no additional data exclusions were performed after curation. \\nFor WSSS4LUAD, we excluded data that had more than one ground truth label.  \\nFor EBRAINS, we excluded classes that had fewer than 30 total WSIs.\\nReplication Replication was successful for all experiments in this study.\\n3 nature portfolio  |  reporting summaryApril 2023\\nRandomization For downstream evaluation that required creating train, validation, test splits, we created them randomly. We created 70% train, 10% \\nvalidation, 20% test splits for captioning on the figure level, i.e., ensuring that extracted images from the same figure are only in the same \\nsplit. For few-shot and supervised analyses, all splits were created stratified by class and on a patient level, i.e., ensuring that slides from the \\nsame patient are only in the same split. \\nBlinding Blinding was not necessary for our study because our experiments were based on digitized histology slides or region-level images.\\nReporting for specific materials, systems and methods\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \\nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \\nMaterials & experimental systems\\nn/a Involved in the study\\nAntibodies\\nEukaryotic cell lines\\nPalaeontology and archaeology\\nAnimals and other organisms\\nClinical data\\nDual use research of concern\\nPlants\\nMethods\\nn/a Involved in the study\\nChIP-seq\\nFlow cytometry\\nMRI-based neuroimaging\\n\"}, {'file_name': 'uni', 'text': \"Nature Medicine | Volume 30 | March 2024 | 850–862 850\\nnature medicine\\nArticle\\nhttps://doi.org/10.1038/s41591-024-02857-3\\nT owards a general-purpose foundation \\nmodel for computational pathology\\nRichard J. Chen\\u2009  \\u20091,2,3,4,5,11, Tong Ding1,6,11, Ming Y . Lu1,2,3,4,7,11, \\nDrew F. K. Williamson\\u2009  \\u20091,2,3,11, Guillaume Jaume1,2,3,4, Andrew H. Song1,2,3,4, \\nBowen Chen1,2, Andrew Zhang\\u2009  \\u20091,2,3,4,8, Daniel Shao1,2,3,4,8, \\nMuhammad Shaban1,2,3,4, Mane Williams1,2,3,4,5, Lukas Oldenburg1, \\nLuca L. Weishaupt1,2,3,4,8, Judy J. Wang1, Anurag Vaidya1,2,3,4,8, Long Phi Le2,8, \\nGeorg Gerber\\u2009  \\u20091, Sharifa Sahai1,2,3,4,9, Walt Williams1,6 & \\nFaisal Mahmood\\u2009  \\u20091,2,3,4,10 \\nQuantitative evaluation of tissue images is crucial for computational \\npathology (CPath) tasks, requiring the objective characterization of \\nhistopathological entities from whole-slide images (WSIs). The high \\nresolution of WSIs and the variability of morphological features present \\nsignificant challenges, complicating the large-scale annotation of data for \\nhigh-performance applications. T o address this challenge, current efforts \\nhave proposed the use of pretrained image encoders through transfer \\nlearning from natural image datasets or self-supervised learning on publicly \\navailable histopathology datasets, but have not been extensively developed \\nand evaluated across diverse tissue types at scale. We introduce UNI, a \\ngeneral-purpose self-supervised model for pathology, pretrained using \\nmore than 100\\u2009million images from over 100,000 diagnostic H&E-stained \\nWSIs (>77\\u2009TB of data) across 20 major tissue types. The model was evaluated \\non 34 representative CPath tasks of varying diagnostic difficulty. In addition \\nto outperforming previous state-of-the-art models, we demonstrate \\nnew modeling capabilities in CPath such as resolution-agnostic tissue \\nclassification, slide classification using few-shot class prototypes, and \\ndisease subtyping generalization in classifying up to 108 cancer types in the \\nOncoTree classification system. UNI advances unsupervised representation \\nlearning at scale in CPath in terms of both pretraining data and downstream \\nevaluation, enabling data-efficient artificial intelligence models that can \\ngeneralize and transfer to a wide range of diagnostically challenging tasks \\nand clinical workflows in anatomic pathology.\\nThe clinical practice of pathology involves performing a large range \\nof tasks: from tumor detection and subtyping to grading and staging, \\nand, given the thousands of possible diagnoses, a pathologist must \\nbe adept at solving an incredibly diverse group of problems, often \\nsimultaneously1–4. Contemporary computational pathology (CPath) has \\nexpanded this array even further by enabling prediction of molecular \\nalterations5,6, prognostication7–9, and therapeutic response predic-\\ntion10, among other applications11–14. With a vast array of tasks, training \\nmodels from scratch has practical limitations due to challenges in \\ngathering pathologist annotations, building large histology collections \\nReceived: 28 August 2023\\nAccepted: 5 February 2024\\nPublished online: 19 March 2024\\n Check for updates\\nA full list of affiliations appears at the end of the paper. \\u2009e-mail: faisalmahmood@bwh.harvard.edu\\nNature Medicine | Volume 30 | March 2024 | 850–862\\n 851\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nTCGA nonsmall cell lung carcinoma subset (TCGA-NSCLC)79 are com-\\nmonly used to benchmark pretrained encoders using weakly super-\\nvised multiple instance learning (MIL) algorithms15,37,40,80, they source  \\ntissue slides only from a single organ and are often used for predict-\\ning binary disease states81, which is not reflective of the broader array \\nof disease entities seen in real-world anatomic pathology practice. \\nInstead, we assess the generalization capabilities of UNI across diverse \\ntissue types and disease categories by constructing a large-scale, hier-\\narchical, and rare cancer classification task for CPath that follows the \\nOncoTree cancer classification system77. Using in-house BWH slides, \\nwe defined a dataset that comprises 5,564 WSIs from 43 cancer types \\nfurther subdivided into 108 OncoTree codes, with at least 20 WSIs \\nper OncoTree code. A total of 90 out of the 108 cancer types are  \\ndesignated as rare cancers as defined by the RARECARE project 82 \\nand the National Cancer Institute’s Surveillance, Epidemiology, and  \\nEnd Results (NCI-SEER) Program. The dataset forms the basis of two \\ntasks that vary in diagnostic difficulty: 43-class OncoTree cancer type \\nclassification (OT-43), and 108-class OncoTree code classification \\n(OT-108) (Fig. 2a and Supplementary Table 4). The goal of these large \\nmulti-class classification task is not necessarily clinical utility but to \\nassess the capabilities of the foundation model and richness of the \\nfeature representations in comparison with other models. T o assess \\nscaling trends, we also pretrain UNI across varying data scales, with \\nMass-100K subsetted to create Mass-22K (16\\u2009million images, 21,444 \\nWSIs) and Mass-1K (1\\u2009million images, 1,404 WSIs). We also assess model \\nscale by ablating UNI using two different ViT architecture sizes: ViT-Base \\n(or ViT-B) and ViT-Large (or ViT-L). Last, we also assess the impact  \\nof self-supervised learning algorithm choice, compared also  \\nagainst MoCoV3 (ref. 24). For weakly supervised slide classification, we \\nfollow the conventional paradigm of first pre-extracting patch-level \\nfeatures from tissue-containing patches in the WSI using a pretrained \\nencoder, followed by training an attention-based MIL (ABMIL) algo-\\nrithm83. T o reflect the label complexity challenges of these tasks, we \\nreport top-K  accuracy (K \\u2009=\\u20091,\\u20093,\\u20095) as well as weighted F1 score and \\narea under the receiver operating characteristic curve (AUROC) per-\\nformance. Additional details regarding the OT-43 and OT-108 tasks, \\nexperimental setup, implementation details and performance are \\nprovided in Methods, Supplementary Tables 1–11 and Supplementary \\nTables 12–18, respectively.\\nOverall, we demonstrate model and data scaling capabilities of \\nself-supervised models in UNI, with the scaling trend for UNI on OT-43 \\nand OT-108 shown in Fig. 2c,e. On OT-43 and OT-108, we observe a +4.2% \\nperformance increase (P\\u2009<\\u20090.001, two-sided paired permutation test) in \\ntop-1 accuracy when scaling UNI using VIT-L from Mass-1K to Mass-22K, \\nand a similar +3.5% performance increase (P\\u2009<\\u20090.001) on OT-108. From \\nMass-22K to Mass-100K, performance increases further: +3.7% and \\n+3.0% on OT-43 and OT-108, respectively (P\\u2009<\\u20090.001). Similar trends are \\nobserved using VIT-B, with performance plateauing from Mass-22K to \\nMass-100K (Supplementary Tables 13 and 16). Supplementary Tables 14 \\nand 17 show the impact of data diversity and pretraining length, with \\nmonotonic improvement from 50,000 to 125,000 training iterations \\non both tasks. Overall, these scaling trends align with findings observed \\nin many ViT models applied to natural images21,31,75, in which the perfor-\\nmance of larger ViT variants improves as the pretraining dataset grows. \\nExploring other self-supervised learning algorithms, we also trained \\nMoCoV3 (ref. 24) (using ViT-L and ResNet-50 backbones) on Mass-1K, \\nwhich performed worse against DINOv2 (Supplementary Table 18). T o \\nscale performance with increasing model and data size, the choice of \\nalgorithms and their hyper-parameters is also important in developing \\nCPath foundation models.\\nWe compare UNI using ViT-L pretrained on Mass-100K to publicly \\navailable pretrained encoders used in CPath, on OT-43 and OT-108 \\ntasks: ResNet-50 (ref. 84) pretrained on ImageNet-1K; CTransPath 37 \\npretrained on TCGA and PAIP (Pathology AI Platform)85; and REMEDIS38 \\npretrained on TCGA. We observe that UNI outperforms all baselines \\nfor single diseases, and acquiring data for rare diseases. These factors \\nhave led to the reliance on transfer learning techniques in CPath, which \\nhave proven effective in tasks such as metastasis detection15, mutation \\nprediction16,17, prostate cancer grading18 and outcome prediction9,19,20.\\nThe transfer learning, generalization and scaling capabilities of \\nself-supervised (or pretrained) models are dependent on the size \\nand diversity of the training data21–23. In general computer vision, the \\ndevelopment and evaluation of many fundamental self-supervised \\nmodels24–27 are based on the ImageNet Large Scale Visual Recogni -\\ntion Challenge 28,29 and other large datasets 30–32. Such models have \\nalso been described as ‘foundation models’ due to their ability to \\nadapt to a wide range of downstream tasks when pretrained on mas -\\nsive amounts of data33,34. In CPath, The Cancer Genome Atlas (TCGA; \\n~29,000 formalin-fixed paraffin-embedded and frozen H&E whole-slide \\nimages (WSIs), 32 cancer types)35 similarly serves as the basis for many \\nself-supervised models 36–46 along with other histology datasets 47–53, \\nwith a number of prior works demonstrating great progress in learning \\nmeaningful representations of histology tissue for clinical pathology \\ntasks37,38,54–66. However, current pretrained models for CPath remain \\nconstrained by the limited size and diversity of pretraining data, given \\nthat the TCGA comprises mostly primary cancer histology slides,  \\nand by the limited evaluation of generalization performance across \\ndiverse tissue types, and many pan-cancer analyses and popular clinical \\ntasks in CPath are also based on annotated histology regions of inter-\\nest (ROIs) and slides from TCGA6,9,16,17,61,67–74. Addressing these limita-\\ntions is critical in the broader development of foundation models in  \\nCPath that can generalize and transfer to real-world clinical settings \\nwith widespread applications.\\nIn this work we build upon these prior efforts by introducing a \\ngeneral-purpose, self-supervised vision encoder for pathology, UNI, \\na large vision transformer (ViT-Large or ViT-L)75 pretrained on one of \\nthe largest histology slide collections created for self-supervised learn-\\ning, termed ‘Mass-100K’ . Mass-100K is a pretraining dataset that con-\\nsists of more than 100\\u2009million tissue patches from 100,426 diagnostic \\nH&E WSIs across 20 major tissue types collected from Massachusetts  \\nGeneral Hospital (MGH) and Brigham and Women’s Hospital (BWH), \\nas well as the Genotype–Tissue Expression (GTEx) consortium76, and \\nprovides a rich source of information for learning objective charac -\\nterizations of histopathologic biomarkers (Fig. 1a and Supplementary \\nTables 1–3). In the pretraining stage, we use a self-supervised learn -\\ning approach called DINOv2 (ref. 22), which has been shown to yield \\nstrong, off-the-shelf representations for downstream tasks without the  \\nneed for further fine-tuning with labeled data (Fig. 1b). We demonstrate \\nthe versatility of UNI on diverse machine learning settings in CPath, \\nincluding ROI-level classification, segmentation and image retrieval, \\nand slide-level weakly supervised learning (Fig. 1c). In total, we assess \\nUNI on 34 clinical tasks across anatomic pathology and a range of diag-\\nnostic difficulty, such as nuclear segmentation, primary and metastatic \\ncancer detection, cancer grading and sub typing, biomarker screening \\nand molecular subtyping, organ transplant assessment, and several \\npan-cancer classification tasks that include subtyping to 108 cancer \\ntypes in the OncoTree cancer classification system77 (Figs. 1d and 2a). \\nIn addition to outperforming previous state-of-the-art models such as \\nCTransPath37 and REMEDIS38, we also demonstrate capabilities such \\nas resolution-agnostic tissue classification and few-shot class proto-\\ntypes for prompt-based slide classification (Fig. 2d), highlighting the \\npotential of UNI as a foundation model for the further development of \\nartificial intelligence (AI) models in anatomic pathology.\\nResults\\nPretraining scaling laws in CPath\\nA pivotal characteristic of foundation models lies in their capability \\nto deliver improved downstream performance on various tasks when \\ntrained on larger datasets. Although datasets such as CAMELYON16 \\n(Cancer Metastases in Lymph Nodes Challenge 2016 (ref. 78) and the \\nNature Medicine | Volume 30 | March 2024 | 850–862 852\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nby a wide margin. On OT-43, UNI achieves a top-5 accuracy of 93.8% \\nand an AUROC of 0.976, outperforming the next best-performing \\nmodel (REMEDIS) by +6.3% and +0.022 on these respective metrics \\n(both P\\u2009<\\u20090.001) (Fig. 2b and Supplementary Table 12). On OT-108 we \\nobserve a similar margin of performance increase, +10.8% and +0.020 \\n(P\\u2009<\\u20090.001), respectively, over REMEDIS (Fig. 2c and Supplementary \\nUNI BRCA\\nPatch-level classification\\nPatch-level segmentation\\nUNI\\nSlide-level classification\\nAggregator\\nIDC ILC\\nUNIUNIUNI\\nGBM LGG\\nGBMLGG\\nClassifier\\nClassifier\\nSeg. network\\nMasked global crops\\nLocal crops\\nOriginal patch V iew 1 V iew 2\\nGlobal crops\\nUNI\\nUNI\\nteacher\\nIndividualAgg. IndividualAgg. IndividualAgg. \\nReconstruction loss Alignment loss\\nReconstruct Align\\nAgg. features\\nfrom UNI\\nAgg. features \\nfrom UNI teacher\\nHeart\\nKidney\\nEndocrine\\nLung\\nBrain\\nMale genital tract\\nBreast\\nSkin\\nFemale genital tract\\nBowel\\nLiver biliary tract\\nL ymphatic system\\nEsophagogastric\\nSoft tissue\\nPancreas\\nBladder\\nBone\\nHead and neck\\nEye\\nNumber of slides\\nBRCA\\nRetrieval and prototyping\\nUNI UNI UNI\\nUNI\\nQuery\\nOrgans\\n0 2,000 4,000 6,000 8,000 10,000\\nPeritoneum\\na b\\nc\\nd\\n36 54 72 90\\n40\\n60\\n80\\n100\\n36\\n54\\n72\\n90\\n38\\n57\\n76\\n95\\n30\\n45\\n60\\n75\\n36\\n54\\n72\\n90\\n32\\n48\\n64\\n80\\n38\\n57\\n76\\n95\\n28\\n42\\n56\\n70\\n22\\n33\\n44\\n55\\n26\\n39\\n52\\n65\\n30\\n45\\n60\\n75\\n40\\n60\\n80\\n100\\n36547290\\n406080100 38\\n57\\n76\\n95\\n26\\n39\\n52\\n65\\n28\\n42\\n56\\n70\\n20\\n30\\n40\\n50\\n38\\n57\\n76\\n95\\n36\\n54\\n72\\n90\\n24\\n36\\n48\\n60\\n36\\n54\\n72\\n90\\n28\\n42\\n56\\n70\\n36\\n54\\n72\\n90\\n28\\n42\\n56\\n70\\n22\\n33\\n44\\n55\\nCRC tissue class. \\n(CRC-100K)\\nCRC MSI screening\\n(TCGA)\\n \\nBRCA subtyping \\n(BACH)\\n \\nOncoTree class.\\n(OT108)\\n \\nPan-cancer class.\\n(OP43)\\n \\nHeart transplant assess.\\n(BWH-EMB)\\n \\nProstate ISUP grading\\n(PANDA)\\nBrain tumor F-subtyping\\n(EBRAINS)\\n \\nBrain tumor C-subtyping\\n(EBRAINS)\\n \\nGlioma \\nhistomolecular\\nsubtyping \\n(TCGA+)\\n \\n \\n \\nGlioma IDH1\\nscreening\\n(TCGA+)\\n \\n \\nBRCA F-subtyping\\n(BRACS)\\n \\nBRCA C-subtyping\\n(BRACS)\\n \\nNSCLC subtyping\\n(TCGA+)\\n \\nRCC subtyping\\n(DHMC)\\n \\nRCC subtyping\\n(TCGA+)\\n \\nPan-cancer seg.\\n(SegPath)\\n \\nBreast metastasis det.\\n(CAMELYON16)\\n \\nPRAD tissue class.\\n(AGGC)\\nPan-cancer class.\\n(TCGA uniform)\\n \\nCRC polyp class.\\n(UniToPatho)\\n \\nESCA \\nsubtyping\\n(TCGA+)\\n \\n \\nCRC screening\\n(HunCRC)\\n \\nCRC \\ntissue class\\n(HunCRC)\\n \\n \\nTIL det.\\n(TCGA)\\n \\nUNI\\nCT ransPath\\nResNet-50 (IN)\\nREMEDIS\\nROI classification\\nROI segmentation\\nSlide classification\\nROI retrieval\\nBreast metastasis det.\\n(C17-WILDS)\\n \\nRCC tissue class.\\n(TCGA+)\\n \\n \\nFig. 1 | Overview of UNI. UNI is a general-purpose, self-supervised vision encoder \\nfor anatomic pathology based on the vision transformer architecture, achieving \\nstate-of-the-art performance across 34 clinical tasks in anatomic pathology.  \\na, Slide distribution of Mass-100K, a large-scale and diverse pretraining dataset \\nof 100\\u2009million tissue patches sampled from over 100,000 diagnostic WSIs across \\n20 major organ types. b, UNI is pretrained on Mass-100K using the DINOv2 \\nself-supervised training algorithm22, which consists of a mask image modeling \\nobjective118 and a self-distillation objective25. c, UNI generally outperforms other \\npretrained encoders across 34 clinical tasks in anatomical pathology (average \\nperformance of the 8 SegPath tasks reported). d, The evaluation tasks consist of \\nROI-level classification, segmentation, retrieval and prototyping, and slide-level \\nclassification tasks. Further details are given in Methods. class., classification; \\nseg., segmentation; det., detection; assess., assessment.\\nNature Medicine | Volume 30 | March 2024 | 850–862\\n 853\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nTable 15). Overall, we find that UNI is able to classify rare cancers in \\nOT-43 and OT-108 with wide margins of performance improvement \\nover all pretrained encoders.\\nWeakly supervised slide classification\\nFurthermore, we investigate UNI’s capabilities across a diverse \\nrange of 15 slide-level classification tasks, which include breast \\n0 20 40 60 80 100\\n0\\n0.2\\n0.4\\n0.6\\nTop-1 accuracy\\n0 20 40 60 80 100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nTop-1 accuracy\\n0 0.25 0.5 0.75 1.0\\n0.976 (0.971–0.981)\\n0.954 (0.946–0.961)\\n0.956 (0.949–0.962)\\n0.862 (0.850–0.873)\\n1 – Specificity\\n0\\n0.25\\n0.5\\n0.75\\n1.0\\nSensitivity\\n0 0.25 0.5 0.75 1.0\\n1 – Specificity\\n0\\n0.25\\n0.5\\n0.75\\n1.0\\nSensitivity\\n0\\nBreast metastasis det.\\n(CAMELYON16, n = 129 slides)\\nNSCLC subtyping\\n(TCGA + Ext., n = 1,091 slides)\\nRCC subtyping\\n(TCGA + Ext., n = 872 slides)\\nRCC subtyping\\n(DHMC, n = 147 slides)\\nCRC screening\\n(HunCRC, n = 50 slides)\\nBRCA C-subtyping\\n(BRACS, n = 87 slides)\\nBRCA F-subtyping\\n(BRACS, n = 87 slides) Prostate ISUP grading\\n(PANDA, n = 954 slides)\\nGlioma IDH1 screening\\n(TCGA + Ext., n = 873 slides)\\nGlioma histomolecular subtyping\\n(TCGA + Ext., n = 873 slides)\\nBrain tumor C-subtyping\\n(EBRAINS, n = 573 slides)\\nBrain tumor F-subtyping\\n(EBRAINS, n = 573 patients)\\nHeart transplant assess.\\n(BWH-EMB, n = 332 slides)\\nPan-cancer class.\\n(OT-43, n = 1,620 slides)\\nOncoTree class.\\n(OT-108, n = 1,620 slides)\\n0.25\\n0.5\\n0.75\\n1.0\\nSupervised performance\\n1 2 4 8 16 32\\nTraining labels per class\\n0\\n0.25\\n0.5\\n0.75\\n1.0\\nFew-shot performance\\ncb\\nUNI\\nCT ransPath\\nREMEDIS\\nResNet-50 (IN)\\na\\nUNI\\nCT ransPath\\nREMEDIS\\nResNet-50 (IN)\\ne\\nPretraining data size \\n(in millions of images)\\nPretraining data size \\n(in millions of images)\\nUNI\\nCT ransPath\\nREMEDIS\\nResNet-50 (IN)\\nd\\nf UNI CT ransPath ResNet-50 (IN)REMEDIS\\nh ig j\\n1 2 4 8 16 32\\nTraining labels per class\\n0\\n0.25\\n0.5\\n0.75\\n1.0\\nFew-shot performance\\n1 2 4 8 16 32\\nTraining labels per class\\n0\\n0.15\\n0.3\\n0.45\\n0.6\\nFew-shot performance\\n1 2 4 8 16 32\\nTraining labels per class\\n0\\n0.15\\n0.3\\n0.45\\n0.6\\nFew-shot performance\\nUNI\\nCT ransPath\\nREMEDIS\\nResNet-50 (IN)\\nUNI\\nCT ransPath\\nREMEDIS\\nResNet-50 (IN)\\nOT-43 and OT-108\\n17 organ t ypes\\n43 cancer t ypes\\n108 OncoT ree codes\\n0 0.1 0.2 0.3\\n0.7\\n0.8\\n0.9\\n1.0\\n0 0.1 0.2 0.3\\n0.7\\n0.8\\n0.9\\n1.0\\n0.972 (0.968–0.976)\\n0.952 (0.946–0.956)\\n0.959 (0.955–0.963)\\n0.869 (0.860–0.877)\\n  \\nS o f t  t i s s u e\\nU p p e r\\nSkin\\nB r a in\\nG l\\nL o w e r\\nG l\\nB l a d d e r B o n e\\nPancreas\\nL y m p h a t i c\\nL u n g\\nL i v e r K i d n e y\\nH e a d  a n d\\nF e m a l e\\ng e n .\\nE n d o c rine\\nBreast\\nn e c k\\nM\\na leg e n .\\nFig. 2 | Slide-level tasks for OT -43 and OT -108, and slide-level task performance. \\na, Organ and OncoTree code distribution for the slide-level OT-43 and OT-108 \\nclassification tasks. All comparisons with UNI are evaluated on 43-way cancer \\ntype classification and 108-way OncoTree code classification tasks with OT-43 \\nand OT-108, respectively. Further details regarding data distribution are \\nprovided in Supplementary Table 4. Gen., genitalia; GI, gastrointestinal.  \\nb,d, Comparison of macro-averaged AUROC of UNI and other pretrained \\nencoders for OT-43 (b) and OT-108 (d) (n\\u2009=\\u20091,620 slides each). c,e, T op-1 accuracy \\nof UNI across different pretraining data scales (Mass-1K, Mass-22K, Mass-100K) for \\nOT-43 (c) and OT-108 (e) (n\\u2009=\\u20091,620 slides each). f, Supervised performance of UNI \\nand its comparisons across 15 weakly supervised slide-level classification tasks. \\nDashed lines represent the average performance of each model across all tasks. \\nAll data are given as balanced accuracy, except for ISUP grading, which is given as \\nquadratic weighted Cohen’s κ. Error bars represent 95% confidence intervals and \\nthe centers correspond to computed values of each metric as specified above. \\nDetailed results for all tasks are provided in Supplementary Tables 12–35. Ext., \\nexternal test set. g–j, Few-shot slide-level performance with K\\u2009∈\\u2009{1,\\u20092,\\u20094,\\u20098,\\u200916,\\u200932} \\nslides per class reported for four tasks. g, RCC subtyping (train, TCGA; test, \\nCPTAC-DHMC; n\\u2009=\\u2009872 slides). h, BRCA fine-grained subtyping (BRACS, n\\u2009=\\u200987 \\nslides). i, Brain tumor coarse-grained subtyping (EBRAINS, n\\u2009=\\u2009573 slides). j, \\nISUP grading (PANDA, n\\u2009=\\u2009954 slides). Boxes indicate quartile values of model \\nperformance (n\\u2009=\\u20095 runs), and whiskers extend to data points within 1.5-fold the \\ninterquartile range. Few-shot results for all tasks are given in Extended Data Fig. 1.\\nNature Medicine | Volume 30 | March 2024 | 850–862 854\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\ncancer metastasis detection (CAMELYON16)78, International Society of  \\nUrological Pathology (ISUP) grading in prostate cancer (Prostate Cancer \\nGrade Assessment, PANDA)18, cardiac transplant assessment (in-house \\nBWH slides)86, and brain tumor subtyping (EBRAINS; representing 30 \\nrare cancers defined by the RARECARE project), among others. Similar \\nto OT-43 and OT-108 evaluation, we compare the pre-extracted fea-\\ntures from UNI with that of other pretrained encoders using ABMIL83. \\nGiven that CTransPath and REMEDIS were trained using almost all TCGA \\nslides, the reported performance of these models on TCGA tasks may be \\ncontaminated with data leakage and thus unfairly inflated. Additional \\ndetails regarding slide tasks, experimental setup, and performance are \\nprovided in Methods, Supplementary Tables 19–21 and Supplementary \\nTables 22–35, respectively.\\nAcross all 15 slide-level tasks, UNI consistently outperforms other \\npretrained encoders (average performance increases of +26.4% over \\nResNet-50, +8.3% over CTransPath, and +10.0% over REMEDIS), with \\ngreater improvements observed on tasks classifying rare cancer types \\nor characterized by higher diagnostic complexity (Fig. 2f). On prostate \\nISUP grading (PANDA), UNI achieves a quadratic weighted Cohen’s κ  \\nof 0.946, outperforming the next best-performing model (REMEDIS) by \\n+0.014 (P\\u2009<\\u20090.05) (Supplementary Table 29). On hierarchical classifica-\\ntion tasks (which also involve rare disease categories) such as glioma \\nbiomarker prediction (2-class IDH1 mutation prediction and 5-class \\nhistomolecular subtyping using TCGA87,88 and EBRAINS89) and brain \\ntumor subtyping (12-class coarse-grained and 30-class fine-grained \\nbrain tumor subtyping using EBRAINS), UNI outperforms the next \\nbest-performing model (either CTransPath or REMEDIS), by +2.0% \\n(P\\u2009=\\u20090.076), +6.4% (P\\u2009=\\u20090.001), +19.6% (P\\u2009<\\u20090.001) and +16.1% (P\\u2009<\\u20090.001) \\n(Supplementary Tables 31–34). Similar to OT-43 and OT-108, we find \\nthat UNI has the largest impact on the evaluation of brain tumor sub-\\ntyping tasks, which involve only rare cancer types.\\nOn comparison of existing leaderboards, we find that ABMIL with \\nUNI features outperforms many sophisticated MIL architectures. On \\nbreast cancer metastasis detection (CAMELYON16), ABMIL with UNI \\noutperforms all state-of-the-art MIL methods on this task (Supple -\\nmentary Table 36), and is one of the few MIL results that outperforms \\nthe human pathologist performance (AUROC of 0.966) without time \\nconstraints in the original challenge 78. On tasks with detailed com -\\nparisons such as prostate ISUP grading (PANDA) and cellular-mediated \\nallograft rejection (BWH-EMB), ABMIL with UNI outperforms methods  \\nsuch as WholeSIGHT90 and CRANE86 (Supplementary Tables 37 and 38).  \\nAlthough many of these comparisons are not equivalent due to  \\nthe use of ResNet-50 with ImageNet transfer (ResNet-50 IN) features, \\nwe note that their proposed MIL architectures are often motivated \\nand developed specifically for solving these challenging tasks. Our \\ncomparisons highlight the strength of having a better-pretrained \\nencoder versus MIL architecture.\\nData contamination is a concern in foundation models trained \\non large collections of public datasets 91–95. Although labels may not \\nbe explicitly leaked into the model during self-supervised training, \\nmodels pretrained on the evaluated test set may exhibit optimisti -\\ncally biased performance, observed in other CPath studies96. We addi-\\ntionally compare UNI against CTransPath and REMEDIS on TCGA test \\nsets from the nonsmall cell lung cancer (NSCLC) subtyping, renal cell  \\ncarcinoma (RCC) subtyping, glioma IDH1 mutation prediction and gli-\\noma histomolecular subtyping tasks, observing performance decreases \\nwhen comparing the in-domain versus out-of-domain performance. On \\nNSCLC subtyping, REMEDIS outperforms UNI on TCGA evaluation \\n(97.3% versus 94.7%), but underperforms on CPTAC (Clinical Proteomic \\nTumor Analysis Consortium) evaluation (79.0% versus 96.3%) (Supple-\\nmentary Table 23). On glioma IDH1 mutation prediction, CTransPath \\nand REMEDIS outperform UNI on TCGA evaluation (89.1% and 81.9% \\nversus 80.8%), but underperform on EBRAINS evaluation (83.6% and \\n79.2% versus 85.6%) (Supplementary Tables 31 and 32). We emphasize \\nthat data contamination exists only in how the models are used, not \\nin the models themselves, which have been shown to transfer well in \\nsettings independent of TCGA 38,59,97. Given that many CPath studies  \\nuse the TCGA for studying diverse cancer types, UNI is more flexible \\nthan CTransPath and REMEDIS in developing pathology AI models on \\npublic histology datasets and benchmarks.\\nLabel efficiency of few-shot slide classification\\nWe additionally evaluate UNI in few-shot MIL across all slide-level tasks. \\nFew-shot learning is an evaluation scheme that studies the generaliza-\\ntion capabilities of models on new tasks (C  classes) given a limited \\nnumber of examples (K training samples per class, also called supports \\nor shots). For all pretrained encoders, we trained an ABMIL model with \\nK\\u2009∈\\u2009{1,\\u20092,\\u20094,\\u20098,\\u200916,\\u200932} training examples per class, where K is limited to \\n32 due to small support sizes in rare disease categories. Given that the \\nperformance can fluctuate depending on which K examples are chosen \\nfor each class, we repeat experiments over five runs with C\\u2009×\\u2009K training \\nexamples randomly sampled each time. Additional details regard -\\ning few-shot MIL experimentation and performance are provided in \\nMethods and Extended Data Fig. 1.\\nUNI generally outperforms other pretrained encoders and with \\nsuperior label efficiency across all tasks, especially in classifying rare \\ndiseases (Fig. 2g–j and Extended Data Fig. 1). When comparing the \\n4-shot performance of UNI with that of other encoders (using the \\nmedian performance), the next best-performing encoder needs up \\nto eightfold as many training examples per class to reach the same \\n4-shot performance of UNI. On prostate ISUP grading (PANDA), UNI is \\nconsistently twice as label efficient across all few-shot settings (Fig. 2j). \\nOn challenging rare cancer subtyping tasks such as fine-grained brain \\ntumor subtyping (EBRAINS), the 4-shot performance of UNI outper -\\nforms other encoders by a large margin, matched only by the 32-shot \\nperformance of REMEDIS (Fig. 2i). Overall, our comprehensive evalu-\\nation of slide classification tasks demonstrates UNI’s potential as a \\nfoundational model that can be used in histopathology workflows that \\nscreen for rare and underrepresented diseases.\\nSupervised ROI classification in linear classifiers\\nIn addition to slide-level tasks, we also assess UNI on a diverse range of  \\n11 ROI-level tasks, which include colorectal tissue and polyp classifi-\\ncation (CRC-100K-NONORM98, HunCRC99, UniT oPatho100), prostate \\nadeno carcinoma (PRAD) tissue classification (Automated Gleason \\nGrading Challenge 2022 (AGGC)101), pan-cancer tumor-immune lympho-\\ncyte detection (TCGA-TILS67), 32-class pan-cancer tissue classification \\n(TCGA Uniform Tumor68), and others. For evaluation and compari-\\nsons, we perform logistic regression and K-nearest neighbors (KNN) on \\ntop of the pre-extracted features of each encoder, a common practice \\nreferred to as linear probing and KNN probing, which measure discrimi-\\nnative performance and the representation quality of pre-extracted  \\nfeatures, respectively23. We evaluate all tasks using balanced accuracy, \\nwith PRAD tissue classification evaluated using weighted F1 score101. \\nAdditional details regarding ROI tasks, experimental setup and per-\\nformance are provided in Methods and Supplementary Tables 39–60.\\nAcross all 11 ROI-level tasks, UNI outperforms nearly all baselines \\non all tasks, with average performance increases of +18.8%, +7.58% and \\n+5.75% on linear probing for ResNet-50, CTransPath and REMEDIS, \\nrespectively (Fig. 3a ). On KNN probing, UNI similarly outperforms \\nResNet-50, CTransPath and REMEDIS with average performance \\nincreases of +15.6%, +8.6% and +9.4%. We find larger gains on challeng-\\ning tasks such as PRAD tissue classification (in weighted F1 score, +0.131, \\nP\\u2009<\\u20090.001; +0.020, P\\u2009<\\u20090.001; +0.027, P\\u2009<\\u20090.001) and esophageal carci-\\nnoma subtyping (+25.3%, P\\u2009<\\u20090.001; +10.1%, P\\u2009<\\u20090.001; +5.5%, P\\u2009<\\u20090.001) \\ncompared with the other three pretrained encoders, respectively. \\nFigure 3b shows the UNI predictions on prostate cancer grading, in \\nwhich a simple linear classifier trained with pre-extracted UNI features \\ncan achieve high agreement with pathologist annotations (Extended \\nData Fig. 2). On 32-class pan-cancer tissue classification (19 out of  \\nNature Medicine | Volume 30 | March 2024 | 850–862\\n 855\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n0\\n224\\n2  res.\\n448\\n2  res.\\n896\\n2  res.\\n1,344\\n2  res.\\n0.25\\n0.5\\n0.75\\n1.0\\n0\\nCRC tissue class.\\n(CRC-100K, n = 7,180 ROIs)\\nBreast metastasis det.\\n(C17-WILDS, n = 85,054 ROIs)\\nRCC tissue class.\\n(TCGA + Ext., n = 2,968)\\nESCA subtyping\\n(TCGA + Ext., n = 178,187 ROIs)\\nCRC polyp class.\\n(UniToPatho, n = 2,399 ROIs)\\nCRC MSI screening\\n(TCGA, n=32,361 ROIs)\\nPan-cancer class.\\n(TCGA, n = 55,360 ROIs)\\nPan-cancer TIL det.\\n(TCGA, n = 56,275 ROIs)\\nPRAD tissue class.\\n(AGGC, n = 345,021 ROIs)\\nCRC tissue class.\\n(HunCRC, n = 22,655 ROIs)\\nBRCA subtyping\\n(BACH, n = 80 ROIs)\\n0.25\\n0.5\\n0.75\\n1.0\\nb\\nd\\nUNI\\nCT ransPath ResNet-50 (IN)\\nREMEDIS\\nROI image 224 2  resolution 448 2  resolution 896 2  resolution 1,344 2  resolution\\nMHSA visualization\\nPer-head attention weight\\n1.0\\n0\\ne\\nStroma Benign G3 G4 G5\\nSupervised performance\\nGround t ruth Prediction Prediction (1)\\n1\\n2\\nPrediction (2) c\\n       \\na UNI CT ransPath ResNet-50 (IN)REMEDIS\\nSupervised performance\\n \\n0\\n0.25\\n0.5\\n0.75\\n1.0\\nRetrieval performance\\nACC@1 ACC@3 ACC@5 MV Acc@5\\nFig. 3 | ROI-level tasks. a, Supervised linear probe performance of UNI and \\nits comparisons across 11 ROI-level classification tasks. All results are given \\nas balanced accuracy except for PRAD tissue classification, which is given as \\nweighted F1 score. Dashed lines represent the average performance of each model \\nacross all tasks. Error bars represent 95% confidence intervals and the centers \\ncorrespond to computed values of each metric as specified above. Detailed results \\nfor all tasks are provided in Supplementary Tables 39–60. b, Examples of UNI on \\nROI classification for PRAD tissue classification in AGGC. Left: ground-truth ROI-\\nlevel labels overlaid on the WSI. Right: predicted patch labels. ROIs are enlarged \\nfor better visualization, with further comparisons shown in Extended Data Fig. 2.  \\nc, ROI retrieval performance of UNI on PRAD tissue classification (AGGC, \\nn\\u2009=\\u2009345,021 ROIs). We report Recall@K for K\\u2009∈\\u2009{1,\\u20093,\\u20095} and the mean recall, with \\nerror bars representing 95% confidence intervals and the centers corresponding \\nto computed values of each metric. d, Supervised KNN probe performance of \\nUNI across various image resolutions (res., in pixels) in BRCA subtyping in BACH \\n(n\\u2009=\\u200980 ROIs). Retrieval performance for all tasks is provided in Extended Data \\nFig. 3 and Supplementary Tables 63–68. e, Multi-head self-attention (MHSA) \\nheatmap visualization of UNI across different image resolutions (in pixels) in \\nBACH. Each colored square represents a 16\\u2009×\\u200916\\u2009pixel patch token encoded by UNI, \\nwith heatmap color corresponding to the attention weight of that patch token \\nto the global [CLS] (that is, classification) token of the penultimate layer in UNI. \\nT op and bottom, respectively: visualizations for the invasive- and normal-labeled \\nimages, with further visualizations and interpretations provided in Extended Data \\nFigs. 4–6. Scale bars: b, ground truth and prediction, 2\\u2009mm; prediction(1) and \\nprediction(2), 200\\u2009µm; insets, 30\\u2009µm; e, ROI image, 32\\u2009µm; 2242, 64\\u2009pixels; 4482, \\n128\\u2009pixels; 8962, 256\\u2009pixels; 1,3442, 384\\u2009pixels.\\nNature Medicine | Volume 30 | March 2024 | 850–862 856\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n32 of which are rare cancers), UNI achieves the highest overall balanced \\naccuracy and AUROC of 65.7% and 0.975, respectively, outperforming \\nthe next best-performing model (REMEDIS) by +4.7% and +0.017 (both \\nP\\u2009<\\u20090.001).\\nWe also compare UNI’s performance against that on the official \\nleaderboards. For tumor-immune lymphocyte detection, compared \\nwith the best model in the ChampKit benchmark, which reports an \\nAUROC of 0.974 and a false-negative rate (FNR) of 0.246, UNI has an \\nAUROC of 0.978 and an FNR of 0.193 (without stain normalization) \\n(Supplementary Table 61). For breast cancer metastasis detection \\n(CAMELYON17-WILDS leaderboard), compared with the best model \\nto date, which has accuracies of 95.2% and 96.5% on the out-of-domain \\nvalidation and test sets, UNI reaches 97.4% and 98.3%, respectively \\n(Supplementary Table 62). We note that many of these comparisons \\nare end-to-end fine-tuned with transfer learning from natural images \\n(and not from pathology). Although not equivalent in experimentation \\nto UNI, these comparisons highlight the versatility of UNI given that \\nout-of-the-box evaluation using linear classifiers is competitive with \\nstate-of-the-art techniques using end-to-end fine-tuning.\\nROI retrieval\\nIn addition to using representations in UNI to build task-specific classi-\\nfiers, representations can also be used for image retrieval. Retrieval is \\nsimilar to KNN in that we evaluate how well a query image can retrieve \\nother images of the same class, given that visually similar images should \\nbe closer in representation space than visually distinct images. Differ-\\nent to KNN evaluation, we consider the accuracy of retrieval, that is, \\nAcc@K for K\\u2009∈\\u2009{1,\\u20093,\\u20095}, in which the retrieval is successful if a correctly \\nlabeled image is within the top-K  retrieved images, and MVAcc@5, \\nwhich uses the majority vote of the top-5 retrieved images. We evalu-\\nate histology image retrieval on six ROI-level tasks (tasks with at least \\n5 classes). Additional details regarding ROI retrieval experimentation \\nand performance are provided in Methods, Extended Data Fig. 3 and \\nSupplementary Tables 63–68.\\nUNI outperforms other encoders on all tasks, demonstrating \\nsuperior retrieval performance across diverse settings. On PRAD tis-\\nsue classification (AGGC), UNI outperforms the next best-performing \\nencoders (REMEDIS) by +4% and +3.3% on Acc@1 and MVAcc@5, respec-\\ntively (both P\\u2009<\\u20090.001) (Fig. 2c). On colorectal cancer (CRC) tissue clas-\\nsification (CRC-100K), the gap between the top performing encoders \\nis relatively smaller (by +3.1%, P\\u2009<\\u20090.001 and +0.01%, P\\u2009=\\u20090.188, respec-\\ntively, compared with REMEDIS), presumably because the different \\ntissue types have very distinct morphology, as shown by the relatively \\nhigh classification performance in linear probing. On the more chal -\\nlenging 32-class pan-cancer tissue classification task, which contains \\nmany rare cancer types, UNI outperforms the second-best performing \\nencoder (REMEDIS) by a large margin of +4.6% for Acc@1 and +4.1% for \\nMVAcc@5 (both P\\u2009<\\u20090.001).\\nRobustness to high image resolution\\nAlthough visual recognition models are commonly evaluated on resized \\n224\\u2009×\\u2009224\\u2009pixel (2242\\u2009pixel) images, image resizing changes the microns \\nper pixel (mpp) and may alter the interpretation of morphological \\nfeatures such as cellular atypia. We study how feature quality in UNI \\nis affected at varying resolutions in breast invasive carcinoma (BRCA) \\nsubtyping (Grand Challenge on Breast Cancer Histology images, BACH) \\n(2242\\u2009pixels at 2.88\\u2009mpp to 1,3442\\u2009pixels at 0.48\\u2009mpp) and CRC polyp \\nclassification (UniT oPatho) (2242\\u2009pixels at 3.60\\u2009mpp to 1,7922\\u2009pixels at \\n0.45\\u2009mpp) with linear and KNN probing. Additional details regarding \\nmultiple resolution experimentation and performance are provided \\nin Methods, Extended Data Fig. 4 and Supplementary Tables 45, 46, \\n51 and 52.\\nOn both tasks we demonstrate the robustness of UNI to different \\nimage resolutions, as well as biases introduced into image resizing for \\nhigh-resolution ROI tasks. When scaling the image resolutions used \\nfor evaluation, we observe that other encoders have worse perfor-\\nmance degradation, with KNN performance decreases of −18.8% in \\nCTransPath and −32.5% in REMEDIS on BRCA subtyping (224 2\\u2009pixels  \\nversus 1,3442\\u2009pixels), compared with −6.3% in UNI. In CRC polyp clas-\\nsification, although other encoders do not have significant perfor -\\nmance decreases (2242\\u2009pixels versus 1,7922\\u2009pixels), UNI increases by \\n+5.1% via KNN probe. Figure 2e and Extended Data Figs. 5 and 6 show \\nhow UNI highlights finer-grained visual features when evaluating \\nhigh-resolution images. In CRC polyp classification, resizing to 2242\\u2009pix-\\nels obscures important fine-grained details localizing the crypts that \\nare otherwise detected at high resolution by UNI. These observations \\nsuggest that UNI can encode semantically meaningful representations \\nagnostic to most image resolutions, which can be valuable in CPath \\ntasks known to be optimal at different image magnifications.\\nROI cell type segmentation\\nWe assess UNI on the largest, public ROI-level segmentation dataset, \\nSegPath102, a dataset for segmenting eight major cell types in tumor \\ntissue: epithelial cells, smooth muscle cells, red blood cells, endothe-\\nlial cells, leukocytes, lymphocytes, plasma cells, and myeloid cells. All \\npretrained encoders are fine-tuned end-to-end using Mask2Former103, \\na flexible framework commonly used for evaluating the off-the-shelf \\nperformance of pretrained encoders22,104. Given that the SegPath data-\\nset divides the cell types into separate dense prediction tasks (eight \\ntasks in total), each encoder is individually fine-tuned per cell type, \\nwith the dice score used as the primary evaluation metric. Additional \\ndetails regarding segmentation tasks and performance are provided \\nin Methods and Supplementary Table 69.\\nAlthough hierarchical vision backbones such as Swin transformers \\n(CTransPath) and convolutional neural networks (CNNs; ResNet-50 and \\nREMEDIS) have well-known advantages over vision transformers (UNI) \\nfor segmentation, we observe that UNI still outperforms all compari-\\nsons on a majority of cell types in SegPath. On individual segmenta -\\ntion tasks for the epithelial, smooth muscle and red blood cell types, \\nUNI achieves dice scores of 0.827, 0.690 and 0.803, respectively, out-\\nperforming the next best-performing encoder (REMEDIS) by +0.003 \\n(P\\u2009=\\u20090.164), +0.016 (P\\u2009<\\u20090.001) and +0.008 (P\\u2009=\\u20090.001), respectively. \\nAcross all eight cell types in SegPath, UNI achieves the overall perfor-\\nmance with an average dice score of 0.721, outperforming ResNet-50 \\n(0.696), CTransPath (0.695) and REMEDIS (0.716). Extended Data \\nFig. 7 shows segmentation visualizations for all cell types by UNI and \\nother encoders, with all comparisons performing well in matching the \\nground truth segmentation. Overall, we find that UNI can outperform \\nstate-of-the-art CNNs and hierarchical vision models on segmentation \\ntasks, extending its versatility in less conventional settings.\\nFew-shot ROI classification with class prototypes\\nSimilar to slide-level classification, we also assess the label efficiency \\nof UNI on ROI-level tasks. We evaluate all pretrained encoders using \\nthe nonparametric SimpleShot framework105, a strong baseline in the \\nfew-shot classification literature that proposes averaging extracted \\nfeature vectors of each class as the support examples in K\\u2009=\\u20091 nearest \\nneighbors (or nearest centroid) classification106. These averaged fea-\\nture vectors can also be viewed as ‘class prototypes’ , a set of one-shot \\nexemplars that are unique in representing semantic information such \\nas class labels (for example, lung adenocarcinoma (LUAD) versus lung \\nsquamous cell carcinoma (LUSC) morphologies). At test time, unseen \\ntest examples are assigned the label of the nearest class prototype via \\nEuclidean distance (Fig. 4a). For all pretrained encoders, we evaluate \\ntheir pre-extracted features using SimpleShot with K\\u2009∈\\u2009{1,\\u20092,\\u20094,\\u20098,\\u2009…,\\u2009256} \\ntraining examples per class for a majority of tasks, with experiments \\nrepeated over 1,000 runs where C\\u2009×\\u2009K training examples are randomly \\nsampled for each run. Additional details regarding few-shot ROI experi-\\nmentation and performances are provided in Methods and Extended \\nData Fig. 8.\\nNature Medicine | Volume 30 | March 2024 | 850–862\\n 857\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n1 2 4 8 16 32 64 128 256\\nTraining labels per class\\n0\\n0.25\\n0.5\\n0.75\\nFew-shot performance\\n1 2 4 8 16 32 64 128 256\\nTraining labels per class\\n0\\n0.25\\n0.5\\n0.75\\nFew-shot performance\\n1 2 4 8 16 32 64 128 256\\nTraining labels per class\\n0\\n0.25\\n0.5\\n0.75\\nFew-shot performance\\na b\\nf WSI Similarity heatmap and top-5 retrieved patches\\n1 2 4 8 16 32\\nTraining slides per class\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nFew-shot performance\\n(Top-K = 5)\\n1 2 4 8 16 32\\nTraining slides per class\\n0\\n0.25\\n0.50\\n0.75\\n1.00\\nFew-shot performance\\n(Top-K = 5)\\ng\\nUNI (Slide prototyping)\\nUNI (MIL)\\nUNI (Slide prototyping)\\nUNI (MIL)\\nSimilarity with LUSC prototype\\nLow\\nHigh\\nc d e\\nUNI CT ransPath\\nResNet-50 (IN)REMEDIS\\nPRCC CHRCCCCRCC\\nPRCC\\nEmbedding space\\nPrediction\\nQuery patch\\nCCRCC\\nprototype\\nPRCC \\nprototype\\nCHRCC \\nprototype\\nWSI (PRCC)\\nSimilarity with CCRCC prototype\\nLow\\nHigh\\nSimilarity heatmap\\nSimilarity with PRCC\\nprototype \\nLow\\nHigh\\nTop-K patches\\nCCRCC\\nCHRCC\\nPRCC\\nAverage similarity\\nPRCC\\nPrediction\\nSimilarity\\nUNIUNIUNI\\nUNI\\nUNI UNI UNI\\nUNIUNIUNI\\nFig. 4 | Few-shot ROI- and slide-level prototyping. a, Prototypical few-shot \\nROI classification via SimpleShot. A class prototype is constructed by averaging \\nthe extracted features from ROIs of the same class. For a test ROI, SimpleShot \\nassigns the class of the most similar class prototype (smallest Euclidean distance) \\nas the predicted ROI label. b, Prototypical few-shot slide classification via MI-\\nSimpleShot. Using a pre-computed set of ROI-level class prototypes (sharing the \\nsame class labels as the slide), MI-SimpleShot predicts the slide label using the \\nclass prototype with the highest average similarity of top-K patches queried from \\nthe WSI. The similarity heatmap visualizes the similarity between the ground-\\ntruth class prototype and each patch in the WSI. c–e, Few-shot ROI classification \\nperformance via SimpleShot on three tasks, with boxes indicating quartiles \\nof model performance (n\\u2009=\\u20091,000 runs) and whiskers extending to data points \\nwithin 1.5-fold the interquartile range. c, Pan-cancer tissue classification (TCGA, \\nn\\u2009=\\u200955,360 ROIs). d, CRC polyp classification (UniT oPatho, n\\u2009=\\u20092,399 ROIs).  \\ne, PRAD tissue classification (AGGC, n\\u2009=\\u2009345,021 ROIs). Few-shot ROI \\nperformances for all tasks are provided in Extended Data Fig. 8. f,g, Few-shot \\nslide classification performance and similarity heatmaps via MI-SimpleShot \\nfor NSCLC subtyping (train, TCGA; test, CPTAC; n\\u2009=\\u20091,091 slides) (f) and RCC \\nsubtyping (train, TCGA; test, CPTAC-DHMC; n\\u2009=\\u2009872 slides) (g). In both tasks, \\nusing pre-extracted features from UNI, we compare MI-SimpleShot in the same \\nfew-shot settings as ABMIL (boxes indicate quartile values of model performance \\nwith n\\u2009=\\u20095 runs and whiskers extend to data points within 1.5-fold the interquartile \\nrange), and visualize similarity heatmaps and the top-5 similar patches (indicated \\nin red bounding boxes) for a LUSC (f) and CCRCC (g) slide. Scale bars: WSI, 2\\u2009mm; \\ntop-5 retrieved patches, 56\\u2009µm. Further details, comparisons and visualizations \\nare provided in Methods and Extended Data Figs. 8–10.\\nNature Medicine | Volume 30 | March 2024 | 850–862 858\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nAcross various tasks and evaluation settings, we find that UNI is \\na strong few-shot learner and is much more label efficient than other \\npretrained encoders. When comparing the median 8-shot perfor -\\nmance of UNI with that of other encoders, UNI consistently exceeds \\nthe 128-shot and 256-shot performance of the next best-performing \\nencoder on many tasks (Fig. 4c–e and Extended Data Fig. 8). We note \\nthat the variance in 1- and 2-shot performances for all encoders can \\nbe high due to the choice of ROIs randomly selected as prototypes, \\npotentially affected by H&E stain variability. However, given that the \\nnumber of support examples increases in forming the class prototypes, \\nwe observe a monotonic decrease in variance of few-shot performance \\nruns (0.32–1.59% standard deviation across tasks in UNI’s 256-shot per-\\nformance), which demonstrates performance stability in permuting \\ntraining examples to average as class prototypes in SimpleShot. Still, \\nwe observe that the lowest few-shot performance of UNI can some-\\ntimes exceed the maximum few-shot performance reported across \\n1,000 runs of other encoders. In pan-cancer tissue classification, the \\nlowest-performing run for UNI in 2-shot, 8-shot and 32-shot evalu-\\nation outperforms the best possible run for ResNet-50, CTransPath \\nand REMEDIS, respectively. These findings demonstrate the superior \\nlabel efficiency and representation quality of UNI, given that averag-\\ning the extracted features from only a few ROIs can create effective \\nclass prototypes.\\nPrompt-based slide classification using class prototypes\\nAlthough weakly supervised learning via MIL has shifted slide-level \\nclassification such that ROI annotations are no longer required 81, \\naccessing and curating histology slide collections may still exist as \\nbarriers for clinical tasks that address rare and underrepresented dis-\\neases. From observing the strong retrieval performance and few-shot \\ncapabilities in UNI, we re-visit the problem of few-shot slide classifica-\\ntion using class prototypes. Similar to textual prompting55, we used \\nthe class prototypes from SimpleShot also as 'prompts' for majority \\nvoting on the top-K retrieved patches (top-K pooling), which we call \\nmultiple instance SimpleShot (MI-SimpleShot) (Fig. 4b). We evaluate \\nMI-SimpleShot on the same folds as trained ABMIL models in few-shot \\nslide classification, with prototypes created using annotated ROIs (from \\ntraining slides) from the pan-cancer tissue classification task 68. We \\nalso compare MI-SimpleShot using other pretrained encoders, as well \\nas the MIL baseline for UNI. We also develop similarity heatmaps that \\nshow the normalized Euclidean distances of all patches in a slide with \\nrespect to the class prototype of the ground-truth label, with patholo-\\ngist annotations of tissue regions that match the slide label outlined in \\nblue. Additional details regarding MI-SimpleShot experimentation and \\nperformance are provided in Methods, Extended Data Figs. 9 and 10  \\nand Supplementary Tables 70 and 71.\\nUsing only a few annotated ROI examples per class as prototypes, \\nwe demonstrate the potential of applying UNI with MI-SimpleShot as a \\nsimple but highly efficient system for slide-level disease subtyping and \\ndetection. On NSCLC and RCC subtyping (trained on TCGA and tested \\non external cohorts), MI-SimpleShot with top-5 pooling achieves better \\nperformance than ABMIL when using 1, 2 and 4 training slides per class \\nfor creating prototypes, and achieves similar performance to ABMIL \\nwhen using more slides (Fig. 4f,g). Using similarity heatmaps, we also \\nobserve that retrieved patches of UNI (corresponding to the slide label) \\nhave strong agreement with pathologist annotations, as observed in the \\nright-hand side of Fig. 4f,g for LUSC and clear cell renal cell carcinoma \\n(CCRCC) slides. We believe that the effectiveness of MI-SimpleShot can \\nbe attributed to not requiring trainable parameters (ABMIL models may \\nstill over- and under-fit in few-shot settings) and the strong representa-\\ntion quality of UNI features for ROI retrieval. Although other pretrained \\nencoders can be used for learning prototypes in MI-SimpleShot, UNI \\nis potentially less sensitive to H&E staining variability. This is seen \\nin the high standard deviation of one-shot performances for RCC \\nsubtyping (both in ABMIL in Extended Data Fig. 1 and MI-SimpleShot \\nin Extended Data Fig. 9), with only one site used for learning a class \\nprototype in MI-SimpleShot. This is also underscored in SimpleShot \\nevaluation of breast metastasis detection (CAMELYON17-WILDS), given \\nthat CTransPath and REMEDIS have larger performance disparities than \\nUNI between the two out-of-domain hospital test cohorts (accuracy \\ndifferences of 12.3% and 12.8% versus 5.1%, respectively), alluding to \\nthe potential effects of H&E stain intensity skewing retrieval perfor -\\nmance (Supplementary Table 42). In Extended Data Fig. 10 we observe \\ninstances of incorrect retrieval performance with respect to the pre-\\ndicted label and the pathologist annotations. Overall, our evaluation \\nof UNI via MI-SimpleShot showcases how visual-centric foundation \\nmodels with strong retrieval capabilities may enable applications in \\nanatomic pathology.\\nDiscussion\\nIn this study, we demonstrate the versatility of UNI, a general-purpose, \\nself-supervised model pretrained on one of the largest histology slide \\ncollections (for self-supervised learning) to date in CPath. We curated \\nMass-100K, a pretraining dataset containing more than 100\\u2009million \\ntissue patches from 100,426 WSIs across 20 major organ types, includ-\\ning normal tissue, cancerous tissue and other pathologies. Using the \\nDINOv2 self-supervised learning approach (demonstrated to scale \\nto large datasets)22, we developed and validated a ViT-L (pretrained \\non Mass-100K) that consistently outperforms other histopathology \\nimage encoders. Depending on the task, although CTransPath and \\nREMEDIS may achieve similar performances, our findings suggest \\nthat these encoders have limitations with regard to retrieval capabili-\\nties, label efficiency and potential biases to H&E staining intensity in \\nout-of-domain evaluation.\\nAs a visual-centric foundation model that may enable versatile \\nclinical applications in CPath, several challenges emerged in devel -\\noping UNI with regard to how factors such as model and data scal -\\ning would affect transfer performance. Although many empirical \\nstudies explore these components to achieve good generalization of \\nnatural images, many solutions may not be translatable due to differ-\\nences between pathology and natural images. For example, although \\nMoCoV3 has a lower but still competitive performance against DINOv2 \\non ImageNet, the same training configurations mirrored for develop-\\ning a ViT-L on Mass-1K demonstrate large gaps in performance on \\nOT-108. Following our study, we note several other studies that have \\nrecently emerged in training on larger histology slide datasets and \\ncollections107–109. Distinct from prior and recent works, our study is \\nunique in providing unique insights into scaling laws and transfer learn-\\ning capabilities of self-supervised models in CPath. Although model \\nand data scale are important components for building visual-centric \\nself-supervised learning, we find that the self-supervised learning \\n(SSL) algorithm choice is the most impactful, with MoCoV3 (ViT-L on \\nMass-1K) under-performing not only against its DINOv2 counterpart, \\nbut also against CTransPath and REMEDIS. Increasing model scale \\n(ViT-B to ViT-L) and data scale (Mass-1K and Mass-100K) does reflect \\nperformance increase, but note that performances of UNI ablations \\non OT-43 and OT-108 are relatively close and have consistent improve-\\nment over CTransPath and REMEDIS, which suggests that competitive \\npretrained encoders can still be developed with smaller models and less \\ndata. In tandem with the many clinical applications demonstrated by \\nUNI, we believe that our testing of the aforementioned factors would \\nguide CPath practitioners in developing their own foundation models \\nusing private in-house slide collections.\\nWith regard to the wide range of clinical tasks to which UNI can \\nbe applied, compared with other encoders, we find that UNI excels in \\nclassifying rare and underrepresented diseases, such as the 90 out of \\n108 rare cancer types in the OT-108 benchmark, the 30 rare brain tumor \\ndiagnoses in the EBRAINS Digital Tumor Atlas, and the 19 out of 32 can-\\ncer subtypes in pan-cancer tissue classification sourced from TCGA. On \\nthese tasks and others, UNI demonstrates consistent and significant \\nNature Medicine | Volume 30 | March 2024 | 850–862\\n 859\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nperformance increases over the next best-performing encoder (REMEDIS  \\nor CTransPath). We hypothesize that UNI’s performance is attributed \\nto the strong representation quality of the pre-extracted features, as \\nseen in few-shot ROI and slide classification using class prototypes. \\nIn weakly supervised paradigms in which rare cancer types are infre-\\nquent and underrepresented in current slide datasets, MI-SimpleShot \\nusing UNI shows that annotating four slides per class can outperform \\ntask-specific MIL algorithms. Overall, we believe that UNI and other \\nvisual-centric foundation models that are being developed can be \\ntransformative in enabling creative clinical applications that would \\nordinarily require orders of magnitude more data.\\nOn comparison with public leaderboards, we believe that UNI \\nalso presents an important shift from task-specific model develop -\\nment to generalist AI models110 in CPath. Beyond the 34 clinical tasks \\nevaluated in this study, UNI evaluated out of the box is competitive \\nwhen compared with published results of other works, outperforming \\nleading models that are often trained end-to-end or that use carefully \\ndesigned training recipes implemented for solving these specific \\npublic challenges. Altogether, our findings highlight the strength of \\nhaving a better-pretrained encoder versus developing task-specific \\nmodels that target narrow clinical problems, which we hope would \\nshift research directions in CPath toward the development of gener-\\nalist AI models that would have greater performance and flexibility \\nin targeting diverse clinical applications in pathology. Following the \\nconventional nomenclature of self-supervised models in computer \\nvision22,75, labels such as ‘foundation model’ may create misleading \\nexpectations.\\nOur study has several limitations. Based on the ViT-L architec -\\nture, UNI lacks vision-specific biases for solving dense prediction \\ntasks in CPath, and we note that performance increases for cell type \\nsegmentation in SegPath are not as drastic as observed in other tasks. \\nWe envision further improvement as better recipes emerge for adapt-\\ning ViT architectures for segmentation tasks 111. Our study also does \\nnot evaluate the best-performing ViT-Giant architecture in DINOv2, \\nan even larger model that would likely translate well in CPath but \\ndemands more computational resources for pretraining. Although \\nour study organizes the largest collection of clinical tasks for evalu -\\nating pretrained models in CPath (to our knowledge), other clinical \\ntasks, such as those in cytopathology or hematopathology, are not \\nrepresented in our analyses. Due to the breadth of our evaluation and \\nsmall (or missing) validation sets for certain tasks, hyper-parameters \\nwere fixed, which follows other works in CPath 25,37,40,112,113. Further \\nhyper-parameter tuning and other training recipes may be likely to \\nimprove results further; however, our evaluation protocol was imple-\\nmented for ranking the representation quality of pretrained encoder \\nbackbones. In developing UNI, although Mass-100K was developed \\nintentionally to not overlap significantly with most public histology \\ncollections, biases such as data contamination and image acquisition \\nshift should be further studied if the same model is re-used across \\nmany applications, especially if it were to have a disparate impact on \\ndiverse populations114. UNI is a unimodal model for CPath, meaning \\nthat multimodal capabilities such as cross-modal retrieval and visual \\nquestion answering remain out of scope, which we explore in concur-\\nrent work115,116. Last, UNI is also only a ROI-level model for CPath, with \\nthe majority of clinical tasks in pathology performed at the slide or \\npatient level. Future work will focus on using UNI as the building block \\nfor slide-level self-supervised models117 and general slide-level pathol-\\nogy AI development in anatomic pathology.\\nOnline content\\nAny methods, additional references, Nature Portfolio reporting sum-\\nmaries, source data, extended data, supplementary information, \\nacknowledgements, peer review information; details of author contri-\\nbutions and competing interests; and statements of data and code avail-\\nability are available at https://doi.org/10.1038/s41591-024-02857-3.\\nReferences\\n1. Song, A. H. et al. Artificial intelligence for digital and \\ncomputational pathology. Nat. Rev. Bioeng. 1, 930–949 (2023).\\n2. Bera, K., Schalper, K. A., Rimm, D. L., Velcheti, V. & Madabhushi, A. \\nArtificial intelligence in digital pathology: new tools for diagnosis \\nand precision oncology. Nat. Rev. Clin. Oncol. 16, 703–715 (2019).\\n3. Lipkova, J. et al. Artificial intelligence for multimodal data \\nintegration in oncology. Cancer Cell 40, 1095–1110 (2022).\\n4. Heinz, C. N., Echle, A., Foersch, S., Bychkov, A. & Kather, J. N. The \\nfuture of artificial intelligence in digital pathology: results of a survey \\nacross stakeholder groups. Histopathology 80, 1121–1127 (2022).\\n5. Coudray, N. et al. Classification and mutation prediction from \\nnon-small cell lung cancer histopathology images using deep \\nlearning. Nat. Med. 24, 1559–1567 (2018).\\n6. Kather, J. N. et al. Deep learning can predict microsatellite \\ninstability directly from histology in gastrointestinal cancer.  \\nNat. Med. 25, 1054–1056 (2019).\\n7. Mobadersany, P. et al. Predicting cancer outcomes from histology \\nand genomics using convolutional networks. Proc. Natl Acad.  \\nSci. USA 115, E2970–E2979 (2018).\\n8. Amgad, M. et al. A population-level digital histologic biomarker \\nfor enhanced prognosis of invasive breast cancer. Nat. Med. 30, \\n85–97 (2024).\\n9. Chen, R. J. et al. Pan-cancer integrative histology-genomic \\nanalysis via multimodal deep learning. Cancer Cell 40, 865–878 \\n(2022).\\n10. Vanguri, R. S. et al. Multimodal integration of radiology, pathology \\nand genomics for prediction of response to PD-(L)1 blockade in \\npatients with non-small cell lung cancer. Nat. Cancer 3, 1151–1164 \\n(2022).\\n11. Cooper, M., Ji, Z. & Krishnan, R. G. Machine learning in \\ncomputational histopathology: challenges and opportunities. \\nGenes Chromosomes Cancer 62, 540–556 (2023).\\n12. Graham, S. et al. Screening of normal endoscopic large bowel \\nbiopsies with interpretable graph learning: a retrospective study. \\nGut 72, 1709–1721 (2023).\\n13. Ozyoruk, K. B. et al. A deep-learning model for transforming the \\nstyle of tissue images from cryosectioned to formalin-fixed and \\nparaffin-embedded. Nat. Biomed. Eng. 6, 1407–1419 (2022).\\n14. Lu, M. Y. et al. AI-based pathology predicts origins for cancers of \\nunknown primary. Nature 594, 106–110 (2021).\\n15. Lu, M. Y. et al. Data-efficient and weakly supervised computational \\npathology on whole-slide images. Nat. Biomed. Eng. 5, 555–570 \\n(2021).\\n16. Kather, J. N. et al. Pan-cancer image-based detection of clinically \\nactionable genetic alterations. Nat. Cancer 1, 789–799 (2020).\\n17. Fu, Y. et al. Pan-cancer computational histopathology reveals \\nmutations, tumor composition and prognosis. Nat. Cancer 1, \\n800–810 (2020).\\n18. Bulten, W. et al. Artificial intelligence for diagnosis and Gleason \\ngrading of prostate cancer: the PANDA challenge. Nat. Med. 28, \\n154–163 (2022).\\n19. Foersch, S. et al. Multistain deep learning for prediction of \\nprognosis and therapy response in colorectal cancer. Nat. Med. \\n29, 430–439 (2023).\\n20. Chen, R. J. et al. Multimodal co-attention transformer for survival \\nprediction in gigapixel whole slide images. In Proceedings of  \\nthe IEEE/CVF International Conference on Computer Vision, \\n4015–4025 (2021).\\n21. He, K. et al. Masked autoencoders are scalable vision learners. In \\nProceedings of the IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 16000–16009 (2022).\\n22. Oquab, M. et al. DINOv2: learning robust visual features without \\nsupervision. Preprint at https://doi.org/10.48550/arxiv.2304.07193 \\n(2023).\\nNature Medicine | Volume 30 | March 2024 | 850–862 860\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n23. Balestriero, R. et al. A cookbook of self-supervised learning. \\nPreprint at https://doi.org/10.48550/arxiv.2304.12210 (2023).\\n24. Chen, X., Xie, S. & He, K. An empirical study of training \\nself-supervised vision transformers. In Proceedings of the  \\nIEEE/CVF International Conference on Computer Vision (2021).\\n25. Caron, M. et al. Emerging properties in self-supervised vision \\ntransformers. In Proceedings of the IEEE/CVF international \\nConference on Computer Vision, 9650–9660 (2021).\\n26. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple \\nframework for contrastive learning of visual representations.  \\nIn International Conference on Machine Learning, 1597–1607 \\n(PMLR, 2020).\\n27. Grill, J.-B. et al. Bootstrap your own latent: a new approach \\nto self-supervised learning. Adv. Neural Inf. Process. Syst. 33, \\n21271–21284 (2020).\\n28. Deng, J. et al. ImageNet: a large-scale hierarchical image \\ndatabase. In 2009 IEEE Conference on Computer Vision and \\nPattern Recognition, 248–255 (IEEE, 2009).\\n29. Russakovsky, O. et al. ImageNet large scale visual recognition \\nchallenge. International Journal of Computer Vision 115, 211–252 \\n(2015).\\n30. Sun, C., Shrivastava, A., Singh, S. & Gupta, A. Revisiting \\nunreasonable effectiveness of data in deep learning era. In \\nProceedings of the IEEE International Conference on Computer \\nVision, 843–852 (2017).\\n31. Zhai, X., Kolesnikov, A., Houlsby, N. & Beyer, L. Scaling vision \\ntransformers. In Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 12104–12113 (2022).\\n32. Goyal, P., Mahajan, D., Gupta, A. & Misra, I. Scaling and \\nbenchmarking self-supervised visual representation learning. \\nIn Proceedings of the IEEE/CVF International Conference on \\nComputer Vision, 6391–6400 (2019).\\n33. Bommasani, R. et al. On the opportunities and risks of foundation \\nmodels. Preprint at https://doi.org/10.48550/arxiv.2108.07258 \\n(2021).\\n34. Yuan, L. et al. Florence: A new foundation model for computer \\nvision. Preprint at https://doi.org/10.48550/arxiv.2111.11432 (2021).\\n35. Weinstein, J. N. et al. The Cancer Genome Atlas Pan-Cancer \\nanalysis project. Nat. Genet. 45, 1113–1120 (2013).\\n36. Chen, R. J. & Krishnan, R. G. Self-supervised vision transformers \\nlearn visual concepts in histopathology. In Learning Meaningful \\nRepresentations of Life, NeurIPS 2021 (2022).\\n37. Wang, X. et al. Transformer-based unsupervised contrastive \\nlearning for histopathological image classification. Med. Image \\nAnal. 81, 102559 (2022).\\n38. Azizi, S. et al. Robust and data-efficient generalization of \\nself-supervised machine learning for diagnostic imaging.  \\nNat. Biomed. Eng. 7, 756–779 (2023).\\n39. Kang, M., Song, H., Park, S., Yoo, D. & Pereira, S. Benchmarking \\nself-supervised learning on diverse pathology datasets. In \\nProceedings of the IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 3344–3354 (2023).\\n40. Li, B., Li, Y. & Eliceiri, K. W. Dual-stream multiple instance learning \\nnetwork for whole slide image classification with self-supervised \\ncontrastive learning. In Proceedings of the IEEE/CVF Conference \\non Computer Vision and Pattern Recognition, 14318–14328 (2021).\\n41. Lazard, T., Lerousseau, M., Decencière, E. & Walter, T. Giga-SSL: \\nself-supervised learning for gigapixel images. In Proceedings \\nof the IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition, 4304–4313 (2023).\\n42. Schirris, Y., Gavves, E., Nederlof, I., Horlings, H. M. & Teuwen, J. \\nDeepSMILE: contrastive self-supervised pre-training benefits \\nMSI and HRD classification directly from H&E whole-slide images \\nin colorectal and breast cancer. Med. Image Anal. 79, 102464 \\n(2022).\\n43. Vu, Q. D., Rajpoot, K., Raza, S. E. A. & Rajpoot, N. Handcrafted \\nHistological Transformer (H2T): unsupervised representation of \\nwhole slide images. Med. Image Anal. 85, 102743 (2023).\\n44. Zhao, Y. et al. Predicting lymph node metastasis using \\nhistopathological images based on multiple instance learning \\nwith deep graph convolution. In Proceedings of the IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition, \\n4837–4846 (2020).\\n45. Wang, X. et al. RetCCL: clustering-guided contrastive learning \\nfor whole-slide image retrieval. Med. Image Anal. 83, 102645 \\n(2023).\\n46. Filiot, A. et al. Scaling self-supervised learning for histopathology \\nwith masked image modeling. Preprint at https://doi.org/10.1101/ \\n2023.07.21.23292757 (2023).\\n47. Srinidhi, C. L., Kim, S. W., Chen, F.-D. & Martel, A. L. Self-supervised \\ndriven consistency training for annotation efficient histopathology \\nimage analysis. Med. Image Anal. 75, 102256 (2022).\\n48. Koohbanani, N. A., Unnikrishnan, B., Khurram, S. A., \\nKrishnaswamy, P. & Rajpoot, N. Self-Path: self-supervision for \\nclassification of pathology images with limited annotations.  \\nIEEE Trans. Med. Imaging 40, 2845–2856 (2021).\\n49. Ciga, O., Xu, T. & Martel, A. L. Self supervised contrastive learning \\nfor digital histopathology. Machine Learning with Applications 7, \\n100198 (2022).\\n50. Lin, T. et al. SGCL: spatial guided contrastive learning on whole- \\nslide pathological images. Med. Image Anal. 89, 102845 (2023).\\n51. Tellez, D., Litjens, G., van der Laak, J. & Ciompi, F. Neural image \\ncompression for gigapixel histopathology image analysis.  \\nIEEE Trans. Pattern Anal. Mach. Intell. 43, 567–578 (2021).\\n52. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. & Zou, J.  \\nA visual–language foundation model for pathology image \\nanalysis using medical Twitter. Nat. Med. 29, 2307–2316  \\n(2023).\\n53. Jiang, C. et al. Hierarchical discriminative learning improves visual \\nrepresentations of biomedical microscopy. In Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, \\n19798–19808 (2023).\\n54. Saldanha, O. L. et al. Self-supervised attention-based deep \\nlearning for pan-cancer mutation prediction from histopathology. \\nNPJ Precis. Oncol. 7, 35 (2023).\\n55. Lu, M. Y. et al. Visual language pretrained multiple instance \\nzero-shot transfer for histopathology images. In Proceedings \\nof the IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition (CVPR), 19764–19775 (2023).\\n56. Mokhtari, R. et al. Interpretable histopathology-based prediction \\nof disease relevant features in inflammatory bowel disease \\nbiopsies using weakly-supervised deep learning. In Medical \\nImaging with Deep Learning 479–495 (PMLR, 2023).\\n57. Jaume, G. et al. Modeling dense multimodal interactions between \\nbiological pathways and histology for survival prediction. Preprint \\nat https://doi.org/10.48550/arxiv.2304.06819 (2023).\\n58. Hörst, F. et al. Histology-based prediction of therapy response to \\nneoadjuvant chemotherapy for esophageal and esophagogastric \\njunction adenocarcinomas using deep learning. JCO Clin. Cancer \\nInform. 7, e2300038 (2023).\\n59. Wagner, S. J. et al. Transformer-based biomarker prediction from \\ncolorectal cancer histology: a large-scale multicentric study. \\nCancer Cell 41, 1650–1661 (2023).\\n60. Hörst, F. et al. CellViT: vision transformers for precise cell \\nsegmentation and classification. Preprint at https://doi.org/ \\n10.48550/arxiv.2306.15350 (2023).\\n61. Kaczmarzyk, J. R. et al. ChampKit: a framework for rapid \\nevaluation of deep neural networks for patch-based histo-\\npathology classification. Computer Methods and Programs in \\nBiomedicine 239, 107631 (2023).\\nNature Medicine | Volume 30 | March 2024 | 850–862\\n 861\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n62. Zhang, J. et al. Gigapixel whole-slide images classification using \\nlocally supervised learning. In International Conference on \\nMedical Image Computing and Computer-Assisted Intervention, \\n192–201 (Springer, 2022).\\n63. Nasrallah, M. P. et al. Machine learning for cryosection pathology \\npredicts the 2021 WHO classification of glioma. Med. 4, 526–540 \\n(2023).\\n64. Li, H. et al. Task-specific fine-tuning via variational information \\nbottleneck for weakly-supervised pathology whole slide image \\nclassification. In Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 7454–7463 (2023).\\n65. Ikezogwo, W. O. et al. Quilt-1M: One million image-text pairs for \\nhistopathology. In Advances in Neural Information Processing \\nSystems (2023).\\n66. Zhang, D. et al. Inferring super-resolution tissue architecture  \\nby integrating spatial transcriptomics with histology.  \\nNat. Biotechnol., https://doi.org/10.1038/s41587-023-02019-9 \\n(2024).\\n67. Saltz, J. et al. Spatial organization and molecular correlation of \\ntumor-infiltrating lymphocytes using deep learning on pathology \\nimages. Cell Rep. 23, 181–193 (2018).\\n68. Komura, D. et al. Universal encoding of pan-cancer histology by \\ndeep texture representations. Cell Rep. 38, 110424 (2022).\\n69. Kalra, S. et al. Yottixel: an image search engine for large archives \\nof histopathology whole slide images. Med. Image Anal. 65, \\n101757 (2020).\\n70. Schmauch, B. et al. A deep learning model to predict RNA-seq \\nexpress ion of tumours from whole slide images. Nat. Commun. 11, \\n3877 (2020).\\n71. Graham, S. et al. One model is all you need: multi-task learning \\nenables simultaneous histology image segmentation and \\nclassification. Med. Image Anal. 83, 102685 (2023).\\n72. Diao, J. A. et al. Human-interpretable image features derived \\nfrom densely mapped cancer pathology slides predict diverse \\nmolecular phenotypes. Nat. Commun. 12, 1613 (2021).\\n73. Wulczyn, E. et al. Deep learning-based survival prediction for \\nmultiple cancer types using histopathology images. PLoS One 15, \\ne0233678 (2020).\\n74. Riasatian, A. et al. Fine-tuning and training of DenseNet for \\nhistopathology image representation using TCGA diagnostic \\nslides. Med. Image Anal. 70, 102032 (2021).\\n75. Dosovitskiy, A. et al. An image is worth 16×16 words: transformers \\nfor image recognition at scale. In International Conference on \\nLearning Representations (2021).\\n76. GTEx Consortium Human genomics. The Genotype-Tissue \\nExpression (GTEx) pilot analysis: multitissue gene regulation in \\nhumans. Science 348, 648–660 (2015).\\n77. Kundra, R. et al. OncoTree: a cancer classification system  \\nfor precision oncology. JCO Clin. Cancer Inform. 5, 221–230 \\n(2021).\\n78. Bejnordi, B. E. et al. Diagnostic assessment of deep learning \\nalgorithms for detection of lymph node metastases in women \\nwith breast cancer. JAMA 318, 2199–2210 (2017).\\n79. Campbell, J. D. et al. Distinct patterns of somatic genome \\nalterations in lung adenocarcinomas and squamous cell \\ncarcinomas. Nat. Genet. 48, 607–616 (2016).\\n80. Shao, Z. et al. TransMIL: transformer based correlated multiple \\ninstance learning for whole slide image classification. In 35th \\nConference on Neural Information Processing Systems (2021).\\n81. Campanella, G. et al. Clinical-grade computational pathology \\nusing weakly supervised deep learning on whole slide images. \\nNat. Med. 25, 1301–1309 (2019).\\n82. Gatta, G. et al. Burden and centralised treatment in Europe of rare \\ntumours: results of RARECAREnet – a population-based study. \\nLancet Oncol. 18, 1022–1039 (2017).\\n83. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple \\ninstance learning. In Proceedings of the 35th International \\nConference on Machine Learning, 2132–2141 (2018).\\n84. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for \\nimage recognition. In Proceedings of the IEEE Conference on \\nComputer Vision and Pattern Recognition, 770–778 (2016).\\n85. Kim, Y. J. et al. PAIP 2019: liver cancer segmentation challenge. \\nMed. Image Anal. 67, 101854 (2021).\\n86. Lipkova, J. et al. Deep learning-enabled assessment of cardiac \\nallograft rejection from endomyocardial biopsies. Nat. Med. 28, \\n575–582 (2022).\\n87. Brennan, C. W. et al. The somatic genomic landscape of \\nglioblastoma. Cell 155, 462–477 (2013).\\n88. Cancer Genome Atlas Research Network. Comprehensive, \\nintegrative genomic analysis of diffuse lower-grade gliomas.  \\nN. Engl. J. Med. 372, 2481–2498 (2015).\\n89. Roetzer-Pejrimovsky, T. et al. The Digital Brain Tumour Atlas, an \\nopen histopathology resource. Sci. Data 9, 55 (2022).\\n90. Pati, P. et al. Weakly supervised joint whole-slide segmentation \\nand classification in prostate cancer. Preprint at https://doi.org/ \\n10.48550/arxiv.2301.02933 (2023).\\n91. Jacovi, A., Caciularu, A., Goldman, O. & Goldberg, Y. Stop \\nuploading test data in plain text: practical strategies for mitigating \\ndata contamination by evaluation benchmarks. Preprint at  \\nhttps://doi.org/10.48550/arxiv.2305.10160 (2023).\\n92. Magar, I. & Schwartz, R. Data contamination: from memorization \\nto exploitation. In Proceedings of the 60th Annual Meeting of \\nthe Association for Computational Linguistics (Volume 2: Short \\nPapers), 157–165 (2022).\\n93. Brown, T. et al. Language models are few-shot learners.  \\nAdv. Neural Inf. Process. Syst. 33, 1877–1901 (2020).\\n94. Dodge, J. et al. Documenting large webtext corpora: a case study \\non the colossal clean crawled corpus. In Proceedings of the 2021 \\nConference on Empirical Methods in Natural Language Processing, \\n1286–1305 (2021).\\n95. Kapoor, S. & Narayanan, A. Leakage and the reproducibility crisis \\nin machine-learning-based science. Patterns 4(9), 100804 (2023).\\n96. Xiang, J. & Zhang, J. Exploring low-rank property in multiple instance \\nlearning for whole slide image classification. In The Eleventh \\nInternational Conference on Learning Representations (2022).\\n97. Niehues, J. M. et al. Generalizable biomarker prediction from \\ncancer pathology slides with self-supervised deep learning: \\na retrospective multi-centric study. Cell Rep. Med. 4, 100980 \\n(2023).\\n98. Kather, J. N. et al. Predicting survival from colorectal cancer \\nhistology slides using deep learning: a retrospective multicenter \\nstudy. PLoS Med. 16, e1002730 (2019).\\n99. Pataki, B. Á. et al. HunCRC: annotated pathological slides \\nto enhance deep learning applications in colorectal cancer \\nscreening. Sci. Data 9, 370 (2022).\\n100. Barbano, C. A. et al. UniToPatho, a labeled histopathological \\ndataset for colorectal polyps classification and adenoma \\ndysplasia grading. In 2021 IEEE International Conference on Image \\nProcessing (ICIP), 76–80 (IEEE, 2021).\\n101. Huo, X. et al. Comprehensive AI model development for Gleason \\ngrading: from scanning, cloud-based annotation to pathologist–\\nAI interaction. Preprint at https://doi.org/10.2139/ssrn.4172090 \\n(2022).\\n102. Komura, D. et al. Restaining-based annotation for cancer histology \\nsegmentation to overcome annotation-related limitations among \\npathologists. Patterns 4, 100688 (2023).\\n103. Cheng, B., Misra, I., Schwing, A. G., Kirillov, A. & Girdhar, R.  \\nMasked-attention mask transformer for universal image \\nsegmentation. In 2021 IEEE/CVF Conference on Computer Vision \\nand Pattern Recognition (CVPR) (2022).\\nNature Medicine | Volume 30 | March 2024 | 850–862 862\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n104. Fang, Y. et al. EVA: exploring the limits of masked visual \\nrepresentation learning at scale. In Proceedings of the IEEE/CVF  \\nConference on Computer Vision and Pattern Recognition, \\n19358–19369 (2023).\\n105. Wang, Y., Chao, W.-L., Weinberger, K. Q. & van der Maaten, L.  \\nSimpleShot: revisiting nearest-neighbor classification for few- \\nshot learning. Preprint at https://doi.org/10.48550/arxiv. \\n1911.04623 (2019).\\n106. Snell, J., Swersky, K. & Zemel, R. Prototypical networks for \\nfew-shot learning. In Advances in Neural Information Processing \\nSystems 30 (2017).\\n107. Vorontsov, E. et al. Virchow: a million-slide digital pathology \\nfoundation model. Preprint at https://doi.org/10.48550/arxiv. \\n2309.07778 (2023).\\n108. Campanella, G. et al. Computational pathology at health system \\nscale: self-supervised foundation models from three billion \\nimages. Preprint at https://doi.org/10.48550/arxiv.2310.07033 \\n(2023).\\n109. Lai, J. et al. Domain-specific optimization and diverse evaluation \\nof self-supervised models for histopathology. Preprint at  \\nhttps://doi.org/10.48550/arxiv.2310.13259 (2023).\\n110. Moor, M. et al. Foundation models for generalist medical artificial \\nintelligence. Nature 616, 259–265 (2023).\\n111. Chen, Z. et al. Vision transformer adapter for dense predictions. \\nIn The Eleventh International Conference on Learning \\nRepresentations (2023).\\n112. Wang, X. et al. SCL-WC: cross-slide contrastive learning for \\nweakly-supervised whole-slide image classification. Advances  \\nin Neural Information Processing Systems 35, 18009–18021  \\n(2022).\\n113. Kolesnikov, A., Zhai, X. & Beyer, L. Revisiting self-supervised  \\nvisual representation learning. In Proceedings of the IEEE/CVF  \\nConference on Computer Vision and Pattern Recognition, \\n1920–1929 (2019).\\n114. Chen, R. J. et al. Algorithmic fairness in artificial intelligence for \\nmedicine and healthcare. Nat. Biomed. Eng. 7, 719–742 (2023).\\n115. Lu, M. Y. et al. Towards a visual-language foundation model for \\ncomputational pathology. Preprint at https://doi.org/10.48550/\\narxiv.2307.12914 (2023).\\n116. Lu, M. Y. et al. A foundational multimodal vision language AI \\nassistant for human pathology. Preprint at https://doi.org/ \\n10.48550/arxiv.2312.07814 (2023).\\n117. Chen, R. J. et al. Scaling vision transformers to gigapixel images \\nvia hierarchical self-supervised learning. In Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition \\n(2022).\\n118. Zhou, J. et al. iBOT: image BERT pre-training with online tokenizer. \\nIn International Conference on Learning Representations (2022).\\nPublisher’s note Springer Nature remains neutral with regard to \\njurisdictional claims in published maps and institutional affiliations.\\nSpringer Nature or its licensor (e.g. a society or other partner) holds \\nexclusive rights to this article under a publishing agreement with \\nthe author(s) or other rightsholder(s); author self-archiving of the \\naccepted manuscript version of this article is solely governed by the \\nterms of such publishing agreement and applicable law.\\n© The Author(s), under exclusive licence to Springer Nature America, \\nInc. 2024\\n1Department of Pathology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA. 2Department of Pathology, Massachusetts General \\nHospital, Harvard Medical School, Boston, MA, USA. 3Cancer Program, Broad Institute of Harvard and MIT, Cambridge, MA, USA. 4Cancer Data Science \\nProgram, Dana-Farber Cancer Institute, Boston, MA, USA. 5Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA. 6Harvard \\nJohn A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA. 7Electrical Engineering and Computer Science, \\nMassachusetts Institute of Technology (MIT), Cambridge, MA, USA. 8Health Sciences and Technology, Harvard-MIT, Cambridge, MA, USA. 9Department \\nof Systems Biology, Harvard University, Cambridge, MA, USA. 10Harvard Data Science Initiative, Harvard University, Cambridge, MA, USA. 11These authors \\ncontributed equally: Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K. Williamson. \\u2009e-mail: faisalmahmood@bwh.harvard.edu\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nMethods\\nLarge-scale visual pretraining\\nMass General Brigham institutional review board approved the ret -\\nrospective analysis of internal pathology images and corresponding \\nreports used in this study. All internal digital data, including WSIs, \\npathology reports and electronic medical records were de-identified \\nbefore computational analysis and model development. Patients were \\nnot directly involved or recruited for the study. Informed consent \\nwas waived for the retrospective analysis of archival pathology slides. \\nIn developing and evaluating self-supervised models in CPath, an \\nimportant and relatively under-discussed challenge is the difficulty \\nin developing large-scale models that can also be used for evaluation \\non public histology datasets. For natural images, ImageNet-1K is an \\nintegral dataset for the model development and evaluation lifecycle \\nof self-supervised learning methods. Specifically, models are first \\npretrained on the training set of ImageNet-1K and then evaluated \\nwith fine-tuning and linear probe performance on the validation set \\n(treated as the test set), reported as a community-accepted ‘goodness \\nof fit’113,119, with further evaluation of generalization performance via \\nother downstream tasks such as fine-grained classification and activity \\nvideo recognition. Although such off-the-shelf self-supervised learning \\nmethods can readily be adapted to CPath, we note that there is consid-\\nerably less public data for pretraining in CPath than natural images and \\nthat pretraining on large, public collections of histology slides also \\nrestricts their adaptability to public CPath benchmarks. Specifically, \\nthe development of many self-supervised pathology models has been \\nlimited to pretraining on TCGA35, one of the largest and most diverse \\npublic histology datasets for CPath, with many models opting to use the \\nentire TCGA collection to realize data scaling benefits in self-supervised \\nlearning37,38,117. However, their applicability to public CPath benchmarks \\nmay be restricted to transductive inference 37,40,41,44,46,57,117, given that \\nmany popular clinical tasks in CPath are also derived from TCGA (for \\nexample, pan-cancer analyses6,9,16,17,61,67–74) and thus extensive evaluation \\nof out-of-domain, generalization performance is limited. Although \\ndatasets such as CAMELYON78,120 and PANDA18 can be used to evaluate \\nTCGA-pretrained models, we note that these datasets are limited to \\nsingle tissue types with limited disease categories.\\nDataset curation for Mass-100K.  T o overcome this limitation, we \\ndeveloped Mass-100K, a large-scale and diverse pretraining data -\\nset consisting of in-house histology slides from MGH and BWH, and \\nexternal histology slides from the GTEx consortium. Following natural \\nimage datasets, we also created three partitions of Mass-100K that \\nvary in size to evaluate the data scaling laws, an empirical observation \\nfound in natural language and image foundation models that scaling \\nof dataset size would also increase model performance 21–23,75. Analo-\\ngous to ImageNet-22K and ImageNet-1K, we developed the Mass-22K \\ndataset, which contains 16,059,454 histology image patches sampled \\nfrom 21,444 diagnostic formalin-fixed paraffin-embedded (FFPE) H&E \\nWSIs across 20 major tissue types consisting mostly of cancer tissue, \\nas well as its subset, Mass-1K (1,064,615 images, 1,404 WSIs). All his -\\ntology slides in Mass-22K and Mass-1K were collected from BWH, and \\nscanned using an Aperio GT450 scanner or a Hamamatsu S210 scanner. \\nT o make the image dataset sizes approximately equivalent to that of \\nImageNet-22K and ImageNet-1K, we sample approximately 800 image \\npatches from histology tissue regions of each WSI, with image resolu-\\ntions of 256\\u2009×\\u2009256\\u2009pixels at ×20 magnification. For slide preprocessing, \\nwe adapted the WSI preprocessing in the CLAM (clustering-constrained \\nattention-based multiple-instance learning) toolbox15, which performs \\ntissue segmentation at a low resolution via binary thresholding of \\nthe saturation channel in RGB\\u2009→\\u2009HSV color space; median blurring, \\nmorphological closing and filtering of contours below a minimum \\narea to smooth tissue contours and remove artifacts; and patch  \\ncoordinate extraction of non-overlapping 256\\u2009×\\u2009256 tissue patches  \\nin the segmented tissue regions of each WSI at ×20 magnification.  \\nThe distribution of major tissue types in Mass-22K and Mass-1K are \\ngiven in Supplementary Tables 2 and 3, respectively\\nInspired by even larger natural image datasets such as L VD-142M22 \\nand JFT-300M30, we developed Mass-100K, which combines Mass-22K \\nwith further in-house FFPE H&E histology slide collections (includ -\\ning renal and cardiac transplant tissue) and GTEx 76, which consists \\nof 24,782 noncancerous, human autopsy WSIs. Additional in-house \\nslides were collected from both BWH and MGH, and scanned using an \\nAperio GT450 scanner or a Hamamatsu S210 scanner. We purposefully \\nexcluded other public histology slide collections such as TCGA, CPTAC \\nand PAIP for the external evaluation of UNI. Altogether, Mass-100K \\nincludes 100,426 histology slides, with the distribution of major tissue \\ntypes given in Supplementary Table 1. Following the slide preprocess-\\ning protocol reported above, sampling approximately 800 histology \\ntissue patches per WSI in Mass-100K yielded 75,832,905 images at \\n256\\u2009×\\u2009256\\u2009pixels at ×20 magnification. For high-resolution fine-tuning in \\nDINOv2, we sampled an additional 24,297,995 images at 512\\u2009×\\u2009512 pixels \\nat ×20 magnificatin, which altogether yielded 100,130,900 images for \\npretraining in Mass-100K.\\nNetwork architecture and pretraining protocol. For large-scale visual \\npretraining on Mass-100K we used DINOv2 22, a state-of-the-art \\nself-supervised learning method based on student–teacher knowledge \\ndistillation for pretraining large ViT architectures. DINOv2 is an exten-\\nsion of two previous methods, DINO25 and iBOT118, and uses two main \\nloss objectives: self-distillation loss (that is, alignment loss in Fig. 1b) \\nand masked image modeling loss (that is, reconstruction loss in Fig. 1b), \\nto achieve state-of-the-art results in linear probe accuracy. DINOv2 \\nalso demonstrates capabilities in understanding the semantic layout \\nof histopathology images when pretrained using knowledge distilla-\\ntion117. Self-distillation, introduced in BYOL27 for CNN pretraining and \\nDINO25 for ViT pretraining, minimizes the predictive categorical dis-\\ntributions from the teacher (UNI T eacher in Fig. 1b) and student net-\\nwork (UNI in Fig. 1b) obtained from two augmented views of the same \\nimage by minimizing their cross-entropy loss. The teacher is updated \\nas an exponential moving average of previous iterations of the student. \\nMasked image modeling using an online tokenizer, introduced in \\niBOT118, involves strategically masking specific regions in an input \\nimage and training the model to predict the masked regions based on \\nthe remaining contextual information. This approach captures \\nhigh-level visual features and context, inspired by masked language \\nmodeling in BERT121. Specifically, we denote two augmented views of \\nan input image x as u and v, which are subsequently randomly masked. \\nThe masked images of u and v are represented as ̂u and ̂v, respectively. \\nWhile u and v are propagated through the teacher network, the student \\nnetwork receives ̂u and ̂v as inputs. For the self-distillation objective, \\nwe compute cross-entropy loss between the [CLS] (that is, classifica-\\ntion) token from the teacher network and the [CLS] token from the \\nstudent network. For the masked image modeling objective, DINOv2 \\nuses the output of the masked tokens from the student network to \\npredict the patch tokens from the teacher network, where the teacher \\nnetwork can be regarded as an online tokenizer. We used DINOv2 \\nbecause an important property for pretrained vision models in histo-\\npathology is linear probe performance, given that these models are \\noften used as frozen feature extractors for pre-extracting patch fea-\\ntures in weakly supervised slide-level tasks. Although other ViT-based \\nself-supervised methods have demonstrated superior fine-tuning \\nperformance21,122, their linear probe performance is not comparable, \\nand note that full fine-tuning in ROI-level and slide-level tasks is not \\nalways feasible due to cost in collecting annotations.\\nFor smaller-scale visual pretraining on Mass-1K and Mass-22K \\nwe used iBOT, which has the same loss objectives introduced above \\nfor DINOv2. We note that iBOT and DINOv2 are overlapping methods \\nthat exist in the same family of ViT pretraining techniques, given that \\nboth methods extend the original DINO method (which introduced \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nstudent–teacher knowledge distillation for ViTs), with iBOT extend-\\ning DINO via the introduction of an online tokenizer component for \\nmasked image modeling, and DINOv2 extending iBOT via the intro-\\nduction of additional modifications, thereby improving training \\nstability and efficiency for larger ViT architectures. These six modi-\\nfications can be summarized as follows: untying of the head weights \\nbetween the above loss objectives instead of tying these objectives as \\nperformed in iBOT118; Sinkhorn–Knopp centering instead of teacher \\nsoftmax-centering performed in iBOT118; KoLeo regularization to \\nimprove token diversity123; high-resolution fine-tuning toward the \\nend of pretraining124; an improved code implementation that imple-\\nments FlashAttention125, fully sharded data-parallel training and an \\nefficient stochastic depth; and an improved pretraining recipe of the \\nViT-Large architecture on large-scale datasets. Last, although iBOT and \\nDINOv2 use the same two loss objectives, the training recipes of these \\nmethods were developed for different data scales: iBOT was developed \\nfor ViT-Base and ViT-Large models on ImageNet-1K and ImageNet-22K, \\nwhile DINOv2 was developed for ViT-Large and ViT-Giant models on \\nL VD-142M, which is a dataset of 142\\u2009million curated natural images. \\nT o leverage the improved training recipe for ViT-Large on large-scale \\ndatasets in DINOv2 while also making comparisons fair to iBOT-trained \\nViT-Base models, we excluded the first two modifications of DINOv2 \\nthat modified the iBOT loss objective (untying of head weights and use \\nof Sinkhorn–Knopp centering), as outlined in Supplementary Table 5. \\nHigh-resolution fine-tuning was also conducted on the last 12,500 \\niterations of pretraining (out of 125,000 iterations in total).\\nEvaluation setting\\nComparisons and baselines.  For slide- and ROI-level evaluation, \\nwe compare UNI against three pretrained encoders commonly used \\nin the CPath community. For comparison to models with ImageNet \\nTransfer, we compare against a ResNet-5084 pretrained on ImageNet28 \\n(truncated after the third residual block, 8,543,296 parameters), which \\nis a commonly used baseline in many slide-level tasks15,20. For com-\\nparison to the current state-of-the-art encoders, we compare against \\nCTransPath37, which is a Swin transformer126 using the ‘tiny’ configu-\\nration with a window size of 14 (Swin-T/14, 28,289,038 parameters) \\npretrained mostly on the TCGA via MoCoV3 (ref. 24), and REMEDIS38, \\na ResNet-152\\u2009×\\u20092 (232,230,016 parameters) initialized with the ‘Big \\nTransfer’-medium protocol127 on ImageNet-22K and then pretrained \\nwith SimCLR26. Regarding data distributions, CTransPath was pre-\\ntrained using 29,753 WSIs across 25 anatomic sites in TCGA (including \\nboth FFPE and frozen tissue slides) and 2,457 WSIs from PAIP85 across six \\nanatomic sites, with 15,580,262 tissue patches and 32,120 WSIs used for \\npretraining altogether. REMEDIS was pretrained with a random sample \\nof ~50\\u2009million patches from 29,018 WSIs also across 25 anatomic sites \\nin TCGA. For self-supervised learning, CTransPath was trained using \\nthe MoCoV3 (ref. 24) algorithm for 100 epochs, with ~1.56\\u2009×\\u2009109 (or \\n1.56\\u2009billion) images seen during pretraining, and REMEDIS was trained \\nusing the SimCLR algorithm for a maximum of 1,000 epochs, with \\nupwards of ~50\\u2009×\\u2009109 (or 50\\u2009billion) images seen during pretraining. In \\nour implementation of these pretrained encoders, we use the truncated \\nResNet-50 implementation provided by CLAM15, and use the official \\nmodel checkpoints for CTransPath and REMEDIS. The image embed-\\ndings outputted by these models are 1,024, 768 and 4,096, respectively. \\nSimilar to ResNet-50 and other ResNet models in which the penultimate \\nfeature layer before the classification head is a grid-like feature map of \\n[1\\u2009×\\u20097\\u2009×\\u20097\\u2009×\\u20094,096]-dimensions, we apply a two-dimensional (2D) adap-\\ntive average pooling layer to output a single [1\\u2009×\\u20094,096]-dimensional \\nimage embedding. For all images used in ROI tasks and extracted \\npatches for MIL in slide tasks, across all models, all feature extraction \\noperations are performed on resized 224\\u2009×\\u2009224 images at ×20 magni-\\nfication. We note that the Swin-T/14 architecture used by CTransPath \\nhas constraints in which it can take only image dimensions in which \\nthe length is divisible by 224. We also note that although CTransPath \\nwas pretrained on ×10 magnification, it demonstrates state-of-the-art \\nperformance at ×20 magnification55,59,128. All pretrained encoders use \\nImageNet mean and standard deviation parameters for image nor-\\nmalization (including UNI). T o compare against transfer learning \\nfrom a general pathology task, we also trained a ViT-L/16 architecture \\n(initialized with ImageNet-22K transfer) end-to-end on the 32-class \\npan-cancer tissue classification task in TCGA. In several benchmarking \\ntasks, we note that this ablation study performed worse than UNI, even \\non in-domain tasks such as pan-cancer tumor-immune lymphocyte \\ndetection in TCGA (Supplementary Table 72).\\nLast, we note that although many slide and ROI tasks are created \\nusing annotated data from the TCGA, CTransPath and REMEDIS were \\nalso trained using almost all slides in the TCGA, which can result in \\ninformation leakage that inflates the performance of these models on \\nTCGA benchmarks. When possible, we report evaluation on external \\ncohorts outside of TCGA for all tasks. This may not be possible for \\nall tasks, given that the official train–validation–test folds may all be \\ndeveloped using TCGA.\\nWeakly supervised slide classification.  Training and evaluation \\nfor weakly supervised slide classification tasks follow the con -\\nventional two-stage MIL paradigm consisting of pre-extraction of \\nROI-level features as instances from non-overlapping tissue patches \\nof segmented tissue regions of the WSI, and the learning of a trainable \\npermutation-invariant pooling operator that aggregates patch-level (or \\ninstance) features into a single slide-level (or bag) feature. For slide pre-\\nprocessing, we use the same WSI preprocessing pipeline as described \\nin the dataset curation section, which uses the CLAM toolbox15, with \\nadditional patch feature extraction using a pretrained encoder per-\\nformed on the patched coordinates. Images are resized down to \\n224\\u2009×\\u2009224\\u2009pixels and normalized using ImageNet mean and standard \\ndeviation parameters. As a quality control, we performed the additional \\nfollowing steps: first, for slides with under- or over-segmented tissue \\nmasks, we adjusted the segmentation parameters in CLAM (threshold \\nvalue and downsample level) to segment only tissue regions; second, \\nwe removed slides that were nonH&E and nonFFPE; and third, for \\nslides that did not have a downsample level equivalent to ×20 mag-\\nnification in their WSI pyramidal format, we patched the tissue into \\nnon-overlapping 512\\u2009×\\u2009512\\u2009pixel tissue patches at ×40 magnification \\nand then later resized these images to 224\\u2009×\\u2009224\\u2009pixels during feature \\nextraction. Pre-extracted features for all pretrained encoders used \\nthe same set of patch coordinates for feature extraction of each WSI.\\nFor comparison of pre-extracted features of pretrained encoders \\nin weakly supervised learning, we used the ABMIL algorithm83 across all \\ntasks, which is a canonical weakly supervised baseline in slide classifica-\\ntion tasks. We use the two-layer gated variant of the ABMIL architecture \\nwith all input embeddings mapped to an embedding dimension of \\n512 in the first fully connected layer, followed by hidden dimensions \\nof 384 in the following intermediate layers. For regularization, we use \\ndropout with P\\u2009=\\u20090.10 applied to the input embeddings and P\\u2009=\\u20090.25 \\nafter each intermediate layer in the network. Aside from the first fully \\nconnected layer, which is dependent on the embedding dimension \\nof the pre-extracted features, all comparisons used the same ABMIL \\nmodel configuration. We trained all ABMIL models using the AdamW \\noptimizer129 with a cosine learning rate scheduler, a learning rate of \\n1\\u2009×\\u200910−4, cross-entropy loss, and a maximum of 20 epochs. We addition-\\nally performed early stopping on the validation loss if a validation fold \\nwas available. For all slide classification tasks, we case-stratified and \\nlabel-stratified the slide dataset into train–validation–test folds, or \\nused official folds if available. Given that CTransPath and REMEDIS  \\nwere pretrained using all slides in TCGA, we considered TCGA  \\nslide tasks in which additional external evaluation was possible (for \\nexample, NSCLC subtyping was included due to availability of LUAD \\nand LUSC slides in CPTAC, whereas BRCA subtyping was excluded). \\nFor glioma IDH1 mutation prediction and histomolecular subtyping, \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\ntrain–validation–test folds were additionally site-stratified to mitigate \\npotential batch effects.\\nLinear and K-nearest neighbors probe evaluation in ROI classifica-\\ntion. For ROI-level classification tasks, we follow previous works that \\nuse logistic regression (linear) probing and KNN probing130 to evaluate, \\nrespectively, discriminative transfer performance and the representa-\\ntion quality of pre-extracted feature embeddings on downstream \\ntasks23. For linear probing, following the practice recommended by \\nthe self-supervised learning community, we fix the ℓ2 regularization \\ncoefficient λ to \\n100\\nMC\\n, where M is the embedding dimension and C is the \\nnumber of classes, and use the L-BFGS solver131 with a maximum of \\n1,000 iterations113. KNN probing is an additional evaluation technique \\nadvocated by the self-supervision community for measuring repre-\\nsentation quality of pre-extracted features25,132,133. In comparison with \\nlinear probing, KNN probing is nonparametric (aside from the choice \\nof K), given that it classifies unseen test examples based on only their \\nfeature similarity to labeled training examples (for example, similar \\nexamples in representation space should also be visually similar and \\nshare the same class label). We use the KNN implementation from \\nScikit-Learn134, trained using K\\u2009=\\u200920 and Euclidean distance as the dis-\\ntance metric, following observed stability of this evaluation setup of \\nother self-supervision works25. For all ROI tasks, we approximately \\ncase-stratified and label-stratified datasets into train–test folds or used \\nofficial folds if available.\\nFor all tasks, we resize images to 224\\u2009×\\u2009224\\u2009pixels (or 448\\u2009×\\u2009448\\u2009pixels  \\nif available) and normalize using ImageNet mean and standard devia-\\ntion parameters. Additionally, we note that many ROI datasets consist \\nof images with high image resolutions, with image resizing to a fixed \\n224\\u2009×\\u2009224\\u2009pixels or 448\\u2009×\\u2009448\\u2009pixels resolution also changing the image \\nmagnification and mpp. For example, resizing ROIs in the CRC polyp \\nclassification task in UniT oPatho (ROIs having an original image resolu-\\ntion of 1,812\\u2009×\\u20091,812\\u2009pixels at 0.45\\u2009mpp) to 224\\u2009×\\u2009224\\u2009pixels would change \\nthe magnification to 3.6\\u2009mpp. For CRC polyp classification as well as \\nBRCA subtyping (BACH), we carry out evaluations using resized image \\nresolutions of {2242\\u2009pixels, 4482\\u2009pixels, 8962\\u2009pixels, 1,7922\\u2009pixels} and \\n{2242\\u2009pixels, 4482\\u2009pixels, 8962\\u2009pixels, 1,3442\\u2009pixels}, with multiples of \\n224 chosen due to constraints with CTransPath. T o pre-extract features \\nfrom high-resolution images, for ViTs such as the plain ViT-large archi-\\ntecture in UNI and the hierarchical Swin transformer-T architecture in \\nCTransPath, the forward passes of these architectures are not modified, \\nand interpolation of positional embeddings is performed to have the \\nsame sequence length as patch tokens in the ROI. T o illustrate, in the \\npatch embedding layer of our ViT-Large architecture in UNI that has a \\npatch token size of 16\\u2009×\\u200916, a 224\\u2009×\\u2009224\\u2009pixel image would be converted \\ninto a [14\\u2009×\\u200914\\u2009×\\u2009D]-dimension 2D grid of patch embeddings using a 2D \\nconvolutional layer (kernel and stride size of 16, three incoming chan-\\nnels from RGB-input image inputs and D-dimension outgoing channels \\nset as a hyper-parameter for feature embedding length), followed by \\nflattening and transposing (now a [196\\u2009×\\u2009D]-dimension sequence of \\npatch embeddings), which can now be used in transformer attention \\n(called ‘patchifying’). For a 1,792\\u2009×\\u20091,792\\u2009pixel image in CRC polyp clas-\\nsification, patchifying this image using the same patch embedding \\nlayer would result in a [112\\u2009×\\u2009112\\u2009×\\u2009D]\\u2009→\\u2009[12,544\\u2009×\\u2009D]-dimension sequence \\nof patch embeddings. Feeding this sequence into the forward pass of \\ntransformer attention, although computationally expensive, is still \\ntractable via memory-efficient implementations such as FlashAttention \\nor MemEffAttention. For positional embedding interpolation, we used \\nthe implementation provided in DINO25. For multi-head self-attention \\n(MHSA) visualization, we visualize the weights from the last attention \\nlayer using the notebook implementation provided by the HIPT code-\\nbase117, which we note is applicable only for plain VIT architectures.\\nROI retrieval. T o assess the quality of embeddings produced by dif-\\nferent encoders for content-based image retrieval of histopathology \\nimages, we use ROI-level classification datasets, in which the goal is \\nto retrieve similar images (that is, images with the same class label) to \\na given query image. For each benchmark, we first embed all images \\ninto a low-dimensional feature representation using the pretrained \\nencoders. We treat each image in the test set as a query. Each query \\nimage is compared with each image from the ROI-level classification \\ntraining set, which serves as a database of candidates (keys). Note that \\nno supervised learning takes place in these experiments and the class \\nlabels are used only for evaluation purposes (that is, to assess whether \\nretrieved images share the same class label as the query). We first \\ncenter the database of keys by subtracting their Euclidean centroid \\nfrom each embedding followed by ℓ2 normalization of each key to unit \\nlength. For each new query, we apply the same shift and normalization \\nsteps and then measure it against each key in the database via the ℓ2 \\ndistance metric, where lower distance is interpreted as higher similar-\\nity. The retrieved images are sorted by their similarity scores and their \\ncorresponding class labels are used to evaluate the success of a given \\nretrieval using Acc@K for K\\u2009∈\\u20091,\\u20093,\\u20095 and MVAcc@5, which are described \\nin Evaluation metrics.\\nROI-level cell type segmentation.  For training and evaluation of \\nROI-level cell type segmentation tasks, we follow previous works in \\nusing Mask2Former, which is a flexible framework commonly used for \\nevaluating off-the-shelf performance of pretrained vision encoders103.  \\nIn the case of the ViT architecture, which is nonhierarchical, we addi-\\ntionally use the ViT-Adapter framework alongside the Mask2Former \\nhead111. For both ViT-Adapter and Mask2Former, we use the same \\nhyper-parameters used for ADE20k semantic segmentation. Specifi-\\ncally, we use the AdamW129 optimizer along with a step learning rate \\nschedule. The initial learning rate was set to 0.0001 and a weight decay \\nof 0.05 was applied. T o adjust the learning rate specifically for the \\nbackbone, we apply a learning rate multiplier of 0.1. Additionally, we \\ndecay the learning rate by a factor of 10 at 0.9 and 0.95 fractions of the \\ntotal number of training steps. For all backbones, we fine-tune the full \\nmodel for 50 epochs with a batch size of 16. The model’s performance \\non the validation set is evaluated every 5 epochs, and the optimal model \\nbased on validation performance is saved for testing. T o augment \\nthe data, we use the large-scale jittering (LSJ) augmentation135, with \\na random scale sampled from a range of 0.5–2.0, followed by a fixed \\nsize crop to 896\\u2009×\\u2009896\\u2009pixels to accommodate the size constraints of \\nCTransPath. At inference time, we resize the image dimensions to their \\nnearest multiples of 224.\\nFew-shot ROI classification and prototype learning.  For few-shot \\nclassification, we follow previous works using the SimpleShot frame-\\nwork to evaluate the few-shot learning performance of prototypical \\nrepresentations of self-supervised models105,136. Prototypical (or pro-\\ntotype) learning is a longstanding task in the few-shot learning commu-\\nnity106,137,138, and it has also been posed (in many related forms) in CPath \\nas well43,139–142. In contrast with traditional few-shot learners based on \\nmeta-learning, SimpleShot and related works demonstrate that strong \\nfeature representations combined with specific transformations and \\nsimple classifiers can reach state-of-the-art performance on few-shot \\ntasks105,136,143. SimpleShot is similar to nearest neighbors classification, \\nin which the training set (called ‘supports’ in few-shot learning litera-\\nture) is drawn from C classes (‘ways’) with K examples per class (‘shots’) \\nfor predicting unseen images in the test set (‘queries’). Instead of near-\\nest neighbors, SimpleShot uses a nearest-centroid approach based on \\nProtoNet106, in which the average feature vector (centroid) for each \\nclass is used as a prototypical ‘one-shot’ example for labeling the query \\nset via distance similarity. As noted, these averaged feature vectors \\ncan also be viewed as ‘class prototypes’ , a set of one-shot representa-\\ntive examples that are unique in representing semantic information \\nsuch as class labels (for example, LUAD versus LUSC morphologies). \\nGiven that SimpleShot is a simple and surprisingly strong baseline \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nin the few-shot learning community and popularized in evaluating \\nself-supervised models136, we adopt this baseline in evaluating UNI \\nand its comparisons in few-shot ROI classification tasks. We follow the \\nrecommendations in SimpleShot that suggest centering (subtracting \\nthe mean computed on the support set) and ℓ2 normalizing the support \\nset before computing the class prototypes, with the query set also \\ntransformed (also centered using the mean of the support set) before \\nnearest centroids classification.\\nConventional few-shot learners on natural image classification \\ntasks are evaluated by drawing 10,000 C-way, K-shot episodes from the \\ntraining set with 15 query images per class as the test set. For equiva-\\nlent comparison with metrics in linear and KNN probing, we instead \\ndraw 1,000 C-way, K-shot episodes but use all images in the test set \\nper episode. Due to the relatively larger number of training examples \\navailable in ROI tasks than that of slide tasks, we vary the number of  \\nlabeled examples per class from K \\u2009∈\\u2009{1,\\u20092,\\u20094,\\u20098,\\u200916,\\u200932,\\u2009…256} or  \\nthe maximum number of labeled examples available for a given class.  \\nT o compare with linear and KNN probing that use all training examples,  \\nwe also evaluate SimpleShot by averaging all training examples per \\nclass, which we denote as ‘1-NN’ in Supplementary Tables 40–60.\\nPrompt-based slide classification using multiple instance Simple-\\nShot. T o evaluate the quality of extracted representations serving as the \\nclass prototype for slide classification tasks, we adapt class prototypes \\nfrom SimpleShot (described above) as ‘prompts’ (similar to the use of \\ntextual prompts in zero-shot classification55), which we describe as \\nMI-SimpleShot. As described in the main text, we use two slide-level \\ndatasets (NSCLC and RCC subtyping datasets), which have matching \\nROI training examples from datasets that can be used as the support \\nset. In brief, we use the annotated LUAD and LUSC ROIs from the TCGA \\nUniform Tumor dataset for NSCLC subtyping, and annotated CCRCC, \\npapillary renal cell carcinoma (PRCC) and chromophobe renal cell \\ncarcinoma (CHRCC) ROIs from the TCGA Uniform Tumor dataset for \\nRCC subtyping. The TCGA Uniform Tumor dataset (described further \\nin Methods) consists of 271,170 256\\u2009×\\u2009256\\u2009pixel ROIs at around 0.5\\u2009mpp \\nof 32 cancer types annotated and extracted from 8,736 H&E FFPE diag-\\nnostic histopathology WSIs. We note that the number of annotated \\nROIs per slide ranges from 10 to 70 examples in the TCGA-LUAD, -LUSC, \\n-CCRCC, -PRCC and -CHRCC cohorts. For each class, we first embed \\nROIs in the support set into a low-dimensional feature representation \\nusing the pretrained encoders, followed by average pooling of all ROI \\nfeatures in the class. The average-pooled feature representations are \\nconsidered as the class prototypes, which are used as prompts for \\nlabeling the top-K ROIs for each slide in the query set via normalized \\nEuclidean distance similarity. The slide-level prediction is then made \\nby majority voting of the top-K ROI predictions. For each benchmark, \\nwe evaluate MI-SimpleShot with both top-5 average pooling and top-\\n50 average pooling and on {1,\\u20092,\\u20094,\\u20098,\\u200916,\\u200932} training slides per class, \\nsimilar to our evaluation in few-shot slide classification using the same \\nfive folds as the trained ABMIL models, with prototypes created from \\nthe annotated ROIs in the same training slides. We note little perfor-\\nmance change in considering the average scores of the top-5 and top-\\n50 patches per class prototype. T o compare with the performance \\nthat uses all training slides with ROI annotations, we also evaluate \\nMI-SimpleShot by averaging all training ROI feature representations per \\nclass, with results detailed in Supplementary Tables 70 and 71. T o create \\nsimilarity heatmaps, we visualize the normalized Euclidean distances of \\nall patches in a slide with respect to the ground-truth class prototype.\\nEvaluation metrics. We report balanced accuracy, weighted F1 score, \\nand AUROC for classification tasks. Balanced accuracy is computed by \\ntaking the unweighted average of the recall of each class, which takes \\ninto account class imbalance in the evaluation set. Weighted F1 score is \\ncomputed by averaging the F1 score (the harmonic mean of precision \\nand recall) of each class, weighted by the size of its respective support \\nset. AUROC is the area under the receiver operating characteristic \\ncurve plotting true-positive rate against the false-positive rate as the \\nclassification threshold is varied. Additionally, we compute quadratic \\nweighted Cohen’s κ (inter-annotator agreement between two sets of \\nlabels, for example, ground truth and predictions) which we perform \\nfor ISUP grading (PANDA), and top-K accuracy for K\\u2009∈\\u2009{1,\\u20093,\\u20095} (for a given \\ntest sample, a sample is scored correctly if the ground-truth label is \\namong the top-K labels predicted) for OT-43 and OT-108. For retrieval, \\nwe consider Acc@K for K\\u2009∈\\u2009{1,\\u20093,\\u20095}, which represent the standard top-K \\naccuracy scores in retrieving images with the same class label as the \\nquery. Specifically, a retrieval is considered successful if at least one \\nimage among the top-K retrieved images has the same class label as \\nthe query. We also report MVAcc@5, which, compared with Acc@5, \\nmore strictly requires that the majority vote of the top-5 retrieved \\nimages be in the same class as the query for retrieval to be considered \\nsuccessful. For segmentation, we report the Dice score (same defini-\\ntion as the F1 score), the precision and recall, macro averaged across \\nall images and classes.\\nStatistical analysis. For all semi- and fully supervised experiments, \\nwe estimate 95% confidence intervals for the model performance with \\nnonparametric bootstrapping using 1,000 bootstrap replicates. For \\nstatistical significance, we use a two-sided paired permutation test with \\n1,000 permutations to assess observed differences in the performance \\nof the two models. For all few-shot settings, we report results using box \\nplots that indicate quartile values of model performance (n\\u2009=\\u20095 runs) \\nwith whiskers extending to data points within 1.5-fold the interquartile \\nrange. For ROI-level few-shot classification, for each C-way, K-shot \\nsetting, we randomly sample K training examples per C classes with \\n1,000 repeated experiments (called ‘episodes’ or ‘runs’) evaluated on \\nthe entire test set. For slide-level few-shot classification, we follow the \\nsame setting as above but with the number of runs limited to 5 due to \\nsmall support sizes in rare disease categories.\\nTasks, datasets and comparisons to leaderboard\\nIn this section we outline the data preprocessing, number of samples  \\nper class, train–validation–test folds and other details per dataset  \\n(which may also span multiple tasks). We also add context and com-\\nparisons of our results to existing leaderboards and baselines of \\nother studies when possible, and note that comparisons may not \\nalways be equivalent due to differences in hyper-parameters, splits \\nand pre-extracted features (many existing baselines may not use \\nhistopathology-specific pretrained encoders). In comparing against \\nleaderboards and in comparisons, we adopt the metrics used in public \\nevaluation, elaborated further in the table captions.\\nOncoTree cancer classification based on in-house BWH data  \\n(43 cancer types, 108 OncoTree codes). As described in the main text, \\nOncoTree cancer classification is a large-scale hierarchical classifica-\\ntion task for CPath that follows the OncoTree (OT) cancer classification \\nsystem77. This task was devised to assess the generalization capabilities \\nof pretrained models in classifying diverse disease categories and tissue \\ntypes. Using in-house BWH slides, we defined a dataset consisting of \\n5,564 WSIs from 43 cancer types further subdivided into 108 OncoTree \\ncodes, with at least 20 WSIs per OncoTree code. The dataset forms the \\nbasis of two tasks that vary in diagnostic difficulty: 43-class cancer \\ntype classification (OT-43) and 108-class OncoTree code classifica-\\ntion (OT-108). Due to the small support sizes for several OncoTree \\ncodes in OT-108, all ABMIL models were trained using train–test folds \\nand without early stopping. For training and evaluation, we approxi-\\nmately label-stratified the dataset into 71:29 train–test folds (a ratio of \\n3,944:1,620 slides) using the same folds for OT-43 and OT-108, with 15 \\nslides used per OncoTree code in the test set and a minimum of 5 slides \\nused per OncoTree code in the training set. The hierarchical classifica-\\ntion of the coarse- and fine-grained task is reported in Supplementary \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nTable 4. Except for bladder urothelial carcinoma (BLCA), invasive ductal \\ncarcinoma (IDC), invasive lobular carcinoma (ILC), colon adenocarci-\\nnoma (COAD), rectum adenocarcinoma (READ), uterine endometrioid \\ncarcinoma (UEC), stomach adenocarcinoma (STAD), head and neck \\nsquamous cell carcinoma (HNSC), diffuse large B cell lymphoma not \\notherwise specified (DLBCLNOS), melanoma (MEL), LUAD, LUSC, \\npancreatic adenocarcinoma (PAAD), PRAD, cutaneous squamous \\ncell carcinoma (CSCC), small-cell lung cancer (SCLC), adenocarci-\\nnoma of the gastroesophageal junction (GEJ) and chronic lymphocytic  \\nleukemia/small lymphocytic lymphoma (CLLSLL), cancer types in \\nthis task are rare cancers designated by the RARECARE project82 and \\nthe National Cancer Institute’s Surveillance, Epidemiology and End \\nResults (NCI-SEER) Program. We note that slides in the training fold \\nof OT-43 and OT-108 were included in OP-1K and OP-22K pretraining, \\nwith the test set held out from these pretraining sources (following \\npractices in ImageNet).\\nDue to storage limitations in repeatedly extracting features for \\nall non-overlapping tissue patches per WSI for all pretrained models \\n(including intermediate checkpoints), we sampled 200 representative \\npatches per WSI for feature extraction. T o select these patches, we \\nfirst extracted ResNet-50IN features, followed by clustering144, used \\npreviously in other works such as WSISA145, DeepAttnMISL146,147, and \\nothers148. We note that these works are inspired by visual bag-of-words \\n(vBOW)149,150, which has been adapted to pathology for formulating \\nhigh-resolution ROIs and WSIs as smaller but representative collec-\\ntions of tissue patches via clustering applied to deep features151,152, with \\ndownstream applications such as MIL145–148 and retrieval69,153. For all \\npretrained encoders, we extract features from the same sampled col-\\nlection of patches. Although additional computational steps were taken \\nto derive these sampled patches, we note that this does not fall under \\ntransductive inference, given that the entire test set (all WSI samples) \\nis never made visible to any learning component (clustering is fitted \\nper WSI, with ‘samples’ defined at the slide level instead of the patch \\nlevel). T o validate this approach as having comparable performance \\nusing features for all tissue patches per WSI, we compare the perfor-\\nmance of sampled versus full features of UNI, CTransPath, REMEDIS \\nand ResNet-50IN, which we also report in Supplementary Tables 12 and \\n15. We observe not only marginal performance decrease when using \\nsampled features (maximum decrease of −0.9% in top-1 accuracy, \\n−0.007 in AUROC), but also performance increases for many models. \\nFor REMEDIS we observe that the performance of ABMIL models col-\\nlapses when using full features, with top-1 accuracy performances of \\n4.0% and 11.8%, respectively, on OT-43 and OT-108 (compared with \\n59.3% and 41.2%, respectively, with sampled features). We hypoth-\\nesize that these performance increases are due to the difficult nature \\nof OT-43 and OT-108, with patch sampling reducing the input data \\ncomplexity for ABMIL (for example, instead of finding diagnostically \\nrelevant features in a bag of 10,000+ patches, only 200 representative \\npatches are considered).\\nBreast metastasis detection based on CAMELYON16 (2 classes). \\nThe breast metastasis detection task from the Cancer Metastases in \\nLymph Nodes Challenge 2016 (CAMELYON16) consists of 400 H&E FFPE \\nhistopathology WSIs of sentinel lymph node from Radboud University \\nMedical Center and the University Medical Center Utrecht for meta-\\nstasis detection78. We removed one mislabeled slide from the test set, \\nresulting in 399 slides (239 normal, 160 metastasis). For training and \\nevaluation we used the official train–test folds and label-stratified the \\ntraining set into 90:10 train–validation, resulting in 61:7:32 train–valida-\\ntion–test folds (243:27:129 slides). In addition to internal comparisons, \\nwe also compare our results with the leaderboard taken at the time of \\nthe challenge, provide a chronological timeline of best-performing \\nmodels reported in recent peer-reviewed literature, and add context to \\nthe comparison of state-of-the-art methods in Supplementary Table 36. \\nWe note that comparisons with UNI may not be equivalent, with many \\nproposed methods using ResNet-50IN features and also more sophis-\\nticated MIL architectures.\\nNSCLC subtyping based on TCGA and CPTAC (LUAD versus LUSC, \\n2 classes). The NSCLC subtyping task consists of NSCLC H&E FFPE \\ndiagnostic histopathology WSIs sourced from TCGA and CPTAC for \\nclassifying two subtypes: primary LUAD and LUSC cases79,154,155. For \\nquality control, in TCGA we excluded slides with missing or incorrect \\nmetadata, which resulted in 1,041 slides (529 LUAD and 512 LUSC). In \\nCPTAC we excluded slides that were frozen tissue, nontumor tissue or \\nwere not labeled as having acceptable tumor segments, which resulted \\nin 1,091 slides (578 LUAD and 513 LUSC). For training and evaluation, \\nwe label-stratified the TCGA-NSCLC cohort into 80:10:10 train–valida-\\ntion–test folds (848:97:98 slides), with external evaluation using the \\nheld-out CPTAC cohort.\\nRCC subtyping based on DHMC (CCRCC versus PRCC versus \\nCHRCC versus ROCY versus Benign, 5 classes). The RCC subtyping  \\ntask consists of 563 RCC H&E FFPE diagnostic histopathology WSIs  \\n(485 resections and 78 biopsies) from the Dartmouth-Hitchcock  \\nMedical Center (DHMC) for classifying five subtypes: primary CCRCC, \\n344 slides), PRCC (101 slides) and CHRCC (23 slides), renal oncocyto-\\nmas (ROCY , 66 slides) and benign cases (29 slides)156. For training and  \\nevaluation of both tasks, we used a modified configuration of the \\ntrain–validation–test folds with a 70:4:26 ratio (393:23:147 slides), \\nwith eight CHRCC cases moved from the test to the train fold due to  \\nCHRCC being absent in the train fold.\\nRCC subtyping based on TCGA, DHMC and CPTAC (CCRCC versus \\nPRCC versus CHRCC, 3 classes).  The RCC subtyping task consists \\nof 1,794 RCC H&E FFPE diagnostic histopathology WSIs from TCGA, \\nDHMC and CPTAC for classifying three subtypes: primary CCRCC, PRCC \\nand CHRCC156–160. For quality control, in TCGA we excluded slides with \\nmissing low-resolution downsamples, which resulted in 922 slides (519 \\nCCRCC, 294 PRCC and 109 CHRCC). In the DHMC set we filtered out \\noncocytomas in the previously described DHMC-Kidney cohort, which \\nresulted in 468 slides (344 CCRCC, 101 PRCC and 23 CHRCC). In CPTAC \\nwe excluded slides that were frozen tissue, nontumor tissue or were not \\nlabeled as having acceptable tumor segments, which resulted in 404 \\nslides (404 CCRCC). For training and evaluation, we label-stratified \\nthe TCGA-NSCLC cohort into 80:10:10 train–validation–test folds \\n(736:89:97 slides), with external evaluation on the held-out DHMC \\nand CPTAC cohorts. Given that CPTAC includes only CCRCC cases, we \\ncombined DHMC and CPTAC into a single evaluation cohort.\\nCRC screening based on HunCRC (4 classes).  The CRC screening \\ntask consists of 200 H&E FFPE diagnostic histopathology WSIs of \\ncolorectal biopsies from the Hungarian Colorectal Cancer Screening \\n(HunCRC) dataset from Semmelweis University99. In this dataset we \\ndefined a 4-way coarse-grained subtyping task using the categories of \\nnegative (10 slides), non-neoplastic lesion (38 slides), CRC (46 slides), \\nand adenoma (106 slides), in which the ground-truth label was set by \\nthe study’s pathologist. For training and evaluation we label-stratified \\nthe HunCRC slide dataset into 50:25:25 train–validation–test folds \\n(158:21:21 slides).\\nBRCA coarse- and fine-grained subtyping based on BRACS (3 and \\n7 classes). The BRCA coarse- and fine-grained subtyping tasks consist \\nof 547 breast carcinoma H&E slides from 187 patients sourced from the \\nBreast Carcinoma Subtyping (BRCA) task sourced from IRCCS Fon-\\ndazione Pascale, The Institute for High-Performance Computing and \\nNetworking (ICAR) of the National Research Council (CNR), and IBM \\nResearch-Zurich161. In this dataset we defined a 3-way coarse-grained \\nsubtyping task using the ‘benign tumor’ , ‘atypical tumor’ and ‘malignant \\ntumor’ labels. Furthermore, we define a 7-way fine-grained subtyping \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\ntask that subtypes benign tumors as ‘normal’ , ‘pathological benign’ , \\n‘usual ductal hyperplasia’ , atypical tumors as ‘flat epithelial atypia’ \\nand ‘atypical ductal hyperplasia’ , and malignant tumors as ‘ductal \\ncarcinoma in situ’ and ‘invasive carcinoma’ . The hierarchical classifica-\\ntion of the coarse- and fine-grained tasks is reported in Supplementary \\nTable 19. For training and evaluation of both tasks, we used the official \\ntrain–validation–test folds with a 72:12:16 ratio (395:65:87 slides), using \\nthe same folds for both coarse- and fine-grained tasks.\\nGlioma IDH1 mutation prediction and histomolecular subtyping \\nbased on TCGA and EBRAINS (2 and 5 classes).  The glioma IDH1 \\nmutation prediction and histomolecular subtyping task consists of \\n1,996 H&E FFPE diagnostic histopathology WSIs from cases of glioblas-\\ntoma, astrocytoma and oligodendroglioma with molecular status from \\nthe TCGA and the EBRAINS Digital Tumor Atlas87–89. We first defined a \\n5-way glioma histomolecular subtyping task with the following labels: \\nIDH1-mutant astrocytomas (257 slides), IDH1-mutant glioblastomas  \\n(93 slides), IDH1-mutant and 1p/19q codeleted oligodendro gliomas \\n(408 slides), IDH1 -wild-type glioblastomas (1,094 slides), and \\nIDH1-wild-type astrocytomas (144 slides). Additionally, we defined a \\nsimpler 2-way task that predicts only IDH1 status: IDH1-wild-type (1,238 \\nslides) and IDH1-mutant (756 slides). All brain tumors in these tasks are \\ndesignated as rare cancers by the RARECARE project and the NCI-SEER \\nprogram. The hierarchical classification of the coarse- and fine-grained \\ntasks is reported in Supplementary Table 21. For training and evaluation \\nof both tasks, we approximately label-stratified the TCGA-GBMLGG \\n(TCGA Glioblastoma lower-grade glioma) dataset into a train–valida-\\ntion–test fold with a 47:22:31 ratio (525:243:355 slides), with external \\nevaluation using the held-out EBRAINS cohort (873 slides), using the \\nsame folds for both coarse- and fine-grained tasks.\\nBrain tumor coarse- and fine-grained subtyping based on EBRAINS \\n(12 and 30 classes) . The brain tumor coarse- and fine-grained sub-\\ntyping tasks consists of 2,319 H&E FFPE diagnostic histopathology  \\nWSIs from the EBRAINS Digital Tumor Atlas sourced from the  \\nUniversity of Vienna89. With an original dataset size of 3,114 slides, \\nwe defined a 30-way fine-grained brain tumor subtyping task lim-\\nited to diagnostic labels that have at least 30 slides: IDH1-wildtype \\nglioblastoma (474 slides), pilocytic astrocytoma (173 slides), menin-\\ngothelial meningioma (104 slides), pituitary adenoma (99 slides), \\nIDH1-mutant and 1p/19q codeleted anaplastic oligodendroglioma \\n(91 slides), ganglioglioma (88 slides), hemangioblastoma (88 slides), \\nadamantinomatous craniopharyngioma (85 slides), IDH1-mutant and \\n1p/19q codeleted oligodendroglioma (85 slides), atypical meningioma \\n(83 slides), schwannoma (81 slides), IDH1-mutant diffuse astrocytoma \\n(70 slides), transitional meningioma (68 slides), diffuse large B cell \\nlymphoma of the central nervous system (CNS) (59 slides), gliosar-\\ncoma (59 slides), fibrous meningioma (57 slides), anaplastic epend-\\nymoma (50 slides), IDH1-wild-type anaplastic astrocytoma (47 slides), \\nmetastatic tumors (47 slides), DH1-mutant anaplastic astrocytoma (47 \\nslides), ependymoma (46 slides), anaplastic meningioma (46 slides), \\nsecretory meningioma (41 slides), lipoma (38 slides), hemangioperi-\\ncytoma (34 slides), IDH1-mutant glioblastoma (34 slides), non-WNT/\\nNon-SHH medulloblastoma (32 slides), Langerhans cell histiocytosis \\n(32 slides), angiomatous meningioma (31 slides), and hemangioma \\n(30 slides). From the same 2,319 slide dataset in the fine-grained task, \\nwe also defined a 12-way coarse-grained brain tumor subtyping task \\nthat groups the above labels into the following categories: adult-type \\ndiffuse gliomas (837 slides), meningiomas (430 slides), mesenchymal, \\nnon-meningothelial tumors involving the CNS (190 slides), tumors \\nof the sellar region (184 slides), circumscribed astrocytic gliomas \\n(173 slides), ependymal tumors (96 slides), hematolymphoid tumors \\ninvolving the CNS (91 slides), glioneuronal and neuronal tumors (88 \\nslides), cranial and paraspinal nerve tumors (81 slides), pediatric-type \\ndiffuse low-grade gliomas (70 slides), metastatic tumors (47 slides), \\nand embryonal tumors (32 slides). All brain tumors in these tasks are \\ndesignated as rare cancers by the RARECARE project and the NCI-SEER \\nprogram. The hierarchical classification of the coarse- and fine-grained \\ntasks is reported in Supplementary Table 20. For training and evalu-\\nation of both tasks, we approximately label-stratified the dataset into \\na train–validation–test fold with a 50:25:25 ratio (1,151:595:573 slides), \\nusing the same folds for both coarse- and fine-grained tasks.\\nProstate ISUP grading based on PANDA (6 classes). The ISUP grading \\ntask is derived from the PANDA challenge, which consists of 10,616 pros-\\ntate cancer core needle biopsies of prostate cancer sourced from the \\nRadboud University Medical Center and the Karolinska Institute18,162. \\nEach slide is assigned an ISUP score that defines prostate cancer grade \\n(6-class grading task). For quality control, we follow prior work90 in \\nexcluding slides that were erroneously annotated (https://www. \\nkaggle.com/competitions/prostate-cancer-grade-assessment/  \\ndiscussion/169230) or had noisy labels (https://www.kaggle.com/\\ncompetitions/prostate-cancer-grade-assessment/discussion/169230), \\nwhich resulted in 9,555 slides (2,603 G0, 2,399 G1, 1,209 G2, 1,118 G3, \\n1,124 G4, 1,102 G5). For training and evaluation, we label-stratified \\nPANDA into 80:10:10 train–validation–test folds (7,647:954:954 \\nslides). In addition to internal comparisons, we also re-evaluate our \\nresults using the same splits of public MIL baselines of recent work90. \\nIn evaluation with public baselines, we adopt the evaluation strategy in \\nWholeSIGHT90 of also evaluating the Karolinska and Radboud cohorts \\nseparately. Supplementary Table 30 reports the performance of UNI \\nand its internal comparisons with the public splits, with Supplementary \\nTable 37 reporting our results against the public MIL baselines. We also \\nnote the same caveat from the CAMELYON description in this task, given \\nthat comparisons with public MIL performances may not be equivalent \\ndue to using ResNet-50IN features, but note that these baselines also \\nadopt more sophisticated MIL architectures.\\nEndomyocardial assessment based on in-house BWH data  \\n(2 classes). The BWH-EMB dataset consists of 5,021 H&E FFPE histo-\\npathology WSIs from 1,688 in-house endomyocardial biopsies \\n(EMBs) collected from BWH for cellular-mediated allograft rejection \\n(ACR) (2,444 ACR, 2,577 others)86. For training and evaluation, we \\ncase- and label-stratified the dataset into train–validation–test folds \\n(3,547:484:900 slides, 1,192:164:332 patients), with evaluation per-\\nformed at the patient level. In addition to internal comparisons, we also \\ncompare our results with the CRANE86 results (which shares the same \\nsplits) (Extended Data Fig. 8). We also note the same caveat from the \\nCAMELYON description in this task, given that comparison with UNI \\nmay not be equivalent due to CRANE using ResNet-50IN features, but \\nnote that this baseline also uses multi-task learning with other clinical \\nendpoints for EMB assessment.\\nCRC tissue classification based on CRC-100K (9 classes). The CRC \\ntissue classification task is based on the CRC-100K dataset, which con-\\nsists of 107,180 224\\u2009×\\u2009224\\u2009pixel ROIs at 0.5\\u2009mpp annotated and extracted \\nfrom H&E FFPE diagnostic histopathology WSIs of 136 colorectal ade-\\nnocarcinoma samples from the National Center for Tumor Diseases \\n(NCT) biobank and the University Medical Center Mannheim (UMM) \\npathology archive98. ROIs were labeled with the following 9 classes: \\nadipose (11,745 ROIs), background (11,413 ROIs), debris (11,851 ROIs), \\nlymphocyte (12,191 ROIs), mucus (9,931 ROIs), smooth muscle (14,128 \\nROIs), normal colon mucosa (9,504 ROIs), cancer-associated stroma \\n(10,867 ROIs) and colorectal adenocarcinoma epithelium (15,550 \\nROIs). For training and evaluation we used the official case-stratified \\ntrain–test folds (100,000:7,180 ROIs), with the training fold con-\\nstructed from 100,000 ROIs (86 WSIs) from the NCT biobank and \\nUMM pathology archive (referred to as ‘NCT-CRC-HE-100K’), and the \\ntest fold constructed from 7,180 ROIs (50 WSIs) from the NCT biobank \\n(referred to as ‘CRC-VAL-HE-7K’). Additionally, we use the version of \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nNCT-CRC-HE-100K without stain normalization. We use the same folds \\nfor linear probe, KNN and SimpleShot evaluation. We evaluate this \\ndataset on ROIs of 224\\u2009×\\u2009224\\u2009pixels at 0.5\\u2009mpp.\\nBreast metastasis detection based on CAMELYON17-WILDS  \\n(2 classes).  The breast metastasis detection task is based on  \\nthe patch-based variant of the CAMELYON17 dataset 120 (called \\nPatchCAMELYON or ‘PCAM’)163, with folds created by WILDS164 for test-\\ning the models’ robustness under distribution shift. The dataset con-\\nsists of 417,894 96\\u2009×\\u200996\\u2009pixel histopathology ROIs at ~0.92–1.00\\u2009mpp \\nextracted from WSIs of breast cancer metastases in lymph nodes sec-\\ntions, obtained from the CAMELYON17 challenge120. The ROI label refers \\nto whether the patch contains tumor. For training and evaluation, we \\nused the official train–validation–test folds provided by WILDS. The \\ntraining set contains 302,436 patches from three hospitals, and the \\nmodel is evaluated on two out-of-distribution (OD) datasets contain-\\ning 34,904 patches (ValOD) and 80,554 patches (T estOD) collected from \\ntwo other hospitals, respectively. We bilinearly upsampled all images \\nto 224\\u2009×\\u2009224\\u2009pixels for equivalent comparisons with CTransPath. In \\naddition to internal comparisons, we also compare our results with \\nthe public leaderboard on the WILDS benchmark (https://wilds.stan-\\nford.edu/leaderboard/), which we report in Supplementary Table 62. \\nThe in-domain validation fold was not combined with the training set \\nor used for hyper-parameter tuning. We note that comparisons with \\npublic results may not be equivalent to our evaluation, because many \\nmethods are end-to-end fine-tuned with transfer learning from natural \\nimages (and not from pathology).\\nCRC tissue classification based on HunCRC (9 classes).  The CRC \\ntissue classification task is based on the HunCRC dataset, which \\nconsists of 101,398 512\\u2009×\\u2009512\\u2009pixel ROIs at 0.48\\u2009mpp, annotated and \\nextracted from the same 200 H&E FFPE diagnostic histopathology \\nWSIs of colorectal biopsies also described in the slide-level task99. ROIs \\nwere labeled with the following nine classes: adenocarcinoma (4,315 \\nROIs), high-grade dysplasia (2,281 ROIs), low-grade dysplasia (55,787 \\nROIs), inflammation (763 ROIs), tumor necrosis (365 ROIs), suspicious \\nfor invasion (570 ROIs), resection edge (534 ROIs), technical artifacts \\n(3,470 ROIs), and normal (31,323 ROIs). For training and evaluation \\nwe case-stratified and approximately label-stratified the dataset into \\ntrain–test folds (151:49 cases, 76,753:22,655 ROIs) for use in linear probe, \\nKNN and SimpleShot evaluation. We evaluate this dataset on resized \\nROIs of 448\\u2009×\\u2009448\\u2009pixels at 0.55\\u2009mpp.\\nBRCA subtyping based on BACH (4 classes). The BRCA subtyping \\ntask is based on the Breast Carcinoma Subtyping (BACH) dataset, \\nwhich consists of 400 2,048\\u2009×\\u20091,536\\u2009pixel ROIs at 0.42\\u2009mpp, annotated  \\nand extracted from H&E FFPE diagnostic histopathology WSIs of  \\nbreast carcinoma samples from the International Conference on  \\nImage Analysis and Recognition (ICIAR) 2018 grand challenge on breast \\ncancer histology images (BACH)165. ROIs were labeled with the following \\nfour classes: normal (100 ROIs), benign (100 ROIs), in situ carcinoma \\n(100 ROIs) and invasive carcinoma (100 ROIs). For training and evalu-\\nation we label-stratified the dataset into train–test folds (320:80 ROIs) \\nfor use in linear probe, KNN and SimpleShot evaluation. Addition-\\nally, we evaluate this dataset across the following center-cropped and \\nresized image resolutions: 224\\u2009×\\u2009224\\u2009pixels at 2.88\\u2009mpp, 448\\u2009×\\u2009448\\u2009pix-\\nels at 1.44\\u2009mpp, 896\\u2009×\\u2009896\\u2009pixels at 0.72\\u2009mpp and 1,344\\u2009×\\u20091,344\\u2009pixels \\nat 0.48\\u2009mpp.\\nCCRCC tissue classification based on TCGA and HEL (3 classes). \\nThe CCRCC tissue classification task consists of 52,713 256\\u2009×\\u2009256\\u2009pixel \\nand 300\\u2009×\\u2009300\\u2009pixel ROIs at approximately 0.25\\u2009mpp, annotated and \\nextracted from H&E FFPE diagnostic histopathology WSIs of CCRCC \\nsamples from TCGA (502 samples) and Helsinki University Hospital \\n(HEL) (64 samples)166. ROIs were labeled with the following six classes: \\ncancer (13,057 ROIs), normal (8,652 ROIs), stroma (5,460 ROIs), red \\nblood cells (996 ROIs), empty background (16,026 ROIs), and other \\ntextures (8,522 ROIs). For this task we considered only the cancer, \\nnormal and stroma labels due to label imbalance when stratifying \\nby data source and ambiguities in the ‘other’ category. We used ROIs \\nfrom TCGA (21,095 ROIs) and HEL (6,074 ROIs) as the train and test \\ncohorts, respectively (train–test fold with a ratio of 21,095:6,074), \\nwhich we used for linear probe, KNN and SimpleShot evaluation. We \\nevaluate this dataset on resized ROIs of 224\\u2009×\\u2009224\\u2009pixels at approxi-\\nmately 0.29\\u2009mpp.\\nPRAD tissue classification based on AGGC (5 classes). The PRAD \\ntissue classification task is based on the Automated Gleason Grading \\nChallenge 2022 (AGGC) from the National University Hospital and \\nAgency of Science, T echnology and Research (A*STAR) in Singapore101. \\nIt consists of 203 WSIs obtained from prostatectomies (105 training, \\n45 testing) and biopsies (37 training, 16 testing) digitized using an  \\nAkoya Biosciences scanner at ×20 magnification at 0.5\\u2009mpp. Each  \\nslide includes partial pixel-level annotations delineating different  \\nGleason patterns and stromal regions. From the original WSIs and  \\nannotations we built a ROI dataset consisting of 1,125,640 non-  \\noverlapping 256\\u2009×\\u2009256\\u2009pixel ROIs (train–test fold with a ratio of \\n780,619:345,021), which we used for linear probe, KNN and SimpleShot \\nevaluation. ROIs with more than one Gleason pattern were assigned \\nthe most aggressive grade. We evaluate this dataset on resized ROIs  \\nof 224\\u2009×\\u2009224\\u2009pixels at approximately 0.57\\u2009mpp.\\nESCA tissue classification based on UKK, WNS, TCGA and CHA  \\n(11 classes). The ESCA (esophageal carcinoma) tissue classification \\ntask consists of 367,229 256\\u2009×\\u2009256\\u2009pixel ROIs at 0.78\\u2009mpp, annotated \\nand extracted from 320 H&E FFPE diagnostic histopathology WSIs of \\nesophageal adenocarcinoma and adenocarcinoma of the esophagogas-\\ntric junction from four sources: University Hospital Cologne (UKK, 22 \\nslides), Landesklinikum Wiener Neustadt (WNS, 62 slides), TCGA (22 \\nslides) and the University Hospital Berlin–Charité (CHA, 214 slides)167. \\nROIs were labeled with the following 11 classes: adventitia (71,131 ROIs), \\nlamina propria mucosae (2,173 ROIs), muscularis mucosae (2,951 ROIs), \\nmuscularis propria (83,358 ROIs), regression tissue (56,490 ROIs), \\nmucosa gastric (44,416 ROIs), muscosa esophagus (18,561 ROIs), sub-\\nmucosa (22,117 ROIs), submucosal glands (1,516 ROIs), tumor (63,863 \\nROIs) and ulceration (753 ROIs). For training and evaluation we com-\\nbined UKK, WNS and TCGA into one training cohort (189,142 ROIs) and \\nused CHA as a test cohort (178,187 ROIs), with a train–test fold ratio of \\n51:49, which we then used for linear probe, KNN and SimpleShot evalu-\\nation. We evaluate this dataset on resized ROIs of 224\\u2009×\\u2009224\\u2009pixels at \\napproximately 0.89\\u2009mpp.\\nCRC polyp classification based on UniToPatho (6 classes). The CRC \\npolyp classification task is based on the UniT oPatho dataset, which \\nconsists of 9,536 1,812\\u2009×\\u20091,812\\u2009pixel ROIs at 0.44\\u2009mpp, annotated and \\nextracted from 292 H&E FFPE diagnostic histopathology WSIs of colo-\\nrectal polyp samples from the University of Turin100. ROIs were labeled \\nwith the following six classes: normal (950 ROIs), hyperplastic polyp \\n(545 ROIs), tubular adenoma with high-grade dysplasia (454 ROIs), \\ntubular adenoma with low-grade dysplasia (3,618 ROIs), tubulo-villous \\nadenoma with high-grade dysplasia (916 ROIs), and tubulo-villous ade-\\nnoma with low-grade dysplasia (2,186 ROIs). For training and evaluation \\nwe used the official train–test folds (6,270:2,399 ROIs). We evaluate this \\ndataset across the following resized image resolutions: 224\\u2009×\\u2009224\\u2009pixels \\nat 3.60\\u2009mpp, 448\\u2009×\\u2009448\\u2009pixels at 1.80\\u2009mpp, 896\\u2009×\\u2009896\\u2009pixels at 0.90\\u2009mpp, \\nand 1,792\\u2009×\\u20091,792\\u2009pixels at 0.45\\u2009mpp.\\nCRC MSI screening based on TCGA CRC-MSI (2 classes).  The \\nCRC microsatellite instability (MSI) prediction task is based on the \\nTCGA CRC-MSI dataset, which consists of 51,918 512\\u2009×\\u2009512\\u2009pixel ROIs \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nat approxi mately 0.5\\u2009mpp, extracted from H&E FFPE diagnostic his-\\ntopathology WSIs of colorectal adenocarcinoma samples annotated \\nand extracted from TCGA and also pre-normalized using Macenko nor-\\nmalization6. ROIs were labeled with the following two classes according \\nto the patient-level label of the sample: microsatellite instable (15,002 \\nROIs) and microsatellite stable (36,916 ROIs). For training and evalua-\\ntion, we used the official train–test folds (19,557:32,361 ROIs) in linear \\nprobe, KNN, and SimpleShot evaluation. We evaluate this dataset on \\nresized ROIs of 448\\u2009×\\u2009448\\u2009pixels at 0.57\\u2009mpp.\\nPan-cancer tissue classification based on TCGA Uniform Tumor (32 \\nclasses). The pan-cancer tissue classification task is based on the TCGA \\nUniform Tumor dataset, which consists of 271,170 256\\u2009×\\u2009256\\u2009pixel ROIs \\nat around 0.5\\u2009mpp of 32 cancer types annotated and extracted from \\n8,736 H&E FFPE diagnostic histopathology WSIs in TCGA68. Images were \\nlabeled with the following 32 classes: adrenocortical carcinoma (ACC) \\n(4,980 ROIs), bladder urothelial carcinoma (BLCA) (9,990 ROIs), brain \\nlower-grade glioma (LGG) (23,530 ROIs), BRCA (23,690 ROIs), cervical \\nsquamous cell carcinoma and endocervical adenocarcinoma (CESC) \\n(6,270 ROIs), cholangiocarcinoma (CHOL) (900 ROIs), COAD (8,150 \\nROIs), ESCA (3,380 ROIs), glioblastoma multiforme (GBM) (23,740 \\nROIs), HNSC (11,790 ROIs), kidney chromophobe (KICH) (2,460 ROIs), \\nkidney renal clear cell carcinoma (KIRC) (11,650 ROIs), kidney renal \\npapillary cell carcinoma (KIRP) (6,790 ROIs), liver hepatocellular car-\\ncinoma (LIHC) (8,370 ROIs), LUAD (16,460 ROIs), LUSC (16,560 ROIs), \\nlymphoid neoplasm diffuse large B cell lymphoma (DLBC) (840 ROIs), \\nmesothelioma (MESO) (2,090 ROIs), ovarian serous cystadenocar-\\ncinoma (OV) (2,520 ROIs), PAAD (4,090 ROIs), pheochromocytoma \\nand paraganglioma (PCPG) (1,350 ROIs), PRAD (9,810 ROIs), 23) READ \\n(1,880 ROIs), sarcoma (SARC) (13,480 ROIs), skin cutaneous melanoma \\n(SKCM) (10,060 ROIs), STAD (9,670 ROIs), testicular germ cell tumor \\n(TGCT) (6,010 ROIs), thymoma (THYM) (3,600 ROIs), thyroid carci-\\nnoma (THCA) (11,360 ROIs), uterine carcinosarcoma (UCS) (2,120 ROIs), \\nuterine corpus endometrial carcinoma (UCEC) (12,480 ROIs), and uveal \\nmelanoma (UVM) (1,640 ROIs). Except for BLCA, BRCA, COAD, HNSC, \\nLUAD, LUSC, PAAD, PRAD, READ, SKCM, STAD, THCA and UCEC, all \\nother cancer types in this task are designated as rare cancers by the \\nRARECARE project and the NCI-SEER program. For training and evalu-\\nation we case-stratified and approximately label-stratified the dataset \\ninto train–test folds (216,350:55,360 ROIs), for use in linear probe, KNN \\nand SimpleShot evaluation. We evaluate this dataset on resized ROIs \\nof 224\\u2009×\\u2009224\\u2009pixels at approximately 0.57\\u2009mpp. T o mitigate potential \\nbiases from site-specific H&E staining variability in TCGA168, we used \\nMacenko normalization169 to normalize all ROIs.\\nPan-cancer TIL detection based on TCGA-TILS (2 classes) . The \\ntumor-immune lymphocyte (TIL) detection task is based on the \\nTCGA-TILs dataset, which consists of 304,097 100\\u2009×\\u2009100\\u2009pixel histo-\\npathology ROIs at approximately 0.5\\u2009mpp, annotated and extracted \\nfrom H&E FFPE diagnostic histopathology WSIs in TCGA 61,67,170. \\nROIs were labeled with the following two classes: TIL-positive (if \\nthere are at least two TILs present in the image, 54,910 ROIs) and \\nTIL-negative (249,187 ROIs). For training and evaluation we used the \\nofficial train–validation–test folds (209,221:38,601:56,275 ROIs)  \\nand combine the train and validation folds into a single training  \\nfold. We bilinearly upsampled all images to 224\\u2009×\\u2009224\\u2009pixels \\nat 0.20\\u2009mpp for equivalent comparisons with CTransPath. T o  \\nmitigate potential biases from site-specific H&E staining variability \\nin TCGA, we used Macenko normalization169 to normalize all ROIs. \\nIn addition to internal comparisons, we also compare our results \\nwith the ChampKit leaderboard, which we report in Supplementary \\nTable 61. We note that comparisons with public results may not be \\nequivalent to our evaluation, given that many methods are end-to-end \\nfine-tuned with transfer learning from natural images (and not from \\npathology).\\nPan-cancer cell type segmentation based on SegPath (8 cell \\ntypes treated as individual tasks). The cell type segmentation \\ntasks are derived from the SegPath dataset, which consists of 158,687 \\n984\\u2009×\\u2009984\\u2009pixel ROIs at 0.22\\u2009mpp, annotated and extracted from H&E \\nFFPE diagnostic histopathology WSIs of eight major cell types in can-\\ncer tissue from University of T okyo Hospital102. Immunofluorescence \\nand DAPI nuclear staining were performed on ROIs and used as image \\nmasks for the following classes: endothelium (10,647 ROIs), epithelium \\n(26,509 ROIs), leukocyte (24,805 ROIs), lymphocyte (12,273 ROIs), mye-\\nloid cell (14,135 ROIs), plasma cell (13,231 ROIs), red blood cell (25,909 \\nROIs), and smooth muscle (31,178 ROIs). Each cell type in the dataset \\nforms an independent tissue segmentation task with two classes, tis-\\nsue/cell region and non-tissue/cell region. For training and evaluation \\nwe used the official train–validation–test split with an approximate \\n80:10:10 ratio. Furthermore, we compare our results using the public \\nevaluation of this dataset, which we also report in Supplementary \\nTable 69. We note that individual model performances are not made \\npublic in the official dataset, and thus we interpolated the performance \\nbound of the best-performing model for each cell type.\\nComputing hardware and software\\nWe used Python (v3.8.13) and PyT orch171 (v2.0.0, CUDA 11.7) (https://\\npytorch.org) for all experiments and analyses in the study (unless speci-\\nfied), which can be replicated using open-source libraries as outlined \\nbelow. T o train UNI via DINOv2, we modify the vision transformer \\nimplementation maintained by the open-source timm library (v0.9.2) \\nfrom Hugging Face (https://huggingface.co) for the encoder backbone \\nand use the original DINOv2 self-supervised learning algorithm (https://\\ngithub.com/facebookresearch/dinov2) for pretraining, which used \\n4\\u2009×\\u20098 80\\u2009GB NVIDIA A100 GPU (graphics processing unit) nodes config-\\nured for multi-GPU, multi-node training using distributed data-parallel \\n(DDP). All other computations for downstream experiments were \\nconducted on single 24\\u2009GB NVIDIA 3090 GPUs. All WSI processing \\nwas supported by OpenSlide (v4.3.1), openslide-python (v1.2.0), and \\nCLAM (https://github.com/mahmoodlab/CLAM). We use Scikit-learn134 \\n(v1.2.1) for its implementation of K-nearest neighbors, and the logistic \\nregression implementation and SimpleShot implementation provided \\nby the LGSSL codebase (https://github.com/mbanani/lgssl). Imple-\\nmentations of other visual pretrained encoders benchmarked in the \\nstudy are found at the following links: ResNet-50 with ImageNet Trans-\\nfer (https://github.com/mahmoodlab/CLAM), CTransPath (https://\\ngithub.com/Xiyue-Wang/TransPath), and REMEDIS (https://github.\\ncom/google-research/medical-ai-research-foundations). We note \\nthat REMEDIS requires fulfillment of a data use agreement, which \\ncan be accessed and submitted at the PhysioNet website (https://\\nphysionet.org/content/medical-ai-research-foundation)172,173. For \\nmulti-head attention visualization, we used the visualization tools \\nprovided by the HIPT codebase (https://github.com/mahmoodlab/\\nHIPT). For training weakly supervised ABMIL models, we adapted \\nthe training scaffold code from the CLAM codebase (https://github.\\ncom/ mahmoodlab/CLAM). For training semantic segmentation, we \\nuse the original Mask2Former implementation (https://github.com/\\nfacebookresearch/Mask2Former), which is based on detectron2 (ref. \\n174) (v0.6), and required the following older packages for compatibility: \\nPython (v3.8) and PyT orch (v1.9.0, CUDA 11.1). For adding ViT-Adapter \\nto UNI, we adapt its original implementation (https://github.com/\\nczczup/ViT-Adapter) in detectron2 to train it using Mask2Former. \\nPillow (v9.3.0) and OpenCV-python were used to perform basic image \\nprocessing tasks. Matplotlib (v3.7.1) and Seaborn (v0.12.2) were used \\nto create plots and figures. Use of other miscellaneous Python libraries \\nis detailed in the Reporting Summary.\\nReporting summary\\nFurther information on research design is available in the Nature \\nPortfolio Reporting Summary linked to this article.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nData availability\\nTCGA and CPTAC data consisting of whole-slide images and labels can \\nbe accessed through the NIH genomic data commons (https://portal.\\ngdc.cancer.gov) and proteomics data commons (https://proteomic.\\ndatacommons.cancer.gov ), respectively. GTEx data added to the  \\npretraining dataset can be accessed through the GTEx portal (https://www. \\ngtexportal.org/home/). CPTAC data consisting of all publicly available \\ndatasets analyzed in this work can be can accessed in their respective \\ndata portals: CRC-100K (https://zenodo.org/record/1214456), HunCRC \\nROIs (10.6084/m9.figshare.c.5927795.v1), HunCRC slides (10.7937/\\ntcia.9cjf-0127), BACH (https://iciar2018-challenge.grand-challenge.\\norg/Dataset/), TCGA CRC-MSI (https://zenodo.org/record/3832231), \\nCCRCC tissue classification (https://zenodo.org/record/7898308), \\nTCGA-TILs (https://zenodo.org/record/6604094 ), TCGA Uniform \\n(https://zenodo.org/record/5889558), UniT oPatho (https://zenodo.\\norg/record/4643645), ESCA(https://zenodo.org/record/7548828), \\nCAMELYON17-WILDS (https://wilds.stanford.edu/datasets), EBRAINS \\n(10.25493/WQ48-ZGX), DHMC (https://bmirds.github.io/Kidney -\\nCancer), BRACS (https://bracs.icar.cnr.it), PANDA (https://panda.\\ngrand-challenge.org), SegPath (https://zenodo.org/record/7412731) \\nand AGGC (https://zenodo.org/record/6460100). TCGA, CPTAC, Hun-\\nCRC and TCGA-TILS can also be accessed using The Cancer Imaging  \\nArchive175. Links for all datasets are also listed in Supplementary \\nTable 73. We note that data from AGGC were obtained from a public \\ngrand challenge (of the same name (https://aggc22.grand-challenge.\\norg)) with a pending publication 101, with permission granted by the \\nchallenge organizers to present results from this dataset. No internal \\npatient data were specifically collected for this study. This study relies \\non retrospective analysis of anonymized whole-slide images. Following \\ninstitution policies, all requests for data collected or curated in-house \\nwill be evaluated on a case-by-case basis to determine whether the \\ndata requested and the use case comply with intellectual property or \\npatient privacy obligations.\\nCode availability\\nCode and model weights for UNI can be accessed for academic research \\npurposes at https://github.com/mahmoodlab/UNI. We have docu -\\nmented all technical deep learning methods and software libraries used \\nin the study while ensuring that the paper is accessible to the broader \\nclinical and scientific audience.\\nReferences\\n119. Zhai, X., Oliver, A., Kolesnikov, A. & Beyer, L. S4L: self-supervised \\nsemi-supervised learning. In Proceedings of the IEEE/CVF \\nInternational Conference on Computer Vision, 1476–1485 (2019).\\n120. Bandi, P. et al. From detection of individual metastases to \\nclassification of lymph node status at the patient level: the \\nCAMELYON17 challenge. IEEE Trans. Med. Imaging 38, 550–560 \\n(2019).\\n121. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \\nof deep bidirectional transformers for language understanding. \\nIn Proceedings of the 2019 Conference of the North American \\nChapter of the Association for Computational Linguistics: Human \\nLanguage Technologies, Volume 1 (Long and Short Papers)  \\n(2018).\\n122. Tian, K. et al. Designing BERT for convolutional networks: sparse \\nand hierarchical masked modeling. In The Eleventh International \\nConference on Learning Representations (2023).\\n123. Sablayrolles, A., Douze, M., Schmid, C. & Jégou, H. Spreading \\nvectors for similarity search. In International Conference on \\nLearning Representations (2019).\\n124. Touvron, H., Vedaldi, A., Douze, M. & Jegou, H. Fixing the train–\\ntest resolution discrepancy. In Advances in Neural Information \\nProcessing Systems, Vol. 32 (eds Wallach, H. et al.) (Curran \\nAssociates, 2019).\\n125. Dao, T., Fu, D. Y., Ermon, S., Rudra, A. & Ré, C. FlashAttention:  \\nfast and memory-efficient exact attention with IO-awareness.  \\nIn Advances in Neural Information Processing Systems (2022).\\n126. Liu, Z. et al. Swin transformer: hierarchical vision transformer \\nusing shifted windows. In Proceedings of the IEEE/CVF \\nInternational Conference on Computer Vision, 10012–10022 (2021).\\n127. Kolesnikov, A. et al. Big Transfer (BiT): general visual \\nrepresentation learning. In Computer Vision–ECCV 2020: \\n16th European Conference, Glasgow, UK, August 23–28, 2020, \\nProceedings, Part V 16, 491–507 (Springer, 2020).\\n128. Lin, T., Yu, Z., Hu, H., Xu, Y. & Chen, C.-W. Interventional bag \\nmulti-instance learning on whole-slide pathological images. In \\nProceedings of the IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 19830–19839 (2023).\\n129. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. \\nIn International Conference on Learning Representations (2019).\\n130. Bentley, J. L. Multidimensional binary search trees used for \\nassociative searching. Communications of the ACM 18, 509–517 \\n(1975).\\n131. Zhu, C., Byrd, R. H., Lu, P. & Nocedal, J. Algorithm 778: L-BFGS-B: \\nFortran subroutines for large-scale bound-constrained \\noptimization. ACM Transactions on Mathematical Software 23, \\n550–560 (1997).\\n132. Sarıyıldız, M. B., Kalantidis, Y., Alahari, K. & Larlus, D. No reason \\nfor no supervision: improved generalization in supervised \\nmodels. In The Eleventh International Conference on Learning \\nRepresentations (2023).\\n133. Fang, Z. et al. SEED: self-supervised distillation for visual \\nrepresentation. In International Conference on Learning \\nRepresentations (2020).\\n134. Pedregosa, F. et al. Scikit-learn: machine learning in python. \\nJournal of Machine Learning Research 12, 2825–2830 (2011).\\n135. Ghiasi, G. et al. Simple copy-paste is a strong data augmentation \\nmethod for instance segmentation. In Proceedings of the IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition, \\n2918–2928 (2021).\\n136. El Banani, M., Desai, K. & Johnson, J. Learning visual representa-\\ntions via language-guided sampling. In Proceedings of the IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition, \\n19208–19220 (2023).\\n137. Koch, G., Zemel, R. & Salakhutdinov, R. Siamese neural networks \\nfor one-shot image recognition. In Proceedings of the 32nd \\nInternational Conference on Machine Learning (2015).\\n138. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K. &  \\nWierstra, D. Matching networks for one shot learning. In  \\nAdvances in Neural Information Processing Systems 29 (2016).\\n139. Yu, J.-G. et al. Prototypical multiple instance learning for predicting \\nlymph node metastasis of breast cancer from whole-slide \\npathological images. Med. Image Anal. 85, 102748 (2023).\\n140. Yu, Z., Lin, T. & Xu, Y. SLPD: slide-level prototypical distillation for \\nWSIs. In International Conference on Medical Image Computing \\nand Computer-Assisted Intervention, 259–269 (Springer, 2023).\\n141. Quiros, A. C. et al. Mapping the landscape of histomorphological \\ncancer phenotypes using self-supervised learning on unlabeled, \\nunannotated pathology slides. Preprint at https://doi.org/10.48550/\\narxiv.2205.01931 (2022).\\n142. Yang, J., Chen, H., Yan, J., Chen, X. & Yao, J. Towards better \\nunderstanding and better generalization of low-shot classification \\nin histology images with contrastive learning. In International \\nConference on Learning Representations (2021).\\n143. Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B. & Isola, P. \\nRethinking few-shot image classification: a good embedding \\nis all you need? In Computer Vision–ECCV 2020: 16th European \\nConference, Glasgow, UK, August 23–28, 2020, Proceedings,  \\nPart XIV 16, 266–282 (Springer, 2020).\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\n144. Lloyd, S. Least squares quantization in PCM. IEEE Transactions on \\nInformation Theory 28, 129–137 (1982).\\n145. Zhu, X., Yao, J., Zhu, F. & Huang, J. WSISA: making survival \\nprediction from whole slide histopathological images. In \\nProceedings of the IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 7234–7242 (2017).\\n146. Yao, J., Zhu, X. & Huang, J. Deep multi-instance learning for \\nsurvival prediction from whole slide images. In International \\nConference on Medical Image Computing and Computer-Assisted \\nIntervention, 496–504 (Springer, 2019).\\n147. Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N. & Huang, J. Whole \\nslide images based cancer survival prediction using attention \\nguided deep multiple instance learning networks. Med. Image \\nAnal. 65, 101789 (2020).\\n148. Li, R., Yao, J., Zhu, X., Li, Y. & Huang, J. Graph CNN for survival \\nanalysis on whole slide pathological images. In International \\nConference on Medical Image Computing and Computer-Assisted \\nIntervention, 174–182 (Springer, 2018).\\n149. Sivic, J. & Zisserman, A. Video Google: A text retrieval approach \\nto object matching in videos. In Proceedings of the Ninth IEEE \\nInternational Conference on Computer Vision, 1470–1477 (IEEE, \\n2003).\\n150. Fei-Fei, L. & Perona, P. A Bayesian hierarchical model for \\nlearning natural scene categories. In 2005 IEEE Computer \\nSociety Conference on Computer Vision and Pattern Recognition \\n(CVPR’05) Vol. 2, 524–531 (IEEE, 2005).\\n151. Cruz-Roa, A., Caicedo, J. C. & González, F. A. Visual pattern mining \\nin histology image collections using bag of features. Artif. Intell. \\nMed. 52, 91–106 (2011).\\n152. Xu, Y. et al. Weakly supervised histopathology cancer image \\nsegmentation and classification. Med. Image Anal. 18, 591–604 \\n(2014).\\n153. Chen, C. et al. Fast and scalable search of whole-slide images via \\nself-supervised deep learning. Nat. Biomed. Eng. 6, 1420–1434 \\n(2022).\\n154. Gillette, M. A. et al. Proteogenomic characterization reveals \\ntherapeutic vulnerabilities in lung adenocarcinoma. Cell 182, \\n200–225 (2020).\\n155. Satpathy, S. et al. A proteogenomic portrait of lung squamous cell \\ncarcinoma. Cell 184, 4348–4371 (2021).\\n156. Zhu, M. et al. Development and evaluation of a deep neural \\nnetwork for histologic classification of renal cell carcinoma on \\nbiopsy and surgical resection slides. Sci. Rep. 11, 7080 (2021).\\n157. Cancer Genome Atlas Research Network. Comprehensive \\nmolecular characterization of clear cell renal cell carcinoma. \\nNature 499, 43–49 (2013).\\n158. Cancer Genome Atlas Research Network. Comprehensive \\nmolecular characterization of papillary renal-cell carcinoma.  \\nN. Engl. J. Med. 374, 135–145 (2016).\\n159. Davis, C. F. et al. The somatic genomic landscape of chromophobe \\nrenal cell carcinoma. Cancer Cell 26, 319–330 (2014).\\n160. Li, Y. et al. Histopathologic and proteogenomic heterogeneity \\nreveals features of clear cell renal cell carcinoma aggressiveness. \\nCancer Cell 41, 139–163 (2023).\\n161. Brancati, N. et al. BRACS: a dataset for breast carcinoma \\nsubtyping in H&E histology images. Database 2022, baac093 \\n(2022).\\n162. Bulten, W. et al. Automated deep-learning system for Gleason \\ngrading of prostate cancer using biopsies: a diagnostic study. \\nLancet Oncol. 21, 233–241 (2020).\\n163. Veeling, B. S., Linmans, J., Winkens, J., Cohen, T. & Welling, M. \\nRotation equivariant CNNs for digital pathology. In Medical Image \\nComputing and Computer Assisted Intervention–MICCAI 2018: 21st \\nInternational Conference, Granada, Spain, September 16–20, 2018, \\nProceedings, Part II 11, 210–218 (Springer, 2018).\\n164. Koh, P. W. et al. WILDS: a benchmark of in-the-wild distribution \\nshifts. In International Conference on Machine Learning,  \\n5637–5664 (PMLR, 2021).\\n165. Aresta, G. et al. BACH: grand challenge on breast cancer \\nhistology images. Med. Image Anal. 56, 122–139 (2019).\\n166. Brummer, O., Pölönen, P., Mustjoki, S. & Brück, O. Computational \\ntextural mapping harmonises sampling variation and reveals \\nmultidimensional histopathological fingerprints. British Journal of \\nCancer 129, 683–695 (2023).\\n167. Tolkach, Y. et al. Artificial intelligence for tumour tissue \\ndetection and histological regression grading in oesophageal \\nadenocarcinomas: a retrospective algorithm development  \\nand validation study. Lancet Digit. Health 5, e265–e275  \\n(2023).\\n168. Howard, F. M. et al. The impact of site-specific digital histology \\nsignatures on deep learning model accuracy and bias.  \\nNat. Commun. 12, 4423 (2021).\\n169. Macenko, M. et al. A method for normalizing histology slides for \\nquantitative analysis. In 2009 IEEE international Symposium on \\nBiomedical Imaging: From Nano to Macro, 1107–1110 (IEEE, 2009).\\n170. Abousamra, S. et al. Deep learning-based mapping of tumor \\ninfiltrating lymphocytes in whole slide images of 23 types of \\ncancer. Front. Oncol. 11, 806603 (2022).\\n171. Paszke, A. et al. PyTorch: an imperative style, high-performance \\ndeep learning library. In Advances in Neural Information \\nProcessing Systems 32 (2019).\\n172. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and PhysioNet: \\ncomponents of a new research resource for complex physiologic \\nsignals. Circulation 101, e215–e220 (2000).\\n173. Azizi, S. et al. Medical AI research foundations: a repository  \\nof medical foundation models (version 1.0.0). PhysioNet  \\nhttps://doi.org/10.13026/grp0-z205 (2023).\\n174. Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y. & Girshick, R. Detectron2. \\nGitHub https://github.com/facebookresearch/detectron2  \\n(2019).\\n175. Clark, K. et al. The Cancer Imaging Archive (TCIA): maintaining \\nand operating a public information repository. J. Digit. Imaging 26, \\n1045–1057 (2013).\\nAcknowledgements\\nWe thank J. Zhou and T. Darcet for providing insights into the \\ntraining dynamics for iBOT and DINOv2, respectively, and L. Beyer \\nfor providing insights and feedback on evaluating self-supervised \\nmodels. This work was supported in part by the BWH president’s fund, \\nBWH & MGH Pathology, and National Institutes of Health (NIH) NIGMS \\nR35GM138216 (F.M.). G.G. was supported by the BWH President’s \\nScholar Award, NIGMS R35GM149270, NIDDK P30DK034854, and \\nthe Massachusetts Life Sciences Center. R.J.C., D.S. and S.S. were \\nsupported by the NSF Graduate Research Fellowship. T.D. was \\nsupported by the Harvard SEAS Fellowship. M.Y.L. was supported by \\nthe Siebel Scholars program. D.F.K.W. was supported by the NIH NCI \\nRuth L. Kirschstein National Service Award, T32CA251062. L.O. was \\nsupported by the German Academic Exchange (DAAD) Fellowship. We \\nalso thank T. Janicki, R. Kenny and the system administration staff at \\nthe MGB Enterprise Research Infrastructure & Services (ERIS) Research \\nComputing Core for their support with computing resources, and  \\nN. Vatanian, M. Thiagarajan, B. Fevrier-Sullivan and J. Kirby at the  \\nNIH for navigating access to whole-slide imaging data in CPTAC.\\nAuthor contributions\\nR.J.C., F.M., M.Y.L., T.D. and D.F.K.W. conceived the study and \\ndesigned the experiments. R.J.C., L.P.L., D.F.K.W., J.J.W., T.D., M.Y.L, \\nG.J., A.H.S., B.C., D.S., M.S., L.O., A.Z., A.V. and S.S. collected the data \\nfor self-supervised learning. R.J.C., T.D. and M.Y.L. performed model \\ndevelopment for self-supervised learning. R.J.C., M.Y.L, T.D., B.C. and \\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nG.J. organized the datasets and codebases for all downstream tasks \\nregarding ROI classification, ROI segmentation and slide classification. \\nR.J.C, T.D., M.Y.L., A.H.S., G.J., M.S., A.Z., L.L.W. and A.V. performed \\nquality control of the codebase and the results. R.J.C., M.Y.L., T.D. and \\nG.J. carried out analysis of the ROI classification. T.D., M.Y.L., R.J.C., \\nL.L.W., A.Z. and W.W. carried out analysis of the ROI segmentation. \\nR.J.C., T.D., B.C., D.S., M.S., M.W. and L.L.W. carried out analysis of the \\nslide classification. R.J.C., T.D., M.Y.L., D.F.K.W., G.J., A.H.S., M.S., L.P.L., \\nG.G. and F.M. interpreted the results and provided feedback on the \\nstudy. R.J.C., T.D., M.Y.L, D.F.K.W. and F.M. prepared the paper with input \\nfrom all co-authors. F.M. supervised the research.\\nCompeting interests\\nR.J.C., M.Y.L. and F.M. are inventors on a provisional US patent \\n(application no. 63/611,059) filed corresponding to the \\nmethodological aspects of this work. The other authors declare  \\nno competing interests.\\nAdditional information\\nExtended data are available for this paper at  \\nhttps://doi.org/10.1038/s41591-024-02857-3.\\nSupplementary information The online version contains supplementary \\nmaterial available at https://doi.org/10.1038/s41591-024-02857-3.\\nCorrespondence and requests for materials should be addressed to \\nFaisal Mahmood.\\nPeer review information Nature Medicine thanks Andrew Beck, \\nFrancesco Ciompi and Lee Cooper for their contribution to the  \\npeer review of this work. Primary Handling Editor: Lorenzo Righetto,  \\nin collaboration with the Nature Medicine team.\\nReprints and permissions information is available at  \\nwww.nature.com/reprints.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 1 | Few-shot slide classification. T o study the label efficiency \\nof UNI in slide classification, we compare UNI with other pretrained encoders on: \\na. breast metastasis detection in CAMELYON16, b. NSCLC subtyping in CPTAC \\n(trained on TCGA) c. RCC subtyping in CPTAC-DHMC (trained on TCGA),  \\nd. RCC subtyping in DHMC, e. BRCA coarse-grained subtyping in BRACS, f. BRCA \\nfine-grained subtyping in BRACS, g. CRC screening in HunCRC, h. Prostate ISUP \\nGrading in PANDA, i. glioma IDH1 prediction in EBRAINS (trained on TCGA),  \\nj. glioma histomolecular subtyping in EBRAINS (trained on TCGA), k. brain tumor \\ncoarse-grained subtyping in EBRAINS, l. brain tumor fine-grained subtyping in \\nEBRAINS, and m. heart transplant assessment in BWH-EMB. The performance \\nis measured across different few-shot settings with K\\u2009∈\\u20091, 2, 4, 8, 16, 32 training \\nexamples used per class. Boxes indicate quartile values of model performance  \\n(n = 5 runs) and whiskers extend to data points within 1.5\\u2009×\\u2009the interquartile range. \\nOverall, we observe that UNI consistently demonstrates superior label efficiency \\nover other baselines.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 2 | Comparing supervised performance on PRAD tissue \\nclassification in AGCC. Qualitative illustrations comparing UNI to CTransPath, \\nREMEDIS, and ResNet-50 (IN) via KNN probing on PRAD tissue classification \\nin AGCC. UNI achieves better accuracy (acc.) on all three examples. The \\nreported results are based on partial annotations (left-most panel) provided by \\npathologists.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 3 | ROI retrieval. We evaluate content-based image retrieval \\nfor ROI-level classes with at least 5 classes, for a. CRC tissue classification in \\nCRC-100K, b. CRC tissue classification in HunCRC, c. ESCA subtyping on CHA \\n(trained on UKK, WNS and TCGA), d. PRAD tissue classification in AGGC, e. CRC \\npolyp classification in UniT oPatho, and f. pan-cancer tissue classification in \\nTCGA, and. UNI consistently outperforming all pretrained encoders. Error bars \\nrepresent 95% confidence intervals and the center is the computed value of \\nthe corresponding retrieval metric. Detailed performance metrics are further \\nprovided in Supplementary Tables 63–68.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 4 | ROI classification across different image resolutions. \\nT o assess how image resolution affects performance, we compare UNI and other \\nbaselines on various resized and center-cropped ROIs for a. BRCA subtyping and \\nb. CRC polyp classification tasks. The original image sizes are 2048\\u2009×\\u20091536 and \\n1812\\u2009×\\u20091812 pixels, respectively. All models are evaluated on linear, SimpleShot \\n(1-NN), and KNN (20-NN) probe settings. UNI consistently outperforms all \\nbaselines across all resolutions. The performance metrics are further provided in \\nSupplementary Tables 45, 46, 51, 52.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 5 | Multi-head self-attention (MHSA) heatmap \\nvisualization of UNI across different image resolutions in BRCA Subtyping \\nin BACH. Each colored square represents a 16\\u2009×\\u200916 patch token encoded by \\nUNI, with heatmap color corresponding to the attention weight of that patch \\ntoken to the global [CLS] token of the penultimate layer in UNI. We show MHSA \\nvisualizations for resized and center-cropped ROIs at 2242,\\u20094482,\\u20098962,\\u20091,3442 \\nresolutions for the a. normal, b. benign, c. in situ, and d. invasive classes in BACH. \\nIn each, the left-most image is the original H&E ROI and the right four images are \\nthe MHSA visualizations. For comparative purposes, we resize all images within \\nthe figure to have the same dimension, but note that at higher resolutions, each \\ncolored square has an original image resolution of 16\\u2009×\\u200916 pixels at 0.42 mpp. As \\nthe resolution increases, the heatmaps demonstrate increasing and increasingly \\nfine-grained attention focused on epithelial structures, with relatively lower \\nattention on stroma or other background, neither of which are contributory to \\nthe diagnoses in these ROIs.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 6 | Multi-head self-attention (MHSA) heatmap \\nvisualization of UNI across different image resolutions for CRC polyp \\nclassification in UniT oPatho. Each colored square represents a 16\\u2009×\\u200916 patch \\ntoken encoded by UNI, with heatmap color corresponding to the attention \\nweight of that patch token to the global [CLS] token of the penultimate layer \\nin UNI. We show MHSA visualizations for resized and center-cropped ROIs at \\n2242,\\u20094482,\\u20098962,\\u200917922 resolutions for a. normal tissue, b. hyperplastic polyp,  \\nc. tubular adenoma with low-grade dysplasia, d. tubular adenoma with high-\\ngrade dysplasia, e. tubulo-villous adenoma with high-grade dysplasia, and  \\nf. tubulo-villous adenoma with low-grade dysplasia. In each, the left-most image \\nis the original H&E ROI and the right four images are the MHSA visualizations. \\nFor comparative purposes, we resize all images within the figure to have the \\nsame dimension, but note that at higher resolutions, each colored square has an \\noriginal image resolution of 16\\u2009×\\u200916 pixels at 0.48 mpp. As resolution increases, \\nthe heatmaps demonstrate increasing and increasingly fine-grained attention \\nfocused on the crypts, in all cases except the hyperplastic polyp in b, focusing on \\nareas a pathologist would use to make the diagnosis.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 7 | Visualizing segmentation results in SegPath. Using the \\nMask2Former head, we visualize the tissue segmentation of each class in SegPath \\ncreated by all pretrained encoders. Overall, we find that UNI is competitive  \\nwith convolutional and hierarchical models like CTransPath and REMEDIS in \\nmatching the segmentation masks obtained via immunofluorescence and DAPI \\nnuclear staining.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 8 | Few-shot ROI classification using class prototypes. \\nSimilar to slide-level classification, we also assess the label efficiency of UNI on \\nROI-level tasks, and observe superior label efficiency of UNI on most tasks except \\nCRC tissue classification on HunCRC. We evaluate all pretrained encoders using \\nthe nonparametric SimpleShot framework for a. CRC tissue classification in \\nCRC-100K, b. Breast metastasis detection in CAMELYON17-WILDS, c. RCC tissue \\nclassification on HEL (trained on TCGA), d. BRCA subtyping in BACH, e. CRC \\ntissue classification in HunCRC, f. ESCA subtyping on CHA (UKK+WNS+TCGA),  \\ng. PRAD tissue classification in AGGC, h. CRC polyp classification in UniT oPatho, \\ni. CRC MSI screening in TCGA, j. pan-cancer tissue classification in TCGA, and  \\nk. pan-cancer TIL detection in TCGA. The performance is measured across \\ndifferent few-shot settings with K\\u2009∈\\u20091, 2, 4, 8, 16, 32, 64, 128, 256 training examples \\nused per class (or support set size). Boxes indicate quartile values of model \\nperformance (n = 1000 runs) and whiskers extend to data points within 1.5\\u2009×\\u2009the \\ninterquartile range.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 9 | Few-shot slide classification using class prototypes. \\nWe adapt the SimpleShot framework for slide-level classification, called ‘MI-\\nSimpleShot’ . ROI class prototypes are constructed by averaging the pre-extracted \\nROI features for each class using the ‘TCGA Uniform Tumor’ dataset, which we \\nuse as ‘prompts’ for assigning the slide-level label. We assess and compare the \\nfew-shot performance of all pretrained encoders on NSCLC subtyping (a) and \\nRCC subtyping task (b), using the same runs (n = 5) in the few-shot setting for \\nABMIL for K\\u2009∈\\u20091, 2, 4, 8, 16, 32 training examples used per class. We compare \\nperformance of top-5 and top-50 pooling of nearest patches in the test set, as well \\nas show performance on both the internal test fold in TCGA and external cohort. \\nBoxes indicate quartile values of model performance (n = 5 runs) and whiskers \\nextend to data points within 1.5\\u2009×\\u2009the interquartile range. Overall, we observe \\nthat the formed prototypes by UNI can be used to classify slides based on the \\nMI-SimpleShot frame- work. a. On NSCLC subtyping, we observe that 2-shot and \\n4-shot performance from UNI outperforms the 32-shot performance of all other \\nmodels. b. On RCC subtyping, 1-shot performance of UNI also outperforms the \\n32-shot performance of other models. We also observe that MI-SimpleShot can \\nbe combined with other pretrained encoders as well, but generally require more \\nannotated ROIs for creating prototypes.\\nNature Medicine\\nArticle https://doi.org/10.1038/s41591-024-02857-3\\nExtended Data Fig. 10 | Comparing 1-shot similarity heatmaps of pretrained \\nencoders with class prototype. We compare the similarity heatmaps of all \\npretrained encoders using annotated ROIs from a single slide per class for \\nforming class prototypes in MI-SimpleShot (with top-5 pooling) on NSCLC \\nsubtyping (a) and RCC subtyping task (b), with top visualizing example ROIs \\nused for each class, and bottom showing similarity heatmaps. Outlined in blue \\nare pathologist annotations of ROIs that match the label of the histology slide. \\nSimilarity heatmaps are created with respect to the class prototype of the \\ncorrect slide label (indicated in green), with a ✓ indicating a correct prediction \\nand ✗ indicating incorrect prediction. Note that since the visualizations are \\ncreated with respect to the ground truth label, the model may retrieve correct \\npatches that match pathologist annotations but still misclassify the slide. a. On a \\nLUAD slide, we observe strong agreement of the pathologist’s annotations with \\nretrieved LUAD patches by UNI. Although retrieved patches by REMEDIS also \\nhave strong agreement with the pathologist’s annotations, we note that slide was \\nmisclassified as LUSC, indicating that the top-5 retrieved patches of the LUSC \\nprototype was higher than that of the LUAD prototype. Vice versa, ResNet-50IN \\nclassifies the slide correctly but incorrectly retrieves the correct patches that \\nagree with the pathologist’s annotations, indicating that non-LUAD patches in \\nthe slide were more LUAD-like than the pathologist-annotated LUAD patches \\nwith respect to the LUAD prototype. The similarity heatmap for CTransPath \\nboth misclassified the slide and retried incorrect patches. b. On an CCRCC slide, \\nwe observe strong agreement of the pathologist’s annotations with retrieved \\nCCRCC patches by UNI. We observe similar mismatch in predicted class label and \\nretrieved patches, in which REMEDIS classifies the slide correctly but retrieves \\nthe incorrect patches, and CTransPath misclassifies the slide but retrieves the \\ncorrect patches.\\n1 nature portfolio  |  reporting summaryApril 2023\\nCorresponding author(s): Faisal Mahmood\\nLast updated by author(s): Dec 20, 2023\\nReporting Summary\\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \\nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\\nStatistics\\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\\nn/a Confirmed\\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\\nThe statistical test(s) used AND whether they are one- or two-sided \\nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\\nA description of all covariates tested\\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \\nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \\nGive P values as exact values whenever suitable.\\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\\nOur web collection on statistics for biologists contains articles on many of the points above.\\nSoftware and code\\nPolicy information about availability of computer code\\nData collection To collect and process the pretraining dataset,  Python (version 3.8.13) was used along with the following packages: openslide-python (version \\n1.2.0), pillow (version 9.3.0), scikit-learn (version 1.2.1), and CLAM (http://github.com/mahmoodlab/CLAM) for WSI processing.\\n2 nature portfolio  |  reporting summaryApril 2023\\nData analysis We used Python (version 3.8.13) for all experiments and analyses in the study, which can be replicated using open-source libraries as outlined \\nbelow. For task agnostic pretraining, we used 4x8 80GB NVIDIA A100 GPU nodes configured for multi-GPU, multi-node training using \\ndistributed data-parallel (DDP) as implemented by the popular open source deep learning framework PyTorch (version 2.0.0, CUDA 11.7) \\n(pytorch.org). All downstream experiments were conducted on single 24GB NVIDIA 3090 GPUs. For unimodal pretraining of our visual encoder \\nusing DINOv2, we modify the vision transformer implementation maintained by the open-source Timm library (version 0.9.2) from Hugging \\nFace (huggingface.co) for the encoder backbone and use the original DINOv2 implementation (github.com/facebookresearch/dinov2) for \\ntraining. All WSI processing was supported by OpenSlide (version 4.3.1), openslide-python (version 1.2.0), and CLAM (github.com/\\nmahmoodlab/CLAM). We use Scikit-learn (version 1.2.1) for its implementation of K-Nearest Neighbors, and the logistic regression \\nimplementation and SimpleShot implementation provided by the LGSSL codebase (github.com/mbanani/lgssl). Implementations of other \\nvisual pretrained encoders benchmarked in the study are found at the following links: ResNet-50 with ImageNet Transfer (github.com/\\nmahmoodlab/CLAM), CTransPath (github.com/Xiyue-Wang/TransPath), and REMEDIS (github.com/google-research/medical-ai-research-\\nfoundations). For multi-head attention visualization, we used the visualization tools provided by the HIPT codebase (github.com/\\nmahmoodlab/HIPT). For training weakly-supervised ABMIL models, we adapted the training scaffold code from the CLAM codebase \\n(github.com/mahmoodlab/CLAM). For training semantic segmentation, we use the original Mask2Former implementation (github.com/\\nfacebookresearch/Mask2Former) which is based on detectron2 (version 0.6). For  ViT-Adatper, we adapt its original implementation \\n(github.com/czczup/ViT-Adapter) in detectron2 to train it using Mask2Former. Pillow (version 9.3.0) and OpenCV-python were used to \\nperform basic image processing tasks. Matplotlib (version 3.7.1) and Seaborn (version 0.12.2) were used to create plots and figures. \\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \\nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\\nData\\nPolicy information about availability of data\\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \\n- Accession codes, unique identifiers, or web links for publicly available datasets \\n- A description of any restrictions on data availability \\n- For clinical datasets or third party data, please ensure that the statement adheres to our policy \\n \\nTCGA data consisting of whole pathology slides and labels can be accessed through the NIH genomic data commons (https://portal.gdc.cancer.gov). GTEx data \\nadded to the pretraining dataset can be accessed through the GTEx portal (https://www.gtexportal.org/home/). For publicly available datasets we can access the \\ndata and labels at their respective data portals: CRC-100K (https://zenodo.org/record/1214456), HunCRC patches (https://doi.org/10.6084/\\nm9.figshare.c.5927795.v1), HunCRC slides (https://doi.org/10.7937/tcia.9cjf-0127), BACH (https://iciar2018-challenge.grand-challenge.org/Dataset/), TCGA CRC-\\nMSI  (https://zenodo.org/record/3832231). CCRCC tissue classification from TCGA (https://zenodo.org/record/7898308#.ZGXM3-xBxAc). TCGA-TILs  (https://\\nzenodo.org/record/5889558), TCGA Uniform (https://zenodo.org/record/5889558), UniToPatho  (https://zenodo.org/record/4643645), ESCA (https://zenodo.org/\\nrecord/7548828#.ZEnMnNLMJH5), EBRAINS (https://doi.org/10.25493/WQ48-ZGX), DHMC Kidney (https://bmirds.github.io/KidneyCancer/), BRACS (https://\\nwww.bracs.icar.cnr.it/), PANDA (https://panda.grand-challenge.org/data/), SegPath (https://zenodo.org/record/7412731), and AGGC (https://zenodo.org/\\nrecord/6460100). We obtained permission from the challenge organizers of the AGGC dataset to use this dataset. No internal patient data was specifically collected \\nfor this study. This study relies on retrospective analysis of anonymized whole slide images. Following institution policies, all requests for data collected or curated \\nin-house will be evaluated on a case-by-case basis to determine whether the data requested and the use case comply with intellectual property or patient privacy \\nobligations. Data that can be shared would require a formal data transfer agreement. \\nResearch involving human participants, their data, or biological material\\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), \\nand sexual orientation and race, ethnicity and racism.\\nReporting on sex and gender We did not use gender or sex as a covariate in our experimental analysis at any stage of the study. Though not used, data \\npertaining to sex and gender may have been collected in external data for downstream tasks, which were curated by their \\noriginal investigators. We refer readers to their original source for more detailed descriptions. For in-house data used in our \\nOT cancer classification task, we provide the aggregate distribution of self-reported sex as follows: 3080 Female,  2474 Male.\\nReporting on race, ethnicity, or \\nother socially relevant \\ngroupings\\nWe did not collect or use any covariates regarding race, ethnicity, and other social groupings at any stage of the study. \\nPopulation characteristics We did not collect or use any covariates pertaining to population characteristics at any stage of the study. \\nRecruitment No patient recruitment was necessary for using histology whole slide images retrospectively.  \\nEthics oversight Brigham and Women's Hospital IRB committee approved the retrospective analysis of pathology slides and corresponding \\nreports. The study involved retrospective analysis of data and patients were not directly recruited or involved in this study. \\nInformed consent was waived for analyzing pathology data retrospectively. \\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\n3 nature portfolio  |  reporting summaryApril 2023\\nField-specific reporting\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\\nLife sciences study design\\nAll studies must disclose on these points even when the disclosure is negative.\\nSample size No sample size calculations were performed. Given our fixed computational budget (e.g., file storage and number of GPUs available), for our \\ncombined pretraining data, we collected 100,130,900 image patches sampled from 100,426 diagnostic, FFPE H&E whole histopathology slide \\nimages. The performance of our pretrained model, which outperforms all other baselines, suggests that an adequate sample size was \\nobtained. For downstream datasets, see the datasets and evaluation subsection of the Online Methods section in the manuscript for more \\ndetails.\\nData exclusions For pretraining data, no additional data exclusions were performed after curation.\\nReplication Attempts at replication were successful for the reported model results. Code corresponding this work can be accessed at https://\\nwww.github.com/mahmoodlab/UNI\\nRandomization For downstream evaluation that required creating train, validation, test splits, we either used official splits created by the original \\ninvestigators of each dataset when available, or created them randomly. In general, we created random splits straitfied by class (ensuring that \\nthe proportions of each class are similar across splits) and at the patient level if possible (ensuring that slides from the same patient are only in \\nthe same split).\\nBlinding Blinding was not necessary for our study because our experiments were based on digitized histology slides or region-level images.\\nReporting for specific materials, systems and methods\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \\nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \\nMaterials & experimental systems\\nn/a Involved in the study\\nAntibodies\\nEukaryotic cell lines\\nPalaeontology and archaeology\\nAnimals and other organisms\\nClinical data\\nDual use research of concern\\nPlants\\nMethods\\nn/a Involved in the study\\nChIP-seq\\nFlow cytometry\\nMRI-based neuroimaging\\n\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract words from each pdf and\n",
        "word_count = []\n",
        "for i, entry in enumerate(texts):\n",
        "  text = entry['text']\n",
        "  clean_text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "  words = clean_text.split()\n",
        "  word_counts = Counter(words)\n",
        "  word_count.append({\"file_name\" :entry['file_name'], \"word_counts\" : word_counts})\n",
        "print(word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwKfEZwD58Gr",
        "outputId": "25cf6404-51d0-4ac5-8812-6e260b2fdad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'file_name': 'neural-nets', 'word_counts': Counter({'the': 311, 'of': 236, 'a': 160, 'in': 97, 'and': 96, 'to': 80, 'is': 71, 'be': 58, 'n': 53, 'for': 48, 'by': 44, 'state': 41, 'from': 39, 'states': 37, 'memory': 33, 'system': 32, 'are': 31, 'with': 30, 'this': 30, 'stable': 29, 'can': 25, 'memories': 25, '1': 25, 'an': 24, 'on': 24, 'that': 24, 'or': 23, 'as': 23, '0': 23, 'model': 21, 'were': 21, 'was': 21, 'properties': 19, 'neurons': 19, 'algorithm': 19, 'will': 18, 'at': 18, 'collective': 17, 'j': 17, 'but': 17, 'which': 17, 'time': 17, 's': 17, 'such': 16, 'information': 16, 'tij': 16, '2': 16, 't': 16, 'processing': 15, 'i': 15, '100': 15, 'would': 14, 'if': 14, 'random': 14, 'neuron': 14, 'physical': 13, 'it': 13, '10': 13, 'not': 13, '30': 13, 'computational': 12, 'have': 12, 'new': 11, 'number': 11, 'simple': 11, 'space': 11, 'we': 11, 'has': 11, 'stored': 11, 'there': 11, 'b': 11, 'e': 11, 'one': 11, 'large': 10, 'flow': 10, 'synapses': 10, 'each': 10, '7336173158': 10, 'rate': 10, 'when': 10, 'mean': 10, 'w': 10, '3': 10, 'distance': 10, '5': 10, 'given': 9, 'only': 9, 'no': 9, 'made': 9, 'also': 9, 'modeling': 9, 'd': 9, 'probability': 9, 'assigned': 9, 'systems': 8, 'some': 8, 'details': 8, 'our': 8, 'h': 8, 'g': 8, 'points': 8, 'c': 8, 'eq': 8, 'linear': 8, 'than': 8, 'fig': 8, 'between': 8, 'level': 8, 'initial': 8, 'sci': 7, 'neural': 7, 'use': 7, 'contentaddressable': 7, 'error': 7, 'more': 7, 'all': 7, 'general': 7, 'input': 7, 'errors': 7, 'case': 7, 'produce': 7, 'x': 7, 'xa': 7, 'starting': 7, 'storage': 7, 'particular': 7, 'should': 7, 'proc': 6, '1982': 6, 'emergent': 6, 'january': 6, 'any': 6, '9': 6, 'many': 6, 'been': 6, '11': 6, 'then': 6, 'potential': 6, 'must': 6, 'therefore': 6, 'vi': 6, 'ip': 6, 'threshold': 6, 'its': 6, 'thus': 6, 'noise': 6, '02': 6, '20': 6, 'most': 6, 'action': 6, 'average': 6, 'other': 6, 're': 6, 'about': 6, 'm': 6, 'factor': 6, 'l': 6, 'natl': 5, 'acad': 5, '79': 5, 'biophysics': 5, 'networks': 5, 'hopfield': 5, 'phase': 5, 'based': 5, 'circuits': 5, '13': 5, 'essential': 5, 'may': 5, 'both': 5, 'generate': 5, 'using': 5, 'rev': 5, 'basis': 5, 'used': 5, 'regions': 5, 'two': 5, 'small': 5, 'effects': 5, 'near': 5, 'downloaded': 5, 'httpswwwpnasorg': 5, '2025': 5, 'address': 5, 'firing': 5, 'so': 5, 'bits': 5, 'results': 5, 'value': 5, '4': 5, 'terms': 5, 'associative': 5, 'output': 5, 'network': 5, 'p': 5, 'symmetric': 5, 'chosen': 5, 'similar': 5, 'k': 5, 'cybern': 5, 'devices': 4, 'biological': 4, 'described': 4, 'appropriate': 4, 'readily': 4, 'their': 4, 'elementary': 4, 'behavior': 4, 'complex': 4, 'becomes': 4, 'whether': 4, 'spontaneous': 4, 'com': 4, 'yield': 4, 'function': 4, 'known': 4, 'same': 4, 'could': 4, 'content': 4, 'addressable': 4, 'like': 4, 'usually': 4, 'consider': 4, 'set': 4, 'point': 4, 'represents': 4, 'possible': 4, 'those': 4, 'locally': 4, 'within': 4, 'well': 4, 'minima': 4, 'dynamics': 4, 'limit': 4, 'stochastic': 4, 'local': 4, '12': 4, 'usa': 4, 'neu': 4, 'defined': 4, 'changes': 4, 'according': 4, 'randomly': 4, 'way': 4, '16': 4, 'n2': 4, 'matrices': 4, '01': 4, 'cells': 4, 'membrane': 4, 'generated': 4, 'effective': 4, 'present': 4, 'little': 4, 'shaw': 4, '25': 4, 'wandering': 4, 'hamming': 4, 'much': 4, 'these': 4, 'v': 4, 'math': 4, '1977': 4, '1978': 4, 'biol': 4, '1979': 4, 'pp': 3, 'california': 3, '15': 3, 'abstract': 3, 'computers': 3, 'having': 3, 'equivalent': 3, 'evolution': 3, 'asynchronous': 3, 'capacity': 3, 'generalization': 3, 'individual': 3, 'plan': 3, 'because': 3, 'part': 3, 'example': 3, 'nature': 3, 'force': 3, 'change': 3, 'cir': 3, 'item': 3, 'phys': 3, '1941': 3, 'partial': 3, 'even': 3, 'classes': 3, 'whose': 3, 'around': 3, 'particle': 3, 'above': 3, 'until': 3, 'connection': 3, 'values': 3, 'fixed': 3, 'unless': 3, 'perceptrons': 3, 'analysis': 3, 'strong': 3, 'second': 3, 'net': 3, 'real': 3, 'delays': 3, 'positive': 3, 'negative': 3, 'simultaneously': 3, '06': 3, 'si': 3, 'nonlinear': 3, 'true': 3, '21': 3, 'normal': 3, 'potentials': 3, 'inputoutput': 3, 'relationship': 3, '24': 3, 'region': 3, 'single': 3, 'typical': 3, 'roney': 3, '8': 3, '26': 3, 'hebb': 3, '27': 3, 'modification': 3, '6': 3, 'past': 3, 'produced': 3, 'effect': 3, 'remembered': 3, '7': 3, '29': 3, 'examine': 3, 'into': 3, 'being': 3, 'matrix': 3, 'occurred': 3, 'different': 3, 'pi': 3, 'minimum': 3, 'simulations': 3, 'energy': 3, 'always': 3, 'out': 3, 'nervous': 3, 'nominal': 3, 'retained': 3, '19': 3, 'increased': 3, 'choice': 3, 'studied': 3, 'clipped': 3, 'sign': 3, 'signaltonoise': 3, 'ratio': 3, 'added': 3, 'ty': 3, 'higher': 3, '0000': 3, '1980': 3, 'eds': 3, 'r': 3, 'f': 3, 'press': 3, '1969': 3, 'res': 3, 'brain': 3, 'division': 2, 'chemistry': 2, 'biology': 2, 'institute': 2, 'technology': 2, 'john': 2, 'construction': 2, 'components': 2, 'meaning': 2, 'correctly': 2, 'yields': 2, 'entire': 2, 'subpart': 2, 'sufficient': 2, 'size': 2, 'parallel': 2, 'additional': 2, 'familiarity': 2, 'correction': 2, 'weakly': 2, 'sensitive': 2, 'failure': 2, 'electrochemical': 2, 'interconnections': 2, 'schemes': 2, 'few': 2, 'obtain': 2, 'useful': 2, 'larger': 2, 'ask': 2, 'ability': 2, 'consequence': 2, 'interacting': 2, 'ele': 2, 'interactions': 2, 'numbers': 2, 'phenomena': 2, 'magnetic': 2, 'patterns': 2, 'stability': 2, 'categories': 2, 'old': 2, 'shows': 2, 'important': 2, 'spontaneously': 2, 'arise': 2, 'eg': 2, 'collisions': 2, 'integrated': 2, 'hardware': 2, 'categorizer': 2, 'suppose': 2, 'wannier': 2, 'capable': 2, 'might': 2, 'suffice': 2, 'deal': 2, 'ideas': 2, 'form': 2, 'ising': 2, 'describe': 2, 'frictional': 2, 'deterministic': 2, 'problems': 2, 'limiting': 2, 'essence': 2, 'xn': 2, 'let': 2, 'xb': 2, 'started': 2, 'regard': 2, 'vectors': 2, 'dom': 2, 'substantial': 2, 'addition': 2, 'called': 2, 'pitts': 2, 'article': 2, 'ment': 2, '18': 2, 'u': 2, 'readjusts': 2, 'attempt': 2, 'ui': 2, 'although': 2, 'modeled': 2, 'chiefly': 2, 'connections': 2, 'coupling': 2, 'proved': 2, 'interesting': 2, 'studies': 2, 'finding': 2, 'synchronous': 2, 'rons': 2, 'computer': 2, 'global': 2, 'synchrony': 2, 'signal': 2, '2v': 2, '12vj': 2, 'nets': 2, 'stimulus': 2, 's2': 2, 'confusing': 2, '04': 2, 'generally': 2, 'mixed': 2, 'contrast': 2, 'nonlinearity': 2, 'make': 2, 'external': 2, 'computation': 2, 'circuitry': 2, 'rather': 2, 'poten': 2, 'smooth': 2, 'shown': 2, 'often': 2, '22': 2, '23': 2, 'study': 2, 'nonlin': 2, 'ear': 2, 'central': 2, 'provide': 2, 'tion': 2, 'off': 2, 'transmission': 2, 'partially': 2, 'cell': 2, 'taken': 2, 'where': 2, 'synapse': 2, 'q': 2, 'dropping': 2, 'similarity': 2, 'eccles': 2, 'cor': 2, 'over': 2, 'allowed': 2, 'construct': 2, 'need': 2, 'inputs': 2, 'done': 2, 'how': 2, 'representing': 2, 'behaviors': 2, 'special': 2, 'tji': 2, 'ae': 2, 'reached': 2, 'field': 2, 'character': 2, 'architecture': 2, '31': 2, 'suggests': 2, 'slightly': 2, '50': 2, 'trials': 2, 'examined': 2, 'result': 2, 'chaotic': 2, 'statistics': 2, 'occurrence': 2, 'showed': 2, 'fashion': 2, '015': 2, 'carried': 2, 'appear': 2, 'sequences': 2, 'encoded': 2, 'half': 2, 'evolved': 2, 'less': 2, 'rest': 2, 'start': 2, 'ing': 2, 'expectation': 2, 'nn': 2, 'gaussian': 2, 'had': 2, 'experimental': 2, '05': 2, '49': 2, '39': 2, 'nominally': 2, 'final': 2, 'ended': 2, 'ambiguous': 2, 'very': 2, 'range': 2, 'nearest': 2, 'beyond': 2, 'times': 2, 'statistical': 2, 'while': 2, 'reduced': 2, 'ti': 2, 'next': 2, 'increment': 2, 'longer': 2, 'far': 2, 'too': 2, 'mem': 2, 'ories': 2, 'ij': 2, 'sim': 2, 'consistent': 2, 'seven': 2, 'does': 2, 'overload': 2, 'unfamiliar': 2, 'correlations': 2, 'implementation': 2, 'ca': 2, 'nonsymmetric': 2, 'lead': 2, 'cuits': 2, 'chips': 2, 'science': 2, '1973': 2, 'york': 2, 'stanford': 2, 'amari': 2, 'biosci': 2, 'kohonen': 2, 'mit': 2, 'cambridge': 2, 'cooper': 2, 'lundqvist': 2, '17': 2, 'longuethiggins': 2, '1968': 2, 'london': 2, 'mountcastle': 2, 'usavol': 1, '25542558': 1, 'april': 1, 'abilitiesassociative': 1, 'memoryparallel': 1, 'processingcategorizationcontentaddressable': 1, 'memoryfailsoft': 1, 'pasadena': 1, '91125': 1, 'bell': 1, 'laboratories': 1, 'murray': 1, 'hill': 1, 'jersey': 1, '07974': 1, 'contributed': 1, 'hopfweld': 1, 'ganisms': 1, 'emerge': 1, 'col': 1, 'lective': 1, 'aspects': 1, 'neurobiology': 1, 'adapted': 1, 'tegrated': 1, 'producea': 1, 'clude': 1, 'recognition': 1, 'categorization': 1, 'sequence': 1, 'retention': 1, 'themodeling': 1, 'dynamical': 1, 'understand': 1, 'understanding': 1, 'electronics': 1, 'allows': 1, 'us': 1, 'relevant': 1, 'collections': 1, 'perform': 1, 'tasks': 1, 'ments': 1, 'among': 1, 'ponents': 1, 'orientations': 1, 'domains': 1, 'vortex': 1, 'fluid': 1, 'do': 1, 'analogous': 1, 'correlates': 1, 'timese': 1, 'quential': 1, 'origin': 1, 'paper': 1, 'examines': 1, 'fun': 1, 'damental': 1, 'question': 1, '48': 1, 'computa': 1, 'tional': 1, 'neuro': 1, 'anatomy': 1, 'myriad': 1, 'incompletely': 1, 'emer': 1, 'gent': 1, 'insensitive': 1, 'inserted': 1, 'soundwaves': 1, 'reasonable': 1, 'interatomic': 1, 'law': 1, 'ap': 1, 'propriate': 1, 'spirit': 1, 'seek': 1, 'robust': 1, 'against': 1, 'implemented': 1, 'cuit': 1, 'conclusions': 1, 'suggest': 1, 'design': 1, 'delo': 1, 'calized': 1, 'ex': 1, 'tensive': 1, 'kramers': 1, '60': 1, '252': 1, 'retrieving': 1, 'ideal': 1, 'retrieve': 1, 'reference': 1, 'vannier': 1, 'relatively': 1, 'forms': 1, 'hard': 1, 'ware': 1, 'sophisticated': 1, 'ac': 1, 'cessing': 1, 'introduced': 1, 'software': 1, 'havior': 1, 'errorcorrecting': 1, 'co': 1, 'ordinates': 1, 'instanta': 1, 'neous': 1, 'condition': 1, 'either': 1, 'continuous': 1, 'discrete': 1, 'spins': 1, 'equations': 1, 'motion': 1, 'various': 1, 'sys': 1, 'tems': 1, 'particularly': 1, 'include': 1, 'ward': 1, 'anywhere': 1, 'damping': 1, 'moving': 1, 'exemplifies': 1, 'completely': 1, 'description': 1, 'complicated': 1, 'twowell': 1, 'characterized': 1, 'atemperature': 1, 'driving': 1, 'become': 1, 'absolute': 1, 'long': 1, 'remains': 1, 'coordinates': 1, 'x1': 1, 'vector': 1, 'sufficiently': 1, 'proceed': 1, 'knowledge': 1, 'generates': 1, 'total': 1, 'xaany': 1, 'inated': 1, 'attracted': 1, 'regarded': 1, 'potentiallyuseful': 1, 'prescribed': 1, 'mccullough': 1, '2554': 1, 'publication': 1, 'costs': 1, 'defrayed': 1, 'page': 1, 'chargepayment': 1, 'hereby': 1, 'marked': 1, 'advertise': 1, 'accordance': 1, '1734': 1, 'solely': 1, 'indicate': 1, 'fact': 1, '2555': 1, 'maximum': 1, 'ron': 1, 'strength': 1, 'nonconnected': 1, 'tij0': 1, 'instantaneous': 1, 'specified': 1, 'listing': 1, 'represented': 1, 'binary': 1, 'word': 1, 'following': 1, 'algo': 1, 'rithm': 1, 'setting': 1, 'vi0if': 1, 'tvjoi': 1, 'asynchronously': 1, 'evaluates': 1, 'below': 1, 'accord': 1, 'ingly': 1, 'otherwise': 1, 'stated': 1, 'choose': 1, 'superficial': 1, 'similarities': 1, 'per': 1, 'ceptron': 1, '14': 1, 'differences': 1, 'responsible': 1, 'first': 1, 'forward': 1, 'direction': 1, 'backward': 1, 'intractable': 1, 'consequences': 1, 'backcoupling': 1, 'percep': 1, 'tron': 1, 'directly': 1, 'world': 1, 'did': 1, 'questions': 1, 'proper': 1, 'ties': 1, 'finally': 1, 'perceptron': 1, 'required': 1, 'conventional': 1, 'digital': 1, 'evidence': 1, 'nerve': 1, 'propagation': 1, 'effectively': 1, 'exist': 1, 'spite': 1, 'asynchrony': 1, 'implications': 1, 'wish': 1, 'store': 1, 'v8': 1, 'prescription': 1, 'tii': 1, 'definition': 1, 'tijjs': 1, 'vj2vj1': 1, 'hjs': 1, 'bracketed': 1, 'term': 1, 'pseudoorthogonality': 1, 'tivs': 1, 'hs': 1, '2vs': 1, 'vw': 1, 'vf': 1, 'except': 1, 'coming': 1, 'al': 1, 'ways': 1, 'under': 1, 'theories': 1, '1519': 1, 'pattern': 1, 'paired': 1, 's1': 1, 'association': 1, 'simulus': 1, 'meaningless': 1, 'choices': 1, 'regenerate': 1, 'high': 1, 'connected': 1, 'logic': 1, 'processor': 1, 'order': 1, 'easy': 1, 'difficult': 1, 'discuss': 1, 'evolutionary': 1, 'obtains': 1, 'simpleproperties': 1, 'interpretation': 1, 'generating': 1, 'train': 1, 'tialspropagating': 1, 'pulses': 1, 'activitywhen': 1, 'across': 1, 'held': 1, 'resting': 1, 'potentialsare': 1, 'po': 1, 'tential': 1, 'sent': 1, 'lies': 1, 'shorttime': 1, 'neglect': 1, 'parallelpathways': 1, 'carrying': 1, 'enhance': 1, 'extract': 1, 'shortterm': 1, 'putation': 1, 'necessarily': 1, 'focus': 1, 'putoutput': 1, 'logical': 1, 'operations': 1, 'come': 1, 'dependence': 1, 'forces': 1, 'positions': 1, 'particles': 1, 'whereas': 1, 'emphasized': 1, '1419': 1, 'replace': 1, 'lationship': 1, 'dotdash': 1, 'step': 1, 'operation': 1, 'dominantly': 1, 'merely': 1, 'pathway': 1, 'communica': 1, 'granting': 1, 'interconnec': 1, 'tions': 1, 'operating': 1, 'regime': 1, 'synaptic': 1, 'char': 1, 'acter': 1, 'impulses': 1, 'along': 1, 'axons': 1, 'den': 1, 'drites': 1, 'delay': 1, 'generation': 1, 'parameter': 1, '1w': 1, 'arises': 1, 'current': 1, 'leaks': 1, 'influence': 1, 'activated': 1, 'arriving': 1, 'tials': 1, '5i': 1, 'tijvj': 1, 'effectiveness': 1, 'ajaz': 1, 'modelingw': 1, 'volts': 1, 'versus': 1, 'voltage': 1, 'solid': 1, 'line': 1, 'saturating': 1, 'broken': 1, 'lines': 1, 'show': 1, 'approximations': 1, 'developed': 1, 'functioning': 1, 'onoff': 1, 'however': 1, 'relative': 1, 'timing': 1, 'spikes': 1, 'sulted': 1, 'reverberating': 1, 'trains': 1, 'theirs': 1, 'limited': 1, 'formal': 1, 'deeper': 1, 'learning': 1, 'type': 1, '28': 1, 'ingredient': 1, 'relations': 1, 'at41': 1, 'vitvjtaverage': 1, 'calculation': 1, 'history': 1, 'decay': 1, 'vitavg': 1, 'vjtavg': 1, 'initiallyassume': 1, 'ty1has': 1, 'previous': 1, 'experi': 1, 'ence': 1, 'inheritance': 1, 'hebbian': 1, 'property': 1, 'reside': 1, 'groups': 1, 'performs': 1, 'cal': 1, 'culation': 1, 'applications': 1, 'appropri': 1, 'ately': 1, 'coded': 1, 'visual': 1, 'feature': 1, 'extrac': 1, 'previously': 1, 'related': 1, 'entity': 1, 'gestalt': 1, 'categorized': 1, 'collection': 1, 'features': 1, 'define': 1, 'tijvjvj': 1, 'ioj': 1, 'due': 1, 'av1': 1, 'avi': 1, 'vj': 1, '8joi': 1, 'altering': 1, 'causes': 1, 'monotoni': 1, 'cally': 1, 'decreasing': 1, 'continue': 1, 'least': 1, 'isomorphic': 1, 'provides': 1, 'role': 1, 'exchange': 1, 'site': 1, 'tjis': 1, 'spin': 1, 'glass': 1, 'monte': 1, 'carlo': 1, 'calculations': 1, 'removing': 1, 't1': 1, 'restriction': 1, 'element': 1, 'cortical': 1, 'ganglia': 1, 'invertebrates': 1, '32': 1, 'importance': 1, '10010000': 1, 'intense': 1, 'mu': 1, 'tual': 1, 'scale': 1, 'initiated': 1, 'configurations': 1, 'never': 1, 'displayed': 1, 'ergodic': 1, 'through': 1, '4w': 1, 'settled': 1, 'monest': 1, 'three': 1, 'end': 1, 'collect': 1, 'cycle': 1, 'oc': 1, 'casionallyfor': 1, 'third': 1, 'seen': 1, 'bi': 1, 'nary': 1, 'places': 1, 'digits': 1, 'short': 1, 'entropic': 1, 'measure': 1, 'available': 1, 'found': 1, 'theflow': 1, 'necessary': 1, 'slower': 1, 'quan': 1, 'titatively': 1, 'pursued': 1, 'they': 1, 'qualitative': 1, 'why': 1, 'persist': 1, 'tjj': 1, 'vice': 1, 'versa': 1, 'split': 1, 'identical': 1, 'ty1is': 1, 'tijand': 1, 'tj': 1, 'responding': 1, 'finite': 1, 'temperature': 1, 'fore': 1, 'recall': 1, 'severe': 1, 'corre': 1, 'sponding': 1, 't9': 1, 'preprocessedsignals': 1, 'efficient': 1, 'preprocessed': 1, 'coding': 1, 'dna': 1, 'simulate': 1, 'efficiently': 1, 'ignorance': 1, 'evolve': 1, 'stationary': 1, 'averages': 1, 'almost': 1, 'exactly': 1, 'recallable': 1, 'nominallyremembered': 1, 'er': 1, 'rors': 1, 'quite': 1, 'understood': 1, 'sum': 1, 'summation': 1, 'contributes': 1, 'rms': 1, '1n22': 1, 'largethis': 1, 'approximately': 1, 'bit': 1, 'ex22a2': 1, 'dx': 1, '00091': 1, 'ec0o9': 1, '040': 1, 'simulation': 1, 'theoretical': 1, 'scaling': 1, 'demon': 1, 'strated': 1, 'going': 1, 'tained': 1, 'badly': 1, 'expected': 1, '2556': 1, '2557': 1, 'n1': 1, '40': 1, 'nerr': 1, 'tate': 1, 'distribution': 1, 'location': 1, 'obtained': 1, 'accuracy': 1, 'judicious': 1, 'thresholds': 1, 'variables': 1, '1u44j': 1, 'arbitrary': 1, 'what': 1, 'resulting': 1, 'statistically': 1, 'evolutions': 1, 'ran': 1, 'domly': 1, 'tabulated': 1, 'inessential': 1, 'symmetry': 1, '101110': 1, '010001': 1, 'approx': 1, 'imately': 1, '85': 1, 'obvious': 1, 'landed': 1, 'likelihood': 1, 'leads': 1, 'closest': 1, '90': 1, 'fell': 1, 'smoothly': 1, 'chance': 1, 'apparently': 1, 'dominated': 1, 'attractors': 1, 'inates': 1, 'entirely': 1, 'responds': 1, 'resembles': 1, 'desired': 1, 'sibased': 1, 'modi': 1, 'fied': 1, 'hold': 1, 'letting': 1, 'oth': 1, 'ers': 1, 'adjust': 1, 'replacing': 1, 't4in': 1, 'algebraic': 1, 'purposes': 1, 'necessity': 1, 'supposition': 1, 'makinga': 1, 'highly': 1, 'efficiency': 1, 'nn2': 1, 'possibly': 1, 'experimentally': 1, 'ordinary': 1, 'evaluated': 1, 'analytically': 1, '2r12': 1, 'pared': 1, 'unclipped': 1, '2irwith': 1, 'maximal': 1, 'shannon': 1, 'corresponded': 1, 'continually': 1, 'overloads': 1, 'makes': 1, 'irretrievable': 1, 'provision': 1, 'forgetting': 1, '28the': 1, 'saturation': 1, 'itself': 1, 'cause': 1, 'getting': 1, 'tt': 1, 'freely': 1, 'incremented': 1, 'ignored': 1, 'reduce': 1, 'constructed': 1, 'recent': 1, 'distant': 1, 'depends': 1, 'digitizing': 1, 'depth': 1, 'keep': 1, 'written': 1, 'attractive': 1, 'requires': 1, 'delicate': 1, 'balances': 1, 'natural': 1, 'restricted': 1, 'asked': 1, 'tjjis': 1, 'making': 1, 'con': 1, 'tinued': 1, 'descriptionof': 1, 'decreased': 1, '1f2': 1, 'ulations': 1, 'analysisshows': 1, 'fails': 1, 'soft': 1, 'increasing': 1, 'slowly': 1, 'fail': 1, 'close': 1, 'confused': 1, 'tend': 1, 'merge': 1, 'pair': 1, 'sep': 1, 'arated': 1, 'units': 1, '8was': 1, 'eighth': 1, 'up': 1, 'distinct': 1, 'displaced': 1, 'fused': 1, 'categorizes': 1, 'ilarity': 1, 'haves': 1, 'forced': 1, '00000': 1, 'seldom': 1, 'occurs': 1, 'adding': 1, 'uniform': 1, 'raising': 1, 'compared': 1, 'likely': 1, 'generatedby': 1, 'resemble': 1, 'adequately': 1, 'closely': 1, 'recog': 1, 'nition': 1, 'familiar': 1, 'recognized': 1, 'means': 1, 'drastically': 1, 'overloaded': 1, '500': 1, 'none': 1, 'ini': 1, 'tial': 1, 'readjustments': 1, 'occur': 1, '12w': 1, 'fa': 1, 'miliar': 1, 'distinguishable': 1, 'faster': 1, 'kind': 1, 'famil': 1, 'iarity': 1, 'read': 1, 'class': 1, 'abstracting': 1, 'group': 1, 'cases': 1, 'considered': 1, 'tijwas': 1, 'cij': 1, 'ternal': 1, 'correlation': 1, 'now': 1, 'atuj': 1, '2xi': 1, '12xj': 1, 'ill': 1, 'xkl': 1, 'determined': 1, 'primarily': 1, 'xj': 1, '12j1': 1, 'completed': 1, 'pacity': 1, 'stores': 1, 'correlated': 1, 'fol': 1, 'lowed': 1, 'possibility': 1, 'metastable': 1, 'replaced': 1, 'another': 1, 'eas': 1, 'ily': 1, 'minor': 1, 'atu': 1, '2vs1': 1, 'judiciously': 1, 'adjusted': 1, 'spend': 1, 'leave': 1, 'go': 1, 'four': 1, 'im': 1, 'faithfullyfollowed': 1, 'discussion': 1, 'structure': 1, 'nonetheless': 1, 'arose': 1, 'entities': 1, 'gestalts': 1, 'reasonably': 1, 'sized': 1, 'ambiguities': 1, 'solved': 1, 'ordering': 1, 'follow': 1, 'strongly': 1, 'dependent': 1, 'precise': 1, 'mod': 1, 'eling': 1, 'robustness': 1, 'neurobiological': 1, 'brains': 1, 'animals': 1, 'proliferation': 1, 'welldefined': 1, 'functions': 1, 'bridge': 1, 'emergence': 1, 'capabilities': 1, 'elements': 1, 'softfailure': 1, 'wasteful': 1, 'gates': 1, 'standard': 1, 'designs': 1, 'par': 1, 'allel': 1, 'capability': 1, 'rapid': 1, 'solutions': 1, 'work': 1, 'supported': 1, 'national': 1, 'foundation': 1, 'grant': 1, 'dmr8107494': 1, 'contri': 1, 'bution': 1, '6580': 1, 'chemical': 1, 'engineering': 1, 'willows': 1, 'dorsett': 1, 'hoyle': 1, 'robiol': 1, '207237': 1, '255285': 1, 'kristan': 1, 'pinsker': 1, 'willis': 1, 'raven': 1, '241261': 1, 'knight': 1, '1975': 1, 'lect': 1, 'life': 1, '111144': 1, 'smith': 1, 'davidson': 1, '1962j': 1, 'assoc': 1, 'comput': 1, 'mach': 1, '268279': 1, 'harmon': 1, '1964': 1, 'theory': 1, 'ed': 1, 'reissr': 1, 'univ': 1, '2324': 1, 'bio': 1, '175185': 1, 'akikazu': 1, '127136': 1, '1974': 1, '101120': 1, 'marr': 1, 'physiol': 1, '202': 1, '437470': 1, 'springernew': 1, 'york11': 1, 'palm': 1, '36': 1, '1931': 1, 'mcculloch': 1, '1943': 1, 'bull': 1, 'biophys': 1, '115133': 1, 'minsky': 1, 'papert': 1, 'introduction': 1, 'geometry': 1, 'ma14': 1, 'rosenblatt': 1, '1962': 1, 'principles': 1, 'spartan': 1, 'wash': 1, 'ington': 1, 'dc15': 1, 'proceedings': 1, 'nobel': 1, 'symposium': 1, 'academic': 1, '252264': 1, 'liberman': 1, 'oja': 1, '33928': 1, 'roy': 1, 'soc': 1, 'ser': 1, '171327334': 1, '217': 1, '104105': 1, 'memorya': 1, 'systemtheoreticapproach': 1, 'springer': 1, 'york20': 1, 'willwacher': 1, '1976': 1, '181198': 1, 'anderson': 1, 'psych': 1, '84': 1, '413451': 1, 'perkel': 1, 'bullock': 1, 'neurosci': 1, 'sympsumm': 1, '405527': 1, '1972': 1, '177': 1, '850864': 1, 'scheibel': 1, '225271': 1, '281289': 1, 'lett': 1, '74': 1, '146150': 1, '1949': 1, 'organization': 1, 'wiley': 1, 'york28': 1, '1953': 1, 'neurophysiological': 1, 'mind': 1, 'clarendon': 1, 'oxford29': 1, 'kirkpatrick': 1, 'sherrington': 1, '43844403': 1, 'mindful': 1, 'edelmang': 1, 'ma': 1, '3641': 1, 'goldman': 1, 'nauta': 1, '12239341332': 1, 'kandel': 1, 'am': 1, '241': 1, '6170': 1, 'rl2558': 1})}, {'file_name': 'backprop', 'word_counts': Counter({'the': 339, 'of': 166, 'to': 105, 'a': 85, 'in': 84, 'and': 80, 'is': 61, 'units': 58, 'for': 51, 'by': 44, 'that': 40, 'input': 36, 'are': 33, 'output': 30, 'weights': 27, 'each': 27, 'be': 26, 'this': 24, 'e': 24, 'unit': 24, 'from': 23, 'can': 22, 'we': 21, '1': 21, '2': 21, 'an': 20, 'with': 19, 'it': 18, 'have': 18, 'on': 18, 'j': 18, 'layer': 18, 'were': 18, 'as': 17, 'learning': 17, 'procedure': 17, 'which': 17, 'm': 17, 'two': 17, 'one': 16, 'network': 16, 'eye': 16, 'hidden': 14, 'not': 13, 'so': 13, 'weight': 13, 'these': 13, 'amino': 12, 'c': 12, 'vector': 12, 'set': 12, '5': 12, '0': 12, 'connections': 11, 'net': 11, 'i': 11, 'l': 11, '3': 11, 's': 11, '4': 11, '6': 11, 'states': 11, 'at': 11, 'fig': 11, 'after': 11, 'both': 11, 'deprived': 11, 'vision': 11, 'nature': 10, 'between': 10, 'will': 10, 'initially': 10, 'binocular': 10, 'may': 9, 'more': 9, 'task': 9, 'layered': 9, 'layers': 9, 'gradient': 9, 'was': 9, 'reverse': 9, 'kittens': 9, 'visual': 9, 'acuity': 9, 'given': 8, 'or': 8, 'networks': 8, 'desired': 8, 'but': 8, 'r': 8, 'g': 8, '1982': 8, 'h': 8, 'p': 8, 'inputoutput': 8, 'all': 8, 'has': 8, 'only': 8, 'use': 8, 'levels': 8, '_': 8, 'occlusion': 8, 'normal': 8, 'eyes': 8, 'acids': 7, 'if': 7, 'engel': 7, 'hare': 7, 'error': 7, 'person': 7, 'period': 7, 'days': 7, 'acid': 6, 'should': 6, 'cases': 6, 't': 6, 'connected': 6, 'f': 6, '17': 6, '18': 6, 'their': 6, 'then': 6, 'total': 6, 'function': 6, 'its': 6, 'case': 6, 'using': 6, 'first': 6, 'activity': 6, 'second': 6, 'change': 6, 'iterative': 6, 'weeks': 6, 'individual': 5, 'other': 5, 'd': 5, 'time': 5, 'however': 5, 'particular': 5, 'used': 5, 'age': 5, 'partial': 5, 'actual': 5, 'there': 5, 'find': 5, '1986': 5, 'yb': 5, 'carnegie': 5, 'wash': 5, '20': 5, 'because': 5, 'determined': 5, 'learn': 5, 'intermediate': 5, 'lower': 5, 'just': 5, 'about': 5, 'every': 5, 'descent': 5, 'compute': 5, 'derivatives': 5, 'pass': 5, 'people': 5, 'area': 5, 'had': 5, 'cortical': 5, 'nondeprived': 5, 'publishing': 4, 'group1986': 4, 'also': 4, 'possible': 4, 'method': 4, 'during': 4, 'when': 4, 'grant': 4, 'support': 4, 'research': 4, 'representations': 4, 'science': 4, 'california': 4, 'usa': 4, 'internal': 4, 'features': 4, 'state': 4, 'bada': 4, '70': 4, 'ed': 4, 'w': 4, '1984': 4, '1985': 4, '7': 4, '8': 4, 'macko': 4, 'instn': 4, '12': 4, 'y': 4, 'they': 4, 'must': 4, 'bottom': 4, 'within': 4, 'equations': 4, 'linear': 4, 'equivalent': 4, '142': 4, '3f': 4, 'symmetry': 4, 'required': 4, 'sweeps': 4, 'distributed': 4, 'patterns': 4, 'same': 4, 'family': 4, 'triples': 4, 'representing': 4, 'english': 4, 'aeay': 4, 'very': 4, 'corresponding': 4, 'nets': 4, 'local': 4, 'shows': 4, 'indigeneity': 3, '14': 3, 'identical': 3, 'always': 3, 'stable': 3, 'enantiomers': 3, 'isotopic': 3, 'information': 3, 'some': 3, 'williams': 3, 'university': 3, 'new': 3, 'come': 3, 'represent': 3, 'domain': 3, 'earlier': 3, 'methods': 3, 'been': 3, 'many': 3, 'neural': 3, 'structure': 3, 'relatively': 3, 'vectors': 3, 'interesting': 3, '15': 3, 'b': 3, 'analyt': 3, '82': 3, '13': 3, '9': 3, 'estep': 3, '10': 3, 'chem': 3, '24': 3, 'form': 3, 'any': 3, 'top': 3, 'presented': 3, 'applying': 3, 'parallel': 3, 'until': 3, 'extra': 3, 'value': 3, 'bias': 3, 'opposite': 3, 'equation': 3, 'symmetrical': 3, 'pattern': 3, 'receive': 3, 'midpoint': 3, 'sum': 3, 'below': 3, 'necessary': 3, 'derivative': 3, 'where': 3, 'index': 3, 'over': 3, 'simply': 3, 'backward': 3, 'colin': 3, 'trees': 3, 'relationship': 3, 'third': 3, 'term': 3, 'groups': 3, 'group': 3, 'penultimate': 3, 'how': 3, 'aeaw': 3, 'encodes': 3, 'most': 3, 'similar': 3, 'trained': 3, 'ae': 3, 'aw': 3, 'current': 3, 'o': 3, 'shown': 3, 'minima': 3, 'bilateral': 3, 'amblyopia': 3, 'early': 3, 'severe': 3, 'dominance': 3, 'monocularly': 3, '30': 3, 'cycles': 3, 'per': 3, 'degree': 3, '_________': 2, 'fossils': 2, 'ams': 2, 'techniques': 2, 'become': 2, 'racemization': 2, 'complete': 2, 'indigenous': 2, 'nonindigenous': 2, 'organisms': 2, 'such': 2, 'easier': 2, 'becomes': 2, 'distribution': 2, 'composition': 2, 'fossil': 2, 'help': 2, 'general': 2, 'range': 2, 'development': 2, 'whether': 2, '27': 2, 'result': 2, 'part': 2, 'mhe': 2, 'earth': 2, 'following': 2, 'david': 2, 'rumelhart': 2, 'department': 2, 'backpropagation': 2, 'minimize': 2, 'difference': 2, 'captured': 2, 'create': 2, 'simpler': 2, 'aim': 2, 'powerful': 2, 'rule': 2, 'allow': 2, 'appropriate': 2, 'specified': 2, 'directly': 2, 'easy': 2, 'iteratively': 2, 'relative': 2, 'petroleum': 2, 'co': 2, 'sam': 2, 'received': 2, '19': 2, 'accepted': 2, 'july': 2, 'proc': 2, '1974': 2, 'nagy': 2, 'biochem': 2, '1977': 2, 'haering': 2, '1983': 2, '11': 2, 'chemistry': 2, 'biochemistry': 2, 'barrett': 2, 'chapman': 2, 'hall': 2, 'london': 2, 'weinstein': 2, 'protein': 2, 'n': 2, 'life': 2, '81': 2, '296': 2, 'parker': 2, 'perceptrons': 2, 'fixed': 2, 'do': 2, 'what': 2, 'active': 2, 'behaviour': 2, 'simple': 2, 'enough': 2, 'simplest': 2, 'different': 2, 'outputs': 2, 'yi': 2, 'biases': 2, 'sign': 2, '72': 2, '71': 2, 'learned': 2, 'detect': 2, 'numbers': 2, 'inside': 2, 'through': 2, 'accumulated': 2, 'sweep': 2, '09': 2, 'random': 2, '03': 2, 'property': 2, 'solution': 2, 'magnitude': 2, 'off': 2, 'eight': 2, 'above': 2, 'exactly': 2, 'before': 2, 'computed': 2, 'respect': 2, 'forward': 2, 'isomorphic': 2, 'relationships': 2, 'know': 2, 'activating': 2, 'white': 2, 'totally': 2, 'correct': 2, 'black': 2, 'row': 2, 'immediately': 2, 'computing': 2, 'differentiating': 2, 'gives': 2, 'means': 2, 'changing': 2, 'contribution': 2, 'distinction': 2, 'italian': 2, 'representation': 2, 'therefore': 2, 'since': 2, 'separate': 2, 'four': 2, 'remaining': 2, 'make': 2, '02': 2, 'decay': 2, 'considered': 2, 'activities': 2, 'run': 2, 'three': 2, 'into': 2, 'performing': 2, 'store': 2, 'average': 2, 'amount': 2, 'proportional': 2, 'either': 2, 'perform': 2, 'effect': 2, 'no': 2, 'does': 2, 'rapidly': 2, 'significantly': 2, 'improved': 2, 'point': 2, 'space': 2, 'whole': 2, 'discovered': 2, 'le': 2, 'evidence': 2, 'adding': 2, 'receptive': 2, 'fields': 2, '100': 2, 'global': 2, 'minimum': 2, 'tasks': 2, 'poor': 2, 'than': 2, 'dimensions': 2, 'weightspace': 2, 'plausible': 2, 'mit': 2, 'cambridge': 2, 'short': 2, 'majority': 2, 'cats': 2, 'behaviourally': 2, 'although': 2, 'monocular': 2, 'deprivation': 2, '34': 2, 'restore': 2, 'examined': 2, 'ocular': 2, 'followed': 2, 'beginning': 2, 'surprisingly': 2, 'despite': 2, 'introduction': 2, 'acuities': 2, 'tion': 2, 'opened': 2, 'closed': 2, 'recordings': 2, 'made': 2, 'control': 2, 'five': 2, 'further': 2, 'single': 2, '00': 2, 'c155': 2, '40': 2, '50': 2, '60': 2, 'changes': 2, 'intravenous': 2, 'artificial': 2, 'along': 2, 'penetration': 2, 'hemisphere': 2, 'exposure': 2, 'months': 2, '_natu_r_e_v_o_l_': 1, '3_2_3': 1, '_9_0_ct_o_b_e_r_1_98_6': 1, 'letterstonature': 1, '533': 1, 'delineating': 1, 'absolute': 1, 'refined': 1, 'handle': 1, 'smaller': 1, 'samples': 1, 'date': 1, 'enan': 1, 'tiomers': 1, 'enantiomer': 1, 'entirely': 1, 'derived': 1, 'diagenesis': 1, 'lenantiomers': 1, 'ages': 1, 'older': 1, 'poorly': 1, 'preserved': 1, 'prove': 1, 'amenable': 1, 'determination': 1, 'isotope': 1, 'prospects': 1, 'replace': 1, 'ment': 1, 'increases': 1, 'undergo': 1, 'compositions': 1, 'still': 1, 'related': 1, 'original': 1, 'circumstance': 1, 'recognize': 1, 'available': 1, 'concerning': 1, 'constituents': 1, 'modern': 1, 'representatives': 1, 'dates': 1, 'clarify': 1, 'problems': 1, 'stratigraphic': 1, 'controls': 1, 'estimate': 1, 'question': 1, 'finally': 1, 'determining': 1, 'enable': 1, 'us': 1, 'establish': 1, 'nonracemic': 1, 'carbonaceous': 1, 'meteorites': 1, 'terrestrial': 1, 'contamination': 1, 'thanks': 1, 'nsf': 1, 'division': 1, 'sciences': 1, 'ear8352055': 1, 'contributors': 1, 'his': 1, 'presidential': 1, 'young': 1, 'investigator': 1, 'award': 1, 'backpropagating': 1, 'errors': 1, 'geoffrey': 1, 'hintont': 1, 'ronald': 1, 'institute': 1, 'cognitive': 1, 'c015': 1, 'san': 1, 'diego': 1, 'la': 1, 'jolla': 1, '92093': 1, 'computer': 1, 'carnegiemellon': 1, 'pittsburgh': 1, 'philadelphia': 1, '15213': 1, 'describe': 1, 'neuronelike': 1, 'repeatedly': 1, 'adjusts': 1, 'measure': 1, 'adjustments': 1, 'important': 1, 'regularities': 1, 'interactions': 1, 'ability': 1, 'useful': 1, 'distin': 1, 'guishes': 1, 'perceptronconvergence': 1, 'attempts': 1, 'design': 1, 'selforganizing': 1, 'synaptic': 1, 'modification': 1, 'arbitrarily': 1, 'develop': 1, 'giving': 1, 'rules': 1, 'adjust': 1, 'strengths': 1, 'progressively': 1, 'reduce': 1, 'whom': 1, 'correspondence': 1, 'addressed': 1, 'arco': 1, 'exxon': 1, 'phillips': 1, 'texaco': 1, 'inc': 1, 'upjohn': 1, 'acknowledge': 1, 'donors': 1, 'fund': 1, 'administered': 1, 'american': 1, 'chemical': 1, 'society': 1, '16144ac2': 1, '14805ac2': 1, 'acknowledges': 1, 'nserc': 1, 'a2644': 1, 'protsch': 1, 'natn': 1, 'acad': 1, '13311334': 1, '1973': 1, 'schroeder': 1, 'carter': 1, '184': 1, '791793': 1, 'boulton': 1, 'et': 1, 'al': 1, '298': 1, '437441': 1, 'wehmiller': 1, 'quaternary': 1, 'dating': 1, 'mahaney': 1, '171193': 1, 'elsevier': 1, 'amsterdam': 1, 'zumberge': 1, 'je': 1, '415422': 1, 'rev': 1, 'planet': 1, 'sci': 1, '241268': 1, 'chisholm': 1, 'nelson': 1, 'schwarcz': 1, 'hp': 1, '216': 1, '11311132': 1, 'ambrose': 1, 'deniro': 1, '319': 1, '321324': 1, 'lnstn': 1, '404410': 1, '410414': 1, 'amio': 1, '462479': 1, 'johnstone': 1, 'rose': 1, '480524': 1, 'practical': 1, 'chemistrya': 1, 'handbook': 1, 'darbre': 1, '337344': 1, 'wiley': 1, 'york': 1, 'gillespie': 1, 'gowlett': 1, 'hedges': 1, '312': 1, '442444': 1, 'mitterer': 1, 'kriausakul': 1, 'org': 1, 'geochem': 1, '9198': 1, '16': 1, 'k': 1, 'smith': 1, 'origins': 1, '91144': 1, '425430': 1, '73': 1, '576581': 1, 'pillinger': 1, '802': 1, 'neuberger': 1, 'adv': 1, '298383': 1, '1948': 1, '21': 1, '56': 1, '25982600': 1, '22': 1, 'dungworth': 1, 'geo': 1, '135153': 1, '1976': 1, '23': 1, '121': 1, '370377': 1, 'lee': 1, 'exp': 1, 'mar': 1, 'biol': 1, 'ecol': 1, '63': 1, '145149': 1, '25': 1, '413417': 1, '26': 1, 'vallentync': 1, 'geochim': 1, 'cosmochim': 1, 'acta': 1, '28': 1, '157188': 1, '1964': 1, '837840': 1, 'difficult': 1, 'introduce': 1, 'whose': 1, 'feature': 1, 'analysers': 1, 'true': 1, 'hand': 1, 'completely': 1, 'decide': 1, 'under': 1, 'circumstances': 1, 'order': 1, 'achieve': 1, 'amounts': 1, 'deciding': 1, 'demonstrate': 1, 'purpose': 1, 'construct': 1, 'number': 1, 'higher': 1, 'forbidden': 1, 'skip': 1, 'setting': 1, 'coming': 1, 'sequentially': 1, 'starting': 1, 'working': 1, 'upwards': 1, 'xi': 1, 'unitj': 1, 'wii': 1, 'introducing': 1, 'called': 1, 'threshold': 1, 'treated': 1, 'like': 1, 'realvalued': 1, 'nonlinear': 1, '534': 1, 'letters': 1, 'nature__naturey0l': 1, '3239': 1, 'oct0ber1986': 1, '8_8_': 1, 'f4': 1, '88': 1, 'mirror': 1, 'arcs': 1, 'nodes': 1, '1425': 1, '64': 1, 'being': 1, 'adjusted': 1, 'basis': 1, 'values': 1, 'parameters': 1, '01': 1, 'initial': 1, 'uniformly': 1, 'key': 1, 'symmetric': 1, 'middle': 1, 'equal': 1, 'negative': 1, 'having': 1, 'positive': 1, 'note': 1, 'side': 1, 'ratio': 1, 'ensures': 1, 'occur': 1, 'sends': 1, 'unique': 1, 'activation': 1, 'balance': 1, 'nonsymmetrical': 1, 'nonzero': 1, 'activations': 1, 'signs': 1, 'nonsymmetric': 1, 'suppress': 1, 'functions': 1, 'bounded': 1, 'combining': 1, 'inputs': 1, 'nonlinearity': 1, 'greatly': 1, 'simplifies': 1, 'ensure': 1, 'produced': 1, 'sufficiently': 1, 'close': 1, 'finite': 1, 'performance': 1, 'comparing': 1, 'defined': 1, '½2': 1, 'yjcdjc2': 1, 'pairs': 1, 'passes': 1, 'already': 1, 'described': 1, 'propagates': 1, 'back': 1, 'complicated': 1, 'christopher': 1, 'penelope': 1, 'andrew': 1, 'christine': 1, 'margaret': 1, 'arthur': 1, 'victoria': 1, 'james': 1, 'jennifer': 1, 'charles': 1, 'charlotte': 1, 'roberto': 1, 'maria': 1, 'pierro': 1, 'francesca': 1, 'gina': 1, 'emilio': 1, 'lucia': 1, 'marco': 1, 'angela': 1, 'tomaso': 1, 'alfonso': 1, 'sophia': 1, 'expressed': 1, 'father': 1, 'mother': 1, 'husband': 1, 'wife': 1, 'son': 1, 'daughter': 1, 'uncle': 1, 'aunt': 1, 'brother': 1, 'sister': 1, 'nephew': 1, 'niece': 1, 'said': 1, 'produce': 1, 'triple': 1, 'terms': 1, 'encoded': 1, 'proposition': 1, 'represents': 1, 'fivelayer': 1, 'left': 1, 'right': 1, 'squares': 1, 'show': 1, 'hasaunt': 1, 'own': 1, 'encode': 1, 'central': 1, 'activate': 1, 'stands': 1, 'answers': 1, 'marked': 1, 'dots': 1, 'aunts': 1, 'laid': 1, 'out': 1, 'spatially': 1, 'italians': 1, 'starts': 1, 'suppressing': 1, 'aeayyd': 1, 'apply': 1, 'chain': 1, 'aeax': 1, 'aeaxi': 1, 'aeayidydxi': 1, 'get': 1, 'dyi': 1, 'dx': 1, 'substituting': 1, 'x': 1, 'affect': 1, 'level': 1, 'affected': 1, 'aeaxaxiaw': 1, 'ijeax': 1, 'ih': 1, 'nccaccctuccccre_vccol': 1, '32c3': 1, '90ctober1986letterstonature': 1, '53': 1, '__e': 1, '__a': 1, '__': 1, '_0': 1, 'representa': 1, 'tions': 1, 'rectangles': 1, 'excitatory': 1, 'rec': 1, 'tangles': 1, 'inhibitory': 1, 'rectangle': 1, 'mag': 1, 'nitude': 1, 'primarily': 1, 'concerned': 1, 'ignore': 1, 'making': 1, 'isomorphism': 1, 'share': 1, 'tend': 1, 'generalize': 1, 'sensibly': 1, 'tree': 1, 'generation': 1, 'belongs': 1, 'branch': 1, 'explicit': 1, 'encodings': 1, 'capture': 1, 'underlying': 1, 'generalizes': 1, 'correctly': 1, '1500': 1, '0005': 1, '05': 1, '001': 1, 'interpret': 1, 'introduced': 1, 'weightdecay': 1, 'decrementing': 1, 'prolonged': 1, 'balanced': 1, 'final': 1, 'indicates': 1, 'usefulness': 1, 'reducing': 1, 'prevent': 1, 'needing': 1, 'large': 1, 'drive': 1, 'zero': 1, '08': 1, 'synchronous': 1, 'iterations': 1, 'timestep': 1, 'recurrent': 1, 'corresponds': 1, 'mapped': 1, 'complications': 1, 'arise': 1, 'mapping': 1, 'intermedi': 1, 'ate': 1, 'see': 1, 'history': 1, 'preserve': 1, 'ieaw': 1, 'provisos': 1, 'applied': 1, 'searches': 1, 'sequential': 1, 'structures': 1, 'resulting': 1, 'aeavaxjjayj': 1, 'aeaxj': 1, 'wji': 1, 'taking': 1, 'account': 1, 'emanating': 1, 'now': 1, 'seen': 1, 'last': 1, 'repeat': 1, 'successively': 1, 'go': 1, 'way': 1, 'advantage': 1, 'memory': 1, 'alternative': 1, 'scheme': 1, 'reported': 1, 'here': 1, 'accumulate': 1, 'version': 1, 'aweaeaw': 1, 'converge': 1, 'much': 1, 'easily': 1, 'implemented': 1, 'computations': 1, 'hardware': 1, 'without': 1, 'sacrificing': 1, 'simplicity': 1, 'locality': 1, 'acceleration': 1, 'modify': 1, 'velocity': 1, 'instead': 1, 'position': 1, 'wt': 1, 'eaejawt': 1, 'aawt': 1, 'incremented': 1, 'exponential': 1, 'factor': 1, 'determines': 1, 'gradients': 1, 'break': 1, 'start': 1, 'small': 1, 'variants': 1, 'independently': 1, 'personal': 1, 'communication': 1, 'yann': 1, 'cun3': 1, 'cannot': 1, 'done': 1, 'connecting': 1, 'detection': 1, 'binary': 1, 'onedimensional': 1, 'array': 1, 'centre': 1, 'essential': 1, 'alone': 1, 'provides': 1, 'nonsymmetry': 1, 'up': 1, 'insufficient': 1, 'formal': 1, 'proof': 1, 'ref': 1, 'elegant': 1, 'another': 1, 'figure': 1, '104': 1, 'far': 1, 'dealt': 1, 'feedforward': 1, 'equivalence': 1, 'recur': 1, 'rent': 1, 'obvious': 1, 'drawback': 1, 'errorsurface': 1, 'contain': 1, 'guaranteed': 1, 'experience': 1, 'rarely': 1, 'gets': 1, 'stuck': 1, 'worse': 1, 'encountered': 1, 'undesirable': 1, 'few': 1, 'creates': 1, 'provide': 1, 'paths': 1, 'around': 1, 'barriers': 1, 'dimensional': 1, 'subspaces': 1, '_s': 1, '36': 1, '__________________': 1, 'lettersto': 1, 'n_atu_r_e_v_o_l_3_23_9_0c_t_o_b_er_19_86': 1, 'model': 1, 'brains': 1, 'various': 1, 'constructed': 1, 'suggests': 1, 'worth': 1, 'looking': 1, 'biologically': 1, 'ways': 1, 'doing': 1, 'thank': 1, 'system': 1, 'foundation': 1, 'office': 1, 'naval': 1, 'financial': 1, '31': 1, 'rosenblatt': 1, 'principles': 1, 'neurodynamics': 1, 'spartan': 1, 'washington': 1, 'dc': 1, '1961': 1, 'minsky': 1, 'papert': 1, '1969': 1, 'cun': 1, 'cognitiva': 1, '85': 1, '599604': 1, 'hinton': 1, 'processing': 1, 'explorations': 1, 'microstructure': 1, 'cognition': 1, 'vol': 1, 'foundations': 1, 'eds': 1, 'rumclhart': 1, 'mcclelland': 1, '318362': 1, 'kathryn': 1, 'murphy': 1, 'donald': 1, 'mitchell': 1, 'psychology': 1, 'dalhousie': 1, 'halifax': 1, 'nova': 1, 'scotia': 1, 'canada': 1, 'b3h': 1, '4jl': 1, 'neurones': 1, 'cortex': 1, 'adult': 1, 'excited': 1, 'stimulation': 1, 'nevertheless': 1, 'patterned': 1, 'cells': 1, 'activated': 1, 'stimuli': 1, 'apparently': 1, 'useless': 1, 'consequences': 1, 'circum': 1, 'stances': 1, 'reversed': 1, 'implementation': 1, 'forces': 1, 'itself': 1, 'occular': 1, 'promotes': 1, 'recovery': 1, 'effort': 1, 'might': 1, 'good': 1, 'effects': 1, 'physiologi': 1, 'cally': 1, 'optimal': 1, '75': 1, 'attained': 1, 'approximately': 1, 'appeared': 1, 'demonstra': 1, 'consecutive': 1, 'periods': 1, 'nine': 1, 'eyelid': 1, 'suture': 1, 'natural': 1, 'opening': 1, 'sutured': 1, 'physiological': 1, 'least': 1, '48': 1, 'grating': 1, 'thresholds': 1, 'subjected': 1, 'regimemonocular': 1, 'suturing': 1, 'jumping': 1, 'stand': 1, 'none': 1, 'tested': 1, 'physiologically': 1, 'anaesthetized': 1, 'paralysed': 1, 'experimental': 1, 'glass': 1, 'coated': 1, 'platinumiridium': 1, 'electrodes': 1, 'anaesthesia': 1, 'induced': 1, 'present': 1, 'address': 1, 'school': 1, 'optometry': 1, 'univen1ity': 1, 'berkeley': 1, '94720': 1, 'ooc9': 1, 'c164': 1, '________': 1, 'termination': 1, 'cl64': 1, 'previously': 1, 'occluded': 1, 'pentothal': 1, 'maintained': 1, 'respiration': 1, 'supplemented': 1, 'nembutal': 1, 'eeg': 1, 'ekg': 1, 'body': 1, 'temperature': 1, 'expired': 1, 'monitoredthe': 1, 'brought': 1, 'focus': 1, 'tangent': 1, 'screen': 1, '137': 1, 'cm': 1, 'distant': 1, 'kitten': 1, 'contact': 1, 'lenses': 1, 'mm': 1, 'pupils': 1, 'recorded': 1, 'long': 1, 'down': 1, 'medial': 1, 'bank': 1, 'postlateral': 1, 'gyrus': 1, 'contralateral': 1, 'open': 1, 'sampled': 1, 'according': 1, 'established': 1, 'procedures': 1, 'µm': 1, 'region': 1, 'horizontal': 1, 'meridian': 1, 'located': 1, 'centralis': 1, 'longitudinal': 1, 'follow': 1, 'ing': 1, 'representative': 1, 'end': 1, 'recovered': 1, 'rudimentary': 1, '125': 1, 'while': 1, 'rendered': 1, 'blind': 1, 'subsequent': 1, 'slightly': 1, 'limited': 1, 'extent': 1, 'results': 1, 'animal': 1, 'respectively': 1, '254': 1, '335': 1, 'animals': 1, 'remained': 1, 'onethird': 1, 'onehalf': 1, 'peak': 1, 'sensitive': 1, 'brief': 1, 'depriva': 1, 'regimen': 1, 'devastiating': 1, 'permanent': 1, 'upon': 1})}, {'file_name': 'cnn', 'word_counts': Counter({'the': 357, 'of': 213, 'and': 127, 'to': 110, 'in': 100, 'a': 83, 'we': 75, 'on': 66, 'is': 61, 'with': 60, 'are': 50, 'for': 47, 'images': 46, 'by': 46, 'this': 44, 'our': 44, 'convolutional': 42, 'training': 39, 'that': 38, 'layer': 36, 'which': 35, 'layers': 31, 'as': 31, 'network': 30, 'networks': 27, 'error': 26, 'image': 25, 'from': 25, 'test': 24, '2': 24, 'neural': 21, 'learning': 21, 'it': 21, 'kernels': 21, 'an': 20, 'not': 20, 'neurons': 19, 'be': 19, 'results': 18, '3': 18, 'rate': 17, '1': 17, 'two': 17, 'set': 17, 'imagenet': 16, 'each': 16, 'top5': 15, 'models': 15, 'size': 15, 'gpu': 14, 'dataset': 14, 'figure': 14, 'net': 14, 'at': 14, 'rates': 13, 'has': 13, 'but': 13, 'so': 13, 'have': 13, 'all': 13, 'gpus': 13, 'input': 13, 'ﬁrst': 13, 'pooling': 13, 'trained': 12, 'ﬁve': 12, 'overﬁtting': 12, 'model': 12, 'recognition': 12, '4': 12, 'cnns': 12, 'architecture': 12, 'large': 11, 'top1': 11, 'use': 11, 'their': 11, 'only': 11, 'over': 11, 'scheme': 11, 'million': 10, 'than': 10, 'fullyconnected': 10, 'object': 10, 'datasets': 10, '2010': 10, 'visual': 10, 'normalization': 10, 'deep': 9, '12': 9, 'data': 9, 'achieved': 9, 'train': 9, 'performance': 9, 'can': 9, 'if': 9, 'they': 9, 'best': 9, 'one': 9, 'without': 9, 'section': 9, 'using': 9, 'labels': 9, 'neuron': 9, 'kernel': 9, 'cnn': 9, '5': 9, 'predictions': 9, 'make': 8, 'used': 8, 'very': 8, 'also': 8, 'ilsvrc2012': 8, 'learn': 8, 'more': 8, 'labeled': 8, 'most': 8, 'do': 8, 'l': 8, 'j': 8, 'computer': 8, 'parameters': 7, 'consists': 7, 'dropout': 7, 'were': 7, 'well': 7, 'example': 7, 'examples': 7, 'other': 7, 'number': 7, 'features': 7, 'time': 7, 'any': 7, 'since': 7, 'report': 7, 'learned': 7, 'relus': 7, 'different': 7, 'k': 7, 'connected': 7, 'second': 7, '2011': 7, 'vision': 7, 'conference': 7, 'ieee': 7, 'faster': 6, 'current': 6, '6': 6, 'categories': 6, 'such': 6, '11': 6, 'pixel': 6, 'while': 6, 'local': 6, 'these': 6, 'described': 6, 'amount': 6, 'six': 6, 'ilsvrc': 6, '256': 6, 'was': 6, 'or': 6, 'nonlinearity': 6, 'output': 6, 'maps': 6, 'same': 6, 'similar': 6, 'see': 6, 'reduces': 6, 'many': 6, 'last': 6, 'patches': 6, 'w': 6, '7': 6, '2009': 6, 'international': 6, 'pages': 6, 'krizhevsky': 5, 'hinton': 5, '1000': 5, 'previous': 5, 'some': 5, 'three': 5, 'competition': 5, 'machine': 5, 'much': 5, 'et': 5, 'al': 5, 'about': 5, 'however': 5, 'correct': 5, 'ilsvrc2010': 5, 'available': 5, 'contains': 5, 'important': 5, 'roughly': 5, 'validation': 5, 'version': 5, 'where': 5, 'label': 5, 'therefore': 5, 'way': 5, 'rgb': 5, 'relu': 5, 'times': 5, 'units': 5, 'half': 5, 'xy': 5, 'n': 5, 's': 5, 'ﬁfth': 5, 'augmentation': 5, '224': 5, 'random': 5, 'weight': 5, 'table': 5, 'approach': 5, 'feature': 5, 'y': 5, 'classiﬁcation': 4, 'university': 4, 'toronto': 4, 'highresolution': 4, 'ﬁnal': 4, 'ing': 4, 'reduce': 4, 'employed': 4, 'compared': 4, 'larger': 4, 'preventing': 4, 'small': 4, 'eg': 4, '9': 4, 'them': 4, 'been': 4, '15': 4, 'cannot': 4, 'even': 4, 'con': 4, 'standard': 4, 'enough': 4, 'its': 4, 'made': 4, 'several': 4, 'found': 4, 'memory': 4, 'takes': 4, 'between': 4, 'there': 4, 'then': 4, 'out': 4, 'mean': 4, 'activity': 4, 'values': 4, 'pixels': 4, 'eight': 4, 'describe': 4, '25': 4, 'cifar10': 4, 'saturating': 4, 'f': 4, 'shows': 4, 'particular': 4, 'would': 4, 'respectively': 4, 'follow': 4, 'produce': 4, 'computed': 4, 'applying': 4, 'outputs': 4, 'during': 4, 'remaining': 4, 'fourth': 4, '10': 4, 'averaging': 4, 'hidden': 4, 'wi': 4, 'initialized': 4, 'vectors': 4, 'fvs': 4, 'fall': 4, 'feifei': 4, 'd': 4, '2012': 4, 'r': 4, '2004': 4, 'classify': 3, 'classes': 3, '375': 3, 'followed': 3, 'softmax': 3, 'efﬁcient': 3, 'convolution': 3, 'method': 3, 'called': 3, 'entered': 3, '153': 3, '262': 3, 'until': 3, 'thousands': 3, '16': 3, '8': 3, 'human': 3, 'objects': 3, 'pinto': 3, 'new': 3, '13': 3, 'depth': 3, 'slightly': 3, 'still': 3, 'scale': 3, 'implementation': 3, 'far': 3, 'no': 3, 'days': 3, 'gtx': 3, '580': 3, '3gb': 3, 'experiments': 3, 'challenge': 3, 'largescale': 3, 'considered': 3, 'constant': 3, 'given': 3, 'did': 3, 'summarized': 3, 'fourlayer': 3, 'equivalent': 3, 'tanh': 3, 'work': 3, 'kind': 3, 'effect': 3, 'here': 3, 'fx': 3, 'following': 3, 'works': 3, 're': 3, 'consider': 3, 'jarrett': 3, 'average': 3, 'us': 3, 'big': 3, 'certain': 3, 'take': 3, 'pattern': 3, 'computation': 3, 'cire': 3, 'san': 3, '17': 3, 'twogpu': 3, 'onegpu': 3, 'nets': 3, 'comparison': 3, 'response': 3, 'runs': 3, 'form': 3, 'applied': 3, '35': 3, '14': 3, 'overlapping': 3, 'z': 3, 'obtain': 3, 'what': 3, 'throughout': 3, 'weights': 3, 'distribution': 3, 'third': 3, 'every': 3, 'distance': 3, 'top': 3, '48': 3, 'transformed': 3, 'produced': 3, 'batch': 3, 'horizontal': 3, 'ten': 3, 'variable': 3, 'details': 3, 'decay': 3, 'di': 3, 'averages': 3, 'types': 3, '24': 3, 'gives': 3, '164': 3, 'entire': 3, 'notice': 3, 'euclidean': 3, '2007': 3, 'deng': 3, 'arxiv': 3, 'preprint': 3, 'g': 3, 'lecun': 3, 'proceedings': 3, 'sutskever': 2, 'e': 2, '170': 2, 'better': 2, '60': 2, 'maxpooling': 2, '1000way': 2, 'nonsaturating': 2, 'regularization': 2, 'effective': 2, 'secondbest': 2, 'entry': 2, 'approaches': 2, 'methods': 2, 'powerful': 2, 'techniques': 2, 'recently': 2, 'order': 2, 'especially': 2, 'labelpreserving': 2, 'transformations': 2, 'task': 2, '03': 2, 'considerable': 2, 'sets': 2, 'recognized': 2, '21': 2, 'become': 2, 'possible': 2, 'col': 2, 'millions': 2, 'labelme': 2, '23': 2, '22000': 2, 'need': 2, 'capacity': 2, 'means': 2, 'speciﬁed': 2, 'should': 2, 'prior': 2, 'knowledge': 2, 'class': 2, '18': 2, '22': 2, '26': 2, 'namely': 2, 'thus': 2, 'despite': 2, 'expensive': 2, 'highlyoptimized': 2, '2d': 2, 'paper': 2, 'subsets': 2, 'publicly': 2, 'unusual': 2, 'improve': 2, 'problem': 2, 'removing': 2, 'improved': 2, 'bigger': 2, 'tool': 2, 'annual': 2, 'testing': 2, 'fraction': 2, 'probable': 2, 'system': 2, 'patch': 2, 'resulting': 2, 'except': 2, 'raw': 2, 'below': 2, 'according': 2, 'line': 2, 'magnitude': 2, 'demonstrated': 2, 'equivalents': 2, 'x': 2, 'gradient': 2, 'descent': 2, 'nair': 2, '20': 2, 'rectiﬁed': 2, 'linear': 2, 'iterations': 2, 'able': 2, 'traditional': 2, 'tional': 2, 'particularly': 2, 'type': 2, 'contrast': 2, 'nor': 2, 'ﬁt': 2, 'when': 2, 'multiple': 2, 'single': 2, 'turns': 2, 'too': 2, 'across': 2, 'parallelization': 2, 'through': 2, 'employ': 2, 'additional': 2, 'communicate': 2, 'those': 2, 'reside': 2, 'precisely': 2, 'columns': 2, 'independent': 2, 'less': 2, 'because': 2, 'approximately': 2, 'property': 2, 'positive': 2, 'will': 2, 'ﬁnd': 2, 'ai': 2, 'position': 2, 'responsenormalized': 2, 'bi': 2, 'α': 2, 'β': 2, 'course': 2, 'determined': 2, 'inspired': 2, '34': 2, 'neighboring': 2, 'map': 2, 'produces': 2, 'generally': 2, 'overall': 2, 'fully': 2, 'objective': 2, 'cases': 2, 'under': 2, 'prediction': 2, 'responsenormalization': 2, 'both': 2, 'ﬁlters': 2, 'the2242243': 2, '96': 2, 'code': 2, 'layerparts': 2, 'bottom': 2, 'pooled': 2, 'another': 2, '384': 2, '192': 2, '41': 2, 'forms': 2, 'extracting': 2, 'reﬂections': 2, 'factor': 2, 'though': 2, 'highly': 2, 'substantial': 2, 'forced': 2, 'hence': 2, 'add': 2, 'why': 2, 'corresponding': 2, 'drawn': 2, 'gaussian': 2, 'zero': 2, '01': 2, 'ith': 2, 'αi': 2, 'aforementioned': 2, 'technique': 2, 'probability': 2, '05': 2, 'reasonable': 2, '61': 2, 'momentum': 2, '00005': 2, 'vi1': 2, '0': 2, '001': 2, 'biases': 2, 'initialization': 2, 'improving': 2, 'achieves': 2, '471': 2, '282': 2, '457': 2, '257': 2, 'denselysampled': 2, 'sift': 2, 'italics': 2, 'others': 2, '182': 2, 'sixth': 2, 'volutional': 2, 'release': 2, '166': 2, 'pretrained': 2, 'val': 2, '390': 2, 'does': 2, '19': 2, 'variety': 2, 'specialization': 2, 'restricted': 2, 'largely': 2, 'left': 2, 'written': 2, 'column': 2, 'show': 2, 'computing': 2, 'appear': 2, '4096dimensional': 2, 'retrieved': 2, 'retrieval': 2, 'achieving': 2, 'computational': 2, 'increase': 2, 'information': 2, 'berg': 2, 'u': 2, 'meier': 2, 'schmidhuber': 2, 'li': 2, 'hierarchical': 2, 'database': 2, 'h': 2, 'url': 2, 'p': 2, 'perona': 2, 'ge': 2, 'kavukcuoglu': 2, 'm': 2, 'belief': 2, 'systems': 2, 'cvpr': 2, 'volume': 2, 'perronnin': 2, 'v': 2, 'dd': 2, 'cox': 2, 'jj': 2, 'dicarlo': 2, 'plos': 2, 'biology': 2, '2008': 2, 'document': 2, 'analysis': 2, 'alex': 1, 'krizcsutorontoca': 1, 'ilya': 1, 'ilyacsutorontoca': 1, 'geoffrey': 1, 'hintoncsutorontoca': 1, 'abstract': 1, 'lsvrc2010': 1, 'contest': 1, 'into': 1, 'dif': 1, 'ferent': 1, 'considerably': 1, 'stateoftheart': 1, '650000': 1, 'implemen': 1, 'tation': 1, 'operation': 1, 'recentlydeveloped': 1, 'proved': 1, 'variant': 1, 'winning': 1, 'introduction': 1, 'essential': 1, 'im': 1, 'prove': 1, 'collect': 1, 'bet': 1, 'ter': 1, 'relatively': 1, 'tens': 1, 'norb': 1, 'caltech101256': 1, 'cifar10100': 1, 'simple': 1, 'tasks': 1, 'solved': 1, 'quite': 1, 'augmented': 1, 'mnist': 1, 'digitrecognition': 1, 'realistic': 1, 'settings': 1, 'exhibit': 1, 'variability': 1, 'recognize': 1, 'necessary': 1, 'indeed': 1, 'shortcomings': 1, 'widely': 1, 'lect': 1, 'include': 1, 'hundreds': 1, 'fullysegmented': 1, 'immense': 1, 'complexity': 1, 'prob': 1, 'lem': 1, 'lots': 1, 'compensate': 1, 'dont': 1, 'constitute': 1, 'trolled': 1, 'varying': 1, 'breadth': 1, 'strong': 1, 'mostly': 1, 'assumptions': 1, 'nature': 1, 'stationarity': 1, 'statistics': 1, 'locality': 1, 'dependencies': 1, 'feedforward': 1, 'similarlysized': 1, 'fewer': 1, 'connections': 1, 'easier': 1, 'theoreticallybest': 1, 'likely': 1, 'worse': 1, 'attractive': 1, 'qualities': 1, 'relative': 1, 'efﬁciency': 1, 'prohibitively': 1, 'apply': 1, 'luck': 1, 'ily': 1, 'paired': 1, 'facilitate': 1, 'interestinglylarge': 1, 'recent': 1, 'contain': 1, 'severe': 1, 'speciﬁc': 1, 'contributions': 1, 'follows': 1, 'largest': 1, 'date': 1, 'competitions': 1, 'ever': 1, 'reported': 1, 'wrote': 1, 'operations': 1, 'inherent': 1, 'detailed': 1, 'signiﬁcant': 1, 'seems': 1, 'resulted': 1, 'inferior': 1, 'end': 1, 'limited': 1, 'mainly': 1, 'willing': 1, 'tolerate': 1, 'suggest': 1, 'simply': 1, 'waiting': 1, 'belonging': 1, 'collected': 1, 'web': 1, 'labelers': 1, 'ama': 1, 'zons': 1, 'mechanical': 1, 'turk': 1, 'crowdsourcing': 1, 'starting': 1, 'part': 1, 'pascal': 1, 'held': 1, 'uses': 1, 'subset': 1, '50000': 1, '150000': 1, 'performed': 1, 'unavailable': 1, 'customary': 1, 'among': 1, 'variableresolution': 1, 'requires': 1, 'dimen': 1, 'sionality': 1, 'downsampled': 1, 'ﬁxed': 1, 'resolution': 1, 'rectangular': 1, 'rescaled': 1, 'shorter': 1, 'side': 1, 'length': 1, 'cropped': 1, 'central256256': 1, 'preprocess': 1, 'subtracting': 1, 'centered': 1, 'novel': 1, 'sections': 1, '3134': 1, 'sorted': 1, 'estimation': 1, 'importance': 1, '1httpcodegooglecompcudaconvnet': 1, '31': 1, 'solid': 1, 'reaches': 1, 'dashed': 1, 'chosen': 1, 'independently': 1, 'fast': 1, 'varies': 1, 'consis': 1, 'tently': 1, 'function': 1, 'ex1': 1, 'terms': 1, 'nonlinearities': 1, 'slower': 1, 'max0x': 1, 'refer': 1, 'quired': 1, 'reach': 1, 'plot': 1, 'experiment': 1, 'had': 1, 'alternatives': 1, 'tradi': 1, 'claim': 1, 'nonlinearityfx': 1, 'tanhx': 1, 'malization': 1, 'caltech101': 1, 'pri': 1, 'mary': 1, 'concern': 1, 'observing': 1, 'accelerated': 1, 'ability': 1, 'great': 1, 'inﬂuence': 1, '32': 1, 'limits': 1, 'maximum': 1, 'spread': 1, 'wellsuited': 1, 'crossgpu': 1, 'read': 1, 'write': 1, 'anothers': 1, 'directly': 1, 'going': 1, 'host': 1, 'essentially': 1, 'puts': 1, 'trick': 1, 'choosing': 1, 'connectivity': 1, 'crossvalidation': 1, 'allows': 1, 'tune': 1, 'communication': 1, 'acceptable': 1, 'resultant': 1, 'somewhat': 1, 'columnar': 1, 'net2': 1, '2the': 1, 'actually': 1, 'halve': 1, 'fullyconneced': 1, 'biased': 1, 'favor': 1, '33': 1, 'desirable': 1, 'require': 1, 'prevent': 1, 'least': 1, 'happen': 1, 'aids': 1, 'generalization': 1, 'denoting': 1, 'iat': 1, 'expression': 1, 'minn1in2': 1, 'jmax0in2': 1, 'aj': 1, 'xy2': 1, 'sum': 1, 'nadjacent': 1, 'spatial': 1, 'total': 1, 'ordering': 1, 'arbitrary': 1, 'before': 1, 'begins': 1, 'sort': 1, 'implements': 1, 'lateral': 1, 'inhibition': 1, 'real': 1, 'creating': 1, 'activities': 1, 'amongst': 1, 'constants': 1, 'knα': 1, 'βare': 1, 'hyperparameters': 1, 'whose': 1, '104': 1, '075': 1, 'after': 1, 'bears': 1, 'resemblance': 1, 'ours': 1, 'correctly': 1, 'termed': 1, 'brightness': 1, 'subtract': 1, 'veriﬁed': 1, 'effectiveness': 1, 'normalization3': 1, 'summarize': 1, 'groups': 1, 'traditionally': 1, 'neighborhoods': 1, 'adjacent': 1, 'overlap': 1, 'precise': 1, 'thought': 1, 'consisting': 1, 'grid': 1, 'spaced': 1, 'spixels': 1, 'apart': 1, 'summarizing': 1, 'neighborhood': 1, 'zzcentered': 1, 'location': 1, 'unit': 1, 'commonly': 1, '2and': 1, '04': 1, 'nonoverlapping': 1, '2z': 1, 'dimensions': 1, 'observe': 1, 'difﬁcult': 1, 'overﬁt': 1, 'now': 1, 'ready': 1, 'depicted': 1, 'fed': 1, 'maximizes': 1, 'multinomial': 1, 'logistic': 1, 'regression': 1, 'maximizing': 1, 'logprobability': 1, 'size11113': 1, 'stride': 1, 'receptive': 1, 'ﬁeld': 1, 'centers': 1, '3we': 1, 'detail': 1, 'due': 1, 'space': 1, 'constraints': 1, 'parameter': 1, 'ﬁles': 1, 'provided': 1, 'httpcodegooglecompcudaconvnet': 1, 'illustration': 1, 'explicitly': 1, 'showing': 1, 'delineation': 1, 'responsibilities': 1, 'ﬁgure': 1, '150528dimensional': 1, '253440186624648966489643264': 1, '409640961000': 1, 'intervening': 1, 'normalized': 1, '4096': 1, 'reducing': 1, 'although': 1, 'impose': 1, 'bits': 1, 'constraint': 1, 'mapping': 1, 'insufﬁcient': 1, 'primary': 1, 'ways': 1, 'combat': 1, 'easiest': 1, 'common': 1, 'artiﬁcially': 1, 'enlarge': 1, 'distinct': 1, 'allow': 1, 'original': 1, 'little': 1, 'stored': 1, 'disk': 1, 'generated': 1, 'python': 1, 'cpu': 1, 'schemes': 1, 'computationally': 1, 'free': 1, 'generating': 1, 'translations': 1, 'reﬂec': 1, 'tions': 1, 'random224': 1, '256256': 1, 'extracted': 1, 'patches4': 1, 'increases': 1, '2048': 1, 'inter': 1, 'dependent': 1, 'suffers': 1, 'smaller': 1, 'makes': 1, 'four': 1, 'corner': 1, 'center': 1, 'altering': 1, 'intensities': 1, 'channels': 1, 'speciﬁcally': 1, 'perform': 1, 'pca': 1, 'multiples': 1, 'principal': 1, 'components': 1, '4this': 1, 'reason': 1, '3dimensional': 1, 'magnitudes': 1, 'proportional': 1, 'eigenvalues': 1, 'deviation': 1, 'ixy': 1, 'ir': 1, 'xyig': 1, 'xyib': 1, 'xyt': 1, 'quantity': 1, 'p1p2p3α1λ1α2λ2α3λ3t': 1, 'pi': 1, 'λi': 1, 'eigenvector': 1, 'eigenvalue': 1, 'covariance': 1, 'matrix': 1, 'once': 1, 'again': 1, 'point': 1, 'redrawn': 1, 'captures': 1, 'natural': 1, 'identity': 1, 'invariant': 1, 'changes': 1, 'intensity': 1, 'color': 1, 'illumination': 1, '42': 1, 'combining': 1, 'successful': 1, 'errors': 1, 'appears': 1, 'already': 1, 'combination': 1, 'costs': 1, 'recentlyintroduced': 1, 'setting': 1, 'dropped': 1, 'contribute': 1, 'forward': 1, 'pass': 1, 'participate': 1, 'back': 1, 'propagation': 1, 'presented': 1, 'samples': 1, 'architectures': 1, 'share': 1, 'complex': 1, 'coadaptations': 1, 'rely': 1, 'presence': 1, 'robust': 1, 'useful': 1, 'conjunction': 1, 'multiply': 1, 'approximation': 1, 'taking': 1, 'geometric': 1, 'predictive': 1, 'distributions': 1, 'exponentiallymany': 1, 'ex': 1, 'hibits': 1, 'doubles': 1, 'required': 1, 'converge': 1, '11113': 1, 'stochastic': 1, '128': 1, '09': 1, 'words': 1, 'merely': 1, 'regularizer': 1, 'update': 1, 'rule': 1, 'wwas': 1, 'vi': 1, 'ϵwi': 1, 'ϵ': 1, 'wi1': 1, 'iis': 1, 'iteration': 1, 'index': 1, 'vis': 1, 'ϵis': 1, 'derivative': 1, 'respect': 1, 'evaluated': 1, 'zeromean': 1, 'de': 1, 'viation': 1, 'accelerates': 1, 'early': 1, 'stages': 1, 'providing': 1, 'inputs': 1, 'equal': 1, 'adjusted': 1, 'manually': 1, 'heuristic': 1, 'divide': 1, 'stopped': 1, 'reduced': 1, 'termination': 1, '90': 1, 'cycles': 1, 'took': 1, 'nvidia': 1, '1705': 1, 'sparsecoding': 1, 'pub': 1, 'lished': 1, 'classi': 1, 'ﬁers': 1, 'fisher': 1, 'sparse': 1, 'coding': 1, 'com': 1, 'petition': 1, 'tried': 1, 'remainder': 1, 'paragraph': 1, 'interchangeably': 1, 'experience': 1, 'differ': 1, 'extra': 1, '15m': 1, '22k': 1, 'ﬁnetuning': 1, 'lease': 1, 'sev': 1, 'eral': 1, 'classiﬁers': 1, '407': 1, '381': 1, '367': 1, '154': 1, 'asterisk': 1, 'finally': 1, '10184': 1, '89': 1, 'convention': 1, 'literature': 1, 'es': 1, 'tablished': 1, 'split': 1, 'neces': 1, 'sarily': 1, 'differs': 1, 'splits': 1, 'authors': 1, 'affect': 1, 'appreciably': 1, '674': 1, '409': 1, 'attained': 1, 'above': 1, 'published': 1, '781': 1, '609': 1, 'qualitative': 1, 'evaluations': 1, 'dataconnected': 1, 'frequency': 1, 'orientationselective': 1, 'various': 1, 'ored': 1, 'blobs': 1, 'exhibited': 1, 'result': 1, 'connec': 1, 'tivity': 1, 'coloragnostic': 1, 'colorspeciﬁc': 1, 'occurs': 1, 'run': 1, 'modulo': 1, 'renumbering': 1, '5the': 1, '183': 1, 'assigned': 1, 'shown': 1, 'red': 1, 'bar': 1, 'happens': 1, '5right': 1, 'five': 1, 'smallest': 1, 'vector': 1, 'panel': 1, 'qualitatively': 1, 'assess': 1, 'offcenter': 1, 'mite': 1, 'topleft': 1, 'cat': 1, 'plausible': 1, 'leopard': 1, 'grille': 1, 'cherry': 1, 'genuine': 1, 'ambiguity': 1, 'intended': 1, 'focus': 1, 'photograph': 1, 'probe': 1, 'activations': 1, 'induced': 1, 'activation': 1, 'separation': 1, 'say': 1, 'higher': 1, 'levels': 1, 'measure': 1, 'level': 1, 'close': 1, 'l2': 1, 'query': 1, 'dogs': 1, 'elephants': 1, 'poses': 1, 'present': 1, 'supplementary': 1, 'material': 1, 'similarity': 1, 'realvalued': 1, 'vec': 1, 'tors': 1, 'inefﬁcient': 1, 'could': 1, 'autoencoder': 1, 'compress': 1, 'short': 1, 'binary': 1, 'codes': 1, 'auto': 1, 'encoders': 1, 'tendency': 1, 'retrieve': 1, 'patterns': 1, 'edges': 1, 'whether': 1, 'semantically': 1, 'discussion': 1, 'capable': 1, 'record': 1, 'breaking': 1, 'challenging': 1, 'purely': 1, 'supervised': 1, 'notable': 1, 'degrades': 1, 'removed': 1, 'middle': 1, 'loss': 1, 'really': 1, 'simplify': 1, 'unsupervised': 1, 'pretraining': 1, 'expect': 1, 'help': 1, 'power': 1, 'signiﬁcantly': 1, 'obtaining': 1, 'longer': 1, 'orders': 1, 'go': 1, 'match': 1, 'inferotemporal': 1, 'pathway': 1, 'ultimately': 1, 'like': 1, 'video': 1, 'sequences': 1, 'temporal': 1, 'structure': 1, 'provides': 1, 'helpful': 1, 'missing': 1, 'obvious': 1, 'static': 1, 'references': 1, 'rm': 1, 'bell': 1, 'koren': 1, 'lessons': 1, 'netﬂix': 1, 'prize': 1, 'challengeacm': 1, 'sigkdd': 1, 'explorations': 1, 'newsletter': 1, '927579': 1, 'wwwimage': 1, 'netorgchallenges': 1, 'breiman': 1, 'forests': 1, '451532': 1, '2001': 1, 'multicolumn': 1, 'arxiv12022745': 1, 'dc': 1, 'masci': 1, 'lm': 1, 'gambardella': 1, 'highperformance': 1, 'arxiv11020183': 1, 'dong': 1, 'socher': 1, 'lj': 1, 'cvpr09': 1, 'satheesh': 1, 'su': 1, 'khosla': 1, 'httpwwwimagenetorgchallengeslsvrc2012': 1, 'fergus': 1, 'generative': 1, 'few': 1, 'incremental': 1, 'bayesian': 1, 'tested': 1, '101': 1, 'understand': 1, '10615970': 1, 'grifﬁn': 1, 'holub': 1, 'caltech256': 1, 'category': 1, 'technical': 1, '7694': 1, 'cali': 1, 'fornia': 1, 'institute': 1, 'technology': 1, 'httpauthorslibrarycaltechedu7694': 1, 'srivastava': 1, 'i': 1, 'rr': 1, 'salakhutdinov': 1, 'coadaptation': 1, 'detectors': 1, 'arxiv12070580': 1, 'ranzato': 1, 'multistage': 1, '21462153': 1, 'tiny': 1, 'masters': 1, 'thesis': 1, 'department': 1, 'science': 1, 'unpublished': 1, 'manuscript': 1, 'autoencoders': 1, 'contentbased': 1, 'esann': 1, 'le': 1, 'cun': 1, 'b': 1, 'boser': 1, 'js': 1, 'denker': 1, 'henderson': 1, 'howard': 1, 'hubbard': 1, 'ld': 1, 'jackel': 1, 'hand': 1, 'digit': 1, 'backpropagation': 1, 'advances': 1, 'processing': 1, '1990': 1, 'fj': 1, 'huang': 1, 'bottou': 1, 'generic': 1, 'invariance': 1, 'pose': 1, 'lighting': 1, 'society': 1, 'ii97': 1, 'c': 1, 'farabet': 1, 'applications': 1, 'circuits': 1, 'iscas': 1, 'symposium': 1, '253256': 1, 'lee': 1, 'grosse': 1, 'ranganath': 1, 'ay': 1, 'ng': 1, 'scalable': 1, 'unsuper': 1, 'vised': 1, 'representations': 1, 'inproceedings': 1, '26th': 1, '609616': 1, 'acm': 1, 't': 1, 'mensink': 1, 'verbeek': 1, 'csurka': 1, 'metric': 1, 'classiﬁ': 1, 'cation': 1, 'generalizing': 1, 'nearzero': 1, 'cost': 1, 'eccv': 1, 'european': 1, 'florence': 1, 'italy': 1, 'october': 1, 'boltzmann': 1, 'machines': 1, 'proc': 1, '27th': 1, 'realworld': 1, 'hard': 1, 'computa': 1, '41e27': 1, 'doukhan': 1, 'highthroughput': 1, 'screening': 1, 'discovering': 1, 'good': 1, 'biologically': 1, 'representation': 1, '511e1000579': 1, 'bc': 1, 'russell': 1, 'torralba': 1, 'kp': 1, 'murphy': 1, 'wt': 1, 'freeman': 1, 'webbased': 1, 'annotation': 1, 'journal': 1, '771157173': 1, 'sánchez': 1, 'highdimensional': 1, 'signature': 1, 'compression': 1, '16651672': 1, 'py': 1, 'simard': 1, 'steinkraus': 1, 'jc': 1, 'platt': 1, 'practices': 1, 'seventh': 1, '958962': 1, '2003': 1, 'sc': 1, 'turaga': 1, 'jf': 1, 'murray': 1, 'jain': 1, 'roth': 1, 'helmstaedter': 1, 'briggman': 1, 'denk': 1, 'hs': 1, 'seung': 1, 'generate': 1, 'afﬁnity': 1, 'graphs': 1, 'segmentation': 1, '222511538': 1})}, {'file_name': 'deep-learning', 'word_counts': Counter({'the': 506, 'of': 327, 'a': 270, 'in': 250, 'and': 191, 'to': 182, 'for': 120, 'is': 97, 'that': 93, 'learning': 88, 'neural': 88, 'with': 87, 'on': 70, 'networks': 70, 'are': 68, 'by': 66, 'deep': 63, 'as': 57, 'y': 55, 'proc': 48, 'or': 46, 'this': 45, 'j': 44, 'can': 41, 'from': 39, 'be': 39, 'it': 38, 'each': 38, 'an': 37, 'network': 36, 'machine': 35, 'at': 35, 'recognition': 35, 'which': 33, 'input': 33, 'word': 33, 'layer': 32, 'processing': 30, 'speech': 29, 'all': 29, 'such': 28, 'one': 28, 'have': 28, 'units': 28, 'image': 27, 'language': 27, 'convolutional': 27, 'e': 27, 'systems': 26, '2015': 26, 'was': 26, 'g': 26, 'very': 25, 'training': 25, '2014': 25, 'used': 24, 'data': 24, 'representations': 24, 'layers': 24, 'output': 24, 'conference': 24, 'feature': 23, 'many': 22, 'into': 22, 'features': 22, 'using': 22, 'c': 22, 'information': 22, 'images': 20, 'representation': 20, 'we': 20, 'r': 20, 'vector': 19, 'two': 19, 'hidden': 19, 'k': 19, 'time': 19, 'international': 19, 'natural': 18, 'but': 18, 'advances': 18, 'when': 18, 'd': 18, 'et': 18, 's': 18, 'm': 17, 'were': 17, 'more': 17, 'not': 17, 'large': 17, 'its': 17, 'learn': 17, 'l': 17, 'al': 17, 'use': 16, 'their': 16, 'first': 16, 'has': 16, 'other': 16, 'recurrent': 16, 'different': 16, 'vision': 16, 'bengio': 16, 'they': 15, 'pattern': 15, 'few': 15, 'unit': 15, 'hinton': 15, 'p': 15, 'may': 14, 'error': 14, 'weights': 14, 'backpropagation': 14, 'local': 14, 'sequence': 14, 'lecun': 14, 'new': 13, 'these': 13, 'called': 13, 'learned': 13, 'example': 13, 'small': 13, 'function': 13, 'gradient': 13, 'next': 13, 'words': 13, 't': 13, 'set': 12, 'detection': 12, 'simple': 12, 'tasks': 12, 'examples': 12, 'nature': 12, '28': 12, 'inputs': 12, 'fig': 12, 'unsupervised': 12, 'convnets': 12, 'v': 12, 'memory': 12, 'ieee': 12, 'limited': 11, 'object': 11, 'outputs': 11, 'similar': 11, 'i': 11, 'trained': 11, 'rnns': 11, 'computer': 10, 'net': 10, 'process': 10, 'system': 10, 'compute': 10, 'over': 10, '1': 10, 'multilayer': 10, 'b': 10, 'x': 10, 'z': 10, 'through': 10, 'respect': 10, 'architecture': 10, 'been': 10, 'way': 10, '2012': 10, 'distributed': 10, 'translation': 10, 'trans': 10, 'research': 9, 'text': 9, 'could': 9, 'nonlinear': 9, 'good': 9, 'so': 9, 'weight': 9, 'algorithm': 9, 'nets': 9, 'vol': 9, '521': 9, 'macmillan': 9, 'publishers': 9, 'rights': 9, 'reserved': 9, 'where': 9, 'vectors': 9, 'f': 9, 'relu': 9, 'pooling': 9, 'model': 9, 'convnet': 9, 'months': 9, '2013': 9, 'paper': 9, 'results': 8, 'applications': 8, 'form': 8, 'values': 8, 'multiple': 8, 'variations': 8, 'particular': 8, 'procedure': 8, 'major': 8, 'then': 8, 'if': 8, 'models': 8, 'visual': 8, 'linear': 8, 'same': 8, 'yl': 8, 'reviewinsight': 8, '5': 8, '27': 8, 'approach': 8, 'science': 7, 'objects': 7, 'methods': 7, 'complex': 7, 'would': 7, 'human': 7, 'understanding': 7, 'because': 7, 'architectures': 7, 'being': 7, 'only': 7, 'most': 7, 'labelled': 7, 'objective': 7, 'between': 7, 'seen': 7, 'composed': 7, 'change': 7, 'about': 7, 'derivative': 7, 'module': 7, 'gradients': 7, 'applied': 7, '2': 7, '20': 7, 'map': 7, 'much': 7, 'pretraining': 7, 'number': 7, '10': 7, 'sequences': 7, 'face': 7, 'n': 7, 'rnn': 7, 'sentence': 7, 'o': 7, 'machinelearning': 6, 'ability': 6, 'raw': 6, 'internal': 6, 'classification': 6, 'artificial': 6, 'out': 6, 'will': 6, 'shown': 6, 'there': 6, 'how': 6, 'previous': 6, 'space': 6, 'performance': 6, 'weighted': 6, 'sum': 6, 'also': 6, 'derivatives': 6, 'recent': 6, 'well': 6, 'yj': 6, 'last': 6, 'thought': 6, '2009': 6, '3': 6, '4': 6, 'symbol': 6, 'reasoning': 6, 'introduced': 6, 'state': 6, '35': 6, '26': 6, 'signal': 6, 'machines': 6, 'sutskever': 6, '2011': 6, 'h': 6, 'york': 5, 'pixel': 5, 'modules': 5, 'motifs': 5, 'parts': 5, 'intelligence': 5, '14': 5, 'analysis': 5, 'amount': 5, 'category': 5, 'during': 5, 'parameters': 5, 'train': 5, 'sets': 5, 'descent': 5, 'average': 5, 'test': 5, 'require': 5, 'figure': 5, 'fz': 5, 'target': 5, 'wjk': 5, 'zl': 5, 'zk': 5, 'yk': 5, 'long': 5, 'several': 5, 'than': 5, 'bottom': 5, 'past': 5, 'become': 5, 'community': 5, 'up': 5, 'right': 5, 'detectors': 5, 'almost': 5, '2006': 5, 'fast': 5, 'vocabulary': 5, 'better': 5, 'including': 5, 'elements': 5, 'cortex': 5, 'document': 5, 'predict': 5, 'semantic': 5, 'step': 5, 'english': 5, 'french': 5, '32': 5, 'mach': 5, 'w': 5, 'u': 5, 'http': 5, 'press': 5, 'de': 4, 'aspects': 4, 'works': 4, 'present': 4, 'products': 4, 'make': 4, 'techniques': 4, 'conventional': 4, 'decades': 4, 'engineering': 4, 'often': 4, 'classifier': 4, 'deeplearning': 4, 'tion': 4, 'composing': 4, 'level': 4, 'functions': 4, 'irrelevant': 4, 'array': 4, 'typically': 4, 'second': 4, 'combinations': 4, 'key': 4, 'years': 4, 'turned': 4, 'successes': 4, 'advantage': 4, 'progress': 4, 'supervised': 4, 'cars': 4, 'real': 4, 'typical': 4, 'structure': 4, 'audio': 4, 'whereas': 4, 'kind': 4, 'stochastic': 4, 'computing': 4, 'until': 4, 'quickly': 4, 'practical': 4, 'classifiers': 4, 'top': 4, 'position': 4, 'white': 4, 'wolf': 4, 'dog': 4, 'samoyed': 4, 'left': 4, 'get': 4, 'expz': 4, '11': 4, 'computed': 4, 'once': 4, 'zj': 4, 'wkl': 4, 'pose': 4, 'widely': 4, 'groups': 4, 'feedforward': 4, '1990s': 4, 'empirical': 4, 'saddle': 4, 'convolutions': 4, 'standard': 4, '36': 4, 'possible': 4, 'achieved': 4, 'come': 4, 'arrays': 4, '2d': 4, 'maps': 4, 'filter': 4, 'statistics': 4, 'although': 4, 'semantically': 4, 'hierarchies': 4, 'neurons': 4, 'reading': 4, 'jointly': 4, 'convnetbased': 4, 'success': 4, 'segmentation': 4, 'companies': 4, 'generate': 4, 'captions': 4, 'distribution': 4, 'another': 4, 'ref': 4, 'attention': 4, 'context': 4, 'symbols': 4, 'inference': 4, 'modelling': 4, 'decoder': 4, '29': 4, '9': 4, '25': 4, 'dependencies': 4, 'lstm': 4, 'gated': 4, 'acoustics': 4, 'turing': 4, 'anal': 4, 'intell': 4, 'httparxivorg': 4, 'mikolov': 4, 'mohamed': 4, '8': 4, 'bordes': 4, 'chopra': 4, 'cho': 4, 'bottou': 4, 'univ': 4, 'lang': 4, 'ai': 3, 'usa': 3, 'canada': 3, 'toronto': 3, 'technology': 3, 'content': 3, 'con': 3, 'transformed': 3, 'detect': 3, 'allows': 3, 'automatically': 3, 'needed': 3, 'obtained': 3, 'starting': 3, 'important': 3, 'edges': 3, 'positions': 3, 'best': 3, 'intricate': 3, 'predicting': 3, 'question': 3, 'future': 3, 'requires': 3, 'little': 3, 'easily': 3, 'take': 3, 'algorithms': 3, 'developed': 3, 'people': 3, 'produces': 3, 'scores': 3, 'categories': 3, 'before': 3, 'dis': 3, 'adjustable': 3, 'inputoutput': 3, 'hundreds': 3, 'millions': 3, 'indicates': 3, 'stateoftheart': 3, 'brought': 3, 'video': 3, 'review': 3, 'landscape': 3, 'taking': 3, 'those': 3, 'gives': 3, 'far': 3, 'optimization': 3, 'tech': 3, 'after': 3, 'generalization': 3, 'produce': 3, 'since': 3, 'regions': 3, 'red': 3, 'blue': 3, 'regular': 3, 'contain': 3, 'chain': 3, 'rule': 3, 'gets': 3, 'matrices': 3, 'pass': 3, 'total': 3, 'below': 3, 'rectified': 3, 'convert': 3, 'tl': 3, 'value': 3, 'answer': 3, 'wij': 3, 'h2': 3, 'kernel': 3, 'stack': 3, 'background': 3, 'days': 3, 'idea': 3, 'discovered': 3, 'application': 3, 'work': 3, 'learns': 3, 'faster': 3, 'largely': 3, 'communities': 3, 'minima': 3, 'no': 3, 'points': 3, 'show': 3, 'hence': 3, 'interest': 3, 'group': 3, 'researchers': 3, 'cifar': 3, 'whole': 3, 'handwritten': 3, 'detecting': 3, '40': 3, 'some': 3, 'three': 3, 'signals': 3, 'stages': 3, 'patches': 3, 'motif': 3, 'appear': 3, 'discrete': 3, 'convolution': 3, 'vary': 3, 'ing': 3, 'ones': 3, 'timedelay': 3, 'early': 3, 'sign': 3, 'now': 3, 'beyond': 3, 'demonstrated': 3, 'translate': 3, 'generating': 3, 'probability': 3, 'story': 3, 'ngrams': 3, 'related': 3, 'steps': 3, 'encoder': 3, 'like': 3, '37': 3, '34': 3, '30': 3, '12': 3, '13': 3, '55': 3, '22': 3, 'meaning': 3, '86': 3, 'longterm': 3, 'authors': 3, '88': 3, 'questions': 3, 'expect': 3, 'report': 3, 'farabet': 3, 'hierarchical': 3, 'scene': 3, 'joint': 3, 'automatic': 3, 'modeling': 3, 'ar': 3, 'dahl': 3, 'method': 3, 'ot': 3, 'st': 3, 'res': 3, '15': 3, 'weston': 3, 'showed': 3, '18': 3, '2007': 3, '19': 3, 'mit': 3, '2005': 3, 'sciences': 3, 'thesis': 3, 'sparse': 3, 'ranzato': 3, 'salakhutdinov': 3, 'kavukcuoglu': 3, 'largescale': 3, 'vincent': 3, '1991': 3, '1995': 3, '2010': 3, '2008': 3, 'mnih': 3, 'broadway': 2, '10003': 2, 'university': 2, 'operations': 2, 'montréal': 2, 'mountain': 2, 'view': 2, '6': 2, 'road': 2, 'society': 2, 'web': 2, 'filtering': 2, 'increasingly': 2, 'cameras': 2, 'smartphones': 2, 'news': 2, 'interests': 2, 'relevant': 2, 'search': 2, 'class': 2, 'domain': 2, 'expertise': 2, 'design': 2, 'ture': 2, 'extractor': 2, 'classify': 2, 'fed': 2, 'discover': 2, 'representationlearning': 2, 'levels': 2, 'higher': 2, 'composition': 2, 'discrimination': 2, 'locations': 2, 'regardless': 2, 'edge': 2, 'assemble': 2, 'correspond': 2, 'subsequent': 2, 'designed': 2, 'generalpurpose': 2, 'problems': 2, 'highdimensional': 2, 'domains': 2, 'potential': 2, 'drug': 2, 'particle': 2, 'brain': 2, 'effects': 2, 'surprisingly': 2, 'extremely': 2, 'various': 2, 'particularly': 2, 'hand': 2, 'available': 2, 'com': 2, 'currently': 2, 'want': 2, 'containing': 2, 'say': 2, 'desired': 2, 'score': 2, 'reduce': 2, 'numbers': 2, 'func': 2, 'what': 2, 'increase': 2, 'computational': 2, 'should': 2, 'sequential': 2, 'practice': 2, 'consists': 2, 'errors': 2, 'sensible': 2, 'answers': 2, 'handengineered': 2, 'computes': 2, 'components': 2, 'above': 2, 'known': 2, 'recog': 2, 'nition': 2, 'insensitive': 2, 'while': 2, 'sensitive': 2, 'minute': 2, 'samoyeds': 2, 'any': 2, 'shallow': 2, 'connected': 2, 'classes': 2, 'linearly': 2, 'separable': 2, 'middle': 2, 'reproduced': 2, 'permission': 2, 'δx': 2, 'δy': 2, 'yx': 2, 'creates': 2, 'δz': 2, 'equation': 2, 'product': 2, 'equations': 2, 'forward': 2, 'simplicity': 2, 'include': 2, 'commonly': 2, 'logistic': 2, 'cost': 2, 'ezk': 2, 'connection': 2, 'just': 2, 'sigmoid': 2, 'uni0394uni0394': 2, 'correct': 2, 'h1': 2, 'pixels': 2, 'invariant': 2, 'animal': 2, 'powerful': 2, 'generic': 2, 'do': 2, 'general': 2, 'extractors': 2, 'consider': 2, 'able': 2, 'both': 2, 'invariance': 2, 'depth': 2, 'replace': 2, 'trainable': 2, 'despite': 2, 'relatively': 2, 'done': 2, 'worked': 2, 'ent': 2, 'prediction': 2, 'external': 2, 'fixedsize': 2, 'prob': 2, 'result': 2, '0': 2, 'nonlinearities': 2, 'allowing': 2, 'without': 2, 'late': 2, 'forsaken': 2, 'computervision': 2, 'multistage': 2, 'poor': 2, 'configurations': 2, 'problem': 2, 'initial': 2, 'nearly': 2, 'theoretical': 2, 'serious': 2, 'issue': 2, 'instead': 2, 'surface': 2, 'curves': 2, 'green': 2, 'detected': 2, 'lowerlevel': 2, '16': 2, '57': 2, '04': 2, 'max': 2, 'them': 2, 'tive': 2, 'does': 2, 'institute': 2, 'advanced': 2, 'reconstruction': 2, 'final': 2, 'recognizing': 2, 'digits': 2, 'pedestrians': 2, 'especially': 2, 'made': 2, 'graphics': 2, 'gpus': 2, 'times': 2, 'short': 2, 'extracted': 2, 'represented': 2, 'recordbreaking': 2, 'already': 2, 'deployed': 2, 'phones': 2, 'prevent': 2, 'significantly': 2, 'setting': 2, 'had': 2, 'type': 2, 'full': 2, 'recently': 2, 'colour': 2, '1d': 2, 'connections': 2, 'structured': 2, 'series': 2, 'within': 2, 'bank': 2, 'nonlinearity': 2, 'share': 2, 'banks': 2, 'reason': 2, 'highly': 2, 'forming': 2, 'location': 2, 'part': 2, 'name': 2, 'role': 2, 'somewhat': 2, 'reducing': 2, 'shifts': 2, 'backpropagating': 2, 'exploit': 2, 'property': 2, 'phonemes': 2, 'sentences': 2, 'directly': 2, 'classic': 2, 'cells': 2, '43': 2, 'overall': 2, 'monkeys': 2, 'activations': 2, 'highlevel': 2, '160': 2, 'did': 2, 'endtoend': 2, 'going': 2, 'back': 2, '47': 2, 'probabilistic': 2, 'states': 2, 'character': 2, 'handwriting': 2, 'later': 2, 'faces': 2, 'autonomous': 2, 'robots': 2, 'selfdriving': 2, 'mobileye': 2, 'nvidia': 2, 'involve': 2, 'imagenet': 2, 'efficient': 2, 'relus': 2, 'dropout': 2, '62': 2, 'generation': 2, 'ago': 2, 'hardware': 2, 'chips': 2, 'developing': 2, 'enable': 2, 'shows': 2, 'advantages': 2, 'exponential': 2, '70': 2, 'extra': 2, 'cnn': 2, '102': 2, 'given': 2, 'generates': 2, 'sitting': 2, 'standing': 2, 'trees': 2, 'stop': 2, 'active': 2, 'relationships': 2, 'microrules': 2, '71': 2, 'logicinspired': 2, 'paradigms': 2, 'cognition': 2, 'paradigm': 2, 'either': 2, 'chosen': 2, 'rules': 2, 'big': 2, 'activity': 2, 'perform': 2, 'statistical': 2, 'based': 2, 'order': 2, 'end': 2, 'clear': 2, 'dynamic': 2, 'proved': 2, 'symbolic': 2, 'expressions': 2, 'visualizing': 2, 'tsne': 2, 'phrases': 2, 'encoderdecoder': 2, '33': 2, '31': 2, '45': 2, '42': 2, '38': 2, '24': 2, 'unfolded': 2, 'augment': 2, 'shortterm': 2, 'special': 2, 'remember': 2, 'decide': 2, 'effective': 2, 'characters': 2, 'proposals': 2, 'augmented': 2, 'read': 2, 'manipulation': 2, 'list': 2, 'world': 2, 'version': 2, 'lowresolution': 2, 'combine': 2, 'reinforcement': 2, 'strategies': 2, 'krizhevsky': 2, '10901098': 2, 'breakthrough': 2, 'couprie': 2, 'najman': 2, 'tompson': 2, 'jain': 2, 'bregler': 2, 'graphical': 2, 'estimation': 2, 'scale': 2, 'acoustic': 2, 'task': 2, 'online': 2, 'kaggle': 2, 'st1': 2, 'ot1': 2, 'computation': 2, 'delay': 2, 'xiong': 2, 'frey': 2, 'splicing': 2, 'code': 2, 'answering': 2, 'vinyals': 2, 'le': 2, '72': 2, 'classiﬁcation': 2, '21': 2, 'mechanisation': 2, 'processes': 2, '23': 2, 'phd': 2, '1985': 2, '85': 2, 'la': 2, 'des': 2, 'belief': 2, 'comp': 2, 'boltzmann': 2, 'larochelle': 2, 'autoencoders': 2, 'efﬁcient': 2, 'energybased': 2, 'sermanet': 2, 'ng': 2, 'digit': 2, 'gradientbased': 2, 'primate': 2, '1989': 2, '89': 2, 'simard': 2, 'platt': 2, '1994': 2, '1997': 2, 'schmidhuber': 2, 'comput': 2, 'robust': 2, '2004': 2, 'localization': 2, 'humanlevel': 2, 'longrange': 2, 'parsing': 2, 'parallel': 2, 'montufar': 2, 'pascanu': 2, 'hochreiter': 2, 'tell': 2, 'caption': 2, 'arxivorgabs150203044': 2, 'graves': 2, '94': 2, 'generative': 2, '1facebook': 1, '770': 1, '2new': 1, '715': 1, '3department': 1, 'université': 1, 'pavillon': 1, 'andréaisenstadt': 1, 'po': 1, 'box': 1, '6128': 1, 'centreville': 1, 'stn': 1, 'quebec': 1, 'h3c': 1, '3j7': 1, '4google': 1, '1600': 1, 'amphitheatre': 1, 'parkway': 1, 'california': 1, '94043': 1, '5department': 1, 'kings': 1, 'college': 1, 'ontario': 1, 'm5s': 1, '3g4': 1, 'achinelearning': 1, 'powers': 1, 'modern': 1, 'searches': 1, 'social': 1, 'recommendations': 1, 'ecommerce': 1, 'websites': 1, 'consumer': 1, 'identify': 1, 'transcribe': 1, 'match': 1, 'items': 1, 'posts': 1, 'users': 1, 'select': 1, 'structing': 1, 'patternrecognition': 1, 'required': 1, 'careful': 1, 'considerable': 1, 'fea': 1, 'suitable': 1, 'subsystem': 1, 'patterns': 1, 'representa': 1, 'transform': 1, 'slightly': 1, 'abstract': 1, 'enough': 1, 'transformations': 1, 'amplify': 1, 'suppress': 1, 'comes': 1, 'represent': 1, 'presence': 1, 'absence': 1, 'orientations': 1, 'detects': 1, 'spotting': 1, 'arrangements': 1, 'third': 1, 'larger': 1, 'familiar': 1, 'aspect': 1, 'engineers': 1, 'making': 1, 'solving': 1, 'resisted': 1, 'attempts': 1, 'commu': 1, 'nity': 1, 'discovering': 1, 'structures': 1, 'therefore': 1, 'applica': 1, 'ble': 1, 'business': 1, 'government': 1, 'addition': 1, 'beating': 1, 'records': 1, 'recognition14': 1, 'recognition57': 1, 'beaten': 1, 'activ': 1, 'ity': 1, 'molecules8': 1, 'analysing': 1, 'accelerator': 1, 'data910': 1, 'reconstructing': 1, 'circuits11': 1, 'mutations': 1, 'noncoding': 1, 'dna': 1, 'gene': 1, 'expression': 1, 'disease1213': 1, 'perhaps': 1, 'produced': 1, 'promising': 1, 'topic': 1, 'sentiment': 1, 'answering15': 1, 'lan': 1, 'guage': 1, 'translation1617': 1, 'think': 1, 'near': 1, 'increases': 1, 'putation': 1, 'acceler': 1, 'ate': 1, 'common': 1, 'super': 1, 'vised': 1, 'imagine': 1, 'build': 1, 'house': 1, 'car': 1, 'person': 1, 'pet': 1, 'collect': 1, 'houses': 1, 'pets': 1, 'highest': 1, 'unlikely': 1, 'happen': 1, 'measures': 1, 'tance': 1, 'modifies': 1, 'knobs': 1, 'define': 1, 'properly': 1, 'adjust': 1, 'putes': 1, 'decrease': 1, 'increased': 1, 'tiny': 1, 'adjusted': 1, 'opposite': 1, 'direc': 1, 'averaged': 1, 'abstraction': 1, 'dramatically': 1, 'improved': 1, 'rec': 1, 'ognition': 1, 'discovery': 1, 'genomics': 1, 'discovers': 1, 'indicate': 1, 'breakthroughs': 1, 'shone': 1, 'light': 1, 'ann': 1, 'lecun12': 1, 'oshua': 1, 'bengio3': 1, 'geoffrey': 1, 'hinton45': 1, '436': 1, 'doi101038nature14539': 1, 'hilly': 1, 'negative': 1, 'direction': 1, 'steepest': 1, 'closer': 1, 'minimum': 1, 'low': 1, 'practitioners': 1, 'sgd': 1, 'showing': 1, 'adjusting': 1, 'accordingly': 1, 'repeated': 1, 'stops': 1, 'decreasing': 1, 'noisy': 1, 'estimate': 1, 'usually': 1, 'finds': 1, 'compared': 1, 'elaborate': 1, 'niques18': 1, 'measured': 1, 'serves': 1, 'never': 1, 'current': 1, 'twoclass': 1, 'threshold': 1, 'classified': 1, 'belonging': 1, '1960s': 1, 'carve': 1, 'namely': 1, 'halfspaces': 1, 'sepa': 1, 'rated': 1, 'hyperplane19': 1, 'orientation': 1, 'illumination': 1, 'pitch': 1, 'accent': 1, 'difference': 1, 'breed': 1, 'wolflike': 1, 'poses': 1, 'environments': 1, 'backgrounds': 1, 'operating': 1, 'multi': 1, 'dots': 1, 'distort': 1, 'lines': 1, 'note': 1, 'grid': 1, 'panel': 1, 'illustrative': 1, 'tens': 1, 'thousands': 1, 'olah': 1, 'httpcolahgithubio': 1, 'tells': 1, 'us': 1, 'getting': 1, 'multiplied': 1, 'definition': 1, 'partial': 1, 'similarly': 1, 'substituting': 1, 'multiplication': 1, 'zx': 1, 'jacobian': 1, 'constituting': 1, 'backpropagate': 1, 'omitted': 1, 'bias': 1, 'terms': 1, 'max0z': 1, 'sigmoids': 1, 'hyberbolic': 1, 'tangent': 1, 'expzexpz': 1, 'backward': 1, 'multiplying': 1, 'differentiating': 1, '05yl': 1, 'tl2': 1, 'errorderivative': 1, 'dc': 1, 'zz': 1, 'uni0394uni0394z': 1, 'xx': 1, 'compare': 1, 'xi': 1, '437': 1, 'possibly': 1, 'distinguish': 1, 'latter': 1, 'putting': 1, 'former': 1, 'why': 1, 'solves': 1, 'selectivityinvariance': 1, 'dilemma': 1, 'selective': 1, 'methods20': 1, 'arising': 1, 'gaussian': 1, 'allow': 1, 'learner': 1, 'ize': 1, 'examples21': 1, 'option': 1, 'skill': 1, 'avoided': 1, 'mod': 1, 'ules': 1, 'subject': 1, 'mappings': 1, 'transforms': 1, 'selectivity': 1, 'implement': 1, 'tions': 1, 'simultaneously': 1, 'details': 1, 'distinguishing': 1, 'wolves': 1, 'lighting': 1, 'surrounding': 1, 'earliest': 1, 'recognition2223': 1, 'aim': 1, 'ers': 1, 'solution': 1, 'understood': 1, 'mid': 1, '1980s': 1, 'turns': 1, 'smooth': 1, 'independently': 1, '1970s': 1, '1980s2427': 1, 'nothing': 1, 'insight': 1, 'gradi': 1, 'working': 1, 'backwards': 1, 'repeatedly': 1, 'propagate': 1, 'straightforward': 1, 'go': 1, 'popular': 1, 'simply': 1, 'halfwave': 1, 'rectifier': 1, 'maxz': 1, 'smoother': 1, 'tanhz': 1, 'conventionally': 1, 'distorting': 1, 'ignored': 1, 'speechrecognition': 1, 'useful': 1, 'lit': 1, 'tle': 1, 'prior': 1, 'knowledge': 1, 'infeasible': 1, 'trapped': 1, 'rarely': 1, 'conditions': 1, 'always': 1, 'reaches': 1, 'solutions': 1, 'quality': 1, 'strongly': 1, 'suggest': 1, 'packed': 1, 'combinato': 1, 'rially': 1, 'zero': 1, 'dimensions': 1, 'down': 1, 'inside': 1, 'filters': 1, 'horizontally': 1, 'rgb': 1, 'rectangular': 1, 'corresponding': 1, 'flows': 1, 'acting': 1, 'oriented': 1, 'papillon': 1, 'pomeranian': 1, 'arctic': 1, 'fox': 1, 'eskimo': 1, '06': 1, 'siberian': 1, 'husky': 1, '438': 1, 'remainder2930': 1, 'seems': 1, 'downward': 1, 'curving': 1, 'directions': 1, 'objec': 1, 'matter': 1, 'stuck': 1, 'revived': 1, 'around': 1, 'refs': 1, '3134': 1, 'together': 1, 'cana': 1, 'dian': 1, 'intro': 1, 'duced': 1, 'procedures': 1, 'create': 1, 'requiring': 1, 'reconstruct': 1, 'activities': 1, 'progressively': 1, 'initialized': 1, 'added': 1, 'finetuned': 1, 'backpropaga': 1, 'tion3335': 1, 'remarkably': 1, 'advent': 1, 'convenient': 1, 'program37': 1, 'allowed': 1, 'temporal': 1, 'windows': 1, 'coef': 1, 'ficients': 1, 'sound': 1, 'wave': 1, 'probabilities': 1, 'fragments': 1, 'might': 1, 'frame': 1, 'centre': 1, 'window': 1, 'benchmark': 1, 'vocabu': 1, 'lary38': 1, 'give': 1, 'task39': 1, 'versions': 1, 'groups6': 1, 'android': 1, 'smaller': 1, 'helps': 1, 'overfitting': 1, 'leading': 1, 'exam': 1, 'ples': 1, 'transfer': 1, 'lots': 1, 'source': 1, 'rehabilitated': 1, 'stage': 1, 'however': 1, 'easier': 1, 'generalized': 1, 'connectivity': 1, 'adjacent': 1, '4142': 1, 'period': 1, 'favour': 1, 'adopted': 1, 'intensities': 1, 'channels': 1, 'modalities': 1, 'spectrograms': 1, '3d': 1, 'volumetric': 1, 'four': 1, 'ideas': 1, 'behind': 1, 'properties': 1, 'shared': 1, 'types': 1, 'convolu': 1, 'tional': 1, 'organized': 1, 'passed': 1, 'differ': 1, 'twofold': 1, 'correlated': 1, 'distinctive': 1, 'anywhere': 1, 'sharing': 1, 'mathemati': 1, 'cally': 1, 'operation': 1, 'performed': 1, 'junctions': 1, 'merge': 1, 'relative': 1, 'reliably': 1, 'coarsegraining': 1, 'posi': 1, 'maximum': 1, 'patch': 1, 'neighbouring': 1, 'shifted': 1, 'row': 1, 'column': 1, 'thereby': 1, 'dimension': 1, 'creating': 1, 'tortions': 1, 'pool': 1, 'stacked': 1, 'followed': 1, 'fullyconnected': 1, 'sig': 1, 'nals': 1, 'compositional': 1, 'higherlevel': 1, 'combi': 1, 'nations': 1, 'exist': 1, 'sounds': 1, 'syllables': 1, 'previ': 1, 'ous': 1, 'appearance': 1, 'inspired': 1, 'notions': 1, 'neuroscience': 1, 'reminiscent': 1, 'lgnv1v2v4it': 1, 'hierarchy': 1, 'ventral': 1, 'path': 1, 'way44': 1, 'pic': 1, 'explains': 1, 'half': 1, 'variance': 1, 'random': 1, 'infer': 1, 'otemporal': 1, 'cortex45': 1, 'roots': 1, 'neocognitron46': 1, 'supervisedlearning': 1, 'primitive': 1, 'words4748': 1, 'numerous': 1, 'neu': 1, 'ral': 1, 'reading42': 1, 'implemented': 1, 'constraints': 1, 'cheques': 1, 'united': 1, 'optical': 1, 'microsoft49': 1, 'experimented': 1, 'hands5051': 1, 'recognition52': 1, '2000s': 1, 'great': 1, 'abun': 1, 'dant': 1, 'traffic': 1, 'recognition53': 1, 'biological': 1, 'images54': 1, 'connectomics55': 1, 'bodies': 1, 'images3650515658': 1, 'recognition59': 1, 'importantly': 1, 'mobile': 1, '439': 1, '6061': 1, 'upcoming': 1, 'sys': 1, 'tems': 1, 'gaining': 1, 'importance': 1, 'recognition7': 1, 'mainstream': 1, 'competition': 1, 'million': 1, 'contained': 1, '1000': 1, 'spec': 1, 'tacular': 1, 'halving': 1, 'rates': 1, 'compet': 1, 'approaches1': 1, 'came': 1, 'regularization': 1, 'technique': 1, 'niques': 1, 'deforming': 1, 'existing': 1, 'revolution': 1, 'dominant': 1, '458596365': 1, 'stunning': 1, 'demonstration': 1, 'combines': 1, 'hun': 1, 'dreds': 1, 'billions': 1, 'taken': 1, 'weeks': 1, 'software': 1, 'parallelization': 1, 'reduced': 1, 'hours': 1, 'caused': 1, 'google': 1, 'facebook': 1, 'microsoft': 1, 'ibm': 1, 'ahoo': 1, 'twitter': 1, 'adobe': 1, 'growing': 1, 'startups': 1, 'initiate': 1, 'development': 1, 'projects': 1, 'deploy': 1, 'services': 1, 'amenable': 1, 'implemen': 1, 'tations': 1, 'fieldprogrammable': 1, 'gate': 1, '6667': 1, 'intel': 1, 'qualcomm': 1, 'samsung': 1, 'realtime': 1, 'theory': 1, 'expo': 1, 'nential': 1, 'representations21': 1, 'arise': 1, 'power': 1, 'depend': 1, 'underlying': 1, 'datagenerating': 1, 'having': 1, 'appropriate': 1, 'componential': 1, 'binary': 1, '6869': 1, 'brings': 1, 'repre': 1, 'sent': 1, 'makes': 1, 'easy': 1, 'nicely': 1, 'generated': 1, 'focus': 1, 'lighter': 1, 'bold': 1, 'found86': 1, 'exploits': 1, 'achieve': 1, 'shopping': 1, 'outdoor': 1, 'market': 1, 'vegetables': 1, 'fruit': 1, 'stand': 1, 'woman': 1, 'throwing': 1, 'frisbee': 1, 'park': 1, 'girl': 1, 'bed': 1, 'teddy': 1, 'bear': 1, 'boat': 1, 'water': 1, 'giraﬀe': 1, 'forest': 1, 'hardwood': 1, 'f_loor': 1, '440': 1, 'earlier': 1, 'words71': 1, 'presented': 1, 'oneofn': 1, 'component': 1, 'rest': 1, 'vec': 1, 'tors': 1, 'predicted': 1, 'interpreted': 1, 'separate': 1, 'explicitly': 1, 'factorizing': 1, 'corpus': 1, 'individual': 1, 'unreliable': 1, 'tuesday': 1, 'wednesday': 1, 'sweden': 1, 'norway': 1, 'mutually': 1, 'exclusive': 1, 'observed': 1, 'determined': 1, 'ahead': 1, 'experts': 1, '14177276': 1, 'lies': 1, 'heart': 1, 'debate': 1, 'neuralnetworkinspired': 1, 'instance': 1, 'something': 1, 'identical': 1, 'nonidentical': 1, 'instances': 1, 'must': 1, 'bound': 1, 'variables': 1, 'judiciously': 1, 'contrast': 1, 'scalar': 1, 'intui': 1, 'underpins': 1, 'effortless': 1, 'commonsense': 1, 'introduction': 1, 'models71': 1, 'distrib': 1, 'uted': 1, 'counting': 1, 'frequencies': 1, 'occur': 1, 'rences': 1, 'length': 1, 'size': 1, 'account': 1, 'handful': 1, 'corpora': 1, 'treat': 1, 'atomic': 1, 'cannot': 1, 'generalize': 1, 'across': 1, 'associate': 1, 'valued': 1, 'close': 1, 'exciting': 1, 'element': 1, 'maintaining': 1, 'implicitly': 1, 'contains': 1, 'history': 1, 'becomes': 1, 'apply': 1, 'problematic': 1, 'backpropagated': 1, 'grow': 1, 'shrink': 1, 'explode': 1, 'vanish7778': 1, 'thanks': 1, '7980': 1, 'ways': 1, 'them8182': 1, 'found': 1, 'text83': 1, 'sequence75': 1, 'expressed': 1, 'provided': 1, 'tribution': 1, 'chosen177276': 1, 'according': 1, 'depends': 1, 'rather': 1, 'naive': 1, 'performing': 1, 'competitive': 1, 'raises': 1, 'doubts': 1, 'whether': 1, 'sen': 1, 'tence': 1, 'anything': 1, 'manipulated': 1, 'compatible': 1, 'everyday': 1, 'involves': 1, 'simultaneous': 1, 'analogies': 1, 'illustration': 1, 'nonlinearly': 1, 'projected': 1, 'visualization': 1, 'algorithm103': 1, 'englishtofrench': 1, 'network75': 1, 'observe': 1, 'mapped': 1, 'nearby': 1, 'predicts': 1, 'quantity': 1, 'translated': 1, 'translation1875': 1, '105': 1, '115': 1, '125': 1, '135': 1, 'organizations': 1, 'institutions': 1, 'industry': 1, 'company': 1, 'organization': 1, 'school': 1, 'oﬃce': 1, 'agency': 1, 'association': 1, 'body': 1, 'schools': 1, 'agencies': 1, 'daysthe': 1, 'coming': 1, 'quot': 1, 'dispute': 1, 'six': 1, '441': 1, 'contribute': 1, 'plausibility': 1, 'conclusion8485': 1, 'translating': 1, 'here': 1, 'vnet': 1, 'converts': 1, 'surge': 1, 'see': 1, 'mentioned': 1, 'main': 1, 'purpose': 1, 'evidence': 1, 'difficult': 1, 'store': 1, 'long78': 1, 'explicit': 1, 'proposal': 1, 'behaviour': 1, 'time79': 1, 'cell': 1, 'acts': 1, 'accumulator': 1, 'leaky': 1, 'neuron': 1, 'itself': 1, 'copies': 1, 'own': 1, 'realvalued': 1, 'accumulates': 1, 'selfconnection': 1, 'multiplicatively': 1, 'subsequently': 1, 'step87': 1, 'enabling': 1, 'entire': 1, 'goes': 1, 'transcription': 1, 'forms': 1, 'translation177276': 1, 'year': 1, 'tapelike': 1, 'choose': 1, 'write': 1, 'associative': 1, 'memory89': 1, 'yielded': 1, 'excel': 1, 'lent': 1, 'questionanswering': 1, 'benchmarks': 1, 'asked': 1, 'memorization': 1, 'mem': 1, 'ory': 1, 'normally': 1, 'taught': 1, 'among': 1, 'things': 1, 'sorted': 1, 'unsorted': 1, 'accompanied': 1, 'priority': 1, 'keep': 1, 'track': 1, 'adventure': 1, 'game': 1, 'inference90': 1, '15sentence': 1, 'lord': 1, 'rings': 1, 'correctly': 1, 'frodo': 1, 'now89': 1, 'learning9198': 1, 'catalytic': 1, 'effect': 1, 'reviving': 1, 'overshadowed': 1, 'purely': 1, 'focused': 1, 'longer': 1, 'term': 1, 'observing': 1, 'told': 1, 'every': 1, 'sequentially': 1, 'samples': 1, 'optic': 1, 'intelligent': 1, 'taskspeciﬁc': 1, 'highresolution': 1, 'fovea': 1, 'surround': 1, 'endto': 1, 'look': 1, 'combining': 1, 'rein': 1, 'forcement': 1, 'infancy': 1, 'outperform': 1, 'passive': 1, 'systems99': 1, 'impressive': 1, 'play': 1, 'games100': 1, 'area': 1, 'poised': 1, 'impact': 1, 'understand': 1, 'documents': 1, 'selectively': 1, 'attending': 1, 'time7686': 1, 'ultimately': 1, 'rulebased': 1, 'vectors101': 1, 'received': 1, 'february': 1, 'accepted': 1, 'halve': 1, 'rate': 1, 'precipitated': 1, 'rapid': 1, 'adoption': 1, 'labeling': 1, '19151929': 1, '17991807': 1, 'szegedy': 1, 'deeper': 1, 'preprint': 1, 'abs14094842': 1, 'deoras': 1, 'povey': 1, 'burget': 1, 'cernocky': 1, '196201': 1, 'magazine': 1, '8297': 1, 'laboratories': 1, 'summarizing': 1, 'phonetic': 1, 'industrial': 1, '7': 1, 'sainath': 1, 'kingsbury': 1, 'ramabhadran': 1, 'lvcsr': 1, '86148618': 1, 'ma': 1, 'sheridan': 1, 'liaw': 1, 'svetnik': 1, 'quantitative': 1, 'structureactivity': 1, 'chem': 1, 'inf': 1, '263274': 1, 'ciodaro': 1, 'deva': 1, 'seixas': 1, 'damazio': 1, 'topological': 1, 'calorimetry': 1, 'phys': 1, 'conf': 1, '368': 1, '012030': 1, 'higgs': 1, 'boson': 1, 'challenge': 1, 'httpswwwkaggle': 1, 'comchiggsboson': 1, 'helmstaedter': 1, 'connectomic': 1, 'inner': 1, 'plexiform': 1, 'mouse': 1, 'retina': 1, '500': 1, '168174': 1, 'xtxt1': 1, 'xt1x': 1, 'unfold': 1, 'unfolding': 1, 'involved': 1, 'grouped': 1, 'under': 1, 'node': 1, 'black': 1, 'square': 1, 'representing': 1, 'xt': 1, 'depending': 1, 'xtʹ': 1, 'tʹ': 1, 'uvw': 1, 'variant': 1, 'graph': 1, 'logprobability': 1, '442': 1, 'leung': 1, 'lee': 1, 'tissue': 1, 'regulated': 1, 'bioinformatics': 1, 'i121i129': 1, 'reveals': 1, 'insights': 1, 'genetic': 1, 'determinants': 1, 'disease': 1, '347': 1, '6218': 1, 'collobert': 1, 'scratch': 1, '24932537': 1, 'subgraph': 1, 'embeddings': 1, 'arxivorgabs14063676v3': 1, 'jean': 1, 'memisevic': 1, 'aclijcnlp': 1, 'abs14122007': 1, '17': 1, 'q': 1, '31043112': 1, 'bousquet': 1, 'tradeoffs': 1, '161168': 1, 'duda': 1, 'hart': 1, 'wiley': 1, '1973': 1, 'schölkopf': 1, 'smola': 1, 'kernels': 1, '2002': 1, 'delalleau': 1, 'roux': 1, 'curse': 1, 'variable': 1, '107114': 1, 'selfridge': 1, 'pandemonium': 1, 'symposium': 1, '513526': 1, '1958': 1, 'rosenblatt': 1, 'perceptron': 1, 'perceiving': 1, 'automaton': 1, 'rep': 1, '854601': 1, 'cornell': 1, 'aeronautical': 1, 'laboratory': 1, '1957': 1, 'werbos': 1, 'regression': 1, 'tools': 1, 'behavioral': 1, 'harvard': 1, '1974': 1, 'parker': 1, 'logic': 1, 'tr47': 1, 'une': 1, 'procédure': 1, 'dapprentissage': 1, 'pour': 1, 'réseau': 1, 'à': 1, 'seuil': 1, 'assymétrique': 1, 'cognitiva': 1, 'frontière': 1, 'lintelligence': 1, 'artiﬁcielle': 1, 'connaissance': 1, 'neurosciences': 1, '599604': 1, 'rumelhart': 1, 'williams': 1, '323': 1, '533536': 1, '1986': 1, 'glorot': 1, 'rectiﬁer': 1, '14th': 1, '315323': 1, 'dauphin': 1, 'identifying': 1, 'attacking': 1, 'point': 1, 'high': 1, 'dimensional': 1, 'nonconvex': 1, '29332941': 1, 'choromanska': 1, 'henaff': 1, 'mathieu': 1, 'arous': 1, 'loss': 1, 'arxivorgabs14120233': 1, '19th': 1, '17651775': 1, 'osindero': 1, 'teh': 1, 'yw': 1, '15271554': 1, 'novel': 1, 'restricted': 1, 'lamblin': 1, 'popovici': 1, 'greedy': 1, 'layerwise': 1, '153160': 1, 'improves': 1, 'generalizes': 1, 'poultney': 1, '11371144': 1, 'dimensionality': 1, '313': 1, '504507': 1, 'chintala': 1, 'pedestrian': 1, 'httparxivorgabs12120142': 1, 'raina': 1, 'madhavan': 1, 'processors': 1, '26th': 1, 'annual': 1, '873880': 1, '1422': 1, '39': 1, 'yu': 1, 'deng': 1, 'acero': 1, 'contextdependent': 1, 'pretrained': 1, '3342': 1, 'courville': 1, 'perspectives': 1, '17981828': 1, '41': 1, '396404': 1, '1990': 1, 'classifying': 1, 'haffner': 1, '22782324': 1, '1998': 1, 'overview': 1, 'principles': 1, 'modular': 1, 'combined': 1, 'mechanisms': 1, 'interdependent': 1, 'associated': 1, 'hubel': 1, 'wiesel': 1, 'receptive': 1, 'ﬁelds': 1, 'binocular': 1, 'interaction': 1, 'functional': 1, 'cats': 1, 'physiol': 1, '106154': 1, '1962': 1, '44': 1, 'felleman': 1, 'essen': 1, 'cerebral': 1, 'cereb': 1, '147': 1, 'cadieu': 1, 'rival': 1, 'core': 1, 'plos': 1, 'biol': 1, 'e1003963': 1, '46': 1, 'fukushima': 1, 'miyake': 1, 'neocognitron': 1, 'tolerant': 1, 'deformations': 1, '455469': 1, '1982': 1, 'waibel': 1, 'hanazawa': 1, 'shikano': 1, 'phoneme': 1, '328339': 1, '48': 1, 'fogelmansoulié': 1, 'blanchet': 1, 'lienard': 1, 'experiments': 1, 'warping': 1, 'speaker': 1, 'independent': 1, 'isolated': 1, 'eurospeech': 1, '537540': 1, '49': 1, 'steinkraus': 1, 'practices': 1, '958963': 1, '2003': 1, '50': 1, 'vaillant': 1, 'monrocq': 1, 'original': 1, 'localisation': 1, '141': 1, '245250': 1, '51': 1, 'nowlan': 1, '901908': 1, '52': 1, 'lawrence': 1, 'giles': 1, 'tsoi': 1, 'neuralnetwork': 1, '98113': 1, '53': 1, 'ciresan': 1, 'meier': 1, 'masci': 1, 'multicolumn': 1, 'trafﬁc': 1, '333338': 1, '54': 1, 'ning': 1, 'toward': 1, 'phenotyping': 1, 'embryos': 1, 'videos': 1, '13601371': 1, 'turaga': 1, 'afﬁnity': 1, 'graphs': 1, '511538': 1, '56': 1, 'garcia': 1, 'delakis': 1, 'ﬁnder': 1, '14081423': 1, 'osadchy': 1, 'miller': 1, 'synergistic': 1, '11971215': 1, '58': 1, 'goroshin': 1, 'httparxivorgabs14114280': 1, '59': 1, 'taigman': 1, 'yang': 1, 'deepface': 1, 'closing': 1, 'gap': 1, 'veriﬁcation': 1, '17011708': 1, '60': 1, 'hadsell': 1, 'offroad': 1, 'driving': 1, 'field': 1, 'robot': 1, '120144': 1, '61': 1, 'multiscale': 1, 'purity': 1, 'optimal': 1, 'covers': 1, 'httparxivorgabs12022160': 1, 'srivastava': 1, 'overﬁtting': 1, '19291958': 1, '63': 1, 'overfeat': 1, 'integrated': 1, 'httparxivorgabs13126229': 1, '64': 1, 'girshick': 1, 'donahue': 1, 'darrell': 1, 'malik': 1, 'rich': 1, 'accurate': 1, '580587': 1, '65': 1, 'simonyan': 1, 'zisserman': 1, 'httparxivorgabs14091556': 1, '66': 1, 'boser': 1, 'sackinger': 1, 'bromley': 1, 'jackel': 1, 'analog': 1, 'processor': 1, 'programmable': 1, 'topology': 1, 'solid': 1, 'circuits': 1, '20172025': 1, '67': 1, 'fpgabased': 1, 'scaling': 1, 'approaches': 1, 'eds': 1, 'bekkerman': 1, 'bilenko': 1, 'langford': 1, '399419': 1, 'cambridge': 1, '68': 1, '69': 1, 'morton': 1, 'mixture': 1, 'mixtures': 1, 'math': 1, '321347': 1, '29242932': 1, 'ducharme': 1, '932938': 1, '2001': 1, 'embedding': 1, 'phrase': 1, '443': 1, '17241734': 1, '73': 1, 'schwenk': 1, 'continuous': 1, '492518': 1, '74': 1, 'socher': 1, 'lin': 1, 'cy': 1, 'manning': 1, 'scenes': 1, 'recursive': 1, '129136': 1, '75': 1, 'chen': 1, 'corrado': 1, 'dean': 1, 'compositionality': 1, '31113119': 1, '76': 1, 'bahdanau': 1, 'align': 1, 'httparxivorgabs14090473': 1, '77': 1, 'untersuchungen': 1, 'zu': 1, 'dynamischen': 1, 'neuronalen': 1, 'netzen': 1, 'german': 1, 'diploma': 1, 'tu': 1, 'münich': 1, '78': 1, 'frasconi': 1, 'difﬁcult': 1, '157166': 1, '79': 1, '17351780': 1, 'crucial': 1, 'ingredient': 1, '80': 1, 'elhihi': 1, 'httppapersnipsccpaper1102hierarchicalrecurrentneuralnetworksfor': 1, 'longtermdependencies': 1, '81': 1, '82': 1, 'difﬁculty': 1, '30th': 1, '1310': 1, '1318': 1, '83': 1, 'martens': 1, '28th': 1, '1017': 1, '1024': 1, '84': 1, 'lakoff': 1, 'johnson': 1, 'metaphors': 1, 'live': 1, 'chicago': 1, 'rogers': 1, 'mcclelland': 1, 'xu': 1, 'attend': 1, '87': 1, '66456649': 1, 'wayne': 1, 'danihelka': 1, 'abs14105401': 1, 'abs14103916': 1, '90': 1, 'towards': 1, 'aicomplete': 1, 'prerequisite': 1, 'toy': 1, 'httparxivorgabs150205698': 1, '91': 1, 'dayan': 1, 'neal': 1, 'wakesleep': 1, '268': 1, '15581161': 1, '92': 1, '448455': 1, '93': 1, 'manzagol': 1, 'pa': 1, 'extracting': 1, 'denoising': 1, '25th': 1, '10961103': 1, '95': 1, 'gregor': 1, 'approximations': 1, 'coding': 1, '399406': 1, '96': 1, 'susskind': 1, 'mrfs': 1, '22062222': 1, '97': 1, 'thibodeaulaufer': 1, 'alain': 1, 'yosinski': 1, 'backprop': 1, '31st': 1, '226234': 1, '98': 1, 'kingma': 1, 'rezende': 1, 'welling': 1, 'semisupervised': 1, '35813589': 1, '99': 1, 'ba': 1, 'arxivorgabs14127755': 1, '100': 1, 'control': 1, '518': 1, '529533': 1, '101': 1, '133149': 1, 'toshev': 1, 'erhan': 1, 'generator': 1, '103': 1, 'van': 1, 'der': 1, 'maaten': 1, '25792605': 1, 'acknowledgements': 1, 'thank': 1, 'council': 1, 'canadian': 1, 'national': 1, 'foundation': 1, 'office': 1, 'naval': 1, 'support': 1, 'yb': 1, 'fellows': 1, 'author': 1, 'reprints': 1, 'permissions': 1, 'wwwnaturecomreprints': 1, 'declare': 1, 'competing': 1, 'financial': 1, 'readers': 1, 'welcome': 1, 'comment': 1, 'gonaturecom7cjbaa': 1, 'correspondence': 1, 'addressed': 1, 'yanncsnyuedu': 1, '444': 1})}, {'file_name': 'resnet', 'word_counts': Counter({'the': 327, 'and': 164, 'of': 127, 'to': 105, 'a': 104, 'in': 102, 'we': 93, 'is': 88, '3x3': 83, 'conv': 82, 'on': 75, 'are': 70, 'for': 67, 'that': 66, 'layers': 61, 'with': 60, 'residual': 51, '2': 51, 'networks': 45, 'by': 45, '33': 44, 'error': 43, '256': 43, 'fig': 42, 'this': 40, '3': 38, 'training': 36, 'as': 36, 'deep': 35, 'plain': 34, 'our': 33, '512': 33, '64': 32, '11': 31, 'have': 30, '128': 30, 'table': 30, 'layer': 28, 'imagenet': 28, 'identity': 28, 'than': 27, 'from': 27, 'network': 27, '4': 27, 'nets': 26, '40': 25, 'shortcuts': 24, 'learning': 23, 'an': 23, 'be': 23, 'use': 23, '1': 22, 'x': 22, 'depth': 21, '5': 21, 'resnet': 21, 'test': 20, 'mapping': 20, 'j': 20, 'can': 19, 'also': 18, 'shortcut': 18, 's': 18, 'deeper': 17, 'neural': 17, '2015': 17, 'has': 17, 'problem': 17, 'not': 17, 'y': 17, '34layer': 17, 'more': 16, 'at': 16, 'dimensions': 16, 'resnets': 16, 'but': 15, 'coco': 15, 'results': 15, 'which': 15, 'size': 15, 'image': 14, 'detection': 14, '20': 14, 'when': 14, 'degradation': 14, 'output': 14, '34': 14, 'c': 14, 'r': 14, 'recognition': 13, 'convolutional': 13, 'models': 13, '6': 13, '10': 13, 'right': 13, 'it': 13, 'v': 13, 'b': 13, 'accuracy': 12, 'dataset': 12, 'still': 12, 'set': 12, '16': 12, 'all': 12, 'left': 12, 'model': 12, 'so': 12, 'connections': 12, 'k': 12, 'functions': 11, 'these': 11, '60': 11, 'ieee': 11, 'net': 11, 'two': 11, 'feature': 11, 'architectures': 11, 'pool': 11, '18layer': 11, 'bottleneck': 11, 'm': 11, '12': 10, '7': 10, 'shows': 10, 'no': 10, 'library': 10, 'each': 10, 'method': 10, 'n': 10, 'used': 9, 'classiﬁcation': 9, 'stacked': 9, '43': 9, 'other': 9, 'similar': 9, 'better': 9, 'increasing': 9, 'its': 9, 'show': 9, 'authorized': 9, 'licensed': 9, 'limited': 9, 'univ': 9, 'chicago': 9, 'downloaded': 9, 'january': 9, '112025': 9, '170544': 9, 'utc': 9, 'xplore': 9, 'restrictions': 9, 'apply': 9, 'if': 9, 'parameter': 9, 'complexity': 9, 'same': 9, 'function': 9, 'responses': 9, 'f': 9, 'flops': 9, 'validation': 9, '2014': 9, 'd': 9, 'increased': 8, 'vgg': 8, '1000': 8, 'object': 8, '0': 8, '41': 8, 'block': 8, 'or': 8, 'time': 8, 'trained': 8, 'using': 8, 'shown': 8, 'optimization': 8, 'top5': 8, '18': 8, 'e': 8, 'see': 8, 'only': 8, 'p': 8, 'ing': 7, 'evaluate': 7, '21': 7, '32': 7, 'iter': 7, '1e4': 7, 'figure': 7, '8': 7, 'such': 7, 'experiments': 7, 'relu': 7, 'building': 7, 'extra': 7, 'both': 7, 'multiple': 7, 'linear': 7, 'map': 7, 'projection': 7, 'billion': 7, 'testing': 7, 'images': 7, 'present': 6, '1st': 6, 'ilsvrc': 6, 'visual': 6, '28': 6, 'number': 6, '35': 6, 'higher': 6, '36': 6, 'counterpart': 6, 'solvers': 6, 'nonlinear': 6, 'original': 6, 'zero': 6, 'their': 6, 'extremely': 6, 'gains': 6, '30': 6, 'faster': 6, 'highway': 6, '42': 6, 'adopt': 6, 'ﬁlters': 6, 'stride': 6, 'baseline': 6, 'fc': 6, 'option': 6, 'bn': 6, '01': 6, 'iterations': 6, '2012': 6, '152layer': 6, 'resnet34': 6, 'l': 6, 't': 6, 'he': 5, 'zhang': 5, 'sun': 5, 'com': 5, 'easier': 5, 'optimize': 5, 'lower': 5, 'won': 5, '100': 5, 'representations': 5, 'tasks': 5, 'obtain': 5, 'segmentation': 5, '20layer': 5, 'cifar10': 5, 'gradients': 5, 'gradient': 5, 'able': 5, 'then': 5, 'let': 5, 'shallower': 5, 'learned': 5, 'weight': 5, 'good': 5, 'paper': 5, 'few': 5, 'ﬁt': 5, 'hx': 5, 'one': 5, 'simply': 5, 'suggesting': 5, 'over': 5, 'vectors': 5, '44': 5, '24': 5, 'parameters': 5, 'they': 5, 'mappings': 5, 'may': 5, 'after': 5, 'eqn1': 5, 'plainresidual': 5, 'form': 5, 'three': 5, 'middle': 5, 'standard': 5, 'top1': 5, 'rates': 5, 'counterparts': 5, 'difﬁculty': 5, 'data': 5, 'err': 5, 'vgg16': 5, 'resnet101': 5, 'voc': 5, 'bengio': 5, 'g': 5, 'h': 5, 'ren': 4, 'train': 4, 'learn': 4, 'considerably': 4, 'ensemble': 4, '357': 4, 'result': 4, 'im': 4, 'where': 4, 'very': 4, '56layer': 4, 'thus': 4, 'phenomena': 4, 'been': 4, 'start': 4, 'sgd': 4, 'might': 4, 'leads': 4, 'reported': 4, 'example': 4, 'consider': 4, 'there': 4, 'added': 4, 'solution': 4, 'should': 4, 'ﬁnd': 4, 'solutions': 4, 'vision': 4, 'do': 4, 'directly': 4, 'stack': 4, '48': 4, 'case': 4, 'outputs': 4, 'tional': 4, 'eg': 4, '19': 4, 're': 4, 'difﬁculties': 4, 'methods': 4, 'long': 4, 'practice': 4, 'connected': 4, 'input': 4, 'addition': 4, 'ﬁrst': 4, 'although': 4, 'fxwi': 4, 'here': 4, 'performed': 4, 'above': 4, 'maps': 4, 'i': 4, 'average': 4, 'options': 4, 'rate': 4, 'best': 4, 'thin': 4, 'denote': 4, 'compared': 4, 'ilsvrc14': 4, 'designs': 4, '9': 4, '110layer': 4, 'resnet20': 4, 'std': 4, 'pascal': 4, 'rcnn': 4, 'w': 4, 'girshick': 4, 'cvpr': 4, '2013': 4, 'lecun': 4, 'explicitly': 3, 'reference': 3, 'inputs': 3, 'provide': 3, 'evidence': 3, 'gain': 3, 'up': 3, 'place': 3, 'analysis': 3, 'many': 3, 'due': 3, 'places': 3, '22': 3, '49': 3, 'features': 3, 'question': 3, 'easy': 3, '14': 3, 'addressed': 3, 'initial': 3, '23': 3, 'normalization': 3, 'descent': 3, 'indicates': 3, 'us': 3, 'architecture': 3, 'constructed': 3, 'computer': 3, 'pattern': 3, 'fx': 3, 'desired': 3, 'underlying': 3, 'lay': 3, 'ers': 3, 'optimal': 3, 'formulation': 3, 'perform': 3, 'neither': 3, 'nor': 3, 'exhibit': 3, 'previous': 3, 'explore': 3, 'performance': 3, 'lead': 3, 'further': 3, 'localization': 3, 'expect': 3, 'encoding': 3, 'multigrid': 3, 'scales': 3, 'between': 3, 'scale': 3, 'hierarchical': 3, 'basis': 3, '45': 3, 'reformulation': 3, 'preconditioning': 3, 'addressing': 3, '38': 3, '31': 3, 'propagated': 3, 'parameterfree': 3, 'tions': 3, 'approximate': 3, 'ie': 3, 'about': 3, 'cases': 3, 'solver': 3, 'small': 3, 'open': 3, 'elementwise': 3, 'nonlinearity': 3, 'computation': 3, 'comparisons': 3, 'will': 3, 'note': 3, 'fullyconnected': 3, 'tion': 3, 'ob': 3, 'follows': 3, 'follow': 3, 'simple': 3, 'ends': 3, 'pooling': 3, 'softmax': 3, 'vgg19': 3, '56': 3, 'increase': 3, 'details': 3, 'based': 3, 'into': 3, 'padded': 3, 'convolutions': 3, 'implementation': 3, 'side': 3, 'sampled': 3, 'augmentation': 3, 'batch': 3, 'before': 3, 'following': 3, 'dropout': 3, '13': 3, 'comparison': 3, '10crop': 3, 'classes': 3, 'de': 3, '50layer': 3, '1024': 3, '2048': 3, 'blocks': 3, 'argue': 3, 'reducing': 3, 'next': 3, 'googlenet': 3, 'prelunet': 3, '571': 3, 'accurate': 3, 'compare': 3, 'because': 3, 'among': 3, 'smaller': 3, 'stateoftheart': 3, 'singlemodel': 3, 'behaviors': 3, '3232': 3, '2n': 3, 'maxout': 3, '25': 3, '110': 3, 'resnet110': 3, 'large': 3, '80': 3, 'plain20': 3, 'plain56': 3, 'resnet56': 3, '1202layer': 3, 'map5': 3, 'appendix': 3, 'applied': 3, 'pages': 3, '2010': 3, 'fast': 3, 'darrell': 3, 'hinton': 3, 'srivastava': 3, 'krizhevsky': 3, 'schmidhuber': 3, 'tpami': 3, '1998': 3, 'z': 3, 'iniclr': 3, 'microsoft': 2, 'framework': 2, 'ease': 2, 'substantially': 2, 'those': 2, 'previously': 2, 'stead': 2, 'unreferenced': 2, '152': 2, 'having': 2, 'importance': 2, 'solely': 2, 'ex': 2, 'relative': 2, 'local': 2, 'ization': 2, 'introduction': 2, '39': 2, 'classiﬁers': 2, 'endtoend': 2, 'recent': 2, 'reveals': 2, 'leading': 2, 'challenging': 2, '27': 2, 'presented': 2, 'greatly': 2, 'vanishingexploding': 2, 'convergence': 2, 'however': 2, 'con': 2, 'back': 2, 'converging': 2, 'caused': 2, 'overﬁtting': 2, 'systems': 2, 'current': 2, 'hand': 2, 'unable': 2, '2016': 2, 'comparably': 2, 'formally': 2, 'denoting': 2, 'offx': 2, 'hypothesize': 2, 'short': 2, 'cut': 2, 'add': 2, 'computa': 2, 'entire': 2, 'backpropagation': 2, 'implemented': 2, 'common': 2, 'caffe': 2, 'without': 2, 'increases': 2, 'just': 2, 'successfully': 2, 'excellent': 2, 'while': 2, 'generalization': 2, 'competitions': 2, 'strong': 2, 'applicable': 2, 'problems': 2, 'work': 2, 'vlad': 2, 'fisher': 2, 'version': 2, '47': 2, 'quantization': 2, '17': 2, 'repre': 2, 'sent': 2, 'studied': 2, 'early': 2, 'perceptrons': 2, '37': 2, '46': 2, 'centering': 2, 'errors': 2, '15': 2, 'closed': 2, 'nonresidual': 2, 'func': 2, 'always': 2, 'information': 2, 'addi': 2, 'different': 2, 'weights': 2, 'help': 2, 'closer': 2, '29': 2, 'connection': 2, 'important': 2, 'fairly': 2, 'pare': 2, 'width': 2, 'cost': 2, 'except': 2, 'inputoutput': 2, 'ws': 2, 'match': 2, 'economical': 2, 'single': 2, 'observed': 2, 'channel': 2, 'served': 2, 'dis': 2, 'describe': 2, 'mainly': 2, 'design': 2, 'doubled': 2, 'per': 2, 'global': 2, 'weighted': 2, '196': 2, '7x7': 2, 'avg': 2, '4096': 2, 'dotted': 2, 'line': 2, 'eqn2': 2, 'done': 2, 'go': 2, 'sizes': 2, 'resized': 2, 'shorter': 2, 'crop': 2, 'randomly': 2, 'horizontal': 2, 'ﬂip': 2, 'perpixel': 2, 'mean': 2, 'subtracted': 2, 'starts': 2, 'decay': 2, '00001': 2, 'momentum': 2, '09': 2, 'studies': 2, 'fully': 2, 'consists': 2, '50k': 2, 'server': 2, '101layer': 2, '77': 2, '5656': 2, 'conv4': 2, 'numbers': 2, '5020': 2, '50': 2, 'plain34': 2, 'curves': 2, 'bold': 2, '2854': 2, '2503': 2, 'forward': 2, 'signals': 2, 'backward': 2, 'achieve': 2, 'compet': 2, 'itive': 2, 'future': 2, 'zeropadding': 2, 'well': 2, 'manage': 2, '738': 2, '776': 2, 'resnet50': 2, 'resnet152': 2, 'resnet50101152': 2, 'projections': 2, 'v5': 2, 'bninception': 2, '449': 2, '732': 2, 'resulting': 2, 'rightvs': 2, 'converges': 2, '1x1': 2, 'slightly': 2, 'introduced': 2, 'rest': 2, 'particularly': 2, 'below': 2, 'modify': 2, 'efﬁcient': 2, 'witnessed': 2, '3layer': 2, 'remarkably': 2, 'ones': 2, 'evaluation': 2, 'focus': 2, 'pushing': 2, 'convo': 2, 'lutions': 2, 'respectively': 2, 'totally': 2, 'fitnet': 2, '643': 2, '1202': 2, '194m': 2, '793': 2, 'going': 2, 'several': 2, 'lines': 2, 'index': 2, 'deviations': 2, 'order': 2, 'generally': 2, 'aggressively': 2, '0712': 2, 'metric': 2, '95': 2, 'regularization': 2, 'improve': 2, 'tection': 2, '2007': 2, '26': 2, 'university': 2, 'press': 2, 'et': 2, 'al': 2, 'edaldi': 2, 'zisserman': 2, '2011': 2, 'iccv': 2, 'donahue': 2, 'semantic': 2, 'understanding': 2, 'inaistats': 2, 'sutskever': 2, 'hochreiter': 2, 'tu': 2, 'szegedy': 2, 'jegou': 2, 'douze': 2, 'schmid': 2, 'perronnin': 2, 'jia': 2, 'shelhamer': 2, 'report': 2, 'innips': 2, 'tricks': 2, 'trade': 2, 'springer': 2, 'lin': 2, 'eccv': 2, 'raiko': 2, 'alpola': 2, 'transformations': 2, 'towards': 2, 'ripley': 2, 'schraudolph': 2, 'sermanet': 2, 'fergus': 2, 'greff': 2, 'convolu': 2, 'szeliski': 2, 'kaiming': 1, 'xiangyu': 1, 'shaoqing': 1, 'jian': 1, 'research': 1, 'kahe': 1, 'vxiangz': 1, 'vshren': 1, 'jiansunmicrosoftcom': 1, 'abstract': 1, 'difﬁcult': 1, 'reformulate': 1, 'prehensive': 1, 'empirical': 1, 'showing': 1, 'layers8': 1, 'complex': 1, 'ity': 1, 'achieves': 1, 'imagenettest': 1, 'task': 1, 'cif': 1, 'ar10': 1, 'central': 1, 'tremely': 1, 'provement': 1, 'foundations': 1, 'submissions': 1, 'competitions1': 1, 'led': 1, 'series': 1, 'breakthroughs': 1, 'naturally': 1, 'integrate': 1, 'lowmidhigh': 1, 'level': 1, 'multi': 1, 'fashion': 1, 'levels': 1, 'enriched': 1, 'crucial': 1, 'exploit': 1, 'sixteen': 1, 'thirty': 1, 'non': 1, 'trivial': 1, '1httpimagenetorgchallengeslsvrc2015': 1, 'httpmscocoorgdatasetdetectionschallenge2015': 1, 'beneﬁted': 1, 'driven': 1, 'signiﬁcance': 1, 'arisesis': 1, 'stacking': 1, 'obstacle': 1, 'answering': 1, 'was': 1, 'notorious': 1, 'hamper': 1, 'beginning': 1, 'largely': 1, 'normalized': 1, 'intermediate': 1, 'enable': 1, 'tens': 1, 'verging': 1, 'stochastic': 1, 'propagation': 1, 'exposed': 1, 'gets': 1, 'saturated': 1, 'unsurprising': 1, 'degrades': 1, 'rapidly': 1, 'unexpectedly': 1, 'adding': 1, 'suitably': 1, 'tohigher': 1, 'thoroughly': 1, 'veriﬁed': 1, 'typical': 1, 'similarly': 1, 'adds': 1, 'onto': 1, 'exists': 1, 'solutionby': 1, 'construction': 1, 'areidentity': 1, 'copied': 1, 'existence': 1, 'produce': 1, 'conference': 1, '1063691916': 1, '3100': 1, 'doi': 1, '101109cvpr201690': 1, '770': 1, 'relufxx': 1, 'feasible': 1, 'address': 1, 'introducing': 1, 'hoping': 1, 'another': 1, 'orig': 1, 'inal': 1, 'recast': 1, 'intofx': 1, 'extreme': 1, 'were': 1, 'would': 1, 'push': 1, 'realized': 1, 'feedfor': 1, 'ward': 1, 'skipping': 1, 'eas': 1, 'ily': 1, 'libraries': 1, 'modifying': 1, 'comprehensive': 1, 'easily': 1, 'enjoy': 1, 'producing': 1, 'sults': 1, 'effects': 1, 'akin': 1, 'particular': 1, 'deepest': 1, 'ever': 1, 'competition': 1, 'rep': 1, 'resentations': 1, 'win': 1, 'principle': 1, 'generic': 1, 'nonvision': 1, 'related': 1, 'representation': 1, 'encodes': 1, 'respect': 1, 'dictionary': 1, 'ector': 1, 'formulated': 1, 'probabilistic': 1, 'them': 1, 'powerful': 1, 'shallow': 1, 'trieval': 1, 'vector': 1, 'effec': 1, 'tive': 1, 'lowlevel': 1, 'graphics': 1, 'solv': 1, 'partial': 1, 'differential': 1, 'equations': 1, 'pdes': 1, 'widely': 1, 'reformulates': 1, 'system': 1, 'subprob': 1, 'lems': 1, 'subproblem': 1, 'respon': 1, 'sible': 1, 'coarser': 1, 'ﬁner': 1, 'alternative': 1, 'pre': 1, 'conditioning': 1, 'relies': 1, 'variables': 1, 'converge': 1, 'much': 1, 'stan': 1, 'dard': 1, 'unaware': 1, 'nature': 1, 'suggest': 1, 'simplify': 1, 'practices': 1, 'theories': 1, 'multilayer': 1, 'mlps': 1, 'interme': 1, 'diate': 1, 'auxiliary': 1, 'papers': 1, 'propose': 1, 'sponses': 1, 'inception': 1, 'posed': 1, 'branch': 1, 'branches': 1, 'concurrent': 1, 'gating': 1, 'gates': 1, 'datadependent': 1, 'contrast': 1, 'gated': 1, 'approaching': 1, 'represent': 1, 'contrary': 1, 'learns': 1, 'never': 1, 'passed': 1, 'through': 1, 'high': 1, '771': 1, 'way': 1, 'demonstrated': 1, 'necessarily': 1, 'hypothesizes': 1, 'asymptoti': 1, 'cally': 1, 'complicated': 1, 'functions2': 1, 'equiv': 1, 'alent': 1, 'asymptotically': 1, 'approxi': 1, 'mate': 1, 'assuming': 1, 'rather': 1, 'approximatehxw': 1, 'becomes': 1, 'forms': 1, 'asymptot': 1, 'ically': 1, 'hypothesized': 1, 'motivated': 1, 'counterintuitive': 1, 'discussed': 1, 'greater': 1, 'counter': 1, 'part': 1, 'suggests': 1, 'approximating': 1, 'drive': 1, 'toward': 1, 'approach': 1, 'real': 1, 'unlikely': 1, 'op': 1, 'timal': 1, 'precondition': 1, 'perturbations': 1, 'new': 1, 'general': 1, 'pings': 1, 'reasonable': 1, 'every': 1, 'deﬁned': 1, 'considered': 1, 'represents': 1, 'w2σw1x': 1, 'σ': 1, 'denotes': 1, '2this': 1, 'hypothesis': 1, 'biases': 1, 'omitted': 1, 'simplifying': 1, 'tations': 1, 'operation': 1, 'sec': 1, 'ond': 1, 'σy': 1, 'introduce': 1, 'tra': 1, 'attractive': 1, 'simultaneously': 1, 'negligible': 1, 'must': 1, 'equal': 1, 'changing': 1, 'channels': 1, 'wsx': 1, 'square': 1, 'matrixws': 1, 'sufﬁcient': 1, 'matching': 1, 'functionf': 1, 'ﬂexible': 1, 'exper': 1, 'iments': 1, 'involve': 1, 'possible': 1, 'w1x': 1, 'advantages': 1, 'notations': 1, 'simplicity': 1, 'tested': 1, 'various': 1, 'consistent': 1, 'instances': 1, 'cussion': 1, 'baselines': 1, 'inspired': 1, 'philosophy': 1, 'mostly': 1, 'rules': 1, 'ﬁl': 1, 'ters': 1, 'ii': 1, 'halved': 1, 'num': 1, 'ber': 1, 'preserve': 1, 'plexity': 1, 'downsampling': 1, '1000way': 1, 'total': 1, 'worth': 1, 'noticing': 1, 'hasfewer': 1, 'multiplyadds': 1, '772': 1, '112': 1, '224': 1, 'imagenetleft': 1, 'mid': 1, 'dle': 1, 'dimensionstable': 1, '1shows': 1, 'variants': 1, 'insert': 1, 'turn': 1, 'solid': 1, 'performs': 1, 'entries': 1, 'introduces': 1, 'across': 1, 'ran': 1, 'domly': 1, '256480': 1, '224224': 1, 'color': 1, 'convolution': 1, 'activation': 1, 'initialize': 1, 'scratch': 1, 'minibatch': 1, 'divided': 1, 'plateaus': 1, 'to60': 1, '104': 1, 'scores': 1, 'in224256384480640': 1, 'classiﬁ': 1, 'cation': 1, 'million': 1, 'evalu': 1, 'ated': 1, 'ﬁnal': 1, '100k': 1, 'tailed': 1, 'reveal': 1, 'reasons': 1, 'trainingvalidation': 1, 'during': 1, 'pro': 1, 'cedure': 1, '773': 1, 'name': 1, 'conv1': 1, '112112': 1, 'conv2': 1, 'max': 1, 'conv3': 1, '2828': 1, '1414': 1, 'conv5': 1, '1000d': 1, '18109': 1, '36109': 1, '38109': 1, '76109': 1, '113109': 1, 'brackets': 1, 'down': 1, 'sampling': 1, 'conv31': 1, 'conv51': 1, 'plain18': 1, 'resnet18': 1, 'onimagenet': 1, 'center': 1, 'crops': 1, 'plot': 1, '2794': 1, '2788': 1, 'procedures': 1, 'highertraining': 1, 'throughout': 1, 'whole': 1, 'procedure': 1, 'even': 1, 'though': 1, 'space': 1, 'subspace': 1, 'isunlikely': 1, 'vanishing': 1, 'ensures': 1, 'nonzero': 1, 'variances': 1, 'verify': 1, 'healthy': 1, 'norms': 1, 'vanish': 1, 'fact': 1, 'works': 1, 'some': 1, 'extent': 1, 'conjecture': 1, 'exponentially': 1, 'low': 1, 'impact': 1, 'error3': 1, 'reason': 1, 'opti': 1, 'mization': 1, 'pair': 1, 'haveno': 1, 'major': 1, 'observations': 1, 'first': 1, 'situation': 1, 'reversed': 1, 'importantly': 1, 'exhibits': 1, 'generalizable': 1, 'setting': 1, 'second': 1, '3we': 1, 'experimented': 1, 'cannot': 1, 'feasibly': 1, '774': 1, '2807': 1, '933': 1, '915': 1, '2427': 1, '1002': 1, '2452': 1, '746': 1, '2419': 1, '740': 1, '2285': 1, '671': 1, '2175': 1, '605': 1, '2143': 1, 'uses': 1, '843': 1, '789': 1, '244': 1, '71': 1, '2159': 1, '2199': 1, '581': 1, '2184': 1, '2153': 1, '560': 1, '2074': 1, '525': 1, '1987': 1, '460': 1, '1938': 1, 'ofsinglemodel': 1, '666': 1, '68': 1, '494': 1, '482': 1, 'ilsvrc15': 1, 'ofensembles': 1, 'reduces': 1, 'reduced': 1, 'veriﬁes': 1, 'effectiveness': 1, 'last': 1, 'overly': 1, 'eases': 1, 'providing': 1, 'conver': 1, 'gence': 1, 'stage': 1, 'vs': 1, '64d': 1, '256d': 1, 'investigate': 1, 'free': 1, 'projec': 1, 'bet': 1, 'ter': 1, 'zeropadded': 1, 'indeed': 1, 'marginally': 1, 'attribute': 1, 'thirteen': 1, 'dif': 1, 'ferences': 1, 'abc': 1, 'indicate': 1, 'essential': 1, 'reduce': 1, 'mem': 1, 'orytime': 1, 'concerns': 1, 'afford': 1, 'design4': 1, 'fw': 1, 'instead': 1, 'responsible': 1, 'restoring': 1, 'leaving': 1, 'portant': 1, 'replaced': 1, 'highdimensional': 1, 'replace': 1, '2layer': 1, '4deeper': 1, 'nonbottleneck': 1, 'usage': 1, 'practical': 1, 'considerations': 1, '775': 1, 'construct': 1, '101': 1, 'signiﬁcantly': 1, '113': 1, 'vgg1619': 1, '153196': 1, 'bil': 1, 'lion': 1, '50101152layer': 1, 'considerable': 1, 'margins': 1, 'observe': 1, 'en': 1, 'joy': 1, 'signiﬁcant': 1, 'beneﬁts': 1, 'metrics': 1, 'achieved': 1, 'outperforms': 1, 'combine': 1, 'six': 1, 'submitting': 1, 'entry': 1, 'conducted': 1, '10k': 1, 'evaluated': 1, 'intentionally': 1, 'middleright': 1, 'of6n': 1, 'sizes32168': 1, 'are163264': 1, 'subsampling': 1, 'formed': 1, '10way': 1, '6n2': 1, 'summarizes': 1, '1616': 1, '88': 1, '12n': 1, 'pairs': 1, '3n': 1, '938': 1, 'nin': 1, '881': 1, 'dsn': 1, '822': 1, 'params': 1, '25m': 1, '839': 1, '23m': 1, '754': 1, '772016': 1, '125m': 1, '880': 1, '027m': 1, '875': 1, '046m': 1, '751': 1, '066m': 1, '717': 1, '085m': 1, '697': 1, '17m': 1, '661016': 1, 'thecifar10': 1, 'meth': 1, 'ods': 1, 'run': 1, 'times': 1, 'meanstd': 1, 'exactly': 1, 'initialization': 1, 'mini': 1, 'gpus': 1, 'divide': 1, '32k': 1, '48k': 1, 'terminate': 1, '64k': 1, 'determined': 1, '45k5k': 1, 'trainval': 1, 'split': 1, 'augmen': 1, 'tation': 1, 'pixels': 1, 'da3': 1, '232': 1, 'view': 1, '3579': 1, 'suffer': 1, 'phenomenon': 1, 'mnist': 1, 'fundamental': 1, 'overcome': 1, 'demon': 1, 'strate': 1, 'too': 1, 'converging5': 1, '001': 1, 'warm': 1, 'until': 1, '400': 1, 'tinue': 1, 'schedule': 1, 'fewer': 1, '5with': 1, '90': 1, 'epochs': 1, 'reaches': 1, 'plain32': 1, 'plain44': 1, 'resnet32': 1, 'resnet44': 1, 'resnet11056layer': 1, 'residual110': 1, 'residual1202': 1, 'oncifar10': 1, 'dashed': 1, 'errorleft': 1, 'plain110': 1, 'displayedmiddle': 1, 'sorted': 1, 'magnitude': 1, 'cifar': 1, 'top': 1, 'bottom': 1, 'ranked': 1, 'descending': 1, 'yet': 1, 'reluaddition': 1, 'analy': 1, 'sis': 1, 'response': 1, 'strength': 1, 'support': 1, 'ba': 1, 'sic': 1, 'motivation': 1, 'sec31': 1, 'notice': 1, 'magni': 1, 'tudes': 1, 'evidenced': 1, 'individual': 1, 'tends': 1, 'signal': 1, 'less': 1, 'exploring': 1, '200': 1, 'described': 1, '103layer': 1, 'worse': 1, '07': 1, '704': 1, '764': 1, '20072012': 1, 'sets': 1, 'ap': 1, 'pendix': 1, '415': 1, '212': 1, '484': 1, '272': 1, 'unnecessarily': 1, 'maxoutdropout': 1, 'impose': 1, 'regulariza': 1, 'via': 1, 'tracting': 1, 'combining': 1, 'stronger': 1, 'study': 1, 'ms': 1, 'adoptfaster': 1, 'rcnn32': 1, 'interested': 1, 'improvements': 1, 'replacing': 1, 'attributed': 1, 'most': 1, 'tain': 1, 'cocos': 1, 'improvement': 1, 'tracks': 1, 'agenet': 1, '777': 1, 'references': 1, 'simard': 1, 'frasconi': 1, 'longterm': 1, 'dependen': 1, 'cies': 1, 'difﬁcultieee': 1, 'transactions': 1, '52157166': 1, '1994': 1, 'bishop': 1, 'oxford': 1, '1995': 1, 'briggs': 1, 'mccormick': 1, 'tutorial': 1, 'siam': 1, '2000': 1, 'chatﬁeld': 1, 'lempitsky': 1, 'devil': 1, 'bmvc': 1, 'everingham': 1, 'gool': 1, 'williams': 1, 'winn': 1, 'zis': 1, 'serman': 1, 'challengeijcv': 1, '303338': 1, 'malik': 1, 'rich': 1, 'hier': 1, 'archies': 1, 'glorot': 1, 'feedforward': 1, 'goodfellow': 1, 'wardefarley': 1, 'mirza': 1, 'courville': 1, 'networksarxiv13024389': 1, 'constrained': 1, 'spatial': 1, 'pyramid': 1, 'ineccv': 1, 'delving': 1, 'rectiﬁers': 1, 'surpassing': 1, 'humanlevel': 1, 'salakhutdinov': 1, 'improving': 1, 'preventing': 1, 'co': 1, 'adaptation': 1, 'detectorsarxiv12070580': 1, 'untersuchungen': 1, 'zu': 1, 'dynamischen': 1, 'neuronalen': 1, 'netzen': 1, 'diploma': 1, 'thesis': 1, 'munich': 1, '1991': 1, 'shortterm': 1, 'memoryneural': 1, '9817351780': 1, '1997': 1, 'ioffe': 1, 'accelerating': 1, 'internal': 1, 'covariate': 1, 'shift': 1, 'inicml': 1, 'product': 1, 'nearest': 1, 'neighbor': 1, 'search': 1, 'sanchez': 1, 'perez': 1, 'aggregating': 1, 'descriptors': 1, 'compact': 1, 'codes': 1, 'karayev': 1, 'guadarrama': 1, 'embedding': 1, 'arxiv14085093': 1, 'tiny': 1, 'ages': 1, 'tech': 1, '2009': 1, 'boser': 1, 'denker': 1, 'henderson': 1, 'howard': 1, 'hubbard': 1, 'jackel': 1, 'written': 1, 'zip': 1, 'code': 1, 'recognitionneural': 1, '1989': 1, 'bottou': 1, 'orr': 1, 'kr': 1, 'muller': 1, 'backprop': 1, '950': 1, 'cy': 1, 'lee': 1, 'xie': 1, 'gallagher': 1, 'deeply': 1, 'supervised': 1, 'arxiv14095185': 1, 'q': 1, 'chen': 1, 'networkarxiv13124400': 1, 'ty': 1, 'maire': 1, 'belongie': 1, 'hays': 1, 'perona': 1, 'ramanan': 1, 'dollar': 1, 'zitnick': 1, 'objects': 1, 'context': 1, 'incvpr': 1, 'mont': 1, 'ufar': 1, 'pascanu': 1, 'cho': 1, 'regions': 1, 'nair': 1, 'rectiﬁed': 1, 'units': 1, 'restricted': 1, 'boltzmann': 1, 'machines': 1, 'icml': 1, 'dance': 1, 'kernels': 1, 'vocabularies': 1, 'categorization': 1, 'made': 1, 'realtime': 1, 'region': 1, 'proposal': 1, 'nips': 1, 'cambridge': 1, '1996': 1, 'romero': 1, 'ballas': 1, 'kahou': 1, 'chassang': 1, 'gatta': 1, 'fitnets': 1, 'hints': 1, 'o': 1, 'russakovsky': 1, 'deng': 1, 'su': 1, 'krause': 1, 'satheesh': 1, 'ma': 1, 'huang': 1, 'karpathy': 1, 'khosla': 1, 'bernstein': 1, 'challengearxiv14090575': 1, 'saxe': 1, 'mcclelland': 1, 'ganguli': 1, 'exact': 1, 'dynamics': 1, 'arxiv13126120': 1, 'accelerated': 1, 'factorcentering': 1, 'decomposition': 1, 'technical': 1, 'factors': 1, '207226': 1, 'eigen': 1, 'mathieu': 1, 'le': 1, 'cun': 1, 'overfeat': 1, 'integrated': 1, 'simonyan': 1, 'ery': 1, 'largescale': 1, 'arxiv150500387': 1, '150706228': 1, 'liu': 1, 'reed': 1, 'anguelov': 1, 'er': 1, 'han': 1, 'anhoucke': 1, 'rabinovich': 1, 'surface': 1, 'interpolation': 1, '1990': 1, 'locally': 1, 'adapted': 1, 'siggraph': 1, '2006': 1, 'atanen': 1, 'stochas': 1, 'tic': 1, 'secondorder': 1, 'methodsbackpropagation': 1, 'nonlinearities': 1, 'processing': 1, 'fulkerson': 1, 'vlfeat': 1, 'portable': 1, 'algorithms': 1, '2008': 1, 'enables': 1, 'modern': 1, 'statistics': 1, 'splus': 1, '1999': 1, 'zeiler': 1, 'visualizing': 1, '778': 1})}, {'file_name': 'attention', 'word_counts': Counter({'the': 278, 'and': 154, 'of': 152, 'in': 109, 'to': 101, 'a': 74, 'we': 66, 'attention': 56, 'model': 43, 'for': 42, 'is': 38, 'on': 38, 'with': 34, 'models': 33, 'sequence': 32, 'as': 30, 'our': 29, 'this': 29, 'output': 27, 'neural': 24, 'layer': 24, 'selfattention': 23, 'translation': 22, 'positions': 22, 'input': 22, 'are': 21, 'transformer': 21, 'layers': 21, 'decoder': 20, 'each': 20, 'encoder': 19, 'by': 19, 'training': 19, 'all': 18, 'recurrent': 18, '2': 18, 'values': 18, 'used': 17, 'that': 16, 'machine': 16, 'networks': 15, 'two': 15, '8': 15, 'preprint': 15, 'at': 14, 'function': 14, 'learning': 14, 'arxiv': 14, '2014': 13, 'position': 13, 'different': 13, '3': 13, 'length': 13, 'n': 12, 'an': 12, 'from': 12, 'than': 12, 'dk': 12, 'table': 12, 'also': 11, 'be': 11, 'bleu': 11, '1': 11, 'representations': 11, '32': 11, '2016': 11, 'or': 10, 'tasks': 10, '31': 10, 'between': 10, 'positional': 10, 'd': 10, 'base': 10, 'these': 9, 'more': 9, 'work': 9, 'dotproduct': 9, 'multihead': 9, '2017': 9, 'can': 9, 'use': 9, 'number': 9, 'it': 9, 'using': 9, 'dimension': 9, 'keys': 9, 'big': 9, 'google': 8, 'transduction': 8, 'network': 8, 'while': 8, 'wmt': 8, 'such': 8, 'has': 8, 'computation': 8, 'sequential': 8, 'which': 8, 'dependencies': 8, 'operations': 8, '4': 8, 'embeddings': 8, 'research': 7, 'convolutional': 7, 'best': 7, 'architecture': 7, 'results': 7, 'memory': 7, 'have': 7, 'modeling': 7, '5': 7, 'information': 7, 'previous': 7, '16': 7, 'trained': 7, 'single': 7, '6': 7, 'where': 7, 'i': 7, 'set': 7, 'queries': 7, 'dot': 7, 'complexity': 7, 'linear': 7, 'k': 7, 'tokens': 7, 'based': 6, 'mechanisms': 6, 'convolutions': 6, 'over': 6, 'gpus': 6, 'long': 6, 'language': 6, 'encoderdecoder': 6, 'scaled': 6, 'instead': 6, '15': 6, 'learn': 6, 'cost': 6, 'sentence': 6, 'one': 6, 'figure': 6, 'sublayers': 6, 'sublayer': 6, 'embedding': 6, '512': 6, 'learned': 6, 'size': 6, 'dropout': 6, '1020': 6, 'bengio': 6, 'corr': 6, 'mechanism': 5, 'new': 5, 'quality': 5, 'time': 5, 'task': 5, 'englishtofrench': 5, 'stateoftheart': 5, '12': 5, 'architectures': 5, 'ﬁrst': 5, 'representation': 5, 'other': 5, 'processing': 5, 'sequences': 5, 'steps': 5, 'hidden': 5, 'computational': 5, 'case': 5, 'however': 5, '20': 5, '11': 5, 'described': 5, 'section': 5, 'compute': 5, '28': 5, 'previously': 5, 'stack': 5, 'identical': 5, 'feedforward': 5, 'residual': 5, '10': 5, 'dmodel': 5, 'pairs': 5, 'key': 5, 'softmax': 5, 'v': 5, 'large': 5, 'attend': 5, 'encoding': 5, 'about': 5, 'maximum': 5, 'path': 5, 'any': 5, 'e': 5, 'were': 5, '01': 5, 'englishtogerman': 5, 'yoshua': 5, '2015': 5, 'brain': 4, 'noam': 4, 'recurrence': 4, 'entirely': 4, 'signiﬁcantly': 4, 'less': 4, 'train': 4, 'including': 4, 'ensembles': 4, 'score': 4, '35': 4, 'days': 4, '7': 4, 'been': 4, 'state': 4, 'since': 4, 'order': 4, 'implemented': 4, 'conference': 4, 'systems': 4, 'factor': 4, 'but': 4, 'p100': 4, 'convs2s': 4, 'parallel': 4, 'averaging': 4, '23': 4, 'perform': 4, '14': 4, 'most': 4, 'step': 4, 'employ': 4, 'connections': 4, 'outputs': 4, 'similar': 4, 'query': 4, 'compatibility': 4, 'dv': 4, 'products': 4, 'apply': 4, 'into': 4, 'matrix': 4, 'heads': 4, 'dimensionality': 4, 'same': 4, 'another': 4, 'encodings': 4, 'restricted': 4, 'o1': 4, 'many': 4, 'during': 4, 'not': 4, 'rate': 4, '1019': 4, '128': 4, 'quoc': 4, 'le': 4, 'deep': 4, 'shazeer': 3, 'jakob': 3, 'łukasz': 3, 'kaiser': 3, 'simple': 3, 'achieves': 3, '284': 3, 'improving': 3, '410': 3, 'after': 3, 'small': 3, 'fraction': 3, 'shortterm': 3, 'art': 3, '29': 3, 'equal': 3, 'replacing': 3, 'evaluate': 3, 'every': 3, 'nearly': 3, 'codebase': 3, 'various': 3, 'symbol': 3, 'examples': 3, 'lengths': 3, '18': 3, '26': 3, 'performance': 3, 'without': 3, 'their': 3, '22': 3, 'allows': 3, 'bytenet': 3, 'linearly': 3, 'reduced': 3, 'constant': 3, 'convolution': 3, 'then': 3, 'm': 3, '9': 3, 'when': 3, 'fully': 3, 'connected': 3, 'both': 3, 'respectively': 3, 'stacks': 3, 'composed': 3, 'around': 3, 'normalization': 3, 'addition': 3, 'prevent': 3, 'only': 3, 'value': 3, 'consists': 3, 'together': 3, 'q': 3, 'commonly': 3, 'additive': 3, 'faster': 3, 'similarly': 3, 'outperforms': 3, 'product': 3, '64': 3, 'total': 3, 'three': 3, 'come': 3, 'contains': 3, 'allow': 3, 'kernel': 3, 'no': 3, 'types': 3, 'longrange': 3, 'paths': 3, 'bytepair': 3, 'plan': 3, 'separable': 3, 'even': 3, 'sentences': 3, 'pdrop': 3, 'ensemble': 3, 'beam': 3, 'development': 3, 'rows': 3, '255': 3, '254': 3, 'c': 3, '1024': 3, 'observe': 3, 'geoffrey': 3, 'hinton': 3, 'kyunghyun': 3, 'cho': 3, 'luong': 3, 'alex': 3, 'pages': 3, 'ilya': 3, 'sutskever': 3, 'advances': 3, 'need': 2, 'ashish': 2, 'niki': 2, 'uszkoreit': 2, 'llion': 2, 'aidan': 2, 'illia': 2, 'performing': 2, 'connect': 2, 'through': 2, 'propose': 2, 'being': 2, 'eight': 2, 'costs': 2, 'literature': 2, 'gated': 2, 'particular': 2, 'approaches': 2, 'problems': 2, '21': 2, '13': 2, 'random': 2, 'proposed': 2, 'rnns': 2, 'designed': 2, 'involved': 2, 'countless': 2, 'variants': 2, 'tensor2tensor': 2, 'experimented': 2, 'was': 2, 'inference': 2, 'performed': 2, 'nips': 2, 'they': 2, 'parallelization': 2, 'longer': 2, 'batching': 2, 'across': 2, 'factorization': 2, 'tricks': 2, 'distance': 2, 'relying': 2, 'hours': 2, 'reducing': 2, 'gpu': 2, 'required': 2, 'signals': 2, 'effective': 2, 'due': 2, 'effect': 2, 'counteract': 2, 'reading': 2, 'abstractive': 2, 'summarization': 2, '19': 2, 'endtoend': 2, 'aligned': 2, 'shown': 2, 'well': 2, 'its': 2, 'discuss': 2, 'competitive': 2, 'structure': 2, 'x1x': 2, 'z': 2, 'z1z': 2, 'symbols': 2, 'autoregressive': 2, 'generating': 2, 'pointwise': 2, 'left': 2, 'right': 2, 'followed': 2, 'sublayerx': 2, 'third': 2, 'masking': 2, 'offset': 2, 'mapping': 2, 'vectors': 2, 'computed': 2, 'weight': 2, 'weights': 2, 'practice': 2, 'packed': 2, 'functions': 2, 'scaling': 2, '1dk': 2, 'code': 2, 'larger': 2, '322': 2, 'found': 2, 'beneﬁcial': 2, 'projections': 2, 'dimensions': 2, 'projected': 2, 'versions': 2, 'jointly': 2, 'head': 2, 'why': 2, 'components': 2, 'mean': 2, '0': 2, 'variance': 2, 'rdmodeldk': 2, 'h': 2, 'singlehead': 2, 'ways': 2, 'typical': 2, 'ﬂow': 2, 'setting': 2, 'see': 2, '33': 2, 'transformations': 2, 'way': 2, 'dff': 2, '2048': 2, 'convert': 2, '24': 2, 'those': 2, 'relative': 2, 'minimum': 2, 'neighborhood': 2, 'per': 2, 'ologkn': 2, 'onr': 2, 'so': 2, 'ﬁxed': 2, 'corresponds': 2, 'chose': 2, 'because': 2, 'would': 2, 'row': 2, 'sinusoidal': 2, 'may': 2, 'compare': 2, 'convolu': 2, 'tional': 2, 'amount': 2, 'combination': 2, 'requires': 2, 'wordpiece': 2, '25': 2, 'improve': 2, 'involving': 2, 'very': 2, 'could': 2, 'investigate': 2, 'approach': 2, 'further': 2, 'future': 2, 'increasing': 2, 'present': 2, 'dataset': 2, 'consisting': 2, 'source': 2, 'target': 2, 'vocabulary': 2, 'englishfrench': 2, '25000': 2, 'hyperparameters': 2, 'took': 2, 'seconds': 2, 'bottom': 2, 'line': 2, '53': 2, 'optimizer': 2, 'adam': 2, '17': 2, '09': 2, 'varied': 2, 'according': 2, 'regularization': 2, '27': 2, 'better': 2, 'ende': 2, 'enfr': 2, 'deepatt': 2, 'posunk': 2, 'gnmt': 2, 'rl': 2, '246': 2, '1018': 2, '80': 2, '1021': 2, 'label': 2, 'smoothing': 2, 'ϵls': 2, '30': 2, 'hurts': 2, 'reported': 2, 'listed': 2, 'published': 2, '03': 2, 'last': 2, 'checkpoints': 2, 'search': 2, '50': 2, 'estimate': 2, 'variations': 2, 'newstest2013': 2, '60': 2, 'perplexities': 2, 'dev': 2, '492': 2, '258': 2, '501': 2, 'b': 2, '253': 2, '4096': 2, '00': 2, '02': 2, '257': 2, 'overﬁtting': 2, 'replace': 2, 'nal': 2, 'kalchbrenner': 2, 'jimmy': 2, 'ba': 2, 'minhthang': 2, 'li': 2, 'van': 2, 'empirical': 2, 'graves': 2, 'recognition': 2, 'computer': 2, 'vision': 2, 'sepp': 2, 'hochreiter': 2, 'jürgen': 2, 'schmidhuber': 2, 'oriol': 2, 'vinyals': 2, 'mike': 2, 'schuster': 2, 'yonghui': 2, 'wu': 2, 'international': 2, 'iclr': 2, 'structured': 2, 'zhou': 2, 'macherey': 2, 'cao': 2, 'you': 1, 'vaswani': 1, 'avaswanigooglecom': 1, 'noamgooglecom': 1, 'parmar': 1, 'nikipgooglecom': 1, 'uszgooglecom': 1, 'jones': 1, 'lliongooglecom': 1, 'gomez': 1, 'university': 1, 'toronto': 1, 'aidancstorontoedu': 1, 'lukaszkaisergooglecom': 1, 'polosukhin': 1, 'illiapolosukhingmailcom': 1, 'abstract': 1, 'dominant': 1, 'complex': 1, 'include': 1, 'solely': 1, 'dispensing': 1, 'experiments': 1, 'show': 1, 'superior': 1, 'parallelizable': 1, 'requiring': 1, 'english': 1, 'togerman': 1, 'existing': 1, 'establishes': 1, 'singlemodel': 1, 'introduction': 1, 'ﬁrmly': 1, 'established': 1, 'numerous': 1, 'efforts': 1, 'continued': 1, 'push': 1, 'boundaries': 1, 'contribution': 1, 'listing': 1, 'started': 1, 'effort': 1, 'idea': 1, 'crucially': 1, 'aspect': 1, 'parameterfree': 1, 'became': 1, 'person': 1, 'detail': 1, 'tuned': 1, 'evaluated': 1, 'original': 1, 'novel': 1, 'responsible': 1, 'initial': 1, 'efﬁcient': 1, 'visualizations': 1, 'lukasz': 1, 'spent': 1, 'designing': 1, 'parts': 1, 'implementing': 1, 'earlier': 1, 'greatly': 1, 'massively': 1, 'accelerating': 1, '31st': 1, 'beach': 1, 'ca': 1, 'usa': 1, 'typically': 1, 'along': 1, 'aligning': 1, 'generate': 1, 'states': 1, 'ht': 1, 'ht1': 1, 't': 1, 'inherently': 1, 'nature': 1, 'precludes': 1, 'within': 1, 'becomes': 1, 'critical': 1, 'constraints': 1, 'limit': 1, 'recent': 1, 'achieved': 1, 'signiﬁcant': 1, 'improvements': 1, 'efﬁciency': 1, 'conditional': 1, 'latter': 1, 'fundamental': 1, 'constraint': 1, 'remains': 1, 'become': 1, 'integral': 1, 'part': 1, 'compelling': 1, 'transduc': 1, 'tion': 1, 'allowing': 1, 'regard': 1, 'few': 1, 'cases': 1, 'conjunction': 1, 'eschewing': 1, 'draw': 1, 'global': 1, 'reach': 1, 'little': 1, 'twelve': 1, 'background': 1, 'goal': 1, 'forms': 1, 'foundation': 1, 'extended': 1, 'basic': 1, 'building': 1, 'block': 1, 'computing': 1, 'relate': 1, 'arbitrary': 1, 'grows': 1, 'logarithmically': 1, 'makes': 1, 'difﬁcult': 1, 'distant': 1, 'albeit': 1, 'resolution': 1, 'attentionweighted': 1, 'sometimes': 1, 'called': 1, 'intraattention': 1, 'relating': 1, 'successfully': 1, 'variety': 1, 'comprehension': 1, 'textual': 1, 'entailment': 1, 'taskindependent': 1, 'simplelanguage': 1, 'question': 1, 'answering': 1, 'knowledge': 1, 'following': 1, 'sections': 1, 'will': 1, 'describe': 1, 'motivate': 1, 'advantages': 1, 'here': 1, 'maps': 1, 'continuous': 1, 'given': 1, 'generates': 1, 'y1y': 1, 'element': 1, 'consuming': 1, 'generated': 1, 'additional': 1, 'next': 1, 'follows': 1, 'overall': 1, 'stacked': 1, 'halves': 1, 'second': 1, 'wise': 1, 'connection': 1, 'layernormx': 1, 'itself': 1, 'facilitate': 1, 'produce': 1, '6identical': 1, 'inserts': 1, 'performs': 1, 'modify': 1, 'attending': 1, 'subsequent': 1, 'combined': 1, 'fact': 1, 'ensures': 1, 'predictions': 1, 'ican': 1, 'depend': 1, 'known': 1, 'keyvalue': 1, 'weighted': 1, 'sum': 1, 'assigned': 1, 'corresponding': 1, '321': 1, 'call': 1, 'several': 1, 'running': 1, 'divide': 1, 'obtain': 1, 'simultaneously': 1, 'matrices': 1, 'kand': 1, 'attentionqkv': 1, 'softmaxqkt': 1, 'multi': 1, 'plicative': 1, 'algorithm': 1, 'except': 1, 'computes': 1, 'theoretical': 1, 'much': 1, 'spaceefﬁcient': 1, 'highly': 1, 'optimized': 1, 'multiplication': 1, 'suspect': 1, 'grow': 1, 'magnitude': 1, 'pushing': 1, 'regions': 1, 'extremely': 1, 'gradients': 1, 'scale': 1, 'dmodeldimensional': 1, 'project': 1, 'htimes': 1, 'yielding': 1, 'dvdimensional': 1, 'concatenated': 1, 'once': 1, 'again': 1, 'resulting': 1, 'ﬁnal': 1, 'depicted': 1, 'subspaces': 1, 'inhibits': 1, '4to': 1, 'illustrate': 1, 'get': 1, 'assume': 1, 'independent': 1, 'variables': 1, 'i1': 1, 'qiki': 1, 'multiheadqkv': 1, 'concathead1': 1, 'headhwo': 1, 'headi': 1, 'attentionqwq': 1, 'kw': 1, 'vw': 1, 'parameter': 1, 'matriceswq': 1, 'wk': 1, 'wv': 1, 'rdmodeldv': 1, 'wo': 1, 'rhdvdmodel': 1, 'dmodelh': 1, 'full': 1, '323': 1, 'applications': 1, 'uses': 1, 'mimics': 1, 'sequencetosequence': 1, 'place': 1, 'up': 1, 'leftward': 1, 'preserve': 1, 'property': 1, 'implement': 1, 'inside': 1, 'out': 1, 'correspond': 1, 'illegal': 1, 'positionwise': 1, 'applied': 1, 'separately': 1, 'identically': 1, 'relu': 1, 'activation': 1, 'ffnx': 1, 'max0xw1': 1, 'b1w2': 1, 'b2': 1, 'parameters': 1, 'describing': 1, 'innerlayer': 1, '34': 1, 'usual': 1, 'transfor': 1, 'mation': 1, 'predicted': 1, 'nexttoken': 1, 'probabilities': 1, 'share': 1, 'presoftmax': 1, 'transformation': 1, 'multiply': 1, 'make': 1, 'must': 1, 'inject': 1, 'some': 1, 'absolute': 1, 'end': 1, 'add': 1, 'perlayer': 1, 'nis': 1, 'dis': 1, 'kis': 1, 'rthe': 1, 'type': 1, 'on2': 1, 'ond2': 1, 'oknd2': 1, 'ornd': 1, 'bottoms': 1, 'summed': 1, 'there': 1, 'choices': 1, 'sine': 1, 'cosine': 1, 'frequencies': 1, 'pepos2i': 1, 'sinpos100002idmodel': 1, 'pepos2i1': 1, 'cospos100002idmodel': 1, 'posis': 1, 'iis': 1, 'sinusoid': 1, 'wavelengths': 1, 'form': 1, 'geometric': 1, 'progression': 1, '2πto': 1, '10000': 1, '2π': 1, 'hypothesized': 1, 'easily': 1, 'peposk': 1, 'represented': 1, 'pepos': 1, 'produced': 1, 'version': 1, 'extrapolate': 1, 'ones': 1, 'encountered': 1, 'aspects': 1, 'variablelength': 1, 'xizi': 1, 'rd': 1, 'motivating': 1, 'consider': 1, 'desiderata': 1, 'parallelized': 1, 'measured': 1, 'challenge': 1, 'affecting': 1, 'ability': 1, 'forward': 1, 'backward': 1, 'traverse': 1, 'shorter': 1, 'easier': 1, 'hence': 1, 'noted': 1, 'connects': 1, 'sequentially': 1, 'executed': 1, 'whereas': 1, 'terms': 1, 'smaller': 1, 'often': 1, 'translations': 1, 'considering': 1, 'rin': 1, 'centered': 1, 'respective': 1, 'increase': 1, 'width': 1, 'kn': 1, 'does': 1, 'doing': 1, 'onk': 1, 'contiguous': 1, 'kernels': 1, 'dilated': 1, 'longest': 1, 'generally': 1, 'expensive': 1, 'decrease': 1, 'considerably': 1, 'oknd': 1, 'nd2': 1, 'take': 1, 'side': 1, 'beneﬁt': 1, 'yield': 1, 'interpretable': 1, 'inspect': 1, 'distributions': 1, 'appendix': 1, 'do': 1, 'individual': 1, 'clearly': 1, 'appear': 1, 'exhibit': 1, 'behavior': 1, 'related': 1, 'syntactic': 1, 'semantic': 1, 'describes': 1, 'regime': 1, '51': 1, 'data': 1, 'standard': 1, 'englishgerman': 1, '45': 1, 'million': 1, 'encoded': 1, 'shared': 1, '37000': 1, '36m': 1, 'split': 1, '32000': 1, 'batched': 1, 'approximate': 1, 'batch': 1, 'contained': 1, 'containing': 1, 'approximately': 1, '52': 1, 'hardware': 1, 'schedule': 1, 'nvidia': 1, 'throughout': 1, 'paper': 1, '04': 1, '100000': 1, 'modelsdescribed': 1, '300000': 1, 'β1': 1, 'β2': 1, '098': 1, 'ϵ': 1, '109': 1, 'course': 1, 'formula': 1, 'lrate': 1, 'd05': 1, 'minstep_num05step_numwarmup_steps15': 1, 'warmup_stepstraining': 1, 'decreasing': 1, 'thereafter': 1, 'proportionally': 1, 'inverse': 1, 'square': 1, 'root': 1, 'warmup_steps': 1, '4000': 1, '54': 1, 'before': 1, 'added': 1, 'normalized': 1, 'sums': 1, 'scores': 1, 'newstest2014': 1, 'tests': 1, 'flops': 1, '2375': 1, '392': 1, '3992': 1, '2516': 1, '4046': 1, '96': 1, 'moe': 1, '2603': 1, '4056': 1, '404': 1, '2630': 1, '4116': 1, '2636': 1, '4129': 1, '77': 1, '273': 1, '381': 1, 'employed': 1, 'perplexity': 1, 'learns': 1, 'unsure': 1, 'improves': 1, 'accuracy': 1, '61': 1, 'establishing': 1, 'conﬁguration': 1, 'surpasses': 1, 'outperforming': 1, 'obtained': 1, 'written': 1, '10minute': 1, 'intervals': 1, 'averaged': 1, 'penalty': 1, 'α': 1, '06': 1, 'chosen': 1, 'experimentation': 1, 'terminate': 1, 'early': 1, 'possible': 1, 'summarizes': 1, 'compares': 1, 'ﬂoating': 1, 'point': 1, 'multiplying': 1, 'sustained': 1, 'singleprecision': 1, 'ﬂoatingpoint': 1, 'capacity': 1, '62': 1, 'importance': 1, 'measuring': 1, 'change': 1, 'checkpoint': 1, 'vary': 1, 'keeping': 1, 'worse': 1, 'drops': 1, 'off': 1, 'too': 1, '5we': 1, '37': 1, '95': 1, 'tflops': 1, 'k80': 1, 'k40': 1, 'm40': 1, 'unlisted': 1, 'metrics': 1, 'perwordpiece': 1, 'should': 1, 'compared': 1, 'perword': 1, 'ppl': 1, 'params': 1, '106': 1, '100k': 1, '65': 1, '529': 1, '249': 1, '500': 1, '491': 1, '516': 1, '251': 1, '58': 1, '611': 1, '237': 1, '36': 1, '519': 1, '488': 1, '256': 1, '575': 1, '245': 1, '466': 1, '260': 1, '168': 1, '475': 1, '262': 1, '90': 1, '577': 1, '495': 1, '467': 1, '547': 1, 'sinusoids': 1, '300k': 1, '433': 1, '264': 1, '213': 1, 'suggests': 1, 'determining': 1, 'easy': 1, 'sophisticated': 1, 'expected': 1, 'bigger': 1, 'helpful': 1, 'avoiding': 1, 'conclusion': 1, 'presented': 1, 'multiheaded': 1, 'achieve': 1, 'former': 1, 'excited': 1, 'attentionbased': 1, 'them': 1, 'extend': 1, 'modalities': 1, 'text': 1, 'local': 1, 'efﬁciently': 1, 'handle': 1, 'inputs': 1, 'images': 1, 'audio': 1, 'video': 1, 'making': 1, 'generation': 1, 'goals': 1, 'ours': 1, 'available': 1, 'httpsgithubcom': 1, 'tensorflowtensor2tensor': 1, 'acknowledgements': 1, 'grateful': 1, 'stephan': 1, 'gouws': 1, 'fruitful': 1, 'comments': 1, 'corrections': 1, 'inspiration': 1, 'references': 1, 'lei': 1, 'jamie': 1, 'ryan': 1, 'kiros': 1, 'arxiv160706450': 1, 'dzmitry': 1, 'bahdanau': 1, 'align': 1, 'translate': 1, 'abs14090473': 1, 'denny': 1, 'britz': 1, 'anna': 1, 'goldie': 1, 'massive': 1, 'exploration': 1, 'abs170303906': 1, 'jianpeng': 1, 'cheng': 1, 'dong': 1, 'mirella': 1, 'lapata': 1, 'memorynetworks': 1, 'arxiv160106733': 1, 'bart': 1, 'merrienboer': 1, 'caglar': 1, 'gulcehre': 1, 'fethi': 1, 'bougares': 1, 'holger': 1, 'schwenk': 1, 'phrase': 1, 'rnn': 1, 'statistical': 1, 'abs14061078': 1, 'francois': 1, 'chollet': 1, 'xception': 1, 'depthwise': 1, 'arxiv161002357': 1, 'junyoung': 1, 'chung': 1, 'çaglar': 1, 'gülçehre': 1, 'evaluation': 1, 'abs14123555': 1, 'jonas': 1, 'gehring': 1, 'michael': 1, 'auli': 1, 'david': 1, 'grangier': 1, 'denis': 1, 'yarats': 1, 'yann': 1, 'dauphin': 1, 'arxiv170503122v2': 1, 'arxiv13080850': 1, '2013': 1, 'kaiming': 1, 'he': 1, 'xiangyu': 1, 'zhang': 1, 'shaoqing': 1, 'ren': 1, 'jian': 1, 'sun': 1, 'im': 1, 'age': 1, 'proceedings': 1, 'ieee': 1, 'pattern': 1, '770778': 1, 'paolo': 1, 'frasconi': 1, 'gradient': 1, 'nets': 1, 'difﬁculty': 1, 'longterm': 1, '2001': 1, '9817351780': 1, '1997': 1, 'rafal': 1, 'jozefowicz': 1, 'exploring': 1, 'limits': 1, 'arxiv160202410': 1, 'algorithms': 1, 'lasse': 1, 'espeholt': 1, 'karen': 1, 'simonyan': 1, 'aaron': 1, 'den': 1, 'oord': 1, 'ko': 1, 'ray': 1, 'kavukcuoglu': 1, 'timearxiv': 1, 'arxiv161010099v2': 1, 'yoon': 1, 'kim': 1, 'carl': 1, 'denton': 1, 'hoang': 1, 'alexander': 1, 'rush': 1, 'diederik': 1, 'kingma': 1, 'method': 1, 'stochastic': 1, 'optimization': 1, 'oleksii': 1, 'kuchaiev': 1, 'boris': 1, 'ginsburg': 1, 'lstm': 1, 'arxiv170310722': 1, 'zhouhan': 1, 'lin': 1, 'minwei': 1, 'feng': 1, 'cicero': 1, 'nogueira': 1, 'dos': 1, 'santos': 1, 'mo': 1, 'yu': 1, 'bing': 1, 'xiang': 1, 'bowen': 1, 'selfattentive': 1, 'arxiv170303130': 1, 'samy': 1, 'active': 1, 'hieu': 1, 'pham': 1, 'christopher': 1, 'manning': 1, 'arxiv150804025': 1, 'ankur': 1, 'parikh': 1, 'oscar': 1, 'täckström': 1, 'dipanjan': 1, 'das': 1, 'decomposable': 1, 'methods': 1, 'natural': 1, 'romain': 1, 'paulus': 1, 'caiming': 1, 'xiong': 1, 'richard': 1, 'socher': 1, 'reinforced': 1, 'arxiv170504304': 1, 'oﬁr': 1, 'press': 1, 'lior': 1, 'wolf': 1, 'arxiv160805859': 1, 'rico': 1, 'sennrich': 1, 'barry': 1, 'haddow': 1, 'alexandra': 1, 'birch': 1, 'rare': 1, 'words': 1, 'subword': 1, 'units': 1, 'arxiv150807909': 1, 'azalia': 1, 'mirhoseini': 1, 'krzysztof': 1, 'maziarz': 1, 'andy': 1, 'davis': 1, 'jeff': 1, 'dean': 1, 'outrageously': 1, 'sparselygated': 1, 'mixtureofexperts': 1, 'arxiv170106538': 1, 'nitish': 1, 'srivastava': 1, 'krizhevsky': 1, 'ruslan': 1, 'salakhutdi': 1, 'nov': 1, 'journal': 1, '15119291958': 1, 'sainbayar': 1, 'sukhbaatar': 1, 'arthur': 1, 'szlam': 1, 'jason': 1, 'weston': 1, 'rob': 1, 'fergus': 1, 'cortes': 1, 'lawrence': 1, 'lee': 1, 'sugiyama': 1, 'r': 1, 'garnett': 1, 'editors': 1, '24402448': 1, 'curran': 1, 'associates': 1, 'inc': 1, 'vv': 1, '31043112': 1, 'christian': 1, 'szegedy': 1, 'vincent': 1, 'vanhoucke': 1, 'sergey': 1, 'ioffe': 1, 'jonathon': 1, 'shlens': 1, 'zbigniew': 1, 'wojna': 1, 'rethinking': 1, 'inception': 1, 'abs151200567': 1, 'zhifeng': 1, 'chen': 1, 'mohammad': 1, 'norouzi': 1, 'wolfgang': 1, 'maxim': 1, 'krikun': 1, 'yuan': 1, 'qin': 1, 'gao': 1, 'klaus': 1, 'et': 1, 'al': 1, 'googles': 1, 'system': 1, 'bridging': 1, 'gap': 1, 'human': 1, 'arxiv160908144': 1, 'jie': 1, 'ying': 1, 'xuguang': 1, 'wang': 1, 'peng': 1, 'wei': 1, 'xu': 1, 'fastforward': 1, 'abs160604199': 1})}, {'file_name': 'chexnet', 'word_counts': Counter({'the': 209, 'of': 130, 'and': 95, 'in': 61, 'a': 54, 'we': 53, 'on': 48, 'to': 43, 'et': 41, 'al': 41, 'chest': 39, 'chexnet': 38, 'pneumonia': 38, 'with': 36, 'for': 33, '2017': 30, 'that': 28, 'is': 26, 'radiologists': 22, 'model': 20, 'detection': 19, 'deep': 17, 'xrays': 16, 'images': 16, 'image': 16, 'radiologist': 16, 'from': 15, '14': 15, 'f1': 15, 'pathology': 15, 'learning': 14, 'radiology': 13, 'by': 13, '1': 12, 'network': 12, 'chestxray14': 12, 'an': 11, 'dataset': 11, 'xray': 11, 'performance': 11, 'wang': 11, 'diagnosis': 11, 'patient': 11, 'c': 11, 'diseases': 10, 'as': 10, 'using': 10, 'each': 10, 'at': 9, 'convolutional': 9, 'test': 9, 'set': 9, 'which': 9, 'average': 9, 'score': 9, '95': 9, '2016': 9, 'training': 9, 'neural': 8, 'all': 8, 'ci': 8, 'previous': 8, 'imaging': 8, 'classiﬁcation': 8, 'radiologistlevel': 7, 'can': 7, 'practicing': 7, 'ﬁnd': 7, 'state': 7, 'art': 7, 'than': 7, 'are': 7, 'task': 7, 'correctly': 7, 'also': 7, 'most': 7, 'label': 7, 'one': 7, 'labels': 7, 'per': 7, 'radiographs': 7, 'lung': 7, 'journal': 7, 'arxiv': 7, 'preprint': 7, 'y': 6, 'level': 6, 'pneu': 6, 'disease': 6, 'work': 6, 'output': 6, 'probability': 6, 'layer': 6, 'use': 6, '4': 6, '0387': 6, 'bootstrap': 6, 'be': 6, 'not': 6, 'binary': 6, 'networks': 6, 'nodule': 6, 'lower': 6, 'interpretation': 6, 'm': 6, 'pp': 6, 'algorithm': 5, 'detect': 5, 'trained': 5, 'x': 5, 'ra': 5, 'monia': 5, 'this': 5, '2': 5, 'diﬀerence': 5, 'huang': 5, '2015': 5, '2012': 5, 'ing': 5, 'have': 5, 'access': 5, 'or': 5, 'ﬁnal': 5, 'has': 5, 'data': 5, 'pathologies': 5, 'into': 5, 'yao': 5, 'were': 5, 'class': 5, 'p': 4, 'ex': 4, 'our': 4, '121layer': 4, 'available': 4, 'four': 4, 'compare': 4, 'who': 4, '2001': 4, 'pneumo': 4, 'nia': 4, 'university': 4, 'figure': 4, 'detects': 4, 'areas': 4, 'frontalview': 4, 'up': 4, 'thoracic': 4, 'including': 4, '0435': 4, '0442': 4, 'higher': 4, 'other': 4, 'radi': 4, '420': 4, 'results': 4, 'would': 4, 'where': 4, 'presence': 4, 'loss': 4, 'w': 4, 'fully': 4, 'connected': 4, 'imagenet': 4, '2009': 4, 'validation': 4, 'patients': 4, 'there': 4, 'between': 4, 'mass': 4, 'emphysema': 4, 'pleural': 4, 'auroc': 4, 'comparison': 4, 'scores': 4, 'formance': 4, 'raoof': 4, 'left': 4, 'right': 4, 'e': 4, 'classes': 4, 'feature': 4, 'map': 4, 'k': 4, 'medical': 4, '2010': 4, 'world': 4, 'li': 4, 'conference': 4, 'pranav': 3, 'rajpurkar': 3, 'andrew': 3, 'publicly': 3, 'ray': 3, 'frontal': 3, 'best': 3, 'ical': 3, 'care': 3, 'detecting': 3, 'computer': 3, 'de': 3, 'medicine': 3, 'positive': 3, 'input': 3, 'outputs': 3, 'example': 3, 'shown': 3, 'released': 3, 'contains': 3, '0330': 3, '0481': 3, 'table': 3, 'achieves': 3, 'make': 3, 'overlap': 3, 'estimate': 3, 'against': 3, 'automated': 3, 'clinical': 3, 'it': 3, 'health': 3, '0x': 3, 'making': 3, 'after': 3, 'weights': 3, 'standard': 3, 'diﬀerent': 3, 'cardiomegaly': 3, 'eﬀusion': 3, 'inﬁltration': 3, 'consolidation': 3, 'both': 3, 'con': 3, 'does': 3, 'history': 3, 'potchen': 3, '1979': 3, 'pulmonary': 3, 'b': 3, 'identiﬁes': 3, 'lobe': 3, 'large': 3, 'maps': 3, 'predicted': 3, 'pyc': 3, 'cams': 3, 'mc': 3, 'features': 3, 'used': 3, 'cancer': 3, 'mortality': 3, 'mollura': 3, 'communityacquired': 3, 'david': 3, 'marc': 3, 'j': 3, 'american': 3, 'ieee': 3, 't': 3, 'peng': 3, 'international': 3, 'localization': 3, 'radaid': 3, 'developing': 3, 'jeremy': 2, 'irvin': 2, 'develop': 2, 'ceeding': 2, 'algo': 2, 'largest': 2, 'over': 2, 'view': 2, 'academic': 2, 'annotate': 2, 'perfor': 2, 'mance': 2, 'exceeds': 2, 'diologist': 2, 'metric': 2, 'extend': 2, 're': 2, 'more': 2, 'year': 2, 'us': 2, 'cdc': 2, 'method': 2, 'clin': 2, 'franquet': 2, 'epidemiological': 2, 'studies': 2, 'cherian': 2, '2005': 2, 'however': 2, 'expert': 2, 'depart': 2, 'ment': 2, 'net': 2, 'localizes': 2, 'indicative': 2, 'along': 2, 'train': 2, 'recently': 2, '112120': 2, '25': 2, '3': 2, 'recall': 2, 'statistically': 2, 'dense': 2, 'batch': 2, 'nor': 2, 'ioﬀe': 2, 'szegedy': 2, 'such': 2, 'tractable': 2, 'radiography': 2, 'abnormal': 2, 'these': 2, 'among': 2, 'neuman': 2, 'davies': 2, '1996': 2, 'hopstaken': 2, '2004': 2, 'ologists': 2, 'simple': 2, 'outperform': 2, 'published': 2, 'only': 2, 'delivery': 2, 'diagnostic': 2, 'problem': 2, 'absence': 2, 'respectively': 2, 'single': 2, 'optimize': 2, 'weighted': 2, 'cross': 2, 'entropy': 2, 'lxy': 2, 'py': 2, '1x': 2, 'i': 2, 'cases': 2, 'negative': 2, 'improve': 2, 'optimization': 2, 'replace': 2, 'apply': 2, 'sigmoid': 2, 'nonlinearity': 2, 'pretrained': 2, 'deng': 2, 'adam': 2, 'kingma': 2, 'ba': 2, '2014': 2, '10': 2, 'examples': 2, 'randomly': 2, 'split': 2, 'no': 2, 'sets': 2, 'mean': 2, 'pneumothorax': 2, '0708': 2, 'edema': 2, 'fibrosis': 2, '0767': 2, 'thickening': 2, 'hernia': 2, '2chexnet': 2, 'outperforms': 2, '005': 2, 'stanford': 2, '7': 2, 'ence': 2, 'vs': 2, 'report': 2, 'samples': 2, 'signiﬁcantly': 2, 'conclude': 2, 'signiﬁcant': 2, 'contain': 2, 'limitations': 2, 'identify': 2, 'three': 2, 'first': 2, 'been': 2, 'accurate': 2, 'third': 2, 'berbaum': 2, '1985': 2, 'two': 2, 'masses': 2, 'd': 2, 'heart': 2, 'enlarged': 2, 'activation': 2, 'important': 2, 'following': 2, 'sum': 2, 'yc': 2, '13': 2, 'increase': 2, 'predictions': 2, 'zhou': 2, 'let': 2, 'leading': 2, 'its': 2, 'prediction': 2, 'datasets': 2, 'algorithms': 2, 'variety': 2, 'diabetic': 2, 'retinopathy': 2, 'gulshan': 2, 'skin': 2, 'tion': 2, 'esteva': 2, 'arrhythmia': 2, 'hemorrhage': 2, 'identiﬁcation': 2, 'grewal': 2, 'tuberculosis': 2, 'lakhani': 2, 'islam': 2, 'ab': 2, 'demnerfushman': 2, 'order': 2, 'gon': 2, 'calvespereira': 2, '2013': 2, 'early': 2, 'critical': 2, 'aydogdu': 2, 'common': 2, 'global': 2, 'organization': 2, 'experts': 2, 'kesselman': 2, 'eﬀect': 2, 'pediatric': 2, 'thomas': 2, 'mar': 2, 'infections': 2, 'children': 2, 'infectious': 2, 'richard': 2, 'database': 2, 'vision': 2, 'pattern': 2, 'recognition': 2, 'brett': 2, 'respiratory': 2, 'ao': 2, 'ct': 2, 'van': 2, 'lee': 2, 'chen': 2, 'daniel': 2, 'countries': 2, 'sarah': 2, 's': 2, 'edward': 2, 'lu': 2, 'kaylie': 1, 'zhu1': 1, 'brandon': 1, 'yang1': 1, 'hershel': 1, 'mehta1': 1, 'tony': 1, 'duan1': 1, 'daisy': 1, 'ding1': 1, 'aarti': 1, 'bagul1': 1, 'robyn': 1, 'l': 1, 'ball2': 1, 'curtis': 1, 'langlotz3': 1, 'katie': 1, 'shpanskaya3': 1, 'matthew': 1, 'lungren3': 1, 'ng1': 1, 'abstract': 1, 'rithm': 1, 'cur': 1, 'rently': 1, 'containing': 1, '100000': 1, 'achieve': 1, 'sults': 1, 'introduction': 1, 'million': 1, 'adults': 1, 'hospitalized': 1, 'around': 1, '50000': 1, 'die': 1, 'every': 1, 'alone': 1, 'currently': 1, 'diagnosing': 1, 'playing': 1, 'crucial': 1, 'role': 1, 'challenging': 1, 'relies': 1, 'availability': 1, 'present': 1, 'automatically': 1, 'exceeding': 1, 'equal': 1, 'contribution': 1, '1stanford': 1, 'science': 1, '2stanford': 1, 'partment': 1, '3stanford': 1, 'correspondence': 1, 'pranavsrcsstanfordedu': 1, 'jirvin16csstanfordedu': 1, 'project': 1, 'website': 1, 'httpsstanfordmlgroup': 1, 'githubioprojectschexnet': 1, '85': 1, 'cnn': 1, '1chexnet': 1, 'takes': 1, '121': 1, 'inputs': 1, 'heatmap': 1, 'localizing': 1, 'im': 1, 'age': 1, 'individually': 1, 'labeled': 1, 'diﬀer': 1, 'ent': 1, 'arxiv171105225v3': 1, 'cscv': 1, 'dec': 1, '0383': 1, '0309': 1, '0453': 1, '0356': 1, '0282': 1, '0428': 1, '0365': 1, '0291': 1, '0390': 1, '0492': 1, 'avg': 1, '1we': 1, 'harmonic': 1, 'precision': 1, 'models': 1, 'sig': 1, 'niﬁcant': 1, 'connections': 1, 'malization': 1, 'opti': 1, 'mization': 1, 'diﬃ': 1, 'cult': 1, 'appearance': 1, 'often': 1, 'vague': 1, 'di': 1, 'agnoses': 1, 'mimic': 1, 'many': 1, 'benign': 1, 'ities': 1, 'discrepancies': 1, 'cause': 1, 'considerable': 1, 'variabil': 1, 'ity': 1, 'collect': 1, 'annotations': 1, 'subset': 1, 'measure': 1, 'dividual': 1, 'modiﬁcations': 1, 'tremendous': 1, 'beneﬁt': 1, 'settings': 1, 'invaluable': 1, 'populations': 1, 'inadequate': 1, 'specialists': 1, '21': 1, 'formulation': 1, '01indicating': 1, 'ylog': 1, 'logpy': 1, 'ix': 1, 'assigns': 1, 'npn': 1, 'ppn': 1, 'pand': 1, 'nthe': 1, 'number': 1, '22': 1, 'architecture': 1, 'densenet': 1, 'chestxray': 1, 'densenets': 1, 'ﬂow': 1, 'formation': 1, 'gradients': 1, 'through': 1, 'very': 1, 'initialized': 1, 'endtoend': 1, 'parameters': 1, 'β1': 1, '09': 1, 'β2': 1, '0999': 1, 'mini': 1, 'batches': 1, 'size': 1, '16': 1, 'initial': 1, 'rate': 1, '0001': 1, 'decayed': 1, 'factor': 1, 'time': 1, 'plateaus': 1, 'epoch': 1, 'pick': 1, 'lowest': 1, '31': 1, '30805': 1, 'unique': 1, 'automatic': 1, 'extraction': 1, 'methods': 1, 'reports': 1, 'annotated': 1, 'exam': 1, 'ples': 1, '28744': 1, '98637': 1, '1672': 1, '6351': 1, '389': 1, 'before': 1, 'inputting': 1, 'downscale': 1, '224224': 1, 'normalize': 1, 'based': 1, 'deviation': 1, 'augment': 1, 'random': 1, 'horizontal': 1, 'ﬂipping': 1, 'ours': 1, 'atelectasis': 1, '0716': 1, '0772': 1, '08094': 1, '0807': 1, '0904': 1, '09248': 1, '0784': 1, '0859': 1, '08638': 1, '0609': 1, '0695': 1, '07345': 1, '0706': 1, '0792': 1, '08676': 1, '0671': 1, '0717': 1, '07802': 1, '0633': 1, '0713': 1, '07680': 1, '0806': 1, '0841': 1, '08887': 1, '0788': 1, '07901': 1, '0835': 1, '0882': 1, '08878': 1, '0815': 1, '0829': 1, '09371': 1, '0769': 1, '08047': 1, '0765': 1, '08062': 1, '0914': 1, '09164': 1, 'margin': 1, '32': 1, 'collected': 1, 'notations': 1, 'obtained': 1, 'independently': 1, 'prac': 1, 'ticing': 1, 'asked': 1, 'had': 1, '28': 1, 'years': 1, 'experi': 1, 'subspecialty': 1, 'fel': 1, 'lowship': 1, 'did': 1, 'any': 1, 'information': 1, 'knowl': 1, 'edge': 1, 'prevalence': 1, 'entered': 1, 'standardized': 1, 'entry': 1, 'program': 1, '41': 1, 'assess': 1, 'compute': 1, 'individual': 1, 'ground': 1, 'truth': 1, 'resulting': 1, 'ologist': 1, 'across': 1, 'struct': 1, 'conﬁdence': 1, 'intervals': 1, 'cis': 1, 'cal': 1, 'culating': 1, '10000': 1, 'sampled': 1, 'replacement': 1, 'take': 1, '25th': 1, '975th': 1, 'percentiles': 1, 'ta': 1, 'ble': 1, 'summarizes': 1, 'determine': 1, 'whether': 1, 'chexnets': 1, 'sta': 1, 'tistically': 1, 'calculate': 1, 'same': 1, 'if': 1, 'include': 1, 'zero': 1, 'was': 1, 'diologists': 1, '0051': 1, '0005': 1, '0084': 1, '0': 1, 'therefore': 1, '42': 1, 'presented': 1, 'during': 1, 'but': 1, '15': 1, 'diagnoses': 1, 'require': 1, 'lateral': 1, 'thus': 1, 'expect': 1, 'setup': 1, 'provides': 1, 'conservative': 1, 'neither': 1, 'radiolo': 1, 'gists': 1, 'permitted': 1, 'decrease': 1, 'interpreting': 1, 'given': 1, 'abnormality': 1, 'fever': 1, 'multifocal': 1, 'com': 1, 'munity': 1, 'acquired': 1, 'airspace': 1, 'lobes': 1, 'arrive': 1, 'clas': 1, 'siﬁes': 1, 'primary': 1, 'ma': 1, 'lignancy': 1, 'upper': 1, 'adjacent': 1, 'mediastinum': 1, 'rightsided': 1, 'mothroax': 1, 'tube': 1, 'predict': 1, 'collapsed': 1, 'ﬂuid': 1, 'space': 1, 'eﬀu': 1, 'sion': 1, 'focuses': 1, 'f': 1, 'congestive': 1, 'failure': 1, 'identi': 1, 'ﬁes': 1, 'cardiac': 1, 'silhouette': 1, 'highlight': 1, 'particular': 1, 'captions': 1, 'provided': 1, 'cough': 1, 'appropriate': 1, 'rather': 1, 'less': 1, 'speciﬁc': 1, 'terms': 1, '5': 1, 'classify': 1, 'multiple': 1, 'changes': 1, 'instead': 1, 'outputting': 1, 'vec': 1, 'tor': 1, 'tof': 1, 'indicating': 1, 'atelec': 1, 'tasis': 1, 'nod': 1, 'ule': 1, 'pneumotho': 1, 'rax': 1, 'second': 1, 'producing': 1, '14dimensional': 1, 'ele': 1, 'mentwise': 1, 'modify': 1, 'function': 1, 'unweighted': 1, 'losses': 1, 'c1': 1, 'log': 1, 'logpyc': 1, '70': 1, 'val': 1, 'idation': 1, '20': 1, 'ensure': 1, 'tween': 1, 'splits': 1, 'perclass': 1, 'held': 1, 'remaining': 1, 'illustrates': 1, 'considerably': 1, '6': 1, 'interpret': 1, 'produce': 1, 'heatmaps': 1, 'visualize': 1, 'dicative': 1, 'mappings': 1, 'generate': 1, 'feed': 1, 'extract': 1, 'fk': 1, 'kth': 1, 'wck': 1, 'weight': 1, 'obtain': 1, 'salient': 1, 'classi': 1, 'fying': 1, 'having': 1, 'taking': 1, 'their': 1, 'associ': 1, 'ated': 1, 'formally': 1, 'wckfk': 1, 'upscal': 1, 'dimensions': 1, 'overlaying': 1, 'shows': 1, 'several': 1, 'well': 1, '14class': 1, 'related': 1, 'recent': 1, 'advancements': 1, 'enabled': 1, 'surpass': 1, 'professionals': 1, 'wide': 1, 'tasks': 1, 'classiﬁca': 1, 'jpurkar': 1, 'ceived': 1, 'increasing': 1, 'attention': 1, 'pul': 1, 'monary': 1, 'sun': 1, 'daram': 1, 'studied': 1, 'various': 1, 'architectures': 1, 'normalities': 1, 'openi': 1, 'magnitude': 1, 'larger': 1, 'kind': 1, 'bench': 1, 'marked': 1, 'archi': 1, 'tectures': 1, 'exploited': 1, 'statistical': 1, 'dependencies': 1, 'la': 1, 'bels': 1, 'outper': 1, 'forming': 1, '8': 1, 'conclusion': 1, 'accounts': 1, 'proportion': 1, 'morbidity': 1, 'treatment': 1, 'preventing': 1, 'complications': 1, 'death': 1, 'approximately': 1, 'billion': 1, 'procedures': 1, 'examination': 1, 'tool': 1, 'practice': 1, 'screening': 1, 'management': 1, 'thirds': 1, 'population': 1, 'lacks': 1, 'diagnostics': 1, 'according': 1, 'shortage': 1, 'terpret': 1, 'even': 1, 'when': 1, 'equipment': 1, 'avail': 1, 'able': 1, 'increased': 1, 'treatable': 1, 'dis': 1, 'eases': 1, 'show': 1, 'extension': 1, 'multi': 1, 'ple': 1, 'automation': 1, 'hope': 1, 'technology': 1, 'healthcare': 1, 'pertise': 1, 'parts': 1, 'skilled': 1, 'limited': 1, '9': 1, 'acknowledgements': 1, 'like': 1, 'acknowledge': 1, 'center': 1, 'artiﬁcial': 1, 'intelligence': 1, 'infrastructure': 1, 'support': 1, 'aimistanford': 1, 'edu': 1, 'references': 1, 'ozyilmaz': 1, 'aksoy': 1, 'handan': 1, 'gursel': 1, 'g': 1, 'ekim': 1, 'numan': 1, 'requiring': 1, 'mechan': 1, 'ventilation': 1, 'values': 1, 'intensive': 1, 'unit': 1, 'severity': 1, 'tuberk': 1, 'toraks': 1, '58125': 1, '34': 1, 'franken': 1, 'jr': 1, 'ea': 1, 'smith': 1, 'wl': 1, 'ﬁlms': 1, 'upon': 1, 'resident': 1, 'interpre': 1, 'tation': 1, 'investigative': 1, '202124128': 1, 'url': 1, 'httpswwwcdcgovfeatures': 1, 'pneumoniaindexhtml': 1, 'mulholland': 1, 'kim': 1, 'carlin': 1, 'john': 1, 'ostensen': 1, 'harald': 1, 'amin': 1, 'ruhul': 1, 'campo': 1, 'garet': 1, 'greenberg': 1, 'lagos': 1, 'rosanna': 1, 'lucero': 1, 'marilla': 1, 'madhi': 1, 'shabir': 1, 'ized': 1, 'paediatric': 1, 'bulletin': 1, '835353359': 1, 'h': 1, 'dele': 1, 'elaine': 1, 'el': 1, 'manson': 1, 'babyn': 1, 'paul': 1, 'shuckett': 1, 'bruce': 1, 'reliability': 1, 'radiograph': 1, 'res': 1, 'piratory': 1, 'young': 1, '157600604': 1, 'dina': 1, 'kohli': 1, 'rosenman': 1, 'shooshan': 1, 'sonya': 1, 'rodriguez': 1, 'laritza': 1, 'antani': 1, 'sameer': 1, 'thoma': 1, 'george': 1, 'r': 1, 'mcdonald': 1, 'clement': 1, 'preparing': 1, 'collection': 1, 'aminations': 1, 'distribution': 1, 'retrieval': 1, 'informatics': 1, 'association': 1, '23': 1, '2304310': 1, 'jia': 1, 'dong': 1, 'wei': 1, 'socher': 1, 'lijia': 1, 'kai': 1, 'feifei': 1, 'largescale': 1, 'hier': 1, 'archical': 1, 'cvpr': 1, 'ference': 1, '248255': 1, 'andre': 1, 'kuprel': 1, 'novoa': 1, 'roberto': 1, 'ko': 1, 'justin': 1, 'swetter': 1, 'susan': 1, 'blau': 1, 'helen': 1, 'thrun': 1, 'sebastian': 1, 'dermatologistlevel': 1, 'nature': 1, '5427639115118': 1, 'trends': 1, 'rithms': 1, 'european': 1, '181196': 1, '208': 1, 'jo': 1, 'concei': 1, 'catarina': 1, 'ovoa': 1, 'pedro': 1, 'evaluation': 1, 'nonresponders': 1, 'therapeutic': 1, 'advances': 1, '115': 1, '17': 1, 'monika': 1, 'srivastava': 1, 'muktabh': 1, 'mayank': 1, 'ku': 1, 'pulkit': 1, 'varadarajan': 1, 'srikrishna': 1, 'radnet': 1, 'accuracy': 1, 'scans': 1, 'arxiv171004934': 1, 'varun': 1, 'lily': 1, 'coram': 1, 'stumpe': 1, 'martin': 1, 'wu': 1, 'derek': 1, 'narayanaswamy': 1, 'arunacha': 1, 'lam': 1, 'venugopalan': 1, 'subhashini': 1, 'widner': 1, 'kasumi': 1, 'madams': 1, 'tom': 1, 'cuadros': 1, 'jorge': 1, 'development': 1, 'tection': 1, 'retinal': 1, 'fundus': 1, 'pho': 1, 'tographs': 1, 'jama': 1, '3162224022410': 1, 'rm': 1, 'witbraad': 1, 'engelshoven': 1, 'jma': 1, 'dinant': 1, 'gj': 1, 'interobserver': 1, 'variation': 1, 'tract': 1, '598743752': 1, 'gao': 1, 'liu': 1, 'zhuang': 1, 'weinberger': 1, 'kilian': 1, 'q': 1, 'der': 1, 'maaten': 1, 'laurens': 1, 'densely': 1, 'convo': 1, 'lutional': 1, 'arxiv160806993': 1, 'park': 1, 'seyoun': 1, 'yan': 1, 'rongkai': 1, 'junghoon': 1, 'chu': 1, 'linda': 1, 'lin': 1, 'cheng': 1, 'hussien': 1, 'amira': 1, 'rathmell': 1, 'joshua': 1, 'added': 1, 'value': 1, 'computeraided': 1, 'small': 1, 'nodules': 1, 'matched': 1, 'casecontrol': 1, 'study': 1, '162725': 1, 'sergey': 1, 'christian': 1, 'normaliza': 1, 'accelerating': 1, 'reduc': 1, 'internal': 1, 'covariate': 1, 'shift': 1, 'confer': 1, 'machine': 1, '448456': 1, 'mohammad': 1, 'tariqul': 1, 'aowal': 1, 'md': 1, 'abdul': 1, 'min': 1, 'haz': 1, 'ahmed': 1, 'tahseen': 1, 'ashraf': 1, 'khalid': 1, 'normality': 1, 'arxiv170509850': 1, 'soroosh': 1, 'garshasb': 1, 'group': 1, 'writ': 1, 'ology': 1, 'evolving': 1, 'landscape': 1, 'col': 1, 'lege': 1, '13911391144': 1, 'diederik': 1, 'jimmy': 1, 'stochastic': 1, 'arxiv14126980': 1, 'paras': 1, 'sundaram': 1, 'baskaran': 1, 'learn': 1, '162326': 1, 'azene': 1, 'ezana': 1, 'starikovsky': 1, 'anna': 1, 'thelwell': 1, 'aduke': 1, 'iosifescu': 1, 'kimble': 1, 'cary': 1, 'polin': 1, 'ann': 1, 'garra': 1, 'brian': 1, 'destigter': 1, 'kris': 1, 'ten': 1, 'short': 1, 'brad': 1, 'white': 1, 'paper': 1, 'identifying': 1, 'challenges': 1, 'oppor': 1, 'tunities': 1, 'strategies': 1, 'services': 1, 'college': 1, '77495500': 1, 'mark': 1, 'bixby': 1, 'diperna': 1, 'stephanie': 1, 'hellinger': 1, 'jeﬀrey': 1, 'markowitz': 1, 'servaes': 1, 'sabah': 1, 'monuteaux': 1, 'michael': 1, 'shah': 1, 'samir': 1, 'variability': 1, 'hospital': 1, '74': 1, '294298': 1, 'ej': 1, 'gard': 1, 'jw': 1, 'lazar': 1, 'lahaie': 1, 'andary': 1, 'ﬁlm': 1, 'interpretationdirection': 1, 'distraction': 1, 'ininves': 1, 'tigative': 1, 'volume': 1, '404404': 1, 'hannun': 1, 'awni': 1, 'haghpanahi': 1, 'masoumeh': 1, 'bourn': 1, 'codie': 1, 'ng': 1, 'cardiologistlevel': 1, 'volutional': 1, 'arxiv170701836': 1, 'suhail': 1, 'feigin': 1, 'sung': 1, 'arthur': 1, 'sabiha': 1, 'irugulpati': 1, 'lavanya': 1, 'rosenow': 1, 'plain': 1, 'roentgenogram': 1, '1412545558': 1, 'xiaosong': 1, 'yifan': 1, 'le': 1, 'zhiyong': 1, 'bagheri': 1, 'mohammadhadi': 1, 'summers': 1, 'ronald': 1, 'chestxray8': 1, 'hospitalscale': 1, 'benchmarks': 1, 'weaklysupervised': 1, 'thorax': 1, 'arxiv170502315': 1, 'standardization': 1, 'diographs': 1, 'chil': 1, 'dren': 1, 'poblenz': 1, 'eric': 1, 'dagunts': 1, 'dmitry': 1, 'covington': 1, 'ben': 1, 'bernard': 1, 'devon': 1, 'lyman': 1, 'kevin': 1, 'diagnose': 1, 'scratch': 1, 'exploiting': 1, 'dependen': 1, 'cies': 1, 'arxiv171010501': 1, 'bolei': 1, 'khosla': 1, 'aditya': 1, 'lapedriza': 1, 'agata': 1, 'oliva': 1, 'aude': 1, 'torralba': 1, 'antonio': 1, 'fea': 1, 'tures': 1, 'discriminative': 1, 'proceedings': 1, '29212929': 1})}, {'file_name': 'densenet', 'word_counts': Counter({'the': 311, 'and': 169, 'of': 161, 'a': 120, 'to': 117, 'in': 117, '1': 94, 'with': 82, 'layers': 80, '3': 75, 'layer': 66, 'on': 64, '2': 61, 'that': 57, 'network': 54, 'as': 53, 'is': 53, 'networks': 52, 'for': 49, 'w': 46, 'k': 46, 'we': 45, 'by': 44, 'are': 40, 'densenet': 39, 'e': 39, 'conv': 39, 'dense': 38, 'this': 37, 'densenets': 34, 'all': 33, 'from': 32, 'featuremaps': 31, 'training': 31, '5': 31, '12': 31, 'resnets': 29, 't': 29, 'parameters': 29, 'deep': 27, 'be': 26, '4': 26, 'convolutional': 25, 'our': 25, 's': 25, 'each': 24, 'resnet': 24, 'block': 23, '7': 23, 'error': 23, '10': 22, '6': 21, 'can': 20, 'which': 20, 'transition': 20, 'densenetbc': 20, 'y': 20, 'more': 19, 'between': 19, 'imagenet': 19, 'results': 19, '32': 18, 'learning': 17, 'an': 17, 'c10': 17, 'j': 17, 'its': 16, 'number': 16, '11': 16, 'images': 16, 'has': 15, 'l': 15, 'neural': 15, 'it': 15, 'architectures': 15, '8': 15, 'input': 14, 'figure': 14, 'rate': 14, 'residual': 14, 'also': 14, 'set': 14, 'function': 14, 'datasets': 14, 'size': 14, 'model': 14, 'or': 13, 'cif': 13, 'information': 13, 'convolution': 13, 'pooling': 13, 'classiﬁcation': 13, 'data': 13, 'm': 13, '2016': 13, 'train': 12, 'connections': 12, 'have': 12, 'svhn': 12, 'only': 12, 'growth': 12, 'depth': 12, '13': 12, 'blocks': 12, '100': 12, 'two': 12, '14': 12, 'average': 12, 'validation': 12, '2015': 12, 'used': 11, 'feature': 11, 'architecture': 11, 'at': 11, 'net': 11, 'without': 11, 'may': 11, '16': 11, 'c100': 11, 'test': 11, 'preceding': 10, 'models': 10, '28': 10, 'through': 10, 'stochastic': 10, 'features': 10, 'ℓ': 10, 'than': 10, 'similar': 10, 'able': 10, 'they': 9, 'inputs': 9, 'into': 9, 'stateoftheart': 9, 'many': 9, 'different': 9, 'there': 9, 'loss': 9, 'further': 9, 'batch': 9, '40': 9, 'augmentation': 9, '0': 9, 'experiments': 9, '24': 9, 'use': 9, 'same': 9, 'output': 8, 'subsequent': 8, 'proposed': 8, 'over': 8, 'achieve': 8, '20': 8, 'one': 8, 'identity': 8, 'ﬂow': 8, 'o': 8, 'refer': 8, 'fewer': 8, 'ar': 8, 'state': 8, 'parameter': 8, 'im': 8, 'using': 8, 'performance': 8, 'wide': 8, 'three': 8, 'xℓ': 8, 'rates': 8, 'preprint': 8, 'p': 8, 'd': 8, 'g': 8, 'work': 7, 'shown': 7, 'other': 7, 'direct': 7, 'recognition': 7, 'tasks': 7, 'structure': 7, '33': 7, 'con': 7, 'very': 7, 'weights': 7, 'efﬁciency': 7, 'their': 7, 'supervision': 7, 'accuracy': 7, 'trained': 7, 'ﬁrst': 7, 'such': 7, 'shows': 7, 'hℓ': 7, 'bottleneck': 7, 'v': 7, 'weight': 7, 'x': 7, 'right': 7, 'research': 6, '17': 6, 'because': 6, 'small': 6, 'comparable': 6, 'image': 6, 'tion': 6, 'improve': 6, '41': 6, '22': 6, 'nets': 6, 'works': 6, 'following': 6, 'any': 6, 'stride': 6, '56': 6, 'within': 6, 'dropout': 6, 'preactivation': 6, 'c': 6, 'r': 6, 'zhang': 6, 'z': 6, 'arxiv': 6, 'q': 5, 'substantially': 5, 'deeper': 5, 'efﬁcient': 5, 'traditional': 5, 'own': 5, 'competitive': 5, 'most': 5, 'cnns': 5, 'although': 5, 'were': 5, 'highway': 5, 'randomly': 5, 'during': 5, 'better': 5, 'gradient': 5, 'paths': 5, 'these': 5, 'connectivity': 5, 'pattern': 5, 'featuremap': 5, 'sizes': 5, 'but': 5, 'ﬁnal': 5, 'been': 5, '9': 5, '30': 5, '36': 5, 'produced': 5, 'supervised': 5, 'where': 5, 'however': 5, 'pool': 5, 'global': 5, 'θ': 5, 'experiment': 5, '102m': 5, '08m': 5, 'best': 5, 'indicates': 5, 'report': 5, 'singlecrop': 5, 'errors': 5, 'epochs': 5, 'left': 5, 'sun': 5, 'bengio': 5, 'connected': 4, 'huang': 4, 'liu': 4, 'recent': 4, 'if': 4, 'feedforward': 4, 'several': 4, 'com': 4, 'reuse': 4, 'highly': 4, 'object': 4, 'benchmark': 4, 'less': 4, 'approach': 4, '19': 4, 'x0': 4, 'end': 4, 'tions': 4, 'while': 4, 'passed': 4, 'requires': 4, 're': 4, 'learn': 4, 'dropped': 4, 'makes': 4, '21': 4, 'eg': 4, 'ﬁlters': 4, 'throughout': 4, 'much': 4, 'existing': 4, 'fully': 4, 'effective': 4, 'ing': 4, 'achieved': 4, 'was': 4, 'not': 4, 'ex': 4, 'single': 4, 'ℓth': 4, 'multiple': 4, 'eq': 4, '48': 4, 'across': 4, '224': 4, 'setting': 4, '17m': 4, 'lower': 4, 'dataset': 4, '90': 4, 'epoch': 4, 'top1': 4, 'densenet201': 4, 'flops': 4, 'comparison': 4, 'result': 4, 'trend': 4, 'consistently': 4, 'perform': 4, 'overﬁtting': 4, '1001layer': 4, 'settings': 4, 'plot': 4, 'ang': 4, 'cvpr': 4, 'i': 4, 'b': 4, 'hinton': 4, 'lecun': 4, 'h': 4, 'university': 3, 'einberger': 3, 'shorter': 3, 'close': 3, 'convo': 3, 'lutional': 3, 'fashion': 3, 'withl': 3, 'ture': 3, 'reduce': 3, 'evaluate': 3, 'ar10': 3, 'ar100': 3, 'obtain': 3, 'improvements': 3, 'them': 3, 'whilst': 3, 'per': 3, 'visual': 3, 'computer': 3, 'last': 3, 'x1': 3, 'new': 3, 'about': 3, 'gra': 3, 'passes': 3, 'out': 3, 'allow': 3, 'combine': 3, 'large': 3, 'simple': 3, 'di': 3, 'rectly': 3, 'additional': 3, 'lay': 3, 'ers': 3, 'layout': 3, 'before': 3, 'instead': 3, 'effect': 3, 'no': 3, 'show': 3, 'keep': 3, 'maps': 3, 'classiﬁer': 3, 'easy': 3, 'observe': 3, 'smaller': 3, 'signiﬁcantly': 3, 'part': 3, 'initial': 3, 'increasing': 3, 'vision': 3, 'architec': 3, 'optimized': 3, 'supported': 3, 'mappings': 3, 'detection': 3, 'way': 3, 'increase': 3, '35': 3, 'learned': 3, 'deeply': 3, 'internal': 3, 'ladder': 3, '25': 3, 'intermediate': 3, '42': 3, 'composite': 3, 'normalization': 3, 'concatenation': 3, 'followed': 3, 'when': 3, 'divide': 3, 'densenet161': 3, 'compression': 3, '05': 3, 'implementation': 3, 'pixel': 3, 'respectively': 3, 'conﬁgurations': 3, 'compare': 3, '15': 3, '235': 3, '31': 3, 'fractalnet': 3, '110': 3, '10m': 3, '50000': 3, 'adopt': 3, 'scheme': 3, '29': 3, '255': 3, '2012': 3, 'raining': 3, '01': 3, 'memory': 3, 'tional': 3, 'task': 3, 'densenet121': 3, 'densenet169': 3, 'testing': 3, '225': 3, 'resnet101': 3, 'resnet50': 3, 'depths': 3, 'row': 3, 'optimization': 3, 'reduction': 3, 'vs': 3, 'middle': 3, 'hyperparameter': 3, 'compact': 3, 'absolute': 3, 'outputs': 3, 'target': 3, 'national': 3, 'foundation': 3, 'china': 3, 'li': 3, 'nips': 3, '2011': 3, 'icml': 3, '2013': 3, 'incvpr': 3, 'he': 3, 'ren': 3, 'szegedy': 3, 'krizhevsky': 3, 'sutskever': 3, 'applied': 3, 'ieee': 3, 'lee': 3, '2014': 3, 'unsupervised': 3, 'n': 3, 'sermanet': 3, 'densely': 2, 'gao': 2, 'cornell': 2, 'zhuang': 2, 'accurate': 2, 'contain': 2, 'those': 2, 'paper': 2, 'observation': 2, 'introduce': 2, 'every': 2, 'problem': 2, 'strengthen': 2, 'fea': 2, 'parame': 2, 'ters': 2, 'four': 2, 'requiring': 2, 'computation': 2, 'code': 2, 'become': 2, 'introduced': 2, '18': 2, 'recently': 2, 'original': 2, 'authors': 2, 'takes': 2, '100layer': 2, 'dient': 2, 'time': 2, 'related': 2, 'signal': 2, 'via': 2, 'connec': 2, 'dropping': 2, 'fractalnets': 2, 'sev': 2, 'eral': 2, 'parallel': 2, 'volutional': 2, 'short': 2, 'key': 2, 'early': 2, 'later': 2, 'propose': 2, 'ensure': 2, 'nature': 2, 'illustrates': 2, 'schematically': 2, 'never': 2, 'summation': 2, 'concatenating': 2, 'hence': 2, 'introduces': 2, 'nectivity': 2, 'possibly': 2, 'need': 2, 'algorithms': 2, 'changes': 2, 'preserved': 2, 'variations': 2, 'fact': 2, 'recurrent': 2, 'narrow': 2, 'collective': 2, 'knowledge': 2, 'based': 2, 'advantage': 2, 'access': 2, 'gradients': 2, 'plicit': 2, 'tend': 2, 'require': 2, 'outperform': 2, 'current': 2, 'exploration': 2, 'differences': 2, 'cascade': 2, 'multilayer': 2, 'perceptrons': 2, '39': 2, 'few': 2, 'pa': 2, '23': 2, 'various': 2, 'tures': 2, 'provided': 2, 'means': 2, 'bypassing': 2, 'units': 2, 'hundreds': 2, 'success': 2, 'redundancy': 2, 'facilitate': 2, 'uses': 2, '37': 2, 'variant': 2, 'simply': 2, 'linear': 2, 'referred': 2, 'change': 2, 'sufﬁcient': 2, 'power': 2, 'potential': 2, 'increases': 2, 'difference': 2, 'inception': 2, 'concatenate': 2, 'dif': 2, 'ferent': 2, 'complicated': 2, 'dsn': 2, 'directly': 2, 'classiﬁers': 2, 'earlier': 2, '26': 2, 'semisupervised': 2, '38': 2, 'deeplyfused': 2, 'prove': 2, 'comprises': 2, 'nonlinear': 2, 'operations': 2, 'bn': 2, 'rectiﬁed': 2, 'relu': 2, 'convolu': 2, 'denote': 2, 'hℓxℓ': 2, 'add': 2, 'consequently': 2, 'lin': 2, 'downsampling': 2, 'do': 2, 'consist': 2, 'produces': 2, 'k0': 2, 'channels': 2, 'put': 2, 'hyper': 2, 'section': 2, 'relatively': 2, '112': 2, 'softmax': 2, 'explanation': 2, 'once': 2, 'accessed': 2, 'typically': 2, 'design': 2, 'densenetb': 2, 'let': 2, 'densenetc': 2, 'both': 2, 'except': 2, 'twice': 2, 'performed': 2, 'attached': 2, 'basic': 2, 'bc': 2, 'evaluated': 2, 'convolutions': 2, 'follow': 2, 'variants': 2, 'colored': 2, 'natural': 2, '386m': 2, '201': 2, '733': 2, '2820': 2, '175': 2, '164': 2, '462': 2, '2271': 2, '524': 2, '70m': 2, '577': 2, '410': 2, '272m': 2, '583': 2, '374': 2, '592': 2, '451': 2, '2227': 2, '250': 2, '153m': 2, '519': 2, '1964': 2, '190': 2, '346': 2, '1718': 2, 'denotes': 2, 'methods': 2, 'overall': 2, 'blue': 2, 'standard': 2, 'run': 2, 'obtained': 2, 'performs': 2, 'classes': 2, 'aug': 2, 'mentation': 2, '27': 2, 'mark': 2, 'house': 2, 'numbers': 2, 'contains': 2, 'digit': 2, '10crop': 2, '300': 2, '50': 2, 'gpu': 2, 'largest': 2, 'minibatch': 2, '128': 2, 'compensate': 2, 'momentum': 2, '34': 2, '09': 2, 'initialization': 2, 'after': 2, '02': 2, 'top5': 2, '215': 2, '245': 2, '265': 2, '275': 2, 'resnet34': 2, 'resnet152': 2, 'densenet161k48': 2, 'densenetsbc': 2, 'highlight': 2, 'general': 2, 'trends': 2, 'outperforms': 2, 'regularization': 2, '250layer': 2, 'capacity': 2, 'utilize': 2, 'difﬁculties': 2, 'particularly': 2, '30m': 2, 'panel': 2, 'improvement': 2, 'observed': 2, 'est': 2, 'resnet1001': 2, 'densenetbc100': 2, 'third': 2, 'presented': 2, 'par': 2, 'compu': 2, 'tation': 2, 'plots': 2, 'quite': 2, 'some': 2, 'ac': 2, 'top': 2, 'al': 2, 'strong': 2, 'spread': 2, 'encodes': 2, 'columns': 2, 'scale': 2, 'naturally': 2, 'study': 2, 'grants': 2, 'science': 2, 'postdoctoral': 2, 'program': 2, 'sedra': 2, 'u': 2, 'deng': 2, 'fei': 2, 'largescale': 2, '2009': 2, '1989': 2, 'aistats': 2, 'courville': 2, 'segmentation': 2, 'eccv': 2, 'ioffe': 2, 'backpropagation': 2, 'iclr': 2, 'innips': 2, 'chintala': 2, 'srivastava': 2, 'inicml': 2, 'anhoucke': 2, 'gh349cornelledu': 1, 'tsinghua': 1, 'liuzhuang13mailstsinghuaeducn': 1, 'laurens': 1, 'van': 1, 'der': 1, 'maaten': 1, 'facebook': 1, 'ai': 1, 'lvdmaatenfbcom': 1, 'kilian': 1, 'kqw4cornelledu': 1, 'abstract': 1, 'embrace': 1, 'connects': 1, 'whereas': 1, 'connectionsone': 1, 'layerour': 1, 'hasll1': 1, 'f': 1, 'pelling': 1, 'advantages': 1, 'alleviate': 1, 'vanishinggradient': 1, 'propagation': 1, 'encourage': 1, 'sig': 1, 'niﬁcant': 1, 'high': 1, 'formance': 1, 'pretrained': 1, 'available': 1, 'httpsgithubcomliuzhuang13densenet': 1, 'introduction': 1, 'dominant': 1, 'machine': 1, 'originally': 1, 'years': 1, 'ago': 1, 'hardware': 1, 'enabled': 1, 'truly': 1, 'lenet5': 1, 'consisted': 1, 'vgg': 1, 'featured': 1, 'year': 1, 'contributed': 1, 'equally': 1, 'h1': 1, 'x2': 1, 'h2': 1, 'h3': 1, 'h4': 1, 'x3': 1, 'x4': 1, '5layer': 1, 'surpassed': 1, 'barrier': 1, 'increasingly': 1, 'emerges': 1, 'vanish': 1, 'wash': 1, 'reaches': 1, 'beginning': 1, 'publications': 1, 'address': 1, 'problems': 1, 'pass': 1, 'next': 1, 'shortens': 1, 'repeatedly': 1, 'sequences': 1, 'nominal': 1, 'maintaining': 1, 'approaches': 1, 'vary': 1, 'topology': 1, 'procedure': 1, 'share': 1, 'characteristic': 1, 'create': 1, '4700': 1, 'distills': 1, 'insight': 1, 'maxi': 1, 'mum': 1, 'connectall': 1, 'matching': 1, 'preserve': 1, 'obtains': 1, 'cru': 1, 'cially': 1, 'contrast': 1, 'stead': 1, 'theℓth': 1, 'consisting': 1, 'alllℓ': 1, 'll1': 1, 'llayer': 1, 'just': 1, 'counterintuitive': 1, 'tivity': 1, 'tra': 1, 'ditional': 1, 'redundant': 1, 'chitectures': 1, 'viewed': 1, 'reads': 1, 'writes': 1, 'needs': 1, 'make': 1, 'preser': 1, 'vation': 1, 'explicit': 1, 'additive': 1, 'transformations': 1, 'contribute': 1, 'little': 1, 'unrolled': 1, 'num': 1, 'ber': 1, 'larger': 1, 'chitecture': 1, 'explicitly': 1, 'differentiates': 1, 'added': 1, 'adding': 1, 'remaining': 1, 'unchangedand': 1, 'decision': 1, 'besides': 1, 'big': 1, 'improved': 1, 'dients': 1, 'leading': 1, 'helps': 1, 'regularizing': 1, 'reduces': 1, 'ﬁtting': 1, 'param': 1, 'eters': 1, 'stateof': 1, 'theart': 1, 'ork': 1, 'since': 1, 'discovery': 1, 'resurgence': 1, 'popularity': 1, 'revived': 1, 'domain': 1, 'modern': 1, 'ampliﬁes': 1, 'motivates': 1, 'patterns': 1, 'revisiting': 1, 'old': 1, 'ideas': 1, 'already': 1, 'studied': 1, 'literature': 1, '1980s': 1, 'pioneering': 1, 'focuses': 1, 'bylayer': 1, 'descent': 1, 'scales': 1, 'hundred': 1, 'rameters': 1, 'utilizing': 1, 'multilevel': 1, 'skipconnnections': 1, 'found': 1, 'derived': 1, 'purely': 1, 'theoretical': 1, 'framework': 1, 'crosslayer': 1, 'ours': 1, 'amongst': 1, 'effectively': 1, 'endtoend': 1, 'along': 1, 'gating': 1, 'difﬁculty': 1, 'bypass': 1, 'presumed': 1, 'factor': 1, 'eases': 1, 'point': 1, 'pure': 1, 'pressive': 1, 'recordbreaking': 1, 'challeng': 1, 'localization': 1, 'coco': 1, 'cently': 1, '1202layer': 1, 'proves': 1, 'needed': 1, 'highlights': 1, 'great': 1, 'amount': 1, 'partly': 1, 'inspired': 1, 'preactivationalso': 1, '1000': 1, 'orthogonal': 1, 'making': 1, 'help': 1, 'skip': 1, 'width': 1, 'googlenet': 1, 'incep': 1, 'module': 1, 'concatenates': 1, 'generalized': 1, '4701': 1, 'prediction': 1, 'ಯhorseರ': 1, 'adjacent': 1, 'drawing': 1, 'representational': 1, 'tremely': 1, 'exploit': 1, 'throughfeature': 1, 'yielding': 1, 'densed': 1, 'bydifferent': 1, 'variation': 1, 'improves': 1, 'constitutes': 1, 'major': 1, 'compared': 1, 'simpler': 1, 'notable': 1, 'innovations': 1, 'yielded': 1, 'nin': 1, 'includes': 1, 'micro': 1, 'tract': 1, 'auxiliary': 1, 'received': 1, 'troduce': 1, 'lateral': 1, 'autoencoders': 1, 'producing': 1, 'impressive': 1, 'accuracies': 1, 'dfns': 1, 'combining': 1, 'base': 1, 'pathways': 1, 'minimize': 1, 'reconstruction': 1, 'losses': 1, 'consider': 1, 'implements': 1, 'transformation': 1, 'indexes': 1, 'func': 1, 'connect': 1, '1th': 1, 'gives': 1, 'rise': 1, 'skipconnection': 1, 'bypasses': 1, 'transforma': 1, 'ofhℓ': 1, 'combined': 1, 'impede': 1, 'resulting': 1, 'receives': 1, 'hℓx0': 1, 'refers': 1, 'ease': 1, 'plementation': 1, 'tensor': 1, 'motivated': 1, 'deﬁne': 1, 'consecutive': 1, 'ear': 1, 'unit': 1, 'operation': 1, 'viable': 1, 'essential': 1, 'connecteddense': 1, 'see': 1, 'normal': 1, 'ization': 1, 'follows': 1, 'important': 1, 'layerseg': 1, '4702': 1, 'densenet121k': 1, 'densenet169k': 1, 'densenet201k': 1, 'densenet161k': 1, 'max': 1, '1000d': 1, 'fullyconnected': 1, 'note': 1, 'table': 1, 'corresponds': 1, 'sequence': 1, 'bnreluconv': 1, 'tested': 1, 'therefore': 1, 'view': 1, 'addsk': 1, 'regulates': 1, 'how': 1, 'tributes': 1, 'written': 1, 'everywhere': 1, 'unlike': 1, 'replicate': 1, 'layersalthough': 1, 'noted': 1, 'troduced': 1, 'thus': 1, 'computational': 1, 'ﬁnd': 1, 'es': 1, 'pecially': 1, 'layerie': 1, 'bnreluconv1': 1, '1bnreluconv33': 1, 'version': 1, 'produce': 1, '4k': 1, 'compressiont': 1, 'compactness': 1, 'containsm': 1, 'generate': 1, 'θm': 1, 'fac': 1, 'tor': 1, 'transi': 1, 'remains': 1, 'unchanged': 1, 'details': 1, 'ima': 1, 'genet': 1, 'equal': 1, 'fore': 1, 'entering': 1, 'kernel': 1, 'side': 1, 'zeropadded': 1, 'ﬁxed': 1, 'contiguous': 1, 'then': 1, '88': 1, 'l250': 1, 'l190': 1, '2k': 1, 'exact': 1, 'empirically': 1, 'demonstrate': 1, 'effectiveness': 1, 'stateofthe': 1, 'art': 1, 'especially': 1, 'arthe': 1, 'pixels': 1, '4703': 1, 'method': 1, 'params': 1, '1041': 1, '881': 1, '3568': 1, 'allcnn': 1, '908': 1, '725': 1, '3371': 1, '969': 1, '797': 1, '3457': 1, '192': 1, '772': 1, '3239': 1, '1018': 1, '522': 1, '3534': 1, '2330': 1, 'dropoutdroppath': 1, '460': 1, '2373': 1, '187': 1, '661': 1, 'reported': 1, '1363': 1, '641': 1, '4474': 1, '2722': 1, '1166': 1, '523': 1, '3780': 1, '2458': 1, '1202': 1, '491': 1, '110m': 1, '481': 1, '2207': 1, '365m': 1, '417': 1, '2050': 1, '27m': 1, '1126': 1, '546': 1, '3558': 1, '2433': 1, '1001': 1, '1056': 1, '3347': 1, '700': 1, '2755': 1, '2442': 1, '179': 1, '2379': 1, '2020': 1, '167': 1, '2342': 1, '1925': 1, '159': 1, '2415': 1, '176': 1, '362': 1, '1760': 1, '174': 1, '256m': 1, 'surpass': 1, 'competing': 1, 'bold': 1, 'translation': 1, 'andor': 1, 'mirroring': 1, 'ourselves': 1, 'margin': 1, 'sists': 1, 'drawn': 1, 'sets': 1, '10000': 1, 'hold': 1, '5000': 1, 'mirroringshifting': 1, 'widely': 1, 'name': 1, 'preprocessing': 1, 'normalize': 1, 'channel': 1, 'stan': 1, 'dard': 1, 'deviations': 1, 'svhnthe': 1, 'street': 1, 'iew': 1, '73257': 1, '26032': 1, '531131': 1, 'common': 1, 'practice': 1, '6000': 1, 'split': 1, 'select': 1, 'lowest': 1, 'values': 1, 'so': 1, 'range': 1, 'ilsvrc': 1, 'consists': 1, 'million': 1, 'val': 1, 'idation': 1, 'from1': 1, '000': 1, 'apply': 1, 'de': 1, 'scent': 1, 'sgd': 1, '64': 1, 'divided': 1, '75': 1, 'total': 1, '256': 1, 'initially': 1, 'lowered': 1, 'times': 1, '60': 1, 'due': 1, 'constraints': 1, 'decay': 1, 'nesterov': 1, 'dampening': 1, 'ie': 1, '4704': 1, '2502': 1, '2361': 1, '771': 1, '666': 1, '2380': 1, '2208': 1, '685': 1, '2258': 1, '2146': 1, '634': 1, '554': 1, '2233': 1, '2085': 1, '615': 1, '530': 1, 'crop': 1, '075': 1, '125': 1, 'testtime': 1, '43': 1, 'main': 1, 'boldface': 1, 'noticeable': 1, 'orig': 1, 'inate': 1, 'bottom': 1, 'even': 1, 'encouraging': 1, 'fractal': 1, 'droppath': 1, 'surpasses': 1, 'doesnt': 1, 'counterpart': 1, 'explained': 1, 'extremely': 1, 'overﬁt': 1, 'land': 1, 'attribute': 1, 'primarily': 1, 'corre': 1, 'sponding': 1, 'demon': 1, 'strated': 1, 'column': 1, 'drops': 1, 'ﬁnally': 1, 'sug': 1, 'gests': 1, 'increased': 1, 'representa': 1, 'bigger': 1, 'suffer': 1, 'indicate': 1, 'efﬁciently': 1, 'alterna': 1, 'tive': 1, 'particular': 1, 'dimension': 1, 'parameterefﬁcient': 1, 'ample': 1, '100and': 1, 'achieves': 1, 'er': 1, 'ror': 1, 'converges': 1, 'value': 1, 'analyze': 1, 'detail': 1, 'below': 1, 'overﬁttingone': 1, 'positive': 1, 'sideeffect': 1, 'tendency': 1, 'prone': 1, 'prior': 1, 'pronounced': 1, 'relative': 1, '12to': 1, '24lead': 1, 'modest': 1, 'appear': 1, 'counter': 1, '44': 1, 'pare': 1, 'fair': 1, 'elimi': 1, 'nate': 1, 'factors': 1, 'preprocess': 1, 'adopting': 1, 'publicly': 1, 'avail': 1, 'orch': 1, '81': 1, 'replace': 1, 'settingsexactly': 1, 'exception': 1, 'httpsgithubcomfacebookfbresnettorch': 1, '4705': 1, '105': 1, '16test': 1, '3x': 1, '150': 1, '200': 1, 'curves': 1, 'limitations': 1, 'drop': 1, 'ﬁgure': 1, 'reveal': 1, 'example': 1, '20m': 1, 'yields': 1, '101layer': 1, '40m': 1, 'putation': 1, 'worth': 1, 'noting': 1, 'experimental': 1, 'setup': 1, 'implies': 1, 'conceivable': 1, 'extensive': 1, 'searches': 1, 'discussion': 1, 'superﬁcially': 1, 'differs': 1, 'concatenated': 1, 'summed': 1, 'implica': 1, 'seemingly': 1, 'modiﬁcation': 1, 'lead': 1, 'substan': 1, 'tially': 1, 'behaviors': 1, 'compactnessas': 1, 'consequence': 1, 'encourages': 1, 'leads': 1, 'inefﬁciencies': 1, 'temporarily': 1, 'precludes': 1, 'aims': 1, 'varying': 1, 'curacies': 1, 'parison': 1, 'popular': 1, 'alexnet': 1, 'vggnet': 1, 'achieving': 1, 'sults': 1, 'against': 1, 'kept': 1, 'previous': 1, 'graph': 1, 'level': 1, 'quires': 1, 'around': 1, 'line': 1, 'trainable': 1, 'implicit': 1, 'proved': 1, 'individual': 1, 'receive': 1, 'interpret': 1, 'kind': 1, 'supervi': 1, 'sion': 1, 'beneﬁts': 1, 'previously': 1, 'deeplysupervised': 1, 'hidden': 1, 'enforcing': 1, 'discriminative': 1, 'pro': 1, 'vides': 1, 'shared': 1, 'deterministic': 1, 'connectionthere': 1, 'interesting': 1, 'connection': 1, '4706': 1, 'creates': 1, 'tween': 1, 'surrounding': 1, 'connectiv': 1, 'ity': 1, 'probability': 1, 'connectedif': 1, 'ultimately': 1, 'interpretation': 1, 'provide': 1, 'insights': 1, 'regularizer': 1, 'reuseby': 1, 'cess': 1, 'though': 1, 'sometimes': 1, 'conduct': 1, 'investigate': 1, 'ad': 1, 'vantage': 1, 'opportunity': 1, 'compute': 1, 'assigned': 1, 'heatmap': 1, 'serves': 1, 'surrogate': 1, 'dependency': 1, 'red': 1, 'dot': 1, 'position': 1, 'slayers': 1, 'observa': 1, 'made': 1, 'extracted': 1, 'indeed': 1, 'indicating': 1, 'indirections': 1, 'second': 1, 'assign': 1, 'least': 1, 'triangles': 1, 'dicating': 1, 'redun': 1, 'dant': 1, 'low': 1, 'keeping': 1, 'exactly': 1, 'compressed': 1, 'entire': 1, 'seems': 1, 'concentration': 1, 'towards': 1, 'suggesting': 1, 'highlevel': 1, 'late': 1, 'conclusion': 1, 'source': 1, '03': 1, '04': 1, '06': 1, '07': 1, '08': 1, 'classification': 1, 'ﬁlter': 1, 'color': 1, 'av': 1, 'erage': 1, 'l1': 1, 'norm': 1, 'normalized': 1, 'connecting': 1, 'highlighted': 1, 'black': 1, 'rectangles': 1, 'correspond': 1, 'showed': 1, 'hibiting': 1, 'yield': 1, 'consistent': 1, 'accu': 1, 'racy': 1, 'growing': 1, 'signs': 1, 'degradation': 1, 'under': 1, 'multi': 1, 'ple': 1, 'moreover': 1, 'computa': 1, 'performances': 1, 'adopted': 1, 'believe': 1, 'gains': 1, 'detailed': 1, 'tuning': 1, 'hyperparameters': 1, 'schedules': 1, 'rule': 1, 'integrate': 1, 'properties': 1, 'diversiﬁed': 1, 'according': 1, 'representations': 1, 'reduced': 1, 'good': 1, 'extractors': 1, 'build': 1, 'featureseg': 1, 'plan': 1, 'transfer': 1, 'future': 1, 'acknowledgementsthe': 1, 'iii1618134': 1, 'iii1526012': 1, 'iis1149882': 1, 'bill': 1, 'melinda': 1, 'gates': 1, 'interna': 1, 'exchange': 1, 'fellowship': 1, 'council': 1, 'no20150015': 1, 'sup': 1, 'ported': 1, '2011cba00300': 1, '2011cba00301': 1, 'nat': 1, 'ural': 1, 'grant': 1, '61361136003': 1, 'thank': 1, 'daniel': 1, 'geoff': 1, 'pleiss': 1, 'insightful': 1, 'discussions': 1, '4707': 1, 'references': 1, 'cortes': 1, 'gonzalvo': 1, 'kuznetsov': 1, 'mohri': 1, 'adanet': 1, 'adaptive': 1, 'structural': 1, 'artiﬁcial': 1, 'networksarxiv': 1, 'arxiv160701097': 1, 'dong': 1, 'socher': 1, 'lj': 1, 'hierarchical': 1, 'database': 1, 'fahlman': 1, 'lebiere': 1, 'cascadecorrelation': 1, 'gardner': 1, 'kusner': 1, 'upchurch': 1, 'hopcroft': 1, 'manifold': 1, 'traversal': 1, 'changing': 1, 'labels': 1, 'featuresarxiv': 1, 'arxiv151106421': 1, 'gatys': 1, 'ecker': 1, 'bethge': 1, 'algorithm': 1, 'artistic': 1, 'style': 1, 'communications': 1, 'glorot': 1, 'bordes': 1, 'sparse': 1, 'rectiﬁer': 1, 'goodfellow': 1, 'ardefarley': 1, 'mirza': 1, 'maxout': 1, 'gross': 1, 'wilber': 1, 'investigating': 1, 'hariharan': 1, 'arbeláez': 1, 'girshick': 1, 'malik': 1, 'ﬁnegrained': 1, 'localiza': 1, 'delving': 1, 'rectiﬁers': 1, 'surpassing': 1, 'humanlevel': 1, 'iniccv': 1, 'accelerating': 1, 'reducing': 1, 'covariate': 1, 'shift': 1, 'tiny': 1, 'ech': 1, 'larsson': 1, 'maire': 1, 'shakhnarovich': 1, 'ultradeep': 1, 'residuals': 1, 'arxiv160507648': 1, 'boser': 1, 'denker': 1, 'henderson': 1, 'howard': 1, 'hubbard': 1, 'jackel': 1, 'handwritten': 1, 'zip': 1, 'recognitionneural': 1, '14541551': 1, 'bottou': 1, 'haffner': 1, 'document': 1, 'proceed': 1, 'ings': 1, '861122782324': 1, '1998': 1, 'cy': 1, 'xie': 1, 'gallagher': 1, 'tu': 1, 'liao': 1, 'poggio': 1, 'bridging': 1, 'gaps': 1, 'cortex': 1, 'arxiv160403640': 1, 'chen': 1, 'long': 1, 'shelhamer': 1, 'darrell': 1, 'semantic': 1, 'netzer': 1, 'coates': 1, 'bissacco': 1, 'wu': 1, 'ng': 1, 'reading': 1, 'digits': 1, 'orkshop': 1, 'pezeshki': 1, 'fan': 1, 'brakel': 1, 'deconstructing': 1, 'rasmus': 1, 'berglund': 1, 'honkala': 1, 'alpola': 1, 'raiko': 1, 'romero': 1, 'ballas': 1, 'kahou': 1, 'chassang': 1, 'gatta': 1, 'fitnets': 1, 'hints': 1, 'thin': 1, 'russakovsky': 1, 'su': 1, 'krause': 1, 'satheesh': 1, 'ma': 1, 'karpathy': 1, 'khosla': 1, 'bernstein': 1, 'et': 1, 'challenge': 1, 'ijcv': 1, 'neu': 1, 'ral': 1, 'icpr': 1, 'pages': 1, '32883291': 1, 'kavukcuoglu': 1, 'pedestrian': 1, 'multistage': 1, 'springenberg': 1, 'dosovitskiy': 1, 'brox': 1, 'ried': 1, 'miller': 1, 'striving': 1, 'simplicity': 1, 'arxiv14126806': 1, 'salakhutdinov': 1, 'prevent': 1, 'overﬁttingjmlr': 1, 'greff': 1, 'schmidhuber': 1, 'martens': 1, 'dahl': 1, 'importance': 1, 'jia': 1, 'reed': 1, 'anguelov': 1, 'erhan': 1, 'rabinovich': 1, 'going': 1, 'shlens': 1, 'ojna': 1, 'rethinking': 1, 'arg': 1, 'almeida': 1, 'lyman': 1, 'generalizing': 1, 'arxiv160308029': 1, 'ei': 1, 'zeng': 1, 'arxiv160507716': 1, 'wilamowski': 1, 'transactions': 1, '211117931803': 1, '2010': 1, 'ramanan': 1, 'multiscale': 1, 'dag': 1, 'iccv': 1, 'zagoruyko': 1, 'komodakis': 1, 'arxiv160507146': 1, 'augmenting': 1, 'objectives': 1, '4708': 1})}, {'file_name': 'ecg', 'word_counts': Counter({'the': 462, 'of': 269, 'and': 178, 'a': 171, 'to': 161, 'for': 122, 'in': 112, 'data': 81, 'ecg': 79, 'we': 72, 'rhythm': 68, 'as': 53, 'dnn': 52, 'is': 52, 'that': 52, 'by': 51, 'from': 50, 'dataset': 49, 'cardiologist': 47, 'on': 45, 'were': 45, 'with': 44, 'was': 41, 'or': 38, 'at': 35, 'an': 34, 'records': 34, 'our': 34, 'this': 33, 'deep': 32, 'performance': 32, 'record': 32, 'which': 31, 'j': 31, 'classes': 30, 'all': 30, 'atrial': 30, 'nature': 30, 'medicine': 29, 'test': 29, 'class': 29, 'neural': 28, 'committee': 28, 'focus': 27, 'cardiologists': 27, 'training': 27, 'algorithm': 27, 'used': 26, 'be': 26, 'fibrillation': 25, 'study': 25, 'network': 24, 'average': 24, 'each': 24, 'model': 24, 'individual': 24, 'clinical': 23, 'approach': 23, 'sensitivity': 23, '1': 22, 'learning': 22, 'f1': 22, 'consensus': 20, 'tachycardia': 20, 's': 20, 'patients': 19, 'are': 19, 'such': 19, 'classification': 19, 'letters': 18, 'analysis': 18, '10': 18, 'auc': 18, 'm': 18, 'interpretation': 17, 'available': 17, 'ventricular': 17, 'every': 17, 'standard': 16, 'avb': 16, '2017': 16, 'p': 15, 'per': 15, 'one': 15, 'n': 15, 'rhythms': 15, '02': 15, 'networks': 15, 'research': 15, 'irhythm': 15, 'has': 14, 'using': 14, 'similar': 14, 'output': 14, 'sequencelevel': 14, 'predictions': 14, 'set': 14, '00': 14, '04': 14, 'et': 14, 'convolutional': 14, 't': 13, 'not': 13, 'annotation': 13, 'any': 13, 'use': 13, 'r': 13, 'al': 13, 'technologies': 13, 'inc': 12, 'h': 12, 'improve': 12, 'endtoend': 12, 'specificity': 12, 'these': 12, 'can': 12, '2': 12, 'only': 12, 'sinus': 12, 'prediction': 12, 'level': 12, 'other': 12, 'd': 12, 'evaluation': 11, 'roc': 11, 'most': 11, 'raw': 11, 'may': 11, 'have': 11, 'layers': 11, 'input': 11, 'noise': 11, 'averaged': 11, '06': 11, '08': 11, 'information': 11, 'electrocardiogram': 10, 'singlelead': 10, 'ecgs': 10, 'annotated': 10, 'score': 10, 'more': 10, 'than': 10, 'since': 10, 'development': 10, 'multiple': 10, 'limited': 10, 'arrhythmia': 10, 'fig': 10, 'compared': 10, 'setlevel': 10, 'datasets': 10, 'it': 10, 'label': 10, 'cardiol': 10, 'k': 10, 'comput': 10, 'y': 9, 'present': 9, 'diagnostic': 9, 'features': 9, 'diagnoses': 9, 'single': 9, 'wwwnaturecom': 9, 'table': 9, 'sequence': 9, 'scores': 9, 'trigeminy': 9, 'svt': 9, 'b': 9, 'samples': 9, '30s': 9, 'signal': 9, 'reporting': 9, 'availability': 9, '2018': 9, 'proc': 9, 'ay': 9, 'interval': 9, 'eg': 9, 'algorithmic': 8, 'across': 8, '12': 8, 'achieved': 8, 'under': 8, 'mean': 8, 'range': 8, 'arrhythmias': 8, 'number': 8, 'monitor': 8, 'unique': 8, 'detection': 8, 'january': 8, 'both': 8, 'calculated': 8, 'annotations': 8, 'had': 8, 'additional': 8, 'versus': 8, 'ivr': 8, 'lettersnature': 8, 'given': 8, 'where': 8, 'conference': 8, 'residual': 8, 'code': 8, 'software': 8, 'https': 7, 'computer': 7, 'health': 7, 'learn': 7, 'automated': 7, 'classify': 7, 'settings': 7, 'rate': 7, 'expert': 7, 'important': 7, 'dnns': 7, 'they': 7, 'extraction': 7, '16': 7, '328': 7, 'see': 7, 'methods': 7, '2019': 7, 'naturemedicine': 7, 'gold': 7, 'block': 7, 'received': 7, 'curves': 7, 'confusion': 7, 'two': 7, 'junctional': 7, 'made': 7, 'potential': 7, 'will': 7, 'size': 7, 'c': 7, 'patient': 7, 'doiorg101038s4159101802683': 6, 'authors': 6, 'who': 6, 'curve': 6, 'exceeded': 6, 'if': 6, 'years': 6, 'substantial': 6, 'provide': 6, 'perform': 6, 'applications': 6, 'without': 6, 'feature': 6, 'well': 6, 'whether': 6, 'appropriate': 6, 'extended': 6, '25': 6, 'supplementary': 6, 'within': 6, 'cases': 6, 'ppv': 6, 'ear': 6, 'precisionrecall': 6, 'same': 6, 'architecture': 6, 'contained': 6, 'different': 6, 'bigeminy': 6, 'wenckebach': 6, 'intervals': 6, 'examples': 6, 'also': 6, 'review': 6, 'heart': 6, '2015': 6, 'l': 6, 'support': 6, 'international': 6, '2016': 6, '2013': 6, 'ght': 6, 'full': 6, 'collection': 6, 'deidentified': 6, 'statistical': 6, 'sample': 6, 'stanford': 5, 'ca': 5, 'usa': 5, 'care': 5, 'comprehensive': 5, 'wide': 5, 'when': 5, 'against': 5, 'operating': 5, 'broad': 5, 'distinct': 5, '3': 5, 'over': 5, 'many': 5, 'specific': 5, 'computational': 5, 'useful': 5, 'zio': 5, '200': 5, 'age': 5, 'vol': 5, '6569': 5, 'once': 5, 'report': 5, 'call': 5, 'onset': 5, 'offset': 5, 'but': 5, 'analyses': 5, 'described': 5, 'six': 5, 'precision': 5, 'although': 5, 'out': 5, 'certified': 5, 'three': 5, 'matrices': 5, 'between': 5, 'applied': 5, '256': 5, 'accuracy': 5, 'steps': 5, 'no': 5, 'confidence': 5, 'application': 5, 'would': 5, 'before': 5, 'corresponding': 5, 'summary': 5, 'g': 5, 'procedures': 5, 'zhang': 5, 'x': 5, 'ieee': 5, '18': 5, 'extracted': 5, 'reviewed': 5, 'randomly': 5, 'sampled': 5, 'restrictions': 5, '30second': 5, 'description': 5, 'university': 4, 'cardiology': 4, 'contributed': 4, 'computerized': 4, 'workflow': 4, 'however': 4, 'ambulatory': 4, 'validated': 4, 'cardiolo': 4, 'receiver': 4, 'characteristic': 4, '0780': 4, 'findings': 4, 'demonstrate': 4, 'high': 4, 'could': 4, 'tool': 4, 'ing': 4, 'its': 4, 'existing': 4, 'algorithms': 4, 'processing': 4, 'models': 4, 'layer': 4, 'able': 4, 'preprocessing': 4, 'remains': 4, 'work': 4, 'developed': 4, 'median': 4, 'electrocardiograms': 4, '36': 4, 'first': 4, 'second': 4, 'group': 4, 'metrics': 4, 'duration': 4, 'demonstrated': 4, 'results': 4, 'higher': 4, 'labels': 4, 'recall': 4, 'lower': 4, 'f': 4, 'those': 4, '5': 4, 'technicians': 4, 'example': 4, 'publicly': 4, 'overall': 4, 'flutter': 4, 'tion': 4, 'approaches': 4, 'techniques': 4, 'indicated': 4, '23': 4, 'manner': 4, 'achieve': 4, 'must': 4, 'their': 4, 'manual': 4, 'error': 4, '100': 4, 'bpm': 4, '7': 4, 'online': 4, 'e': 4, 'based': 4, 'he': 4, 'med': 4, 'mh': 4, 'policy': 4, 'purposes': 4, 'selected': 4, 'further': 4, 'how': 4, 'chosen': 4, 'length': 4, 'estimates': 4, 'section': 4, 'tests': 4, 'custom': 4, 'period': 4, 'san': 3, 'francisco': 3, 'digital': 3, 'system': 3, 'awni': 3, 'hannun': 3, 'been': 3, 'practicing': 3, 'gists': 3, 'area': 3, '097': 3, 'harmonic': 3, 'value': 3, '0837': 3, 'fixed': 3, 'end': 3, 'confirmed': 3, 'combination': 3, 'widespread': 3, 'digitization': 3, 'advances': 3, 'driven': 3, 'consisting': 3, 'relevant': 3, 'tasks': 3, 'speech': 3, 'strategy': 3, 'medical': 3, 'ability': 3, 'particularly': 3, 'increase': 3, 'supraventricular': 3, 'efforts': 3, 'database': 3, 'novel': 3, 'detect': 3, 'designed': 3, 'ambula': 3, 'tory': 3, '27': 3, 'lead': 3, 'hz': 3, 'collected': 3, '38': 3, 'interannotator': 3, 'agreement': 3, 'shows': 3, 'exhibiting': 3, 'approximately': 3, 'consists': 3, 'greater': 3, '0978': 3, '0977': 3, 'af': 3, 'method': 3, 'unchanged': 3, 'maximum': 3, 'separate': 3, 'part': 3, '4': 3, 'note': 3, 'pattern': 3, 'finally': 3, 'physionet': 3, 'challenge': 3, 'hyperparameter': 3, 'among': 3, 'best': 3, 'new': 3, '0981': 3, '0761': 3, '0996': 3, '0541': 3, '0987': 3, '13': 3, '7544': 3, 'common': 3, 'recent': 3, 'validation': 3, 'clinically': 3, 'final': 3, 'diagnosis': 3, '01': 3, '03': 3, '05': 3, 'manually': 3, 'derived': 3, 'while': 3, 'limitations': 3, 'some': 3, '12lead': 3, 'including': 3, 'long': 3, 'knowledge': 3, 'about': 3, 'reasonable': 3, 'making': 3, 'provides': 3, 'current': 3, 'obtain': 3, 'rare': 3, 'targeted': 3, 'references': 3, 'source': 3, 'march': 3, 'published': 3, 'am': 3, 'decision': 3, '19': 3, 'association': 3, '40': 3, 'recognition': 3, 'machine': 3, 'ren': 3, 'sun': 3, 'into': 3, 'vision': 3, 'v': 3, '15': 3, 'syst': 3, 'biomed': 3, 'acharya': 3, 'u': 3, 'biol': 3, '2014': 3, 'recurrent': 3, 'short': 3, 'complex': 3, 'provided': 3, 'cb': 3, 'access': 3, 'design': 3, 'mp': 3, 'requests': 3, 'should': 3, 'springer': 3, 'according': 3, 'fully': 3, 'construct': 3, 'intentionally': 3, 'oversampled': 3, 'annotate': 3, 'compare': 3, 'combined': 3, 'outputs': 3, 'optimization': 3, 'connections': 3, 'batch': 3, 'de': 3, 'variable': 3, 'upon': 3, 'sciences': 3, 'palo': 2, 'alto': 2, 'pranav': 2, 'rajpurkar': 2, 'masoumeh': 2, 'geoffrey': 2, 'tison': 2, 'critical': 2, 'paradigm': 2, 'scalability': 2, 'previously': 2, 'reported': 2, '91232': 2, '53549': 2, 'monitoring': 2, 'device': 2, 'boardcertified': 2, 'positive': 2, 'predictive': 2, 'toend': 2, 'diagnos': 2, 'interpretations': 2, 'efficiency': 2, 'human': 2, 'accurately': 2, 'triaging': 2, 'urgent': 2, 'conditions': 2, 'obtained': 2, 'abnormalities': 2, 'become': 2, 'increasingly': 2, 'com': 2, 'rates': 2, 'being': 2, 'representations': 2, 'state': 2, 'image': 2, 'go': 2, 'requiring': 2, 'makes': 2, 'furthermore': 2, 'amount': 2, 'advantage': 2, 'employ': 2, 'toward': 2, 'pipeline': 2, 'detecting': 2, 'heartbeat': 2, 'normal': 2, 'ectopic': 2, 'so': 2, 'lack': 2, 'prior': 2, 'small': 2, 'large': 2, 'underwent': 2, 'inputs': 2, 'total': 2, 'continuously': 2, 'time': 2, '106': 2, 'days': 2, 'respectively': 2, '69': 2, '43': 2, 'women': 2, 'consisted': 2, '70': 2, '17': 2, '728': 2, 'make': 2, 'assessed': 2, 'above': 2, 'weighted': 2, 'aucs': 2, 'atrioventricular': 2, 'though': 2, 'majority': 2, 'multiclass': 2, 'hand': 2, 'sensi': 2, 'tivity': 2, '90': 2, 'addition': 2, 'aver': 2, 'performed': 2, 'main': 2, 'materially': 2, 'tables': 2, 'holding': 2, '6': 2, 'unlike': 2, 'plotted': 2, 'rocs': 2, 'figure': 2, 'showing': 2, 'illustrate': 2, 'problematic': 2, 'trained': 2, 'early': 2, 'demonstrates': 2, 'generalize': 2, '0973': 2, '0997': 2, '0842': 2, '0995': 2, '0702': 2, 'frequencyweighted': 2, 'describes': 2, 'interpreta': 2, 'us': 2, 'evaluate': 2, 'shift': 2, 'enable': 2, 'series': 2, 'include': 2, 'heuristics': 2, 'accept': 2, '0983': 2, 'red': 2, 'crosses': 2, 'green': 2, 'dot': 2, 'line': 2, 'represents': 2, 'probabilities': 2, 'sufficient': 2, 'man': 2, 'shared': 2, 'there': 2, 'represent': 2, 'informative': 2, 'artificial': 2, 'recently': 2, 'several': 2, 'algo': 2, 'studies': 2, 'employed': 2, 'form': 2, 'certain': 2, 'simultaneously': 2, 'through': 2, 'impact': 2, 'probably': 2, 'target': 2, 'even': 2, 'characteristics': 2, 'context': 2, 'postprocessing': 2, 'misclassifications': 2, 'revealed': 2, 'very': 2, 'correct': 2, 'beats': 2, 'committeelabeled': 2, 'remaining': 2, 'displayed': 2, 'activation': 2, 'earivr': 2, 'predicted': 2, 'matrix': 2, 'possible': 2, 'color': 2, 'ratio': 2, 'discussed': 2, 'require': 2, 'way': 2, 'labeled': 2, 'discordant': 2, 'uncertainty': 2, 'resourceintensive': 2, 'during': 2, 'summaries': 2, 'accession': 2, 'codes': 2, 'httpsdoiorg101038': 2, 's4159101802683': 2, '26': 2, 'coll': 2, 'hinton': 2, 'physiol': 2, 'guidelines': 2, 'electrocardiography': 2, 'task': 2, 'shah': 2, 'errors': 2, 'cardiac': 2, 'electrocardiol': 2, '2006': 2, '8': 2, '9': 2, 'search': 2, '14': 2, 'felix': 2, 'computing': 2, 'machinery': 2, 'recordings': 2, 'eng': 2, '2012': 2, 'paroxysmal': 2, '20': 2, 'signals': 2, 'inf': 2, 'sci': 2, 'ny': 2, 'segments': 2, 'clifford': 2, '112': 2, 'financial': 2, 'employees': 2, 'national': 2, 'author': 2, 'figures': 2, 'senior': 2, 'read': 2, 'approved': 2, 'manuscript': 2, 'paper': 2, 'institutional': 2, 'america': 2, 'sampling': 2, 'retrospective': 2, 'adult': 2, 'written': 2, 'informed': 2, 'consent': 2, 'board': 2, 'technician': 2, 'balance': 2, 'noted': 2, 'purpose': 2, 'annotators': 2, 'regarding': 2, 'consistency': 2, 'random': 2, 'tuning': 2, 'eight': 2, 'committees': 2, 'providing': 2, 'whenever': 2, 'takes': 2, '128': 2, '34': 2, 'filters': 2, '0': 2, 'factor': 2, 'linear': 2, 'preactivation': 2, 'structure': 2, 'softmax': 2, 'adam': 2, 'parameters': 2, 'found': 2, 'shortterm': 2, 'memory': 2, 'bidirectional': 2, 'nearest': 2, 'therefore': 2, 'identify': 2, 'give': 2, 'ci': 2, 'binary': 2, 'computed': 2, 'complementary': 2, 'less': 2, 'comparisons': 2, 'open': 2, 'httpsirhythmgithubiocardiol_test_set': 2, 'restriction': 2, 'apply': 2, 'license': 2, 'consider': 2, 'basis': 2, 'restricted': 2, 'noncommercial': 2, 'execution': 2, 'agreements': 2, 'simple': 2, 'mach': 2, '45': 2, 'dynamics': 2, '33': 2, 'comparing': 2, 'classifiers': 2, 'plot': 2, 'summarymarch': 2, 'following': 2, 'legend': 2, 'exact': 2, 'statistics': 2, 'central': 2, 'associated': 2, 'effect': 2, 'sizes': 2, 'values': 2, 'bars': 2, 'web': 2, 'saved': 2, 'then': 2, 'manuscripts': 2, 'statement': 2, 'your': 2, 'sections': 2, 'selection': 2, 'behavioural': 2, 'social': 2, '1department': 1, 'science': 1, '2irhythm': 1, 'echnologies': 1, '3division': 1, 'department': 1, 'california': 1, '4department': 1, 'center': 1, 'school': 1, '5veterans': 1, 'affairs': 1, '6these': 1, 'equally': 1, 'haghpanahi': 1, 'email': 1, 'awnicsstanfordedu': 1, 'plays': 1, 'role': 1, 'widely': 1, 'ing2': 1, 'opportunity': 1, 'substantially': 1, 'accu': 1, 'racy': 1, 'variety': 1, 'here': 1, 'develop': 1, 'independent': 1, 'tic': 1, 'reduce': 1, 'misdi': 1, 'agnosed': 1, 'prioritizing': 1, 'fundamental': 1, 'everyday': 1, 'practice': 1, '300': 1, 'million': 1, 'annually': 1, 'worldwide': 1, 'pivotal': 1, 'spectrum': 1, 'acute': 1, 'coronary': 1, 'syndrome4': 1, 'computeraided': 1, 'intro': 1, 'duction': 1, '50': 1, 'ago': 1, 'serving': 1, 'crucial': 1, 'adjunct': 1, 'physician': 1, 'mercial': 1, 'still': 1, 'show': 1, 'misdiagnosis157': 1, 'paradigms': 1, 'benefit': 1, 'largescale': 1, 'presents': 1, 'opportu': 1, 'nity': 1, 'reexamine': 1, 'analy': 1, 'sis': 1, 'improvements': 1, 'past': 1, 'five': 1, 'largely': 1, 'known': 1, 'networks2': 1, 'abstract': 1, 'higherlevel': 1, 'dramatically': 1, 'improved': 1, 'art': 1, 'recognition8': 1, 'recognition9': 1, 'games': 1, '1112': 1, 'recognize': 1, 'patterns': 1, 'extensive': 1, 'engineer': 1, 'handcrafted': 1, 'rules': 1, 'them': 1, 'suited': 1, 'interpret': 1, 'tends': 1, 'increases2': 1, 'positioned': 1, 'take': 1, 'analyze': 1, 'lacking': 1, 'much': 1, 'previous': 1, 'focused': 1, 'aspects': 1, 'reduc': 1, 'tion1314': 1, '1516': 1, 'approached': 1, 'diag': 1, 'nostic': 1, 'handful': 1, 'types': 1, 'fusion': 1, '1720': 1, 'commonly': 1, 'ventric': 1, 'ular': 1, '2125': 1, 'beyond': 1, 'mitbih': 1, 'physionet26': 1, 'episodes': 1, 'constructed': 1, 'recorded': 1, 'food': 1, 'drug': 1, 'administration': 1, 'fdacleared': 1, 'patchbased': 1, 'vector': 1, 'modified': 1, 'ii': 1, 'wear': 1, '130': 1, 'cardiologistlevel': 1, 'haghpanahi26': 1, 'codie': 1, 'bourn2': 1, 'mintu': 1, 'urakhia45': 1, 'andrew': 1, 'ng1': 1, 'corrected': 1, 'publisher': 1, 'correction': 1, '65': 1, 'calculat': 1, '1a': 1, 'secondwhich': 1, 'intervaland': 1, 'help': 1, 'capture': 1, 'whereas': 1, 'existence': 1, '091': 1, 'significance': 1, 'till28': 1, 'materi': 1, 'ally': 1, 'vice': 1, 'versa': 1, 'exception': 1, 'pairs': 1, 'commit': 1, 'tee': 1, 'trend': 1, 'tended': 1, 'follow': 1, 'aged': 1, 'frequency': 1, 'cardi': 1, 'ologist': 1, 'consistent': 1, '8761': 1, 'slightly': 1, 'retrained': 1, 'heldout': 1, '8768': 1, 'primary': 1, '1ab': 1, 'formance': 1, 'pres': 1, 'ents': 1, 'met': 1, 'fixing': 1, 'gist': 1, 'discordance': 1, 'predic': 1, 'tions': 1, '2a': 1, '2b': 1, 'exhibit': 1, 'highlighting': 1, 'generally': 1, 'generalizability': 1, 'external': 1, 'httpsphysionetorgchal': 1, 'lenge2017': 1, 'four': 1, 'keeping': 1, 'tun': 1, '8528': 1, 'stopping': 1, 'hidden': 1, '3658': 1, 'performers': 1, 'competition': 1, '724': 1, '083': 1, 'dnnbased': 1, '95': 1, 'cia': 1, 'sequencea': 1, 'setb': 1, '09660980': 1, '0965': 1, '09320998': 1, '0801': 1, '0831': 1, '0677': 1, '0686': 1, '0988': 1, '09830993': 1, '09531000': 1, '0828': 1, '0808': 1, '0772': 1, '09911000': 1, '09761000': 1, '0847': 1, '0870': 1, '0853': 1, '0913': 1, '08890937': 1, '0940': 1, '08701000': 1, '0596': 1, '0482': 1, '0536': 1, '09891000': 1, '09591000': 1, '0818': 1, '0632': 1, '0720': 1, '09800993': 1, '0979': 1, '09461000': 1, '0664': 1, '0789': 1, '0692': 1, '0679': 1, '09730989': 1, '0947': 1, '08980996': 1, '0844': 1, '0768': 1, '0685': 1, '0975': 1, '09710979': 1, '09760998': 1, '0887': 1, '0933': 1, '0852': 1, '0910': 1, '09600985': 1, '0953': 1, '09031000': 1, '0488': 1, '0693': 1, '0451': 1, '0564': 1, 'rigeminy': 1, '0998': 1, '09951000': 1, '09791000': 1, '0907': 1, '0864': 1, '0812': 1, '09801000': 1, '0980': 1, '09341000': 1, '0681': 1, '0566': 1, '0769': 1, '09670989': 1, '09381000': 1, '0591': 1, '0738': 1, '0807': 1, '0753': 1, 'adnn': 1, 'bdnn': 1, 'goldstandard': 1, 'naturemedicine66': 1, 'demonstration': 1, 'classweighted': 1, 'sensitivities': 1, 'compel': 1, 'revisit': 1, 'whose': 1, 'improves': 1, 'learning2': 1, 'leverage': 1, 'clear': 1, 'oppor': 1, 'tunities': 1, 'bring': 1, 'closer': 1, 'ideal': 1, 'sys': 1, 'tem29': 1, 'emphasize': 1, 'enough': 1, 'predict': 1, 'mul': 1, 'tiple': 1, 'subspecialized': 1, 'believe': 1, 'nearly': 1, 'represented': 1, 'employs': 1, 'various': 1, 'selectionreduction': 1, 'classification30': 1, 'step': 1, 'handengineered': 1, 'deri': 1, 'vations': 1, 'ultimate': 1, 'aim': 1, 'fibrillation3132': 1, 'contrast': 1, 'fundamentally': 1, 'accomplish': 1, 'classspecific': 1, 'words': 1, '0941': 1, '0710': 1, '0861': 1, '0731': 1, '0858': 1, '0829': 1, '0921': 1, '0993': 1, '0380': 1, '0445': 1, '0991': 1, '0611': 1, '0867': 1, '0984': 1, '0634': 1, '0729': 1, '0749': 1, '0803': 1, '0859': 1, '0901': 1, '0950': 1, '0408': 1, '0487': 1, '0652': 1, '0986': 1, '0651': 1, 'specificity1': 1, '10ppv': 1, '67': 1, 'ner': 1, 'along': 1, 'asyetunrecognized': 1, 'way2': 1, 'predicting': 1, 'multi': 1, 'ple': 1, 'properties': 1, 'serve': 1, 'ample': 1, 'evidence': 1, 'suggest': 1, 'currently': 1, 'recognized': 1, 'fraction': 1, 'diagnosis3334': 1, 'decades': 1, 'ago335': 1, 'until': 1, 'constrained': 1, 'rithmic': 1, 'deeper': 1, 'extraction33': 1, 'classification25': 1, 'good': 1, 'fibrillation222336': 1, 'arrhyth': 1, 'mias21': 1, '20213738': 1, 'promising': 1, 'do': 1, 'encountered': 1, '34layer': 1, 'enabled': 1, 'orders': 1, 'magnitude': 1, 'larger': 1, 'kind26': 1, 'approaches39': 1, 'fourier': 1, 'wavelet': 1, 'transforms40': 1, 'needed': 1, 'strong': 1, '56': 1, 'trials': 1, 'paired': 1, 'properly': 1, 'prelim': 1, 'inary': 1, 'cus': 1, 'tomize': 1, 'institution': 1, 'populationspecific': 1, 'institutionspecific': 1, 'pro': 1, 'vider': 1, 'confirmation': 1, 'tings': 1, 'expand': 1, 'capability': 1, 'overreader': 1, 'least': 1, 'ours': 1, 'ultimately': 1, 'tailored': 1, 'applying': 1, 'rithm': 1, 'sequentially': 1, 'result': 1, 'nontrivial': 1, 'falsepositive': 1, 'faced': 1, 'problem': 1, 'incorporate': 1, 'mechanisms': 1, 'taking': 1, 'increased': 1, 'epidemiology': 1, 'similarly': 1, 'finding': 1, 'appears': 1, 'recapitulate': 1, 'similarity': 1, 'discordances': 1, 'appear': 1, 'having': 1, 'conclusions': 1, 'reasonably': 1, 'drawn': 1, 'difficult': 1, 'definitively': 1, 'ascertain': 1, 'andor': 1, 'factors': 1, 'explain': 1, 'examined': 1, 'interestingly': 1, '941': 1, '784': 1, 'misclassified': 1, 'showed': 1, 'mistakes': 1, 'idioventricular': 1, 'differ': 1, 'below': 1, 'min': 1, 'ute': 1, 'periods': 1, 'ventricu': 1, 'lar': 1, 'close': 1, 'aberrant': 1, 'conduction': 1, 'resulting': 1, 'qrs': 1, 'plexes': 1, 'waveform': 1, 'appearance': 1, 'recategorize': 1, 'exceeds': 1, '082': 1, '077': 1, '10b': 1, '10a': 1, 'percentage': 1, 'category': 1, 'gradient': 1, 'scale': 1, 'naturemedicine68': 1, 'determined': 1, 'perfor': 1, 'mance': 1, 'signaltonoise': 1, 'leaves': 1, 'room': 1, 'ment': 1, 'greatest': 1, 'earlier': 1, 'limitation': 1, 'facing': 1, 'tailoring': 1, 'applica': 1, 'additionally': 1, 'systematic': 1, 'differences': 1, 'decreased': 1, 'took': 1, 'precautions': 1, 'limit': 1, 'establishing': 1, 'protocols': 1, 'addi': 1, 'cis': 1, 'acceptably': 1, 'narrow': 1, 'subgroup': 1, 'agesex': 1, 'quantity': 1, 'implies': 1, 'prevalencedependent': 1, 'expected': 1, 'broader': 1, 'population': 1, 'content': 1, 'statements': 1, 'asso': 1, 'ciated': 1, 'accepted': 1, 'october': 1, 'schläpfer': 1, 'wellens': 1, 'computerinterpreted': 1, 'benefits': 1, '11831192': 1, 'lecun': 1, 'bengio': 1, '521': 1, '436444': 1, 'holst': 1, 'ohlsson': 1, 'peterson': 1, 'edenbrandt': 1, 'confident': 1, 'interpreting': 1, 'clin': 1, '410418': 1, '1999': 1, 'schlant': 1, 'american': 1, 'college': 1, 'cardiologyamerican': 1, 'force': 1, 'assessment': 1, 'therapeutic': 1, 'cardiovascular': 1, '473481': 1, '1992': 1, 'rubin': 1, '385390': 1, '2007': 1, 'guglin': 1, 'thatai': 1, 'int': 1, '232237': 1, 'poon': 1, 'okin': 1, 'kligfield': 1, '235238': 1, '2005': 1, 'amodei': 1, 'english': 1, 'mandarin': 1, '33rd': 1, '173182': 1, 'delving': 1, 'rectifiers': 1, 'surpassing': 1, 'humanlevel': 1, 'imagenet': 1, '10261034': 1, 'silver': 1, 'mastering': 1, 'game': 1, 'tree': 1, '529': 1, '484489': 1, '11': 1, 'gulshan': 1, 'diabetic': 1, 'retinopathy': 1, 'retinal': 1, 'fundus': 1, 'photographs': 1, 'jama': 1, '316': 1, '24022410': 1, 'esteva': 1, 'dermatologistlevel': 1, 'skin': 1, 'cancer': 1, '542': 1, '115118': 1, 'poungponsri': 1, 'yu': 1, 'adaptive': 1, 'filtering': 1, 'reduction': 1, 'neurocomputing': 1, '117': 1, '206213': 1, 'ochoa': 1, 'mena': 1, 'noisetolerant': 1, '3rd': 1, 'compute': 1, '277282': 1, 'mateo': 1, 'rieta': 1, 'versatile': 1, 'technol': 1, '90101': 1, 'pourbabaee': 1, 'roshtkhari': 1, 'khorasani': 1, 'screening': 1, 'trans': 1, 'cybern': 1, '99': 1, '110': 1, 'javadi': 1, 'arani': 1, 'sajedin': 1, 'ebrahimpour': 1, 'modular': 1, 'mixture': 1, 'experts': 1, 'negatively': 1, 'correlated': 1, 'process': 1, 'control': 1, '289296': 1, 'heartbeats': 1, '89': 1, '389396': 1, 'banupriya': 1, 'karpagavalli': 1, 'beat': 1, 'probabilistic': 1, 'challenges': 1, 'opportunities': 1, 'ahead': 1, '3137': 1, 'rahhal': 1, 'active': 1, '345': 1, '340354': 1, '21': 1, '405': 1, '8190': 1, '22': 1, 'zihlmann': 1, 'perekrestenko': 1, 'tschannen': 1, 'httpsdoiorg1022489cinc2017070060': 1, 'xiong': 1, 'z': 1, 'zhao': 1, 'stiles': 1, 'robust': 1, 'httpsdoiorg1022489cinc2017066138': 1, '24': 1, 'recording': 1, 'physionetcomputing': 1, 'httpsdoiorg1022489cinc2017065469': 1, 'teijeiro': 1, 'garcia': 1, 'castro': 1, 'abductive': 1, 'httpsdoiorg1022489cinc2017166054': 1, 'goldberger': 1, 'physiobank': 1, 'physiotoolkit': 1, 'components': 1, 'resource': 1, 'physiologic': 1, 'circulation': 1, '101': 1, 'e215e220': 1, '2000': 1, 'turakhia': 1, 'utility': 1, 'leadless': 1, '520524': 1, 'acknowledgements': 1, 'funded': 1, 'nvidia': 1, 'fellowship': 1, 'institutes': 1, 'k23': 1, 'hl135274': 1, 'independently': 1, 'sponsor': 1, 'responsibility': 1, 'submit': 1, 'publication': 1, 'contributions': 1, 'responsible': 1, 'ran': 1, 'experiments': 1, 'created': 1, 'writing': 1, 'advised': 1, 'supervisor': 1, 'project': 1, 'submitted': 1, 'competing': 1, 'interests': 1, 'advisor': 1, 'cardiogram': 1, 'consultant': 1, 'none': 1, 'conflicts': 1, 'interest': 1, 'httpsdoiorg101038s4159101802683': 1, 'reprints': 1, 'permissions': 1, 'wwwnaturecomreprints': 1, 'correspondence': 1, 'materials': 1, 'addressed': 1, 'publishers': 1, 'neutral': 1, 'regard': 1, 'jurisdictional': 1, 'claims': 1, 'maps': 1, 'affiliations': 1, 'exclusive': 1, 'licence': 1, 'participants': 1, 'old': 1, 'insurance': 1, 'portability': 1, 'accountability': 1, 'act': 1, 'safe': 1, 'harbor': 1, 'privacy': 1, 'externally': 1, 'opt': 1, 'sharing': 1, 'accordingly': 1, 'necessary': 1, 'appropriately': 1, 'exempted': 1, 'produced': 1, 'includes': 1, 'initial': 1, 'fda': 1, '510k': 1, 'typically': 1, 'mix': 1, 'representation': 1, 'included': 1, 'completely': 1, 'disjointed': 1, 'sets': 1, 'reserving': 1, 'assigned': 1, 'specifically': 1, 'instructions': 1, 'transitions': 1, 'labeling': 1, 'held': 1, 'electrophysiologists': 1, 'referred': 1, 'removed': 1, 'divided': 1, 'members': 1, 'onethird': 1, 'type': 1, 'seconddegree': 1, 'mobitz': 1, 'iihay': 1, 'thirddegree': 1, 'because': 1, 'consequences': 1, 'artifact': 1, 'precluded': 1, 'accurate': 1, 'underlying': 1, 'ecgrelated': 1, 'tractable': 1, 'shortcut': 1, 'architecture41': 1, 'blocks': 1, 'filter': 1, 'width': 1, '322k': 1, 'starts': 1, 'incremented': 1, 'fourth': 1, 'alternate': 1, 'subsamples': 1, 'normalization42': 1, 'rectified': 1, 'adopting': 1, 'design43': 1, 'last': 1, 'specialcased': 1, 'due': 1, 'dropout44': 1, 'after': 1, 'nonlinearity': 1, 'probability': 1, 'connected': 1, 'produces': 1, 'distribution': 1, 'novo': 1, 'initialization': 1, 'weights': 1, 'al9': 1, 'optimizer45': 1, 'default': 1, 'β1': 1, '09': 1, 'β2': 1, '0999': 1, 'mini': 1, 'initialized': 1, '103': 1, 'reduced': 1, 'developmentally': 1, 'loss': 1, 'stopped': 1, 'improving': 1, 'consecutive': 1, 'epochs': 1, 'chose': 1, 'lowest': 1, 'general': 1, 'hyperparameters': 1, 'via': 1, 'grid': 1, 'searched': 1, 'primarily': 1, 'depth': 1, 'experimented': 1, 'cells46': 1, 'recurrence': 1, 'improvement': 1, 'runtime': 1, 'thus': 1, 'abandoned': 1, 'tuned': 1, 'fastest': 1, 'convergence': 1, 'start': 1, 'point': 1, 'rounding': 1, 'boundary': 1, 'does': 1, 'penalize': 1, 'misalignment': 1, 'allows': 1, 'comparison': 1, 'metric': 1, 'telemetry': 1, 'holter': 1, 'whereby': 1, 'abstraction': 1, 'approximating': 1, 'might': 1, 'train': 1, 'contains': 1, 'minor': 1, 'modifications': 1, 'change': 1, 'handle': 1, 'truncated': 1, '13s': 1, 'produce': 1, 'vote': 1, 'assess': 1, 'discrimination': 1, 'strategy2847': 1, 'presented': 1, 'separately': 1, 'sided': 1, 'scores48': 1, 'thresholds': 1, 'relationship': 1, 'recall49': 1, 'especially': 1, 'classimbalanced': 1, 'relative': 1, 'ranges': 1, 'rewards': 1, 'maximize': 1, 'rather': 1, 'favoring': 1, 'helpful': 1, 'setting': 1, 'sensitive': 1, 'imbalance49': 1, 'aggregate': 1, 'measure': 1, 'arithmetic': 1, 'compares': 1, 'individually': 1, 'times': 1, 'agreed': 1, 'pairwise': 1, 'linked': 1, 'article': 1, 'httpsgithub': 1, 'comawniecg': 1, '28': 1, 'till': 1, 'generalisation': 1, 'problems': 1, '171186': 1, '2001': 1, '29': 1, 'smith': 1, 'alin': 1, 'cost': 1, 'path': 1, 'academies': 1, 'press': 1, 'washington': 1, '30': 1, 'lyon': 1, 'mincholé': 1, 'martínez': 1, 'laguna': 1, 'rodriguez': 1, 'light': 1, 'contribution': 1, 'soc': 1, 'interface': 1, 'pii': 1, '20170821': 1, '31': 1, 'carrara': 1, 'distinguish': 1, 'frequent': 1, 'ectopy': 1, 'meas': 1, '18731888': 1, '32': 1, 'zhou': 1, 'ding': 1, 'ung': 1, 'pickwellmacpherson': 1, 'automatic': 1, 'symbolic': 1, 'shannon': 1, 'entropy': 1, 'hong': 1, 'encase': 1, 'ensemble': 1, 'classifier': 1, 'httpsdoi': 1, 'org1022489cinc2017178245': 1, 'nahar': 1, 'imam': 1, 'tickle': 1, 'chen': 1, 'intelligence': 1, 'disease': 1, 'appl': 1, '96104': 1, '35': 1, 'cubanski': 1, 'cyganski': 1, 'antman': 1, 'feldman': 1, 'cardiovasc': 1, 'electrophysiol': 1, '602608': 1, '1994': 1, 'andreotti': 1, 'carr': 1, 'o': 1, 'pimentel': 1, 'mahdi': 1, 'vos': 1, 'featurebased': 1, 'httpsdoiorg1022489cinc2017360239': 1, '37': 1, 'xu': 1, 'mak': 1, 'cheung': 1, 'towards': 1, 'informatics': 1, 'ong': 1, 'ng': 1, 'tan': 1, 'cnn': 1, 'lstm': 1, '102': 1, '278287': 1, '39': 1, 'shashikumar': 1, 'nemati': 1, 'attentionbased': 1, '24th': 1, 'acm': 1, 'sigkdd': 1, 'discovery': 1, 'mining': 1, '715723': 1, 'xia': 1, 'wulan': 1, 'wang': 1, '93': 1, '8492': 1, '41': 1, 'identity': 1, 'mappings': 1, 'european': 1, '630645': 1, '42': 1, 'ioffe': 1, 'szegedy': 1, 'normalization': 1, 'accelerating': 1, 'reducing': 1, 'internal': 1, 'covariate': 1, '448456': 1, '770778': 1, '44': 1, 'srivastava': 1, 'krizhevsky': 1, 'sutskever': 1, 'i': 1, 'salakhutdinov': 1, 'dropout': 1, 'prevent': 1, 'overfitting': 1, 'res': 1, '19291958': 1, 'kingma': 1, 'ba': 1, 'stochastic': 1, '115': 1, '46': 1, 'hochreiter': 1, 'schmidhuber': 1, '17351780': 1, '1997': 1, '47': 1, 'fawcett': 1, 'introduction': 1, 'recognit': 1, 'lett': 1, '861874': 1, '48': 1, 'hanley': 1, 'mcneil': 1, 'areas': 1, 'radiology': 1, '148': 1, '839843': 1, '1983': 1, '49': 1, 'saito': 1, 'rehmsmeier': 1, 'evaluating': 1, 'imbalanced': 1, 'plos': 1, 'e0118432': 1, 'followed': 1, 'accepts': 1, 'afatrial': 1, 'earectopic': 1, 'ivridioventricular': 1, 'vtventricular': 1, 'wishes': 1, 'reproducibility': 1, 'publish': 1, 'transparency': 1, 'policies': 1, 'referees': 1, 'editorial': 1, 'checklist': 1, 'confirm': 1, 'items': 1, 'location': 1, 'text': 1, 'na': 1, 'experimental': 1, 'groupcondition': 1, 'discrete': 1, 'unit': 1, 'measurement': 1, 'indication': 1, 'measurements': 1, 'taken': 1, 'measured': 1, 'repeatedly': 1, 'twosided': 1, 'solely': 1, 'name': 1, 'describe': 1, 'covariates': 1, 'tested': 1, 'assumptions': 1, 'corrections': 1, 'normality': 1, 'adjustment': 1, 'tendency': 1, 'means': 1, 'basic': 1, 'regression': 1, 'coefficient': 1, 'variation': 1, 'deviation': 1, 'null': 1, 'hypothesis': 1, 'testing': 1, 'statistic': 1, 'degrees': 1, 'freedom': 1, 'suitable': 1, 'bayesian': 1, 'choice': 1, 'priors': 1, 'markov': 1, 'chain': 1, 'monte': 1, 'carlo': 1, 'hierarchical': 1, 'designs': 1, 'identification': 1, 'outcomes': 1, 'cohens': 1, 'pearsons': 1, 'indicating': 1, 'clearly': 1, 'defined': 1, 'explicitly': 1, 'what': 1, 'sd': 1, 'se': 1, 'biologists': 1, 'build': 1, 'major': 1, 'bulk': 1, 'python': 1, 'containing': 1, 'postgresql': 1, 'pulled': 1, 'webbased': 1, 'htmlbased': 1, 'reviewing': 1, 'reviewers': 1, 'scroll': 1, 'segmentation': 1, 'pushed': 1, 'json': 1, 'format': 1, 'githubcomawniecg': 1, 'utilizing': 1, 'yet': 1, 'literature': 1, 'editorsreviewers': 1, 'request': 1, 'strongly': 1, 'encourage': 1, 'deposition': 1, 'community': 1, 'repository': 1, 'github': 1, 'submitting': 1, 'applicable': 1, 'identifiers': 1, 'links': 1, 'list': 1, 'fieldspecific': 1, 'please': 1, 'select': 1, 'fit': 1, 'you': 1, 'sure': 1, 'life': 1, 'reference': 1, 'copy': 1, 'document': 1, 'naturecomauthorspoliciesreportingsummaryflatpdf': 1, 'disclose': 1, 'points': 1, 'disclosure': 1, 'negative': 1, 'quantitative': 1, 'indications': 1, 'abnormal': 1, 'matched': 1, 'roughly': 1, 'justified': 1, 'posthoc': 1, 'computations': 1, 'exhibited': 1, 'prevalent': 1, 'equal': 1, 'timing': 1, 'retrospectively': 1, 'cohort': 1, 'exclusions': 1, 'excluded': 1, 'preestablished': 1, 'exclusion': 1, 'criteria': 1, 'order': 1, 'simplify': 1, 'approval': 1, 'processes': 1, 'minors': 1, 'nonparticipation': 1, 'did': 1, 'belongs': 1, 'identified': 1, 'randomization': 1, 'mentioned': 1, 'oversampling': 1, 'pool': 1})}, {'file_name': 'gpt-2', 'word_counts': Counter({'the': 736, 'of': 457, 'a': 367, 'and': 354, 'to': 336, 'in': 314, 'is': 183, 'on': 158, 'as': 134, 'for': 124, 'with': 123, 'are': 102, 'this': 100, 'that': 97, 'et': 96, 'al': 91, 'language': 89, 'from': 86, 'was': 70, 'i': 66, 'we': 64, 'model': 64, 'gpt2': 63, 'it': 61, 'which': 61, 'by': 56, '2018': 55, 'models': 54, 'arxiv': 51, 'preprint': 51, 'webtext': 50, 'be': 50, 'an': 48, 'j': 45, 'test': 43, 'training': 42, 'these': 41, 'performance': 41, 'unsupervised': 38, 'learning': 38, 'have': 37, 'he': 36, 'r': 36, 'multitask': 35, 'tasks': 35, 'm': 34, 'dataset': 33, 'or': 33, 'set': 33, 'not': 33, 'such': 31, 'systems': 31, 'many': 31, 'also': 31, '1': 30, 'but': 30, '2017': 30, 'you': 30, 'datasets': 29, '2016': 29, 'has': 29, 'task': 28, 'when': 28, 'were': 28, 'at': 27, 'tokens': 27, 'neural': 27, 'only': 26, 'table': 26, 'who': 26, 'they': 25, 'its': 25, 'k': 25, 'learners': 24, 'more': 24, 'can': 24, 'one': 24, 'if': 24, 'translation': 23, 'our': 23, 'than': 23, 'some': 23, 'work': 23, 'question': 23, 'overlap': 23, 'l': 23, 'results': 22, 'text': 22, 'been': 22, 'most': 22, 'will': 22, 'word': 22, 'while': 22, 'context': 22, 'c': 22, 'my': 22, 'data': 21, 'just': 21, 'le': 21, '2015': 21, 'what': 21, 'q': 21, 'natural': 20, 'perform': 20, 'like': 20, 'common': 20, 'french': 20, 's': 20, 'any': 19, '3': 19, 'lms': 19, 'used': 19, 'n': 19, 'all': 19, 'de': 19, 'so': 19, 'their': 18, 'there': 18, 'english': 18, 'much': 18, 'where': 18, 'pp': 18, 'his': 18, 'completion': 18, 'out': 17, 'state': 17, 'still': 17, 'would': 17, 'then': 17, '2019': 17, 'shown': 17, 'd': 17, 'chocolate': 17, 'cake': 17, 'processing': 16, 'without': 16, 'using': 16, 'large': 16, 'approach': 16, 'two': 16, 'do': 16, 'how': 16, 'ﬁrst': 16, 'p': 16, 'pacman': 16, 'questions': 15, 'often': 15, 'no': 15, 'about': 15, 'after': 15, 'up': 15, 't': 15, 'completions': 15, 'your': 15, 'la': 15, 'machine': 14, 'since': 14, 'answer': 14, 'information': 14, 'similar': 14, 'use': 14, 'dog': 14, 'both': 14, 'random': 14, 'had': 14, 'e': 14, 'reading': 13, 'supervised': 13, 'trained': 13, 'generated': 13, '4': 13, 'zeroshot': 13, 'modeling': 13, 'current': 13, 'each': 13, 'example': 13, 'very': 13, 'able': 13, 'contexts': 13, 'between': 13, 'other': 13, 'found': 13, 'small': 13, 'based': 13, 'could': 13, 'company': 13, 'answering': 12, 'examples': 12, 'art': 12, 'well': 12, '2': 12, 'same': 12, 'them': 12, 'does': 12, 'sentence': 12, 'b': 12, 'g': 12, 'o': 12, 'outﬁt': 12, 'largest': 11, 'setting': 11, 'now': 11, 'system': 11, 'comprehension': 11, 'way': 11, 'over': 11, 'years': 11, 'sequence': 11, 'reference': 11, 'before': 11, '256': 11, 'people': 11, 'y': 11, 'advances': 11, 'ghost': 11, 'ssh': 11, 'cave': 11, 'new': 10, 'capacity': 10, 'train': 10, 'size': 10, 'networks': 10, 'sampling': 10, 'even': 10, 'due': 10, 'probability': 10, 'us': 10, 'answers': 10, 'war': 10, 'into': 10, 'right': 10, 'top': 10, 'spain': 10, 'sutskever': 9, 'learn': 9, 'coqa': 9, 'transfer': 9, '8': 9, 'samples': 9, 'better': 9, 'need': 9, 'however': 9, 'objective': 9, 'figure': 9, 'order': 9, 'level': 9, 'larger': 9, 'man': 9, 'world': 9, 'did': 9, 'time': 9, 'version': 9, 'lm': 9, 'smallest': 9, 'perplexity': 9, 'lambada': 9, 'those': 9, 'sweden': 9, 'vinyals': 9, 'deep': 9, 'generation': 9, 'v': 9, 'color': 9, 'made': 9, 'safe': 9, 'house': 9, 'summarization': 8, 'across': 8, '7': 8, 'general': 8, 'served': 8, 'mccann': 8, 're': 8, 'suggests': 8, 'may': 8, 'pretraining': 8, 'shows': 8, 'signiﬁcant': 8, 'different': 8, 'articles': 8, 'web': 8, 'im': 8, 'get': 8, 'challenge': 8, 'bpe': 8, 'words': 8, '12': 8, 'few': 8, 'approximately': 8, 'original': 8, '5': 8, 'previous': 8, 'article': 8, 'research': 8, 'ing': 8, 'conference': 8, 'sneakers': 8, 'left': 8, 'muslim': 8, 'until': 8, 'permissions': 8, 'recipe': 8, 'cup': 8, 'que': 8, 'radford': 7, 'speciﬁc': 7, 'begin': 7, 'paragraphs': 7, '2014': 7, 'behavior': 7, 'possible': 7, 'single': 7, 'domains': 7, 'pairs': 7, 'contains': 7, 'representations': 7, 'another': 7, 'demonstrated': 7, 'analysis': 7, 'paper': 7, 'ability': 7, 'whether': 7, 'government': 7, 'schema': 7, 'want': 7, 'created': 7, '40': 7, 'representation': 7, 'benchmark': 7, 'layers': 7, '768': 7, 'ﬁnal': 7, 'according': 7, 'accuracy': 7, 'book': 7, 'hill': 7, 'topk': 7, 'during': 7, 'unicorns': 7, 'fully': 7, 'collins': 7, 'wearing': 7, 'pinky': 7, 'ghosts': 7, 'player': 7, 'xresources': 7, 'sugar': 7, 'document': 6, 'f1': 6, 'promising': 6, 'make': 6, 'likely': 6, 'nlp': 6, '10': 6, 'techniques': 6, 'additional': 6, 'best': 6, 'see': 6, 'result': 6, 'learned': 6, '2013': 6, 'devlin': 6, 'reasoning': 6, 'sequences': 6, 'instead': 6, 'predict': 6, 'en': 6, 'bit': 6, 'go': 6, 'winograd': 6, 'links': 6, 'least': 6, 'preprocessing': 6, 'tokenization': 6, 'strings': 6, 'billion': 6, 'byte': 6, 'including': 6, 'understanding': 6, 'ﬁnetuning': 6, 'simple': 6, 'childrens': 6, 'gpt2s': 6, 'must': 6, 'help': 6, '14': 6, 'president': 6, 'sets': 6, 'already': 6, 'h': 6, 'proceedings': 6, 'through': 6, 'unseen': 6, 'summer': 6, 'casual': 6, 'things': 6, '40was': 6, 'rule': 6, 'reconquista': 6, 'last': 6, 'location': 6, 'york': 6, 'tried': 6, 'captain': 6, 'cygwin': 6, 'submarine': 6, 'her': 6, 'she': 6, 'torch': 6, 'tion': 5, 'conditioned': 5, 'baseline': 5, 'improves': 5, 'improvements': 5, 'suggest': 5, 'towards': 5, 'correct': 5, 'heldout': 5, 'diversity': 5, 'inputs': 5, 'domain': 5, 'require': 5, 'wang': 5, 'improving': 5, 'recent': 5, 'pair': 5, 'approaches': 5, 'scale': 5, 'performing': 5, 'artetxe': 5, 'long': 5, 'dai': 5, 'longer': 5, 'methods': 5, 'commonsense': 5, 'symbols': 5, 'should': 5, 'input': 5, 'kaiser': 5, 'outputs': 5, 'weston': 5, 'amount': 5, 'prior': 5, 'jozefowicz': 5, 'wikipedia': 5, 'diverse': 5, 'crawl': 5, 'content': 5, 'un': 5, 'documents': 5, 'target': 5, 'point': 5, 'million': 5, 'evaluation': 5, 'space': 5, 'bytelevel': 5, 'name': 5, 'code': 5, 'vocabulary': 5, 'layer': 5, 'residual': 5, 'second': 5, 'bert': 5, 'reported': 5, 'average': 5, 'sentences': 5, 'ppl': 5, 'cbt': 5, 'times': 5, 'worse': 5, 'human': 5, 'named': 5, 'overall': 5, 'baselines': 5, 'michael': 5, 'john': 5, '30': 5, 'conneau': 5, 'corpus': 5, 'conversational': 5, 'previously': 5, '6': 5, 'change': 5, '13': 5, 'liu': 5, 'shazeer': 5, 'f': 5, 'ﬁnd': 5, 'shoes': 5, 'fact': 5, 'colors': 5, 'high': 5, 'me': 5, 'day': 5, 'noncherrypicked': 5, 'worth': 5, 'built': 5, 'return': 5, 'march': 5, 'attempt': 5, 'because': 5, 'enlisted': 5, 'returned': 5, 'fonts': 5, 'goes': 5, 'mint': 5, 'egg': 5, 'perez': 5, 'said': 5, 'race': 5, 'france': 5, 'replica': 5, 'hernia': 5, 'relay': 5, 'plus': 4, 'success': 4, 'tested': 4, 'lan': 4, 'guage': 4, 'building': 4, 'combination': 4, '2012': 4, 'rather': 4, 'united': 4, 'states': 4, 'manually': 4, 'distributed': 4, 'progress': 4, 'architectures': 4, 'wide': 4, 'range': 4, 'several': 4, 'exploring': 4, 'function': 4, 'cnn': 4, 'daily': 4, 'mail': 4, 'history': 4, 'vectors': 4, 'peters': 4, 'sufﬁcient': 4, 'con': 4, 'potential': 4, 'allows': 4, 'vaswani': 4, 'infer': 4, 'type': 4, 'pre': 4, 'slightly': 4, 'side': 4, 'experiments': 4, 'step': 4, 'dialog': 4, 'prediction': 4, 'news': 4, 'trinh': 4, '16': 4, 'something': 4, 'always': 4, 'says': 4, 'conversation': 4, 'du': 4, 'avoid': 4, 'humans': 4, 'full': 4, 'thought': 4, 'generate': 4, 'lower': 4, 'unicode': 4, 'own': 4, 'standard': 4, 'greedy': 4, 'evaluate': 4, 'details': 4, 'gpt': 4, 'parameters': 4, 'rate': 4, 'given': 4, 'signiﬁcantly': 4, 'having': 4, 'wikitext2': 4, 'ptb': 4, 'bajgar': 4, 'grave': 4, 'measure': 4, 'quite': 4, 'recommend': 4, 'development': 4, 'metrics': 4, 'summaries': 4, 'country': 4, 'china': 4, 'released': 4, 'emperor': 4, 'good': 4, 'actually': 4, 'memorization': 4, 'images': 4, 'low': 4, 'overlaps': 4, 'various': 4, 'everyone': 4, 'speech': 4, 'under': 4, 'x': 4, 'think': 4, 'socher': 4, 'half': 4, 'heavy': 4, 'light': 4, 'suit': 4, 'too': 4, 'asics': 4, 'again': 4, 'thats': 4, 'dont': 4, 'look': 4, 'went': 4, 'modern': 4, 'granada': 4, 'era': 4, 'empire': 4, 'south': 4, 'army': 4, 'corporal': 4, 'life': 4, 'james': 4, 'three': 4, 'military': 4, 'year': 4, 'directory': 4, 'peppermint': 4, 'frosting': 4, 'buttermilk': 4, 'chopped': 4, 'valley': 4, 'erez': 4, 'others': 4, 'creatures': 4, 'site': 4, 'yemen': 4, 'fonctionnelle': 4, 'jai': 4, 'reussi': 4, 'him': 4, 'par': 4, 'tom': 4, 'catherine': 4, 'taxes': 4, 'amodei': 3, 'demonstrate': 3, 'ex': 3, 'called': 3, 'reach': 3, 'parameter': 3, 'transformer': 3, 'achieves': 3, 'contain': 3, 'occurring': 3, 'demonstrations': 3, 'yet': 3, 'experts': 3, 'move': 3, 'create': 3, 'creating': 3, 'generalization': 3, 'observed': 3, 'recently': 3, 'benchmarks': 3, 'glue': 3, 'date': 3, 'total': 3, 'bowman': 3, 'difﬁcult': 3, 'reddy': 3, 'wmt14': 3, 'kwiatkowski': 3, 'forms': 3, 'first': 3, 'mikolov': 3, 'selfattention': 3, 'minimal': 3, 'available': 3, 'connect': 3, 'competitive': 3, 'estimation': 3, 'probabilities': 3, 'conditional': 3, 'bengio': 3, 'form': 3, 'mod': 3, 'compute': 3, 'performed': 3, 'settings': 3, 'encoders': 3, 'translate': 3, 'explicit': 3, 'evaluated': 3, 'global': 3, 'sufﬁciently': 3, 'capable': 3, 'directly': 3, 'case': 3, 'books': 3, 'kiros': 3, 'source': 3, 'nearly': 3, 'quality': 3, 'whose': 3, 'mostly': 3, 'initial': 3, 'say': 3, 'wrote': 3, 'il': 3, 'means': 3, 'improve': 3, 'ahead': 3, 'expensive': 3, 'platform': 3, 'heuristic': 3, 'include': 3, 'deduplication': 3, 'removed': 3, 'string': 3, 'bytes': 3, 'practical': 3, 'character': 3, 'base': 3, 'added': 3, 'token': 3, 'add': 3, 'multiple': 3, '24': 3, 'normalization': 3, 'network': 3, 'number': 3, 'four': 3, 'improved': 3, 'commonly': 3, 'scaled': 3, 'log': 3, 'canonical': 3, 'artifacts': 3, 'wikitext103': 3, '1bw': 3, 'acc': 3, 'sota': 3, 'gong': 3, 'hoang': 3, '26': 3, 'detokenizers': 3, 'remove': 3, 'noticed': 3, 'paperno': 3, 'being': 3, 'constructed': 3, 'rest': 3, 'seen': 3, 'style': 3, 'increases': 3, 'ﬁlter': 3, 'further': 3, 'appeared': 3, '34': 3, 'paired': 3, 'why': 3, 'matches': 3, 'bottomup': 3, 'sum': 3, '100': 3, 'fan': 3, 'abstractive': 3, 'generations': 3, 'involved': 3, 'shirt': 3, 'metric': 3, 'bleu': 3, 'green': 3, 'national': 3, 'came': 3, 'wars': 3, 'western': 3, 'game': 3, 'river': 3, 'european': 3, 'power': 3, 'conﬁdent': 3, 'lample': 3, 'correctly': 3, 'open': 3, 'alberti': 3, 'cifar10': 3, 'important': 3, 'bloom': 3, 'ﬁlters': 3, 'containing': 3, 'discovered': 3, 'theme': 3, 'together': 3, 'rnn': 3, 'explored': 3, 'mentioned': 3, 'levy': 3, 'evaluating': 3, 'kiela': 3, 'show': 3, 'pointer': 3, 'thanks': 3, 'jones': 3, 'lee': 3, 'z': 3, 'chen': 3, 'international': 3, 'beyond': 3, 'universal': 3, 'yang': 3, 'w': 3, 'uszkoreit': 3, 'standing': 3, 'story': 3, 'ali': 3, 'xiong': 3, 'manning': 3, 'via': 3, 'once': 3, 'less': 3, 'close': 3, 'jacket': 3, 'deﬁnitely': 3, 'off': 3, 'down': 3, 'wear': 3, 'sure': 3, 'end': 3, 'try': 3, 'bad': 3, 'turtleneck': 3, 'feet': 3, 'period': 3, 'mosque': 3, 'cordoba': 3, 'old': 3, 'coast': 3, 'peninsula': 3, 'ai': 3, 'pacmans': 3, 'direction': 3, 'expected': 3, 'tile': 3, 'intersection': 3, 'players': 3, 'began': 3, 'games': 3, 'next': 3, 'exit': 3, 'reenlisted': 3, 'promoted': 3, 'along': 3, 'dall': 3, 'font': 3, 'part': 3, 'folder': 3, '0600': 3, 'allow': 3, 'owner': 3, 'package': 3, 'chroot': 3, 'ﬁle': 3, 'chips': 3, 'crushed': 3, 'started': 3, 'butter': 3, 'cups': 3, 'scientist': 3, 'mountains': 3, 'unicorn': 3, 'animals': 3, 'seem': 3, 'prehistoric': 3, 'incredible': 3, 'vallonpontdarc': 3, 'around': 3, 'preserved': 3, 'exploration': 3, 'diving': 3, 'studio': 3, 'remixes': 3, 'dvd': 3, 'clips': 3, 'ete': 3, 'homme': 3, 'quil': 3, 'avait': 3, 'kerry': 3, 'guerre': 3, 'certain': 3, '2008': 3, 'olympics': 3, 'beijing': 3, 'olympic': 3, 'yes': 3, 'pay': 3, 'alec': 2, 'wu': 2, 'ques': 2, 'com': 2, 'typically': 2, 'supervision': 2, 'millions': 2, 'webpages': 2, '55': 2, 'matching': 2, '127000': 2, 'reﬂect': 2, 'ﬁndings': 2, 'path': 2, 'pro': 2, 'naturally': 2, 'introduction': 2, 'highcapacity': 2, 'krizhevsky': 2, 'changes': 2, 'distribution': 2, 'recht': 2, 'kirkpatrick': 2, 'narrow': 2, 'equal': 2, 'contribution': 2, 'san': 2, 'ml': 2, 'desired': 2, 'behaviors': 2, 'independent': 2, 'lake': 2, 'jia': 2, 'liang': 2, 'classiﬁers': 2, 'alcorn': 2, 'variety': 2, 'highlights': 2, 'short': 2, 'major': 2, 'lack': 2, 'measuring': 2, 'proposed': 2, 'decanlp': 2, 'caruana': 2, '1997': 2, 'mul': 2, 'yogatama': 2, 'metalearning': 2, 'sampled': 2, 'objectives': 2, 'thousands': 2, 'generalize': 2, 'effective': 2, 'promise': 2, 'continue': 2, 'creation': 2, 'design': 2, 'motivates': 2, 'section': 2, 'trend': 2, 'ﬂexible': 2, 'taskspeciﬁc': 2, 'collobert': 2, '2011': 2, 'recurrent': 2, 'transferred': 2, 'line': 2, 'schwartz': 2, 'sentiment': 2, 'downstream': 2, 'archi': 2, 'tecture': 2, 'modiﬁcation': 2, 'usually': 2, 'distri': 2, 'bution': 2, 'length': 2, 'product': 2, 'jelinek': 2, 'mercer': 2, '1980': 2, '2003': 2, 'px': 2, 'els': 2, 'probabilistic': 2, 'framework': 2, 'poutputinput': 2, 'condition': 2, 'decoders': 2, 'finn': 2, 'exempliﬁed': 2, 'provides': 2, 'written': 2, 'format': 2, 'principle': 2, 'subset': 2, 'minimum': 2, 'principled': 2, 'problem': 2, 'becomes': 2, 'practice': 2, 'preliminary': 2, 'setup': 2, 'described': 2, 'develop': 2, 'regardless': 2, 'method': 2, 'main': 2, 'merity': 2, 'varied': 2, 'issues': 2, 'doc': 2, 'mentez': 2, 'translates': 2, 'lie': 2, 'somewhat': 2, 'pour': 2, 'aller': 2, 'cote': 2, 'literally': 2, 'gouvernement': 2, 'throughout': 2, 'achieved': 2, 'included': 2, 'making': 2, 'scrape': 2, 'scraped': 2, 'pages': 2, 'ﬁltering': 2, 'starting': 2, 'social': 2, 'media': 2, 'link': 2, 'interesting': 2, 'extract': 2, 'lecocq': 2, 'sults': 2, 'presented': 2, '22': 2, 'steps': 2, 'ful': 2, 'gillick': 2, 'wordlevel': 2, 'alrfou': 2, 'gap': 2, 'attempts': 2, 'sennrich': 2, 'middle': 2, 'ground': 2, 'symbol': 2, 'points': 2, 'compared': 2, 'requires': 2, 'versions': 2, 'exception': 2, 'adding': 2, 'vocab': 2, 'empirical': 2, '117m': 2, '345m': 2, '1024': 2, '762m': 2, '36': 2, '1542m': 2, 'sizes': 2, 'ba': 2, 'attention': 2, 'factor': 2, '512': 2, 'equivalent': 2, 'sample': 2, '31': 2, 'quantity': 2, 'per': 2, 'units': 2, 'enwik8': 2, 'text8': 2, '998': 2, '108': 2, '834': 2, '863': 2, '6324': 2, 'rare': 2, 'report': 2, 'invertible': 2, 'calculate': 2, 'adaptation': 2, 'observe': 2, 'gains': 2, 'dependencies': 2, 'chelba': 2, 'longrange': 2, '32': 2, 'entities': 2, 'nouns': 2, 'tions': 2, 'met': 2, 'reports': 2, 'cloze': 2, 'choices': 2, 'following': 2, 'introduced': 2, 'choice': 2, 'increased': 2, 'showed': 2, '33': 2, 'tests': 2, '50': 2, '86': 2, '19': 2, 'dehghani': 2, 'errors': 2, 'valid': 2, 'helpful': 2, 'levesque': 2, 'follow': 2, 'achieving': 2, 'trichelair': 2, '35': 2, 'decoding': 2, 'associated': 2, 'seq2seq': 2, 'hint': 2, 'measured': 2, 'rouge': 2, 'gehrmann': 2, 'pervised': 2, 'retrieval': 2, 'nallapati': 2, 'sam': 2, 'reduces': 2, 'qualitatively': 2, 'classic': 2, 'barely': 2, 'outperforms': 2, 'ﬁ': 2, 'nal': 2, 'gets': 2, 'ferred': 2, 'species': 2, 'charles': 2, 'darwin': 2, 'founder': 2, 'project': 2, 'aaron': 2, 'star': 2, 'took': 2, 'author': 2, 'sun': 2, 'ofﬁcial': 2, 'montana': 2, 'reserve': 2, 'city': 2, 'johnson': 2, 'head': 2, 'union': 2, 'shoot': 2, 'evolution': 2, '457': 2, 'connor': 2, 'procedure': 2, '2017b': 2, 'sur': 2, 'ran': 2, 'monolingual': 2, 'trans': 2, 'lation': 2, 'within': 2, 'stored': 2, 'exact': 2, 'etc': 2, 'poor': 2, 'kind': 2, '631': 2, 'extractive': 2, '17': 2, 'got': 2, 'computer': 2, 'vision': 2, 'age': 2, 'instance': 2, 'barz': 2, 'denzler': 2, 'ma': 2, 'issue': 2, 'happening': 2, '8grams': 2, 'recall': 2, 'false': 2, 'positive': 2, 'generating': 2, 'percentage': 2, 'splits': 2, 'phrases': 2, 'potentially': 2, '8gram': 2, 'portion': 2, 'shared': 2, 'gave': 2, 'away': 2, '15': 2, 'actual': 2, '200': 2, 'beneﬁt': 2, 'ways': 2, 'talking': 2, 'hestness': 2, 'generative': 2, 'tracking': 2, 'karpathy': 2, 'iweb': 2, 'davies': 2, 'extensive': 2, 'glove': 2, 'pennington': 2, 'skipthought': 2, 'howard': 2, 'ruder': 2, '2017a': 2, 'studied': 2, 'inference': 2, 'subramanian': 2, 'ramachandran': 2, 'wolf': 2, 'dinan': 2, 'goldberg': 2, 'wieting': 2, 'plan': 2, 'especially': 2, 'unclear': 2, 'surprising': 2, 'cheng': 2, 'daniel': 2, 'recognition': 2, 'labaka': 2, 'agirre': 2, 'cho': 2, 'hinton': 2, 'journal': 2, 'friends': 2, 'schuster': 2, 'almost': 2, 'des': 2, 'ranzato': 2, 'denoyer': 2, 'salakhutdinov': 2, 'ł': 2, 'transformers': 2, 'agents': 2, 'rush': 2, 'zhou': 2, 'repre': 2, 'sentations': 2, 'classiﬁcation': 2, 'volume': 2, 'workshop': 2, 'gomez': 2, 'parmar': 2, 'polosukhin': 2, 'pot': 2, 'bradbury': 2, 'contextualized': 2, 'lazaridou': 2, 'trischler': 2, 'required': 2, '11': 2, 'famous': 2, 'drifts': 2, 'rates': 2, 'truncated': 2, 'generally': 2, 'denim': 2, 'cotton': 2, 'db': 2, 'linen': 2, 'fabrics': 2, 'absolute': 2, 'dark': 2, 'lot': 2, 'keeping': 2, 'id': 2, 'white': 2, 'tie': 2, 'crew': 2, 'youre': 2, 'trying': 2, 'features': 2, 'thing': 2, 'legs': 2, 'keep': 2, 'nice': 2, 'black': 2, 'favorite': 2, 'pick': 2, 'nothing': 2, 'sign': 2, 'know': 2, 'myself': 2, 'tiger': 2, 'killshots': 2, 'wish': 2, 'shoe': 2, 'comfortable': 2, 'never': 2, 'palace': 2, 'imperial': 2, 'parts': 2, 'spanish': 2, 'naples': 2, 'roman': 2, 'ferdinand': 2, 'lived': 2, 'muslims': 2, 'population': 2, 'iberian': 2, 'europe': 2, 'native': 2, 'known': 2, 'moors': 2, 'conquest': 2, 'sometimes': 2, 'referred': 2, 'lasted': 2, 'designed': 2, 'feel': 2, 'tiles': 2, 'includes': 2, 'offset': 2, 'turning': 2, 'coming': 2, 'simply': 2, 'leave': 2, 'ones': 2, 'getting': 2, 'caught': 2, 'chase': 2, 'travels': 2, 'yellow': 2, 'room': 2, 'place': 2, 'theyre': 2, 'blinky': 2, 'soon': 2, 'mode': 2, 'ireland': 2, '1860': 2, 'oneal': 2, '3d': 2, 'browne': 2, 'august': 2, 'olunteers': 2, 'men': 2, '1864': 2, 'completing': 2, 'enlistment': 2, 'june': 2, 'service': 2, 'born': 2, 'december': 2, 'april': 2, 'later': 2, 'assigned': 2, 'division': 2, 'civil': 2, 'worked': 2, 'russian': 2, 'sought': 2, 'syrian': 2, 'border': 2, 'syria': 2, 'veteran': 2, 'remained': 2, 'active': 2, 'duty': 2, 'virginia': 2, '1845': 2, 'regiment': 2, 'discharge': 2, 'campaign': 2, 'battery': 2, 'gunboat': 2, 'boats': 2, 'xmonad': 2, 'xft': 2, 'library': 2, 'speciﬁed': 2, 'here': 2, 'windows': 2, 'group': 2, 'linux': 2, 'installing': 2, 'wasnt': 2, 'script': 2, 'enough': 2, 'back': 2, '0644': 2, 'didnt': 2, 'annoying': 2, 'xsessionrc': 2, 'xproﬁle': 2, 'xsessionerrors': 2, 'separate': 2, 'blue': 2, 'threw': 2, 'jo': 2, 'hand': 2, 'meringue': 2, 'cookies': 2, 'cookie': 2, 'crunchy': 2, 'topping': 2, 'anything': 2, '7inch': 2, '8inch': 2, 'pan': 2, 'whites': 2, 'bottom': 2, 'bowl': 2, 'peaks': 2, 'put': 2, 'water': 2, 'minutes': 2, 'rules': 2, 'comes': 2, 'buy': 2, 'else': 2, 'really': 2, 'cute': 2, 'little': 2, 'ingredients': 2, 'yolk': 2, 'powdered': 2, 'pinch': 2, 'salt': 2, 'tsp': 2, 'sea': 2, 'air': 2, 'melted': 2, 'batter': 2, 'give': 2, 'round': 2, 'parchment': 2, 'bake': 2, 'herd': 2, 'andes': 2, 'spoke': 2, 'perfect': 2, 'unknown': 2, 'dr': 2, 'descendants': 2, 'lost': 2, 'america': 2, 'believe': 2, 'sketched': 2, 'array': 2, 'beasts': 2, 'rough': 2, 'limestone': 2, 'walls': 2, '36000': 2, 'ago': 2, 'caverne': 2, 'pontdarc': 2, 'miles': 2, 'southern': 2, '1000': 2, 'bones': 2, 'oldest': 2, 'decorated': 2, 'woolly': 2, 'rhinoceros': 2, 'mammoths': 2, 'oceans': 2, 'ocean': 2, 'snorkelling': 2, 'night': 2, 'amina': 2, 'qassim': 2, 'months': 2, 'baby': 2, 'missile': 2, 'everything': 2, 'saudi': 2, 'arabia': 2, 'bombing': 2, 'album': 2, 'sessions': 2, 'les': 2, 'premier': 2, 'une': 2, 'qui': 2, 'expliquait': 2, 'fonctionnement': 2, 'reconnaˆıt': 2, 'avant': 2, 'faire': 2, 'fonc': 2, 'tionnement': 2, 'lui': 2, 'london': 2, 'internationale': 2, 'vide': 2, 'dune': 2, 'indeniable': 2, 'secretary': 2, 'passage': 2, 'questionanswer': 2, 'dream': 2, 'days': 2, '137000': 2, 'km': 2, 'birthplace': 2, 'olympia': 2, 'greece': 2, 'panathinaiko': 2, 'stadium': 2, 'athens': 2, 'passing': 2, 'road': 2, 'everest': 2, 'notable': 2, 'turker': 2, 'selected': 2, 'moves': 2, 'paying': 2, 'health': 2, 'live': 2, 'care': 2, 'swedish': 2, 'treatment': 2, 'owners': 2, 'damage': 2, 'car': 2, 'jeffrey': 1, 'rewon': 1, 'child1': 1, 'david': 1, 'luan1': 1, 'dario': 1, 'ilya': 1, 'abstract': 1, 'prehension': 1, 'approached': 1, 'plicit': 1, 'swers': 1, 'exceeding': 1, 'essential': 1, 'creasing': 1, 'loglinear': 1, 'fashion': 1, '15b': 1, 'underﬁts': 1, 'co': 1, 'herent': 1, 'cessing': 1, 'excel': 1, 'expectation': 1, 'brittle': 1, 'sensitive': 1, 'slight': 1, 'speciﬁcation': 1, 'sys': 1, 'tems': 1, 'characterized': 1, '1openai': 1, 'francisco': 1, 'califor': 1, 'nia': 1, 'correspondence': 1, 'alecopenaicom': 1, 'competent': 1, 'generalists': 1, 'eventually': 1, 'label': 1, 'dominant': 1, 'col': 1, 'lect': 1, 'demonstrating': 1, 'imitate': 1, 'identically': 1, 'iid': 1, 'erratic': 1, 'captioning': 1, 'image': 1, 'comings': 1, 'suspicion': 1, 'prevalence': 1, 'contributor': 1, 'robust': 1, 'studying': 1, 'frame': 1, 'titask': 1, 'nascent': 1, 'ports': 1, 'modest': 1, 'ambitious': 1, 'efforts': 1, '17dataset': 1, 'respectively': 1, 'perspective': 1, 'hundreds': 1, 'induce': 1, 'functions': 1, 'realize': 1, 'degree': 1, 'quired': 1, 'brute': 1, 'force': 1, 'setups': 1, '1zeroshot': 1, 'fren': 1, 'detailed': 1, 'descriptions': 1, 'utilize': 1, 'ﬁne': 1, 'tuning': 1, 'wards': 1, 'architec': 1, 'tures': 1, 'contextual': 1, 'cent': 1, 'necessary': 1, 'transferring': 1, 'blocks': 1, 'lines': 1, 'tinue': 1, 'highlighting': 1, 'achieve': 1, 'depending': 1, 'core': 1, 'framed': 1, 'x1': 1, 'x2': 1, 'xn': 1, 'composed': 1, 'variable': 1, 's1': 1, 's2': 1, 'sn': 1, 'sequential': 1, 'dering': 1, 'factorize': 1, 'joint': 1, 'i1': 1, 'psns1': 1, 'sn1': 1, 'tractable': 1, 'es': 1, 'timation': 1, 'conditionals': 1, 'psnk': 1, 'sns1': 1, 'snk1': 1, 'expressiveness': 1, 'expressed': 1, 'estimating': 1, 'variously': 1, 'formalized': 1, 'conditioning': 1, 'implemented': 1, 'architectural': 1, 'algorithmic': 1, 'inner': 1, 'outer': 1, 'loop': 1, 'optimization': 1, 'maml': 1, 'specify': 1, 'wise': 1, 'demon': 1, 'strated': 1, 'mqan': 1, 'dicted': 1, 'toy': 1, 'concerns': 1, 'density': 1, 'discussed': 1, 'stepped': 1, 'optimize': 1, 'unsuper': 1, 'vised': 1, 'convergence': 1, 'conﬁrmed': 1, 'toyish': 1, 'slower': 1, 'explicitly': 1, 'wellposed': 1, 'above': 1, 'messiness': 1, 'wild': 1, 'argues': 1, 'proof': 1, 'concept': 1, 'qa': 1, 'reward': 1, 'signal': 1, 'forward': 1, 'teachers': 1, 'attractive': 1, 'worry': 1, 'overly': 1, 'restrictive': 1, 'internet': 1, 'vast': 1, 'passively': 1, 'interactive': 1, 'communication': 1, 'speculation': 1, 'procurement': 1, 'effect': 1, 'analyzing': 1, '21': 1, 'ﬁction': 1, 'collect': 1, 'unlimited': 1, 'scrapes': 1, 'archives': 1, 'orders': 1, 'magnitude': 1, 'noted': 1, 'uments': 1, 'unintelligible': 1, 'ob': 1, 'cleverest': 1, 'je': 1, 'ne': 1, 'suis': 1, 'pas': 1, 'imbecile': 1, 'fool': 1, 'nowdeleted': 1, 'post': 1, 'aug': 1, 'soheil': 1, 'eid': 1, 'tory': 1, 'candidate': 1, 'riding': 1, 'joliette': 1, 'restera': 1, 'toujours': 1, 'quelque': 1, 'chose': 1, 'remain': 1, 'hate': 1, 'perfume': 1, 'burr': 1, 'parfum': 1, 'listened': 1, 'carefully': 1, '2955': 1, 'heard': 1, 'guys': 1, 'comment': 1, 'fait': 1, 'lautre': 1, 'quel': 1, 'autre': 1, 'sounds': 1, 'stretch': 1, 'consider': 1, 'astu': 1, 'au': 1, 'cinema': 1, 'movies': 1, 'haveyou': 1, 'moviestheater': 1, 'brevet': 1, 'sans': 1, 'garantie': 1, 'translated': 1, 'patented': 1, 'warranty': 1, '1examples': 1, 'glish': 1, '2018s': 1, 'subsample': 1, 'pragmatic': 1, 'assumptions': 1, 'emphasizes': 1, 'curatedﬁltered': 1, 'exceptionally': 1, 'outbound': 1, 'reddit': 1, 'received': 1, 'karma': 1, 'indicator': 1, 'users': 1, 'educational': 1, 'funny': 1, 'resulting': 1, '45': 1, 'html': 1, 'responses': 1, 'dragnet': 1, 'newspaper1': 1, 'extractors': 1, 'dec': 1, 'cleaning': 1, 'gb': 1, 'complicate': 1, '1httpsgithubcomcodelucasnewspaper': 1, 'lapping': 1, 'casing': 1, 'outofvocabulary': 1, 'restrict': 1, 'modelable': 1, 'utf8': 1, 'elegantly': 1, 'ﬁlls': 1, 'requirement': 1, 'encoding': 1, 'effectively': 1, 'interpolates': 1, 'frequent': 1, 'char': 1, 'acter': 1, 'infrequent': 1, 'despite': 1, 'implementations': 1, 'operate': 1, 'imple': 1, 'mentations': 1, 'uni': 1, '130000': 1, 'multisymbol': 1, 'prohibitively': 1, '32000': 1, '64000': 1, 'vocabularies': 1, 'contrast': 1, 'applying': 1, 'sub': 1, 'optimal': 1, 'merges': 1, 'frequency': 1, 'occur': 1, 'variations': 1, 'suboptimal': 1, 'allocation': 1, 'limited': 1, 'slots': 1, 'vent': 1, 'merging': 1, 'categories': 1, 'spaces': 1, 'sig': 1, 'niﬁcantly': 1, 'compression': 1, 'efﬁciency': 1, 'fragmentation': 1, 'combine': 1, 'beneﬁts': 1, 'generality': 1, 'assign': 1, '23': 1, 'largely': 1, 'follows': 1, 'openai': 1, 'dmodel': 1, '1280': 1, '48': 1, '1600': 1, '2architecture': 1, 'hyperparameters': 1, 'modiﬁcations': 1, 'moved': 1, 'subblock': 1, 'preactivation': 1, 'self': 1, 'block': 1, 'modiﬁed': 1, 'initialization': 1, 'accounts': 1, 'accumulation': 1, 'depth': 1, 'weights': 1, 'ization': 1, 'expanded': 1, '50257': 1, 'increase': 1, 'batchsize': 1, 'benchmarked': 1, 'loguniformly': 1, 'spaced': 1, 'summa': 1, 'rized': 1, 'call': 1, 'magni': 1, 'tude': 1, 'tuned': 1, 'underﬁt': 1, 'interested': 1, 'primary': 1, 'operates': 1, 'lossy': 1, 'ponentiated': 1, 'negative': 1, 'unit': 1, 'computing': 1, 'logprobability': 1, 'dividing': 1, 'ofdistribution': 1, 'aggressively': 1, 'standardized': 1, 'disconnected': 1, 'punctuation': 1, 'contractions': 1, 'shufﬂed': 1, 'cbtcn': 1, 'cbtne': 1, 'bpb': 1, 'bpc': 1, '5923': 1, '857': 1, '823': 1, '3914': 1, '4654': 1, '099': 1, '183': 1, '218': 1, '3513': 1, '4599': 1, '8765': 1, '2941': 1, '6585': 1, '116': 1, '117': 1, '3750': 1, '7520': 1, '1560': 1, '5548': 1, '9235': 1, '871': 1, '2276': 1, '4733': 1, '101': 1, '106': 1, '2637': 1, '5572': 1, '1087': 1, '6012': 1, '9345': 1, '880': 1, '1993': 1, '4031': 1, '097': 1, '102': 1, '2205': 1, '44575': 1, '9330': 1, '8905': 1, '1834': 1, '3576': 1, '093': 1, '098': 1, '1748': 1, '4216': 1, '3zeroshot': 1, 'unk': 1, 'extremely': 1, '25': 1, 'penn': 1, 'treebank': 1, 'longterm': 1, 'destructive': 1, '1bws': 1, 'shufﬂing': 1, 'removes': 1, 'structure': 1, '2performance': 1, 'estimates': 1, 'examine': 1, 'cat': 1, 'egories': 1, 'verbs': 1, 'preposi': 1, 'reporting': 1, 'ric': 1, 'automatically': 1, 'omitted': 1, 'ditioned': 1, 'highest': 1, 'steadily': 1, 'closes': 1, 'majority': 1, 'jungle': 1, 'rudyard': 1, 'kipling': 1, 'validation': 1, '933': 1, '891': 1, 'detokenizer': 1, 'applied': 1, 'successfully': 1, '5266': 1, 'investigating': 1, 'predictions': 1, 'continuations': 1, 'useful': 1, 'constraint': 1, 'stopword': 1, 'approximation': 1, 'restricted': 1, 'constrained': 1, 'restriction': 1, 'harmful': 1, '3performance': 1, 'capability': 1, 'resolve': 1, 'ambiguities': 1, 'predicting': 1, 'resolution': 1, 'ambiguity': 1, 'higher': 1, 'formulation': 1, 'visualize': 1, 'partial': 1, 'scoring': 1, '7070': 1, '273': 1, 'contextualize': 1, 'consists': 1, 'dialogues': 1, 'asker': 1, 'answerer': 1, 'capabilities': 1, 'depend': 1, 'ument': 1, 'exceeds': 1, 'collected': 1, 'r1': 1, 'r2': 1, 'rl': 1, 'ra': 1, 'vg': 1, '4122': 1, '1868': 1, '3834': 1, '3275': 1, 'lede3': 1, '4038': 1, '1766': 1, '3662': 1, '3155': 1, 'attn': 1, '3133': 1, '1181': 1, '2883': 1, '2399': 1, 'tldr': 1, '2934': 1, '827': 1, '2658': 1, '2140': 1, 'random3': 1, '2878': 1, '2552': 1, '2098': 1, '2158': 1, '403': 1, '1947': 1, '1503': 1, '4summarization': 1, 'nearing': 1, '89': 1, 'exciting': 1, 'su': 1, 'inspection': 1, 'uses': 1, 'heuristics': 1, 'response': 1, 'duce': 1, 'texttldr': 1, 'pling': 1, '2which': 1, 'repetition': 1, 'encourages': 1, 'coding': 1, 'summary': 1, 'resemble': 1, 'focus': 1, 'confuse': 1, 'cars': 1, 'crash': 1, 'logo': 1, 'hat': 1, '12l': 1, 'selecting': 1, 'drops': 1, '64': 1, 'aggregate': 1, 'demonstrates': 1, 'invoke': 1, '37': 1, 'begun': 1, 'prompt': 1, 'englishfrench': 1, 'wordbyword': 1, 'substitution': 1, 'bilingual': 1, 'lexicon': 1, 'origin': 1, 'ubuntu': 1, 'mark': 1, 'shuttleworth': 1, '820': 1, 'quarterback': 1, 'bay': 1, 'packers': 1, 'rodgers': 1, '811': 1, 'panda': 1, 'animal': 1, 'theory': 1, 'relativity': 1, 'albert': 1, 'einstein': 1, '764': 1, 'ﬁlm': 1, '1977': 1, '714': 1, 'blood': 1, '706': 1, 'regarded': 1, 'psychoanalysis': 1, 'sigmund': 1, 'freud': 1, '693': 1, 'moon': 1, '1969': 1, 'neil': 1, 'armstrong': 1, '668': 1, 'supermarket': 1, 'chain': 1, 'uk': 1, 'tesco': 1, '653': 1, 'meaning': 1, 'shalom': 1, 'peace': 1, '640': 1, 'tzu': 1, '596': 1, 'land': 1, 'mass': 1, 'california': 1, '592': 1, 'algae': 1, 'reproduction': 1, 'parthenogenesis': 1, '565': 1, 'vikram': 1, 'samvat': 1, 'calender': 1, 'india': 1, '556': 1, 'responsible': 1, 'writing': 1, 'declaration': 1, 'independence': 1, 'thomas': 1, 'jefferson': 1, '533': 1, 'boundary': 1, '523': 1, 'plays': 1, 'ser': 1, 'davos': 1, 'thrones': 1, 'peter': 1, 'dinklage': 1, '521': 1, 'appoints': 1, 'chair': 1, 'federal': 1, 'janet': 1, 'yellen': 1, '515': 1, 'process': 1, 'divides': 1, 'nucleus': 1, 'genetically': 1, 'identical': 1, 'nuclei': 1, 'mitosis': 1, '507': 1, 'won': 1, 'mvp': 1, 'awards': 1, 'nba': 1, 'jordan': 1, '502': 1, 'rome': 1, 'tiber': 1, '486': 1, 'impeached': 1, 'andrew': 1, '483': 1, 'department': 1, 'homeland': 1, 'security': 1, 'kelly': 1, '470': 1, 'currency': 1, 'euro': 1, '468': 1, 'palpatine': 1, '465': 1, 'gun': 1, 'permit': 1, '464': 1, '1859': 1, 'basis': 1, 'biological': 1, 'nuclear': 1, 'plant': 1, 'blew': 1, 'russia': 1, 'chernobyl': 1, 'played': 1, 'terminator': 1, 'arnold': 1, 'schwarzenegger': 1, '452': 1, '5the': 1, 'sorted': 1, 'none': 1, 'appear': 1, 'frenchenglish': 1, 'leverage': 1, 'strong': 1, '115': 1, '335': 1, 'prising': 1, 'deliberately': 1, 'nonenglish': 1, 'ﬁrm': 1, 'detector2': 1, 'detected': 1, '10mb': 1, '500x': 1, 'smaller': 1, '38': 1, 'contained': 1, 'generates': 1, 'factoidstyle': 1, 'showcas': 1, 'qualitative': 1, 'highquality': 1, '2httpsgithubcomcld2ownerscld2': 1, 'resource': 1, 'quantita': 1, 'tively': 1, 'seeded': 1, 'helps': 1, '41': 1, 'evalu': 1, 'ated': 1, 'match': 1, 'squad': 1, 'comparison': 1, 'exceed': 1, 'accu': 1, 'racy': 1, 'incredibly': 1, 'returns': 1, '53': 1, 'suggesting': 1, 'assigns': 1, 'calibrated': 1, 'ment': 1, 'hybridize': 1, '3alec': 1, 'himself': 1, 'trivia': 1, 'answered': 1, 'randomly': 1, 'gotten': 1, '267': 1, '066': 1, '750': 1, '234': 1, '909': 1, '1319': 1, '088': 1, '163': 1, '394': 1, '242': 1, '375': 1, '6percentage': 1, 'grams': 1, 'overlapping': 1, 'vs': 1, 'nontrivial': 1, 'nearduplicate': 1, 'overreporting': 1, 'chine': 1, 'increasingly': 1, 'phenomena': 1, 'therefore': 1, 'analyze': 1, 'study': 1, 'normalized': 1, 'lowercased': 1, 'alphanumeric': 1, 'delimiter': 1, 'upper': 1, 'bounded': 1, 'veriﬁed': 1, '1m': 1, 'zero': 1, 'let': 1, 'anal': 1, 'ysis': 1, 'surprisingly': 1, '59': 1, 'optimizes': 1, 'manual': 1, 'inspec': 1, 'duplicated': 1, 'unique': 1, '60': 1, 'worryingly': 1, '132': 1, 'schemata': 1, 'spurious': 1, 'remaining': 1, '4a': 1, 'editors': 1, 'reusing': 1, 'battles': 1, 'korean': 1, 'performs': 1, 'coqas': 1, 'gain': 1, '0510': 1, 'cutoff': 1, 'greater': 1, 'recalculating': 1, 'excluding': 1, 'shifts': 1, '87': 1, '632': 1, '629': 1, 'vides': 1, 'consistent': 1, 'ever': 1, 'notice': 1, 'existing': 1, 'quantifying': 1, 'highly': 1, 'pacts': 1, 'scalable': 1, 'fuzzy': 1, 'ngram': 1, 'veriﬁcation': 1, 'sanity': 1, 'check': 1, 'determining': 1, 'perfor': 1, 'mance': 1, 'attributable': 1, 'inspecting': 1, 'underﬁtting': 1, 'write': 1, 'discovery': 1, 'provided': 1, 'related': 1, '4the': 1, 'func': 1, 'gutenberg': 1, 'supplement': 1, 'conducted': 1, 'thorough': 1, 'capac': 1, 'ity': 1, 'noisier': 1, 'trends': 1, 'hold': 1, 'subtasks': 1, '1b': 1, 'regime': 1, 'functionality': 1, 'documented': 1, 'cells': 1, 'linewidth': 1, 'quotecomment': 1, 'detection': 1, 'spirational': 1, 'observation': 1, 'names': 1, 'languages': 1, 'alternative': 1, 'constructing': 1, 'addition': 1, 'vector': 1, 'inﬂuential': 1, 'early': 1, 'vectorskiros': 1, 'derived': 1, 'largescale': 1, 'initialized': 1, 'pretrained': 1, 'ﬁnetuned': 1, 'chitchat': 1, 'discussion': 1, 'dedicated': 1, 'criti': 1, 'cally': 1, 'area': 1, 'explore': 1, 'explain': 1, 'widespread': 1, 'limit': 1, 'begins': 1, 'adaption': 1, 'rudimentary': 1, 'quantitative': 1, 'suggestive': 1, 'terms': 1, 'applica': 1, 'far': 1, 'useable': 1, 'addi': 1, 'tional': 1, 'undoubtedly': 1, 'outperform': 1, 'trivial': 1, 'establishes': 1, 'clear': 1, 'ceiling': 1, 'output': 1, 'departure': 1, 'currently': 1, 'vestigate': 1, 'come': 1, 'inefﬁciencies': 1, 'unidirectional': 1, 'conclusion': 1, 'zeroshots': 1, 'maximize': 1, 'likelihood': 1, 'supervision5': 1, 'acknowledgements': 1, 'upvoted': 1, 'googlers': 1, 'helped': 1, 'infrastructure': 1, 'zak': 1, 'stone': 1, 'js': 1, 'riehl': 1, 'jonathan': 1, 'hseu': 1, 'russell': 1, 'youlong': 1, 'noam': 1, 'solomon': 1, 'boulos': 1, 'banﬁeld': 1, 'aman': 1, 'gupta': 1, 'sohn': 1, 'finally': 1, 'feedback': 1, 'drafts': 1, 'jacob': 1, 'steinhardt': 1, 'geoffrey': 1, 'irving': 1, 'madison': 1, 'references': 1, 'choe': 1, 'constant': 1, 'guo': 1, 'characterlevel': 1, 'deeper': 1, 'arxiv180804444': 1, 'arxiv190108634': 1, 'li': 1, 'mai': 1, 'ku': 1, 'ws': 1, 'nguyen': 1, 'strike': 1, 'pose': 1, 'easily': 1, 'fooled': 1, 'strange': 1, 'poses': 1, 'familiar': 1, 'objects': 1, 'arxiv181111553': 1, 'ananthanarayanan': 1, 'anubhai': 1, 'bai': 1, 'batten': 1, 'berg': 1, 'casper': 1, 'catanzaro': 1, 'endtoend': 1, 'mandarin': 1, '173182': 1, 'arxiv171011041': 1, 'ap': 1, 'proach': 1, 'arxiv190201313': 1, '5preliminary': 1, 'downloading': 1, 'httpsgithubcomopenaigpt2': 1, 'arxiv160706450': 1, 'kadlec': 1, 'kleindienst': 1, 'embracing': 1, 'abun': 1, 'dance': 1, 'booktest': 1, 'arxiv161000956': 1, 'purging': 1, 'cifar': 1, 'nearduplicates': 1, 'arxiv190200423': 1, 'ducharme': 1, 'vincent': 1, 'jauvin': 1, '3feb11371155': 1, 'pavlick': 1, 'van': 1, 'durme': 1, 'hula': 1, 'xia': 1, 'pappagari': 1, 'mccoy': 1, 'patel': 1, 'looking': 1, 'elmos': 1, 'sentencelevel': 1, 'arxiv181210860': 1, '2814175': 1, 'ge': 1, 'brants': 1, 'koehn': 1, 'robinson': 1, 'measur': 1, 'statistical': 1, 'arxiv13123005': 1, 'bottou': 1, 'karlen': 1, 'kavukcuoglu': 1, 'kuksa': 1, 'scratch': 1, '12aug2493': 1, '2537': 1, 'schwenk': 1, 'barrault': 1, 'bor': 1, 'represen': 1, 'tations': 1, 'arxiv170502364': 1, 'egou': 1, 'parallel': 1, 'arxiv171004087': 1, 'semisupervised': 1, '3079': 1, '3087': 1, 'cohen': 1, 'carbonell': 1, 'transformerxl': 1, 'attentive': 1, 'ﬁxedlength': 1, 'arxiv190102860': 1, 'httpscorpusbyueduiweb': 1, 'gouws': 1, 'arxiv180703819': 1, 'chang': 1, 'mw': 1, 'toutanova': 1, 'bidirectional': 1, 'arxiv181004805': 1, 'roller': 1, 'shuster': 1, 'auli': 1, 'wizard': 1, 'knowledgepowered': 1, 'arxiv181101241': 1, 'lewis': 1, 'dauphin': 1, 'hierarchical': 1, 'arxiv180504833': 1, 'abbeel': 1, 'levine': 1, 'modelagnostic': 1, 'meta': 1, 'fast': 1, 'arxiv170303400': 1, 'deng': 1, 'arxiv180810792': 1, 'brunk': 1, 'subramanya': 1, 'tilingual': 1, 'arxiv151200103': 1, 'tan': 1, 'qin': 1, 'ty': 1, 'frage': 1, 'frequencyagnostic': 1, '13411352': 1, 'joulin': 1, 'usunier': 1, 'continuous': 1, 'cache': 1, 'arxiv161204426': 1, 'zhang': 1, 'ren': 1, 'identity': 1, 'mappings': 1, '630645': 1, 'springer': 1, 'narang': 1, 'ardalani': 1, 'diamos': 1, 'jun': 1, 'kian': 1, 'inejad': 1, 'patwary': 1, 'scaling': 1, 'predictable': 1, 'empirically': 1, 'arxiv171200409': 1, 'bordes': 1, 'chopra': 1, 'goldilocks': 1, 'memory': 1, 'rep': 1, 'resentations': 1, 'arxiv151102301': 1, 'korhonen': 1, 'unlabelled': 1, 'arxiv160203483': 1, 'wiseman': 1, 'entity': 1, 'proves': 1, 'clozestyle': 1, 'arxiv181002891': 1, '56th': 1, 'annual': 1, 'meeting': 1, 'association': 1, 'computational': 1, 'linguistics': 1, 'papers': 1, '328339': 1, 'interpolated': 1, 'markov': 1, 'sparse': 1, 'pattern': 1, 'amsterdam': 1, 'netherlands': 1, 'northholland': 1, 'adversarial': 1, 'read': 1, 'arxiv170707328': 1, 'limits': 1, 'arxiv160202410': 1, 'arxiv170605137': 1, 'feifei': 1, 'visualizing': 1, 'arxiv150602078': 1, 'pascanu': 1, 'rabinowitz': 1, 'veness': 1, 'desjardins': 1, 'rusu': 1, 'milan': 1, 'quan': 1, 'ramalho': 1, 'grabska': 1, 'barwinska': 1, 'overcoming': 1, 'catastrophic': 1, 'forgetting': 1, 'academy': 1, 'sci': 1, 'ences': 1, '201611835': 1, 'zhu': 1, 'zemel': 1, 'urtasun': 1, 'torralba': 1, 'fidler': 1, '32943302': 1, 'imagenet': 1, 'classiﬁ': 1, 'cation': 1, 'convolutional': 1, '10971105': 1, 'palomaki': 1, 'rhinehart': 1, 'parikh': 1, 'epstein': 1, 'kelcey': 1, 'ullman': 1, 'tenenbaum': 1, 'gershman': 1, 'machines': 1, 'behavioral': 1, 'brain': 1, 'sciences': 1, 'unsu': 1, 'corpora': 1, 'arxiv171100043': 1, 'davis': 1, 'morgenstern': 1, 'thirteenth': 1, 'principles': 1, 'knowledge': 1, 'embedding': 1, 'implicit': 1, 'trix': 1, 'factorization': 1, '21772185': 1, 'saleh': 1, 'goodrich': 1, 'sepassi': 1, 'summarizing': 1, 'arxiv180110198': 1, '62946305': 1, 'keskar': 1, 'decathlon': 1, 'arxiv180608730': 1, 'sentinel': 1, 'mixture': 1, 'arxiv160907843': 1, 'corrado': 1, 'dean': 1, 'compositionality': 1, '31113119': 1, 'gulcehre': 1, 'xiang': 1, 'abstrac': 1, 'tive': 1, 'sequencetosequence': 1, 'rnns': 1, 'arxiv160206023': 1, 'kruszewski': 1, 'pham': 1, 'bernardi': 1, 'pezzelle': 1, 'baroni': 1, 'boleda': 1, 'fernandez': 1, 'requiring': 1, 'broad': 1, 'discourse': 1, 'arxiv160606031': 1, 'emnlp': 1, '15321543': 1, 'extraction': 1, 'fea': 1, 'ture': 1, '22nd': 1, '8990': 1, 'acm': 1, 'neumann': 1, 'iyyer': 1, 'gardner': 1, 'clark': 1, 'zettlemoyer': 1, 'arxiv180205365': 1, 'reviews': 1, 'discovering': 1, 'arxiv170401444': 1, 'narasimhan': 1, 'salimans': 1, 'arxiv161102683': 1, 'roelofs': 1, 'schmidt': 1, 'shankar': 1, 'arxiv180600451': 1, 'arxiv180807042': 1, 'sap': 1, 'konstas': 1, 'zilles': 1, 'choi': 1, 'smith': 1, 'uw': 1, '2nd': 1, 'linking': 1, 'lexical': 1, 'sentential': 1, 'discourselevel': 1, 'semantics': 1, '5255': 1, 'marization': 1, 'pointergenerator': 1, 'arxiv170404368': 1, 'haddow': 1, 'birch': 1, 'subword': 1, 'arxiv150807909': 1, 'pal': 1, 'purpose': 1, 'arxiv180400079': 1, 'informa': 1, '31043112': 1, 'gregor': 1, 'rezende': 1, 'lillicrap': 1, 'arxiv151106440': 1, 'emami': 1, 'cheung': 1, 'sule': 1, 'diaz': 1, 'arxiv181101778': 1, 'arxiv180602847': 1, '59986008': 1, 'arxiv150605869': 1, 'fortunato': 1, 'jaitly': 1, '2692': 1, '2700': 1, 'singh': 1, 'bow': 1, 'arxiv180407461': 1, 'dialogbased': 1, '829837': 1, 'arxiv190110444': 1, 'sanh': 1, 'chaumond': 1, 'delangue': 1, 'transfo': 1, 'arxiv190108149': 1, 'dautume': 1, 'kocisky': 1, 'chrzanowski': 1, 'kong': 1, 'ling': 1, 'yu': 1, 'dyer': 1, 'linguistic': 1, 'intel': 1, 'ligence': 1, 'arxiv190111373': 1, 'appendix': 1, '81': 1, 'complement': 1, 'bigger': 1, 'tables': 1, 'sidebyside': 1, '82': 1, 'memorizing': 1, 'repeated': 1, 'quotes': 1, 'speeches': 1, 'gettysburg': 1, 'address': 1, 'occurs': 1, 'argmax': 1, 'decode': 1, 'recovers': 1, 'truncation': 1, 'copies': 1, 'awhile': 1, 'drifting': 1, 'albeit': 1, '100200': 1, 'displays': 1, 'widening': 1, 'quantify': 1, 'groundtruth': 1, 'below': 1, 'repeats': 1, '5cdf': 1, 'whereas': 1, 'median': 1, '83': 1, 'showing': 1, '84': 1, 'robustness': 1, 'handling': 1, 'dis': 1, 'tribution': 1, 'typical': 1, 'pairing': 1, 'bought': 1, 'jake': 1, 'grantham': 1, 'nike': 1, 'racers': 1, 'harris': 1, 'tweed': 1, 'pants': 1, 'wonderfully': 1, 'jeans': 1, 'pseudo': 1, 'menswearprep': 1, 'wool': 1, 'gq': 1, 'key': 1, 'pulling': 1, 'dressed': 1, 'menswear': 1, 'khaki': 1, 'chino': 1, 'ditch': 1, 'tee': 1, 'polo': 1, 'eye': 1, 'storm': 1, 'hybrid': 1, 'streetwear': 1, 'pieces': 1, 'traditional': 1, 'budget': 1, 'item': 1, 'sneakermaker': 1, 'ok': 1, 'hesitate': 1, 'price': 1, 'outﬁts': 1, 'cutcut': 1, 'neckline': 1, 'preferred': 1, 'items': 1, 'bluegray': 1, 'designs': 1, 'store': 1, 'chance': 1, 'might': 1, 'opposed': 1, 'wanted': 1, 'true': 1, 'slim': 1, 'ﬁtting': 1, 'ﬂashy': 1, 'swap': 1, 'hadnt': 1, 'itself': 1, 'chinos': 1, 'whatever': 1, 'alone': 1, 'variables': 1, 'dress': 1, 'wore': 1, 'streamlined': 1, 'cool': 1, 'theyll': 1, 'easiest': 1, 'sneaker': 1, 'pretty': 1, '7random': 1, 'glorious': 1, 'historical': 1, 'attractions': 1, 'mezquita': 1, 'great': 1, 'medina': 1, 'azahara': 1, 'ruins': 1, 'visitable': 1, 'madinat': 1, 'alzahra': 1, 'alandalus': 1, 'alhambra': 1, 'splendid': 1, 'intact': 1, 'synagogues': 1, 'santa': 1, 'marıa': 1, 'blanca': 1, 'toledo': 1, 'synagogue': 1, 'edit': 1, 'medieval': 1, 'existed': 1, '9th': 1, 'century': 1, 'bc': 1, 'slowly': 1, 'transformed': 1, 'secularized': 1, '1150': 1, 'ad': 1, 'prosperous': 1, '1050': 1, 'ce': 1, 'partially': 1, 'characterised': 1, 'series': 1, 'invasions': 1, 'northwestern': 1, 'region': 1, 'eg': 1, 'danube': 1, 'extended': 1, 'gibraltar': 1, 'quickly': 1, 'destroyed': 1, 'assumed': 1, 'title': 1, 'decades': 1, 'restore': 1, 'maintain': 1, 'west': 1, 'successors': 1, 'papal': 1, 'throne': 1, '1148': 1, 'successor': 1, 'king': 1, 'alfred': 1, 'ii': 1, 'continent': 1, 'although': 1, 'territory': 1, '1720': 1, '1092': 1, '1095': 1, 'saw': 1, 'establishment': 1, 'kingdom': 1, 'castile': 1, 'aragon': 1, 'dynasty': 1, 'exist': 1, 'considered': 1, 'occasion': 1, 'christianinﬂamed': 1, 'organized': 1, 'conﬂicts': 1, 'themselves': 1, 'reign': 1, 'among': 1, 'nonmuslims': 1, 'followed': 1, 'driven': 1, 'compelled': 1, 'victory': 1, 'ix': 1, 'battle': 1, 'poitiers': 1, '1189': 1, 'jerusalem': 1, '1187': 1, 'saladin': 1, 'retreated': 1, 'north': 1, 'moorish': 1, 'replaced': 1, 'christian': 1, 'resumed': 1, '1229': 1, '1492': 1, 'quered': 1, '8random': 1, 'pink': 1, 'opposite': 1, 'red': 1, 'front': 1, 'accomplished': 1, 'travelling': 1, 'traveling': 1, 'overﬂow': 1, 'bug': 1, 'calculation': 1, 'position': 1, 'exploit': 1, 'navigation': 1, 'situations': 1, 'proximity': 1, 'headed': 1, 'toward': 1, 'apart': 1, 'behind': 1, 'encounters': 1, 'conﬁguration': 1, 'doubleback': 1, 'fake': 1, 'changing': 1, 'directions': 1, 'rapidly': 1, 'approaching': 1, 'displacing': 1, 'farther': 1, 'pac': 1, 'respond': 1, 'movement': 1, 'jump': 1, 'cliff': 1, 'spot': 1, 'vanishing': 1, 'possibilities': 1, 'paths': 1, 'obvious': 1, 'wants': 1, 'goal': 1, 'easy': 1, 'chasing': 1, 'catch': 1, 'ﬁght': 1, 'enemies': 1, 'reasons': 1, 'unlike': 1, 'cross': 1, 'dodge': 1, 'projectiles': 1, 'buster': 1, 'speciﬁcally': 1, 'leaving': 1, 'press': 1, 'bar': 1, 'reveal': 1, 'houses': 1, 'doors': 1, 'hiding': 1, 'note': 1, 'sees': 1, 'unless': 1, 'beware': 1, 'glitch': 1, 'attempting': 1, 'gone': 1, 'manner': 1, '9random': 1, 'goeldin': 1, 'unskilled': 1, 'laborer': 1, 'november': 1, 'goldein': 1, 'survived': 1, 'patrick': 1, '1854': 1, 'artillery': 1, 'oregon': 1, 'east': 1, '1874': 1, 'superintendent': 1, 'cemetery': 1, 'willets': 1, 'benjamin': 1, 'shoemaker': 1, 'orange': 1, 'county': 1, '1862': 1, 'newly': 1, 'formed': 1, '124th': 1, 'sixtyone': 1, 'october': 1, 'february': 1, 'camped': 1, 'brandy': 1, 'station': 1, 'civilian': 1, '1867': 1, 'artiﬁcer': 1, 'william': 1, 'probably': 1, 'ﬁll': 1, 'combat': 1, 'leadership': 1, 'void': 1, 'crossing': 1, 'collinss': 1, 'record': 1, 'qualities': 1, 'earn': 1, 'promotion': 1, 'obviously': 1, 'overcome': 1, 'serious': 1, 'problems': 1, 'sacketts': 1, 'harbor': 1, '1853': 1, 'twentytwo': 1, '1858': 1, 'month': 1, '1861': 1, 'charges': 1, 'discharged': 1, 'internal': 1, 'affairs': 1, 'engineer': 1, 'engi': 1, 'neering': 1, 'graduate': 1, 'studies': 1, 'college': 1, 'engineering': 1, 'employed': 1, 'pfeiffer': 1, 'colonel': 1, '1870': 1, 'pfeiffers': 1, 'home': 1, 'england': 1, 'serve': 1, 'corps': 1, 'engineers': 1, 'experience': 1, 'dutyendoftextmoscow': 1, 'july': 1, 'reuters': 1, 'ukbased': 1, 'sources': 1, 'moscow': 1, 'downplay': 1, 'buildup': 1, 'oppo': 1, 'sition': 1, 'maintaining': 1, 'support': 1, 'donald': 1, 'trumps': 1, 'build': 1, 'wall': 1, 'usmexico': 1, 'calling': 1, 'coup': 1, 'usled': 1, 'envoy': 1, 'flynn': 1, 'gestures': 1, 'usbased': 1, 'political': 1, 'analyst': 1, 'ahmet': 1, 'koryat': 1, 'sentenced': 1, 'shot': 1, 'desertion': 1, '1829': 1, 'revolution': 1, 'joined': 1, 'father': 1, '1836': 1, '2d': 1, 'infantry': 1, '25th': 1, 'numbered': 1, '4500': 1, 'brothers': 1, 'brigade': 1, 'connecticut': 1, '1846': 1, 'mexican': 1, 'staff': 1, 'brigadier': 1, 'sullivan': 1, 'antonio': 1, '1863': 1, 'commanded': 1, 'twentythree': 1, 'charge': 1, 'ﬁftyfour': 1, 'mortars': 1, 'pinta': 1, 'fall': 1, '1865': 1, 'lieutenant': 1, '10random': 1, 'pain': 1, 'handled': 1, 'freetype': 1, 'interface': 1, 'handles': 1, 'rendering': 1, 'xmonadurxvt': 1, 'speciﬁes': 1, 'bunch': 1, 'resources': 1, 'launch': 1, 'window': 1, 'manager': 1, 'individual': 1, 'programs': 1, 'urxvtbackgroundbackground': 1, 'settins': 1, 'anyway': 1, 'syntax': 1, 'urxvtfontxftyour': 1, 'heresizethe': 1, 'herexftfallback': 1, 'basic': 1, 'urxvt': 1, 'fixing': 1, 'keys': 1, 'symptom': 1, 'matter': 1, 'modﬁed': 1, 'ﬁxed': 1, 'completely': 1, 'workable': 1, 'typing': 1, 'username': 1, 'mouse': 1, 'conﬁgured': 1, 'doing': 1, 'unnecessary': 1, 'installed': 1, 'allowing': 1, 'installupgrade': 1, 'quire': 1, 'uninstalling': 1, 'writeup': 1, 'job': 1, 'sudo': 1, 'commands': 1, 'confusing': 1, 'ﬁnally': 1, 'running': 1, 'forgot': 1, 'symlink': 1, 'ﬁles': 1, 'readonly': 1, 'program': 1, 'passwd': 1, 'modiﬁes': 1, 'ownergroup': 1, 'ﬁddle': 1, 'decided': 1, 'reverse': 1, 'permission': 1, '2644': 1, 'start': 1, 'sshadd': 1, 'every': 1, 'makeﬁle': 1, 'colorblue': 1, 'cyan': 1, '11random': 1, 'loves': 1, 'mini': 1, 'jos': 1, 'smorgasbord': 1, 'minty': 1, 'chippy': 1, 'goodness': 1, 'tasted': 1, 'ﬁnished': 1, 'candy': 1, 'canes': 1, 'bark': 1, 'afraid': 1, 'texture': 1, 'pans': 1, 'shorter': 1, 'buttercream': 1, 'stuck': 1, 'fridge': 1, 'topper': 1, 'prepared': 1, 'earlier': 1, 'wont': 1, 'wait': 1, 'sit': 1, 'mixing': 1, 'whipped': 1, 'stiff': 1, 'dish': 1, 'simmering': 1, 'heat': 1, 'warm': 1, 'doesnt': 1, 'take': 1, 'crumbs': 1, 'raw': 1, 'cooled': 1, 'teaspoon': 1, 'calls': 1, 'substitute': 1, 'online': 1, 'card': 1, 'kittens': 1, 'yield': 1, 'print': 1, 'chunks': 1, 'coated': 1, 'topped': 1, 'drizzled': 1, 'sauce': 1, 'momma': 1, 'unsalted': 1, 'temperature': 1, 'granulated': 1, 'eggs': 1, 'glaze': 1, 'cocoa': 1, 'powder': 1, 'ounces': 1, 'semis': 1, 'softened': 1, 'cream': 1, '124': 1, 'ﬂour': 1, 'peppermintjojo': 1, 'pecans': 1, 'ﬁnely': 1, 'leaves': 1, 'fresh': 1, 'sheet': 1, 'failures': 1, 'christmas': 1, 'morning': 1, 'skills': 1, 'beautifully': 1, 'moist': 1, 'beautiful': 1, 'moisture': 1, 'perfectly': 1, 'blended': 1, 'cringe': 1, 'yolks': 1, 'mixed': 1, 'super': 1, 'timeconsuming': 1, 'friend': 1, 'suggested': 1, '69': 1, 'servings': 1, 'delicious': 1, 'prevent': 1, 'sticking': 1, 'tart': 1, 'lined': 1, 'cut': 1, 'cakes': 1, 'sandwiched': 1, '1layer': 1, 'ready': 1, 'preheat': 1, 'oven': 1, '350': 1, 'degrees': 1, 'golden': 1, 'tester': 1, 'clean': 1, '12noncherrypicked': 1, '384': 1, '128': 1, 'humanwritten': 1, 'shocking': 1, 'ﬁnding': 1, 'living': 1, 'remote': 1, 'unexplored': 1, 'researchers': 1, 'distinctive': 1, 'horn': 1, 'ovids': 1, 'fourhorned': 1, 'silverwhite': 1, 'science': 1, 'centuries': 1, 'mystery': 1, 'sparked': 1, 'odd': 1, 'phenomenon': 1, 'nally': 1, 'solved': 1, 'jorge': 1, 'evolutionary': 1, 'biologist': 1, 'university': 1, 'paz': 1, 'panions': 1, 'fountain': 1, 'surrounded': 1, 'rock': 1, 'silver': 1, 'snow': 1, 'ventured': 1, 'reached': 1, 'peak': 1, 'looked': 1, 'crystals': 1, 'astonished': 1, 'touch': 1, 'horns': 1, 'examining': 1, 'bizarre': 1, 'scientists': 1, 'fairly': 1, 'regular': 1, 'stated': 1, 'dialect': 1, 'dialectic': 1, 'believes': 1, 'originated': 1, 'argentina': 1, 'believed': 1, 'arrival': 1, 'origins': 1, 'perhaps': 1, 'civilization': 1, 'incidents': 1, 'pointed': 1, 'knowing': 1, 'indeed': 1, 'alien': 1, 'dna': 1, 'communicate': 1, 'organization': 1, '13conditional': 1, 'outofdistribution': 1, 'cherry': 1, 'cuttingedge': 1, 'technology': 1, 'works': 1, 'chauvetpontdarc': 1, 'reproduced': 1, 'biggest': 1, 'manmade': 1, 'cavern': 1, 'painstakinglyreproduced': 1, 'drawings': 1, '450': 1, 'cavemen': 1, 'women': 1, 'pictured': 1, 'ern': 1, 'unesco': 1, 'heritage': 1, 'orig': 1, 'inal': 1, 'big': 1, 'cats': 1, 'mimics': 1, 'paint': 1, 'ings': 1, '425': 1, 'minute': 1, 'copied': 1, 'elling': 1, 'anamorphic': 1, 'widescreen': 1, 'paw': 1, 'prints': 1, 'bears': 1, 'yours': 1, 'oil': 1, 'billionaire': 1, 'abramovich': 1, 'seems': 1, 'oligarch': 1, 'chelsea': 1, 'fc': 1, 'extras': 1, '300million': 1, 'superyacht': 1, 'eclipse': 1, 'shore': 1, 'undetected': 1, 'wealthy': 1, 'businessman': 1, 'splashing': 1, 'underwater': 1, 'dubbed': 1, 'earths': 1, 'real': 1, 'frontier': 1, 'littleexplored': 1, 'billionaires': 1, 'queuing': 1, 'vessels': 1, 'glimpse': 1, 'dramatic': 1, 'seascapes': 1, 'wildlife': 1, 'worlds': 1, 'spare': 1, 'bank': 1, 'holiday': 1, 'snaps': 1, 'instagram': 1, 'posts': 1, 'trump': 1, 'snap': 1, 'whale': 1, 'orcasub': 1, 'takes': 1, '2000': 1, 'beneath': 1, 'surface': 1, 'pressurised': 1, 'perspex': 1, 'viewing': 1, 'domes': 1, 'optimum': 1, 'uboat': 1, 'worx': 1, 'dive': 1, '984': 1, '300': 1, 'metres': 1, 'snorkel': 1, 'equipped': 1, 'bond': 1, 'villain': 1, 'advanced': 1, 'submerged': 1, 'vehicles': 1, 'convert': 1, 'yacht': 1, 'detached': 1, 'speedy': 1, 'olivers': 1, 'offers': 1, 'mile': 1, 'complete': 1, 'chef': 1, 'butler': 1, '175000': 1, 'sitting': 1, 'youngest': 1, 'grandchild': 1, 'lap': 1, 'wiping': 1, 'tears': 1, 'headscarf': 1, 'girl': 1, 'ears': 1, 'desperately': 1, 'cover': 1, 'aerial': 1, 'bombardment': 1, 'lay': 1, 'awake': 1, 'village': 1, 'yemeni': 1, 'island': 1, 'birim': 1, 'counting': 1, 'explosions': 1, 'cried': 1, 'though': 1, 'landed': 1, 'neighbor': 1, 'shouted': 1, 'husband': 1, 'fell': 1, 'burned': 1, 'tells': 1, 'leading': 1, 'coalition': 1, 'qassims': 1, 'family': 1, 'shelter': 1, 'ﬂeeing': 1, 'boarding': 1, 'sail': 1, 'djibouti': 1, 'pounding': 1, 'bid': 1, 'defeat': 1, 'houthi': 1, 'rebels': 1, '14summaries': 1, 'rerelease': 1, 'titled': 1, 'extra': 1, 'disks': 1, 'unpublished': 1, 'unveiled': 1, 'nouvelles': 1, 'rereleases': 1, 'tout': 1, 'disc': 1, 'necessaire': 1, 'lalbum': 1, 'session': 1, 'lecran': 1, 'quelques': 1, 'ont': 1, 'deja': 1, 'echappes': 1, 'release': 1, 'originally': 1, 'discs': 1, 'origi': 1, 'inedites': 1, 'explained': 1, 'free': 1, 'surgery': 1, 'hed': 1, 'ceived': 1, 'explique': 1, 'loperation': 1, 'gratuite': 1, 'subie': 1, 'soigner': 1, 'hernie': 1, 'permettrait': 1, 'travailler': 1, 'nouveau': 1, 'told': 1, 'operation': 1, 'gratuity': 1, 'promised': 1, 'travel': 1, 'speaking': 1, 'video': 1, 'mr': 1, 'automatic': 1, 'pilot': 1, 'going': 1, 'cold': 1, 'course': 1, '911': 1, 'ledition': 1, 'ligne': 1, 'ministre': 1, 'edition': 1, 'dit': 1, 'ny': 1, 'quune': 1, 'moi': 1, 'tous': 1, 'autres': 1, 'personnes': 1, 'administration': 1, 'evident': 1, 'fois': 1, 'guer': 1, 'sexprimant': 1, 'lors': 1, 'intergouvernementale': 1, 'londres': 1, 'liaison': 1, 'vid': 1, 'eo': 1, 'eclare': 1, 'est': 1, 'moimˆeme': 1, 'dautres': 1, 'mem': 1, 'bres': 1, 'avons': 1, 'pris': 1, 'connaissance': 1, 'cer': 1, 'taines': 1, 'choses': 1, 'pilote': 1, 'automatique': 1, 'parce': 1, 'nous': 1, 'avions': 1, 'possibilite': 1, 'seconde': 1, 'mondiale': 1, 'jusquaux': 1, 'annees': 1, 'difﬁciles': 1, 'froide': 1, 'puis': 1, 'bien': 1, 'sˆur': 1, 'septembre': 1, 'conf': 1, 'erence': 1, 'aware': 1, 'past': 1, 'automated': 1, '15english': 1, 'translations': 1, 'run': 1, 'plans': 1, 'announced': 1, '2007': 1, 'organizers': 1, 'journey': 1, 'harmony': 1, '129': 1, 'carried': 1, '85000': 1, 'mi': 1, 'longest': 1, 'distance': 1, 'tradition': 1, '1936': 1, 'lit': 1, 'trav': 1, 'eled': 1, 'arriving': 1, 'route': 1, 'six': 1, 'continents': 1, 'visited': 1, 'cities': 1, 'silk': 1, 'symbolizing': 1, 'ancient': 1, 'ascent': 1, 'ﬂame': 1, 'mount': 1, 'nepal': 1, 'tibet': 1, 'chinese': 1, 'closed': 1, 'specially': 1, 'event': 1, 'seven': 1, 'visit': 1, 'landmarks': 1, 'climb': 1, 'everywhere': 1, '54yearold': 1, 'ofﬁce': 1, 'shopping': 1, 'mind': 1, 'thinks': 1, 'wonderful': 1, 'hes': 1, 'fourth': 1, 'child': 1, 'treat': 1, 'son': 1, 'buying': 1, 'food': 1, 'bills': 1, 'lead': 1, 'orderly': 1, 'accord': 1, 'laid': 1, 'costs': 1, 'money': 1, 'arent': 1, 'surprised': 1, 'owning': 1, '500': 1, 'kronor': 1, 'spent': 1, 'hospitals': 1, 'medical': 1, 'falls': 1, 'ill': 1, 'decide': 1, 'offer': 1, 'kennel': 1, 'club': 1, 'plains': 1, 'runs': 1, 'hit': 1, 'done': 1, 'killed': 1, 'accident': 1, '54': 1, 'stockholm': 1})}, {'file_name': 'chexpert', 'word_counts': Counter({'the': 425, 'of': 202, 'and': 175, 'to': 96, 'we': 96, 'a': 90, 'in': 88, 'on': 84, 'for': 78, 'as': 62, 'model': 58, 'is': 56, 'uncertainty': 44, 'set': 40, 'labels': 37, 'chest': 36, 'are': 33, 'that': 32, 'which': 31, 'na': 30, 'with': 29, 'dataset': 28, 'using': 28, '3': 28, 'performance': 26, 'by': 26, 'observations': 26, 'label': 26, 'each': 26, 'et': 25, 'al': 25, 'from': 24, 'radiologists': 23, 'report': 23, 'labeler': 22, 'all': 22, 'or': 22, 'our': 21, '1': 21, 'positive': 21, 'mention': 21, 'reports': 20, 'test': 20, 'edema': 20, 'pleural': 20, 'uncertain': 20, 'approach': 20, 'm': 20, 'radiology': 19, 'validation': 18, 'approaches': 18, 'cardiomegaly': 18, 'this': 18, 'atelectasis': 17, 'auc': 17, 'd': 17, 'effusion': 16, 'consolidation': 16, 'large': 15, 'no': 15, '0': 15, 'ground': 15, 'truth': 15, 'models': 15, 'cases': 15, 'j': 15, 'p': 14, 'radiographs': 14, '14': 14, 'these': 14, 'detection': 14, 'negative': 14, 'y': 13, 'different': 13, 'training': 13, 'radiologist': 13, '2018': 13, 'radiograph': 12, 'tasks': 12, 'studies': 12, '2': 12, 'c': 12, 'an': 12, '2017': 12, 'observation': 12, 'u': 12, 'labell': 12, 'labelu': 12, 'e': 12, 'k': 11, 'b': 11, 'labeled': 11, 'output': 11, 'best': 11, 'compare': 11, 'lung': 11, '000': 11, 'classification': 11, 'if': 11, 'evaluation': 11, 'l': 11, 'learning': 10, 'curves': 10, 'at': 10, 'be': 10, 'used': 10, 'not': 10, '025': 10, '050': 10, '075': 10, '100': 10, 'rads': 10, 'rad1': 10, 'rad2': 10, 'rad3': 10, 'radmaj': 10, 'r': 10, 'interpretation': 9, 'probability': 9, 'were': 9, 'find': 9, 'pneumothorax': 9, 'can': 9, 'their': 9, 'negation': 9, 'classified': 9, 'over': 9, 'labeling': 9, 'than': 9, 'chexpert': 8, 'methods': 8, 'present': 8, 'boardcertified': 8, 'pathologies': 8, 'operating': 8, 'most': 8, 'pneumonia': 8, 'other': 8, 'figure': 8, 'final': 8, 'both': 8, 'nih': 8, 'la': 8, 'wang': 8, 'uones': 8, 'rate': 8, 'comparison': 7, 'patients': 7, 'de': 7, 'ob': 7, 'annotated': 7, '5': 7, 'points': 7, 'tion': 7, 'clinical': 7, 'there': 7, 'strong': 7, 'finding': 7, 'into': 7, '1000': 7, 'ing': 7, 'when': 7, 'after': 7, 'against': 7, 'peng': 7, '2014': 7, 'one': 7, 'has': 7, 'binary': 7, 'x': 7, 'been': 7, 'li': 7, 'image': 7, 'delong': 7, 'arxiv': 7, 'preprint': 7, 's': 6, 'have': 6, 'con': 6, 'servations': 6, 'investigate': 6, 'frontal': 6, 'then': 6, 'evaluate': 6, 'roc': 6, 'common': 6, 'ra': 6, 'support': 6, 'association': 6, 'task': 6, 'least': 6, 'data': 6, 'was': 6, 'run': 6, 'sampled': 6, 'mentions': 6, 'three': 6, 'uncer': 6, 'it': 6, 'f1': 6, 'results': 6, 'two': 6, 'whether': 6, 'competition': 6, 'missing': 6, 'such': 6, 'multilabel': 6, 'examples': 6, '2015': 6, 'also': 6, 'network': 6, 'false': 6, 'sensitivity': 6, 'w': 6, 'z': 6, 'lu': 6, 'conference': 5, 'expert': 5, 'stanford': 5, 'datasets': 5, 'ent': 5, 'neural': 5, 'available': 5, 'lateral': 5, 'radiographic': 5, 'consensus': 5, 'per': 5, 'diagnosis': 5, 'automated': 5, 'enlarged': 5, 'devices': 5, 'work': 5, 'table': 5, '4': 5, 'between': 5, 'extraction': 5, 'pattern': 5, 'findings': 5, 'multiple': 5, 'mentioned': 5, 'them': 5, 'pulmonary': 5, 'rules': 5, 'first': 5, 'any': 5, 'procedure': 5, 'use': 5, 'example': 5, 'patient': 5, 'annotations': 5, 'method': 5, 'scores': 5, 'achieves': 5, 'significantly': 5, 'map': 5, 'class': 5, 'better': 5, 'likely': 5, 'but': 5, 'bound': 5, 'proceedings': 5, 'computational': 5, 'science': 4, 'university': 4, 'deep': 4, 'medical': 4, 'imaging': 4, 'presence': 4, 'ap': 4, '200': 4, '500': 4, 'selected': 4, 'pr': 4, 'lie': 4, 'benchmark': 4, 'screening': 4, 'many': 4, 'opacity': 4, 'cardiom': 4, 'lesion': 4, 'tions': 4, 'capture': 4, 'towards': 4, 'pathology': 4, 'consists': 4, 'process': 4, 'where': 4, 'based': 4, 'prevalence': 4, 'radi': 4, 'serves': 4, 'development': 4, 'collection': 4, 'images': 4, 'list': 4, 'study': 4, 'size': 4, 'prenegation': 4, '2009': 4, 'score': 4, 'average': 4, 'do': 4, 'information': 4, 'across': 4, 'un': 4, 'like': 4, 'us': 4, 'train': 4, 'during': 4, 'uignore': 4, 'umulticlass': 4, 'checkpoints': 4, 'loss': 4, 'bels': 4, '1995': 4, 'convolutional': 4, 'below': 4, 'lower': 4, 'upper': 4, 'majority': 4, 'vote': 4, '2016': 4, 'performs': 4, 'worst': 4, 'platt': 4, 'sets': 4, 'cancer': 4, 'yao': 4, 'linguistics': 4, 't': 4, 'recognition': 4, 'ieee': 4, 'david': 3, 'n': 3, 'computer': 3, '224316': 3, '65240': 3, 'automatically': 3, 'uncertainties': 3, 'proaches': 3, 'tional': 3, 'given': 3, 'useful': 3, 'additional': 3, 'above': 3, 'diseases': 3, 'diograph': 3, 'could': 3, 'reference': 3, '003': 3, '005': 3, 'predict': 3, '00': 3, 'fracture': 3, 'those': 3, 'who': 3, 'consisting': 3, '2002': 3, 'review': 3, 'determine': 3, '2008': 3, 'represent': 3, 'mild': 3, 'correctly': 3, 'text': 3, 'up': 3, 'key': 3, 'phrases': 3, 'match': 3, 'effusions': 3, 'may': 3, 'bilateral': 3, 'tain': 3, 'well': 3, 'heart': 3, 'stage': 3, 'phase': 3, 'found': 3, 'second': 3, 'similar': 3, 'ours': 3, '0936': 3, 'ex': 3, 'assigned': 3, 'blank': 3, 'system': 3, 'shown': 3, 'randomly': 3, 'access': 3, 'absent': 3, 'consider': 3, 'algorithm': 3, 'particularly': 3, 'automatic': 3, 'incorporate': 3, 'due': 3, 'however': 3, 'applying': 3, 'take': 3, 'more': 3, 'views': 3, 'ignoring': 3, 'handling': 3, 'sum': 3, 'crossentropy': 3, 'losses': 3, 'ensemble': 3, '30': 3, 'instances': 3, 'convey': 3, 'classifier': 3, 'selftraining': 3, 'tainty': 3, 'unlabeled': 3, 'its': 3, 'zhu': 3, 'wu': 3, 'prediction': 3, 'mean': 3, 'rather': 3, 'multiclass': 3, 'same': 3, 'densenet121': 3, 'unlikely': 3, 'treated': 3, 'receiver': 3, 'characteristic': 3, 'clarkepearson': 3, '1988': 3, 'xu': 3, 'correlated': 3, 'benjamini': 3, 'hochberg': 3, 'few': 3, 'analysis': 3, 'borderline': 3, 'perform': 3, 'localizes': 3, 'assigning': 3, 'calibration': 3, 're': 3, 'history': 3, 'areas': 3, 'natural': 3, 'language': 3, 'parsing': 3, 'annual': 3, 'g': 3, 'yang': 3, 'thorax': 3, 'thoracic': 3, 'localization': 3, 'summers': 3, 'xrays': 3, 'artificial': 2, 'intelligence': 2, 'yifan': 2, 'medicine': 2, 'equal': 2, 'contribution': 2, 'achieve': 2, 'networks': 2, 'manually': 2, 'differ': 2, 'public': 2, 'standard': 2, 'formance': 2, 'examina': 2, 'provide': 2, 'substantial': 2, 'decision': 2, 'progress': 2, 'need': 2, 'human': 2, '001': 2, '011': 2, 'multiview': 2, 'observa': 2, 'extract': 2, 'particular': 2, 'attention': 2, '678': 2, '1278': 2, 'obser': 2, 'vations': 2, 'significance': 2, 'board': 2, 'certified': 2, 'ologists': 2, 'make': 2, 'selection': 2, 'performed': 2, 'fleischner': 2, 'glossary': 2, 'hansell': 2, 'order': 2, 'infection': 2, 'diffuse': 2, 'reticular': 2, 'case': 2, 'extracts': 2, 'classifies': 2, 'distinct': 2, 'classifica': 2, 'aggregation': 2, 'impression': 2, 'radiol': 2, 'ways': 2, 'dence': 2, 'interstitial': 2, 'opacities': 2, 'stable': 2, 'postnegation': 2, 'matched': 2, 'third': 2, 'uni': 2, 'versal': 2, 'dependency': 2, 'parse': 2, 'follow': 2, 'split': 2, 'bird': 2, 'klein': 2, 'loper': 2, 'sentence': 2, 'trained': 2, 'biomedical': 2, 'charniak': 2, 'johnson': 2, '2005': 2, 'mcclosky': 2, '2010': 2, 'finally': 2, 'universal': 2, 'computed': 2, 'stan': 2, 'marneffe': 2, '0833': 2, '0996': 2, '0999': 2, '0993': 2, '0971': 2, '0935': 2, '0854': 2, '0769': 2, 'macroaverage': 2, '0899': 2, 'beler': 2, 'sup': 2, 'port': 2, 'positively': 2, 'another': 2, 'without': 2, 'bel': 2, 'confidently': 2, 'resolved': 2, 'resulting': 2, 'serve': 2, 'algorithms': 2, 'shows': 2, 'higher': 2, 'outperforms': 2, 'ter': 2, 'note': 2, 'differences': 2, 'produced': 2, 'conveyed': 2, 'following': 2, 'phrase': 2, 'cannot': 2, 'exclude': 2, 'rule': 2, 'input': 2, 'either': 2, 'explicitly': 2, 'called': 2, '0934': 2, '0928': 2, 'uzeros': 2, '0932': 2, 'uselftrained': 2, '0939': 2, 'choose': 2, '10': 2, 'times': 2, 'generated': 2, 'o': 2, 'yo': 2, 'vector': 2, 'imputation': 2, 'produce': 2, 'prevalent': 2, 'some': 2, 'thus': 2, 'ignores': 2, 'effective': 2, 'mapping': 2, 'kolesov': 2, 'expect': 2, 'making': 2, 'classifiers': 2, 'semisupervised': 2, '2006': 2, 'setting': 2, 'annotation': 2, 'convergence': 2, '0s': 2, 'yarowsky': 2, 'certain': 2, 'radosavovic': 2, 'upon': 2, 'instead': 2, 'prob': 2, 'ability': 2, 'outputs': 2, 'vinyals': 2, 'dean': 2, '2017a': 2, 'own': 2, 'supervising': 2, 'possible': 2, 'classes': 2, 'p0': 2, 'p1': 2, 'pu': 2, 'architecture': 2, '320': 2, '09': 2, 'fixed': 2, '02': 2, '04': 2, '06': 2, '08': 2, '085': 2, '090': 2, '097': 2, '044': 2, '091': 2, 'space': 2, 'examine': 2, 'superior': 2, 'compute': 2, 'extracted': 2, 'ologist': 2, 'contains': 2, 'full': 2, 'individually': 2, 'binarized': 2, 'gulshan': 2, 'im': 2, 'nonparametric': 2, 'sun': 2, 'delongs': 2, 'testing': 2, 'statistical': 2, 'computing': 2, 'probabilities': 2, 'ble': 2, 'significant': 2, 'forms': 2, 'cardiac': 2, 'would': 2, 'difficult': 2, 'distinguish': 2, 'assignment': 2, 'respectively': 2, 'gradientweighted': 2, 'activation': 2, 'mappings': 2, 'tation': 2, 'solidation': 2, 'performing': 2, 'postprocessing': 2, 'plot': 2, 'com': 2, 'values': 2, 'processing': 2, 'isotonic': 2, 'zadrozny': 2, 'elkan': 2, 'scaling': 2, '1999': 2, 'scaled': 2, 'brier': 2, 'steyerberg': 2, 'lies': 2, 'limitations': 2, 'potchen': 2, '1979': 2, 'berbaum': 2, 'franken': 2, 'smith': 2, '1985': 2, 'selvaraju': 2, 'gradcams': 2, 'gradi': 2, 'layer': 2, 'low': 2, 'resolution': 2, 'important': 2, 'weights': 2, 'publicly': 2, 'hosts': 2, 'national': 2, 'plco': 2, 'gohagan': 2, '2000': 2, 'clinically': 2, 'rubin': 2, 'developing': 2, 'rajpurkar': 2, '2017b': 2, 'kumar': 2, 'grewal': 2, 'srivastava': 2, 'guan': 2, 'features': 2, 'help': 2, 'dis': 2, 'practical': 2, 'journal': 2, 'society': 2, 'effect': 2, 'investigative': 2, 'media': 2, 'meeting': 2, 'dependencies': 2, 'volume': 2, 'comparing': 2, 'under': 2, 'q': 2, 'zheng': 2, 'diagnose': 2, 'disease': 2, 'h': 2, 'international': 2, 'springer': 2, 'lj': 2, 'distillation': 2, 'domain': 2, 'bagheri': 2, 'vision': 2, 'poblenz': 2, 'covington': 2, 'lyman': 2, 'supervised': 2, 'acm': 2, 'thirtythird': 1, 'aaai': 1, 'aaai19': 1, 'jeremy': 1, 'irvin1': 1, 'pranav': 1, 'rajpurkar1': 1, 'michael': 1, 'ko1': 1, 'yu1': 1, 'silviana': 1, 'ciureailcus1': 1, 'chris': 1, 'chute1': 1, 'henrik': 1, 'marklund1': 1, 'behzad': 1, 'haghgoo1': 1, 'robyn': 1, 'ball2': 1, 'katie': 1, 'shpanskaya3': 1, 'jayne': 1, 'seekins3': 1, 'mong3': 1, 'safwan': 1, 'halabi3': 1, 'jesse': 1, 'sandberg3': 1, 'ricky': 1, 'jones3': 1, 'larson3': 1, 'curtis': 1, 'langlotz3': 1, 'bhavik': 1, 'patel3': 1, 'matthew': 1, 'lungren3': 1, 'andrew': 1, 'ng1': 1, '1department': 1, '2department': 1, '3department': 1, 'jirvin16': 1, 'pranavsrcsstanfordedu': 1, 'abstract': 1, 'driven': 1, 'expertlevel': 1, 'variety': 1, 'tains': 1, 'sign': 1, 'detect': 1, 'capturing': 1, 'inher': 1, 'convolu': 1, 'composed': 1, 'release': 1, 'models1': 1, 'introduction': 1, 'radiography': 1, 'globally': 1, 'critical': 1, 'manage': 1, 'ment': 1, 'life': 1, 'threatening': 1, 'level': 1, 'practicing': 1, 'benefit': 1, 'settings': 1, 'improved': 1, 'workflow': 1, 'prioritization': 1, 'largescale': 1, 'global': 1, 'population': 1, 'health': 1, 'initiatives': 1, 'standards': 1, 'metrics': 1, 'copyright': 1, '2019': 1, 'advancement': 1, 'wwwaaaiorg': 1, 'rights': 1, 'reserved': 1, '1httpsstanfordmlgroupgithubiocompetitionschexpert': 1, '049': 1, '010': 1, '006': 1, '004': 1, '027': 1, 'fracture005': 1, 'sists': 1, 'design': 1, 'freetext': 1, 'dif': 1, 'ferent': 1, 'see': 1, 'pay': 1, '590': 1, '16627': 1, '886': 1, '171014': 1, '9114': 1, '9020': 1, '481': 1, '10148': 1, '541': 1, '168473': 1, '8978': 1, '23002': 1, '1226': 1, '6597': 1, '352': 1, '158042': 1, '8423': 1, '6856': 1, '365': 1, '1071': 1, '057': 1, '179714': 1, '9578': 1, '92669': 1, '4939': 1, '4341': 1, '231': 1, '90631': 1, '483': 1, '48905': 1, '2606': 1, '11571': 1, '617': 1, '127165': 1, '6777': 1, '12730': 1, '23976': 1, '150935': 1, '8044': 1, '4576': 1, '244': 1, '15658': 1, '834': 1, '167407': 1, '8922': 1, '29333': 1, '1563': 1, '29377': 1, '1566': 1, '128931': 1, '6871': 1, '17313': 1, '923': 1, '2663': 1, '142': 1, '167665': 1, '8935': 1, '75696': 1, '4034': 1, '9419': 1, '502': 1, '102526': 1, '5464': 1, '2441': 1, '13': 1, '1771': 1, '094': 1, '183429': 1, '9776': 1, '7270': 1, '387': 1, '484': 1, '026': 1, '179887': 1, '9587': 1, '105831': 1, '564': 1, '898': 1, '048': 1, '80912': 1, '4312': 1, 'number': 1, 'contain': 1, 'incorporating': 1, 'sess': 1, 'addi': 1, 'separate': 1, 'out': 1, 'pub': 1, 'licly': 1, 'encourage': 1, 'further': 1, 'inter': 1, 'pretation': 1, 'posi': 1, 'tive': 1, 'prevalences': 1, 'obsevations': 1, 'retrospectively': 1, 'collected': 1, 'hospital': 1, 'october': 1, 'july': 1, 'inpatient': 1, 'outpatient': 1, 'centers': 1, 'along': 1, 'associated': 1, 'sam': 1, 'pled': 1, 'manual': 1, 'feasibility': 1, 'decided': 1, 'relevance': 1, 'conforming': 1, 'societys': 1, 'recommended': 1, 'whenever': 1, 'applicable': 1, 'despite': 1, 'included': 1, 'suggested': 1, 'primary': 1, 'intended': 1, 'absence': 1, 'unremarkable': 1, 'cardiomediastinal': 1, 'silhouette': 1, 'seen': 1, 'atypical': 1, 'chronic': 1, 'fibrotic': 1, 'change': 1, 'focal': 1, 'degenerative': 1, 'changes': 1, 'lumbar': 1, 'spine': 1, 'old': 1, 'right': 1, 'rib': 1, 'fractures': 1, 'underline': 1, 'bolded': 1, 'negations': 1, 'italicized': 1, 'developed': 1, 'rulebased': 1, 'free': 1, 'structured': 1, 'stages': 1, 'section': 1, 'ogy': 1, 'summarizes': 1, 'diographic': 1, 'cu': 1, 'rated': 1, 'vari': 1, 'ous': 1, 'extracting': 1, 'aim': 1, 'classify': 1, 'evi': 1, 'pneumoth': 1, 'orax': 1, 'moder': 1, 'ate': 1, 'bibasilar': 1, 'ambiguity': 1, 'inherent': 1, '3phase': 1, 'pipeline': 1, 'accordingly': 1, 'phases': 1, 'designed': 1, 'obtain': 1, 'tokenized': 1, 'sentences': 1, 'nltk': 1, 'parsed': 1, 'bllip': 1, 'parser': 1, 'mccloskys': 1, 'pendency': 1, 'graph': 1, 'ford': 1, 'corenlp': 1, '591': 1, 'category': 1, '0976': 1, '0998': 1, '0526': 1, '0661': 1, '0647': 1, '0973': 1, '0000': 1, '0909': 1, '0211': 1, '0727': 1, '0879': 1, '0981': 1, '0438': 1, '0924': 1, '0978': 1, '0873': 1, '0962': 1, '0535': 1, '0796': 1, '0985': 1, '0951': 1, '0553': 1, '0707': 1, '0660': 1, '0992': 1, '0703': 1, '0750': 1, '0250': 1, '0817': 1, '0977': 1, '0167': 1, '0762': 1, '0959': 1, '0896': 1, '0900': 1, '0857': 1, '0966': 1, '0914': 1, '0286': 1, '0850': 1, '0975': 1, '0807': 1, '0800': 1, '0933': 1, '0720': 1, '0948': 1, '0770': 1, 'microaverage': 1, '0969': 1, '0952': 1, '0848': 1, 'traction': 1, 'measured': 1, 'micro': 1, 'rows': 1, 'arrive': 1, 'consist': 1, '12': 1, 'signed': 1, 'negatively': 1, 'assign': 1, 'overlap': 1, 'whose': 1, 'develop': 1, 'uncertainly': 1, 'curating': 1, 'conventions': 1, 'adhere': 1, 'indepen': 1, 'dently': 1, 'disagreements': 1, 'discussion': 1, 'employed': 1, 'annotate': 1, 'extrac': 1, 'sider': 1, 'algo': 1, 'rithm': 1, 'notably': 1, 'bet': 1, 'certainty': 1, 'gains': 1, 'extractors': 1, 'metamap': 1, 'dnorm': 1, 'weak': 1, 'extractions': 1, 'applied': 1, 'sev': 1, 'eral': 1, 'variation': 1, 'allowed': 1, 'resolve': 1, 'double': 1, 'matching': 1, 'clude': 1, 'conveys': 1, 'pneu': 1, 'mothorax': 1, 'xxx': 1, 'singleview': 1, 'view': 1, 'maximum': 1, 'explore': 1, 'simple': 1, 'ignore': 1, 'base': 1, 'line': 1, 'optimize': 1, 'masked': 1, '592': 1, '0818': 1, '07590877': 1, '0828': 1, '07690888': 1, '0938': 1, '09050970': 1, '08930975': 1, '08940962': 1, '0811': 1, '07510872': 1, '0840': 1, '07830897': 1, '08980966': 1, '0929': 1, '08880970': 1, '0931': 1, '08970965': 1, '0858': 1, '08060910': 1, '0832': 1, '07730890': 1, '08540944': 1, '0941': 1, '09030980': 1, '09010967': 1, '07760890': 1, '0831': 1, '07700891': 1, '09080971': 1, '08960974': 1, '08990966': 1, '0821': 1, '07630879': 1, '08000909': 1, '0937': 1, '09050969': 1, '08870968': 1, '09040967': 1, 'auroc': 1, 'masking': 1, 'marked': 1, 'formally': 1, 'lx': 1, 'uyo': 1, 'log': 1, 'pyo': 1, '1x': 1, 'logpyo': 1, '0x': 1, 'imagey': 1, 'length': 1, 'taken': 1, 'analogous': 1, 'wise': 1, 'complete': 1, 'deletion': 1, 'gra': 1, 'ham': 1, 'value': 1, 'deleted': 1, 'biased': 1, 'completely': 1, 'random': 1, 'quite': 1, 'almost': 1, 'twice': 1, 'proportion': 1, 'reducing': 1, 'replaced': 1, 'uzeroes': 1, 'zero': 1, 'strategies': 1, 'statistics': 1, 'mimic': 1, 'does': 1, 'semantically': 1, 'distort': 1, 'degrade': 1, 'framework': 1, 'approaching': 1, 'lending': 1, 'way': 1, 'closely': 1, 'tied': 1, 'mlml': 1, 'aims': 1, 'handle': 1, 'partial': 1, 'predictions': 1, 'relabel': 1, 'outputted': 1, 'replace': 1, 'rela': 1, 'beled': 1, 'follows': 1, 'threshold': 1, 'repeating': 1, 'until': 1, 'build': 1, 'tech': 1, 'nique': 1, 'remove': 1, 'iteratively': 1, 'predicting': 1, 'transformed': 1, 'versions': 1, 'inputs': 1, 'target': 1, 'soft': 1, 'continuous': 1, 'hin': 1, 'ton': 1, '3class': 1, 'treating': 1, 'hypothesize': 1, 'allowing': 1, 'net': 1, 'representation': 1, 'time': 1, 'probabil': 1, 'ity': 1, 'softmax': 1, 'restricted': 1, 'experimented': 1, 'several': 1, 'architectures': 1, 'specif': 1, 'ically': 1, 'resnet152': 1, 'inceptionv4': 1, 'se': 1, 'resnext101': 1, 'experiments': 1, 'fed': 1, 'pixels': 1, 'adam': 1, 'optimizer': 1, 'fault': 1, 'βparameters': 1, 'β1': 1, 'β2': 1, '104': 1, 'duration': 1, 'batches': 1, 'batch': 1, '16': 1, 'epochs': 1, 'saving': 1, 'every': 1, '4800': 1, 'itera': 1, '593': 1, '10true': 1, '020022': 1, '010051': 1, '021080': 1, '018071': 1, '031092': 1, '022089': 1, '016024': 1, '004042': 1, '005048': 1, '023085': 1, '011070': 1, '008075': 1, '018031': 1, '005041': 1, '011066': 1, '009048': 1, '003045': 1, '005052': 1, '015049': 1, '012065': 1, '092': 1, '009063': 1, '019079': 1, '007058': 1, '008068': 1, '021078': 1, '016088': 1, '005082': 1, '017083': 1, '014089': 1, '010089': 1, '10precision': 1, '022032': 1, '051070': 1, '069': 1, '080062': 1, '071064': 1, '092056': 1, '089064': 1, '024039': 1, '042082': 1, '081': 1, '048082': 1, '085061': 1, '070074': 1, '075080': 1, '031010': 1, '041034': 1, '066027': 1, '048025': 1, '045045': 1, '052038': 1, '049038': 1, '065050': 1, '066': 1, '063058': 1, '079044': 1, '058059': 1, '068062': 1, '078049': 1, '088059': 1, '082080': 1, '083055': 1, '089063': 1, '089071': 1, 'bounds': 1, 'lap': 1, 'val': 1, 'idation': 1, 'classifying': 1, 'anno': 1, 'tations': 1, 'define': 1, 'area': 1, 'der': 1, 'curve': 1, 'met': 1, 'ric': 1, 'focus': 1, 'call': 1, 'portance': 1, 'atelec': 1, 'tasis': 1, '95': 1, 'twosided': 1, 'confi': 1, 'intervals': 1, 'bestperforming': 1, 'greater': 1, 'worstperforming': 1, 'onesided': 1, 'control': 1, 'hypothesis': 1, 'adjusted': 1, 'pvalue': 1, 'indicates': 1, 'check': 1, 'aucs': 1, 'achieved': 1, 'ta': 1, 'atelecta': 1, 'sis': 1, 'auc0858': 1, 'outper': 1, 'auc0811': 1, 'observe': 1, 'auc0854': 1, 'auc0828': 1, 'ineffective': 1, 'minimal': 1, 'enlargement': 1, 'ignored': 1, 'cause': 1, 'poorly': 1, 'nonborderline': 1, 'enable': 1, 'disambiguate': 1, 'categorize': 1, '594': 1, 'ef': 1, 'fusions': 1, 'top': 1, 'bottom': 1, 'predicted': 1, 'single': 1, 'demonstrates': 1, 'mid': 1, 'predominant': 1, 'car': 1, 'diomegaly': 1, 'consistent': 1, 'cardiogenic': 1, 'accurately': 1, '824': 1, 'independent': 1, 'readers': 1, 'misclassified': 1, 'examination': 1, 'interpre': 1, 'subcaptions': 1, 'provided': 1, 'changed': 1, 'appearance': 1, 'tours': 1, 'nonenlarged': 1, 'therefore': 1, '1s': 1, 'fails': 1, 'optimal': 1, 'insufficient': 1, 'modifi': 1, 'cations': 1, 'hinting': 1, 'servation': 1, 'effectively': 1, 'utilized': 1, 'meant': 1, 'describe': 1, 'ings': 1, 'ports': 1, 'good': 1, 'suspect': 1, 'explanation': 1, 'high': 1, 'possi': 1, 'fact': 1, 'contrast': 1, 'whereas': 1, 'theuzeros': 1, 'ten': 1, 'together': 1, 'versus': 1, 'very': 1, 'signs': 1, 'panel': 1, 'sorts': 1, 'often': 1, 'atelectasispositive': 1, 'consolidationnegative': 1, 'ogists': 1, 'selftrained': 1, 'ran': 1, 'domly': 1, 'eight': 1, 'proce': 1, 'dure': 1, 'described': 1, 'remaining': 1, 'individual': 1, 'recall': 1, 'specificity': 1, 'precision': 1, 'radiolo': 1, 'gist': 1, 'precisionrecall': 1, 'pute': 1, '595': 1, 'convert': 1, 'opposite': 1, 'denoted': 1, 'measure': 1, 'before': 1, 'post': 1, 'techniques': 1, 'namely': 1, 'regres': 1, 'sion': 1, 'others': 1, 'illustrates': 1, 'plots': 1, 'auprc': 1, 'pleu': 1, 'ral': 1, 'consoli': 1, 'dation': 1, 'exceeds': 1, 'besides': 1, 'comparably': 1, 'diologists': 1, 'fore': 1, '0110': 1, 'gression': 1, '0107': 1, '0101': 1, 'acknowledge': 1, 'neither': 1, 'nor': 1, 'had': 1, 'previous': 1, 'decrease': 1, 'diagnostic': 1, 'sta': 1, 'tistical': 1, 'assess': 1, 'difference': 1, 'statistically': 1, 'visualization': 1, 'visualize': 1, 'predicts': 1, 'indicative': 1, 'grad': 1, 'cams': 1, 'highlights': 1, 'portions': 1, 'specifically': 1, 'construct': 1, 'linear': 1, 'weighted': 1, 'feature': 1, 'maps': 1, 'upscale': 1, 'dimensions': 1, 'origi': 1, 'nal': 1, 'overlay': 1, 'illustrated': 1, 'existing': 1, 'main': 1, 'obstacles': 1, 'lack': 1, 'radiologistannotated': 1, 'groundtruth': 1, 'researchers': 1, 'mod': 1, 'els': 1, 'none': 1, 'performances': 1, 'indiana': 1, 'care': 1, 'openi': 1, 'demner': 1, 'fushman': 1, '7470': 1, 'frontalview': 1, 'radio': 1, 'graphs': 1, 'annotators': 1, 'stitute': 1, 'obtained': 1, '185421': 1, 'ages': 1, 'nature': 1, 'mimiccxr': 1, 'cently': 1, 'announced': 1, 'yet': 1, 'commonly': 1, 'chestx': 1, 'ray14': 1, 'introduc': 1, 'made': 1, 'terpretation': 1, 'problematic': 1, 'tracted': 1, 'chex': 1, 'pert': 1, 'introduce': 1, 'radiologistlabeled': 1, 'dards': 1, 'allow': 1, 'robust': 1, 'conclusion': 1, 'vali': 1, 'date': 1, 'relevant': 1, 'hope': 1, 'will': 1, 'velopment': 1, 'improving': 1, 'healthcare': 1, 'delivery': 1, 'worldwide': 1, 'acknowledgements': 1, 'thank': 1, 'luke': 1, 'oakdenrayner': 1, 'susan': 1, 'weber': 1, 'references': 1, 'controlling': 1, 'covery': 1, 'powerful': 1, 'royal': 1, 'series': 1, 'methodological': 1, '289300': 1, 'films': 1, 'resident': 1, 'pediatric': 1, 'diographs': 1, '20124128': 1, '596': 1, 'python': 1, 'analyzing': 1, 'toolkit': 1, 'oreilly': 1, 'inc': 1, 'coarsetofine': 1, 'nbest': 1, 'maxent': 1, 'discriminative': 1, 'reranking': 1, '43rd': 1, '173': 1, '180': 1, 'mc': 1, 'dozat': 1, 'silveira': 1, 'haverinen': 1, 'gin': 1, 'f': 1, 'nivre': 1, 'manning': 1, 'crosslinguistic': 1, 'typology': 1, 'lrec': 1, '45854592': 1, 'oper': 1, 'ating': 1, 'biometrics': 1, '837845': 1, 'demnerfushman': 1, 'kohli': 1, 'rosenman': 1, 'shooshan': 1, 'rodriguez': 1, 'antani': 1, 'thoma': 1, 'mcdonald': 1, 'preparing': 1, 'examinations': 1, 'distribution': 1, 'retrieval': 1, 'american': 1, 'infor': 1, 'matics': 1, '232304310': 1, 'prorok': 1, 'hayes': 1, 'kramer': 1, 'bs': 1, 'prostate': 1, 'colorectal': 1, 'ovarian': 1, 'trial': 1, 'institute': 1, 'organiza': 1, 'status': 1, 'controlled': 1, 'trials': 1, '216251s272s': 1, 'graham': 1, 'real': 1, 'world': 1, 'psychology': 1, '60549576': 1, 'huang': 1, 'zhong': 1, 'guided': 1, 'arxiv180109927': 1, 'v': 1, 'coram': 1, 'stumpe': 1, 'narayanaswamy': 1, 'venugopalan': 1, 'widner': 1, 'madams': 1, 'cuadros': 1, 'diabetic': 1, 'retinopathy': 1, 'retinal': 1, 'fundus': 1, 'photographs': 1, 'jama': 1, '3162224022410': 1, 'bankier': 1, 'macmahon': 1, 'mcloud': 1, 'muller': 1, 'remy': 1, 'terms': 1, '2463697722': 1, 'hinton': 1, 'distilling': 1, 'knowl': 1, 'edge': 1, 'arxiv150302531': 1, 'kamyshenkov': 1, 'litovchenko': 1, 'smekalova': 1, 'golovizin': 1, 'zhavoronkov': 1, 'clas': 1, 'sification': 1, 'incompletely': 1, 'mathematical': 1, 'boosted': 1, 'cas': 1, 'caded': 1, 'convnets': 1, '546552': 1, 'song': 1, 'cao': 1, 'luo': 1, 'noisy': 1, 'iccv': 1, '19281936': 1, 'han': 1, 'xue': 1, 'wei': 1, 'ff': 1, 'identification': 1, 'limited': 1, 'supervision': 1, 'arxiv171106373': 1, 'adap': 1, 'negbio': 1, 'highperformance': 1, 'tool': 1, 'amia': 1, 'summits': 1, 'transla': 1, '2017188': 1, 'probabilistic': 1, 'ma': 1, 'chines': 1, 'comparisons': 1, 'regularized': 1, 'likelihood': 1, 'ad': 1, 'vances': 1, 'margin': 1, '1036174': 1, 'gard': 1, 'lazar': 1, 'lahaie': 1, 'andary': 1, 'film': 1, 'interpretationdirection': 1, 'distraction': 1, '404404': 1, 'lippincottra': 1, 'ven': 1, 'publ': 1, '227': 1, 'east': 1, 'ashington': 1, 'sq': 1, 'philadelphia': 1, 'pa': 1, '19106': 1, 'i': 1, 'dollar': 1, 'girshick': 1, 'gkioxari': 1, 'he': 1, 'omnisupervised': 1, 'arxiv171204440': 1, 'irvin': 1, 'mehta': 1, 'duan': 1, 'ding': 1, 'bagul': 1, 'langlotz': 1, 'shpanskaya': 1, 'lungren': 1, 'ng': 1, 'chexnet': 1, 'radiologistlevel': 1, 'tection': 1, 'arxiv171105225': 1, 'cs': 1, 'stat': 1, '171105225': 1, 'sanghavi': 1, 'zhao': 1, 'lee': 1, 'qadir': 1, 'wilson': 1, 'scale': 1, 'reading': 1, 'dual': 1, 'networksarxiv': 1, 'arxiv180407839': 1, 'das': 1, 'vedantam': 1, 'cogswell': 1, 'parikh': 1, 'batra': 1, 'gradcam': 1, 'why': 1, 'did': 1, 'you': 1, 'say': 1, 'visual': 1, 'explanations': 1, 'via': 1, 'gradientbased': 1, 'corr': 1, 'abs161002391': 1, '7': 1, '2008clinical': 1, 'proach': 1, 'updating': 1, 'business': 1, 'fast': 1, 'implementation': 1, 'op': 1, 'erating': 1, 'signal': 1, 'letters': 1, '211113891393': 1, 'chestxray8': 1, 'hospitalscale': 1, 'xray': 1, 'database': 1, 'benchmarks': 1, 'weaklysupervised': 1, 'local': 1, 'ization': 1, 'cvpr': 1, '34623471': 1, 'honolulu': 1, 'hi': 1, 'tienet': 1, 'textimage': 1, 'embedding': 1, 'ease': 1, 'reporting': 1, 'recogni': 1, '90499058': 1, 'lyu': 1, 'hu': 1, 'bg': 1, 'ji': 1, 'learn': 1, 'facial': 1, 'action': 1, 'unit': 1, '48722792289': 1, 'dagunts': 1, 'bernard': 1, 'scratch': 1, 'exploiting': 1, 'among': 1, 'arxiv171010501': 1, 'prosky': 1, 'weakly': 1, 'resolutions': 1, 'arxiv180307703': 1, 'unsupervised': 1, 'word': 1, 'sense': 1, 'disambiguation': 1, 'rivaling': 1, '33rd': 1, '189196': 1, 'transforming': 1, 'accurate': 1, 'estimates': 1, 'eighth': 1, 'sigkdd': 1, 'knowledge': 1, 'discovery': 1, 'mining': 1, '694699': 1, 'literature': 1, 'survey': 1, 'puter': 1, 'wisconsinmadison': 1, '234': 1, '597': 1})}, {'file_name': 'efficientnet', 'word_counts': Counter({'and': 158, 'the': 127, 'et': 124, 'al': 124, 'to': 92, 'a': 91, 'scaling': 88, 'of': 74, 'for': 72, 'accuracy': 60, '2018': 59, 'in': 58, 'with': 57, 'we': 55, 'our': 53, 'network': 52, 'is': 47, 'by': 44, 'model': 43, 'on': 43, 'width': 38, 'are': 36, 'models': 36, 'resolution': 36, 'depth': 35, '2016': 35, 'that': 34, 'baseline': 33, '2017': 33, 'neural': 32, 'flops': 32, 'networks': 31, 'efﬁcientnet': 29, 'convnets': 29, 'more': 29, 'scale': 29, 'up': 27, 'method': 26, '2019': 26, 'learning': 25, '1': 25, 'he': 25, '2': 25, 'as': 24, 'imagenet': 23, 'huang': 23, 'better': 22, 'compound': 22, 'j': 22, 's': 22, 'from': 21, 'all': 20, 'figure': 20, 'v': 19, 'than': 19, 'different': 19, '3': 18, 'but': 18, 'm': 18, 'cvpr': 18, 'convolutional': 17, 'can': 17, 'search': 17, 'it': 17, 'parameters': 17, 'this': 16, 'transfer': 16, 'table': 16, 'd': 16, 'deep': 16, '1x': 16, 'pp': 16, 'architecture': 15, 'top1': 15, 'convnet': 15, 'also': 15, 'zoph': 15, 'layers': 14, 'szegedy': 14, 'efﬁcientnetb0': 14, 'scaled': 13, 'new': 13, 'achieve': 13, 'be': 13, 'gpipe': 13, 'b': 13, 'higher': 13, 'w': 13, 'k': 13, 'dimensions': 12, 'efﬁcientnetb7': 12, 'tan': 12, 'efﬁciency': 12, 'where': 12, 'results': 12, 'rethinking': 11, 'use': 11, 'other': 11, 'datasets': 11, 'an': 11, '80': 11, 'acc': 11, 'size': 11, '5': 11, 'same': 11, 'r': 11, 'q': 11, 'z': 11, 'using': 10, 'resnet': 10, 'while': 10, 'order': 10, 'fewer': 10, 'resnet50': 10, '4': 10, 'or': 10, 'image': 10, 'howard': 10, 'le': 10, 'have': 10, '101': 10, 'l': 10, 'achieves': 9, 'used': 9, 'nasneta': 9, 'these': 9, 'c': 9, 'dimension': 9, 'bigger': 9, 'i': 9, '102': 9, 'h': 9, 'performance': 8, 'further': 8, 'efﬁcient': 8, 'which': 8, 'best': 8, 'existing': 8, 'example': 8, 'larger': 8, 'resnet152': 8, 'hu': 8, 'work': 8, 'such': 8, 'each': 8, 'methods': 8, 'mobile': 8, 'will': 8, 'x': 8, 'target': 8, 'images': 8, 'inceptionv4': 8, '103': 8, 'g': 8, 'study': 7, 'mobilenets': 7, 'design': 7, 'much': 7, 'stateoftheart': 7, 'efﬁcientnets': 7, 'has': 7, '0': 7, '76': 7, '78': 7, 'con': 7, 'only': 7, 'shows': 7, 'e': 7, 'deeper': 7, 'increase': 7, 'small': 7, 'input': 7, 'capture': 7, 'ﬁrst': 7, '2015': 7, 'zhang': 7, 'lin': 7, 'problem': 7, 'layer': 7, 'n': 7, 'liu': 7, 'p': 7, 'y': 7, 'preprint': 7, 'resource': 6, 'paper': 6, 'observation': 6, 'coefﬁcient': 6, 'particular': 6, '844': 6, '61x': 6, 'faster': 6, '788': 6, 'xie': 6, 'common': 6, 'way': 6, 'zagoruyko': 6, 'αβγ': 6, 'ﬁnegrained': 6, 'sandler': 6, 'net': 6, 'similar': 6, '8': 6, 'krizhevsky': 6, 'shown': 6, 'object': 6, 'cost': 6, 'resolutions': 6, 'li': 6, 'features': 6, '6': 6, 'billions': 6, '77': 6, '10': 6, 'mbconv6': 6, 'mobilenetv2': 6, 'latency': 6, '41m': 6, 'eccv': 6, 'chen': 6, 'arxiv': 6, 'then': 5, 'propose': 5, 'uniformly': 5, 'scales': 5, 'even': 5, '84x': 5, 'well': 5, 'widely': 5, 'accu': 5, 'racy': 5, '843': 5, 'many': 5, 'efﬁcientnetb1': 5, 'details': 5, 'komodakis': 5, 'three': 5, 'balance': 5, 'ratio': 5, 'conventional': 5, 'coefﬁcients': 5, 'channels': 5, 'wider': 5, 'proposed': 5, 'since': 5, '2014': 5, 'kornblith': 5, 'gain': 5, 'han': 5, 'space': 5, 'large': 5, 'constraints': 5, 'fi': 5, 'stage': 5, 'denotes': 5, 'batch': 5, 'any': 5, 'training': 5, 'very': 5, '75': 5, '79': 5, 'd10': 5, 'd20': 5, 'r10': 5, 'real': 5, 'w2': 5, 'r2': 5, 'equation': 5, '556m': 5, 'com': 5, 'hinton': 5, 'vasudevan': 5, 'sun': 5, 'ﬁxed': 4, 'if': 4, 'resources': 4, 'available': 4, 'simple': 4, 'demonstrate': 4, 'effectiveness': 4, 'previous': 4, 'smaller': 4, 'inference': 4, 'cifar100': 4, 'flowers': 4, 'magnitude': 4, 'recently': 4, 'however': 4, 'there': 4, '20': 4, 'number': 4, 'inceptionresnetv2': 4, 'xception': 4, 'senet': 4, '826': 4, 'out': 4, '57x': 4, 'vnets': 4, 'tuning': 4, 'still': 4, 'empirical': 4, 'critical': 4, 'both': 4, 'ing': 4, 'notably': 4, 'efﬁcientnetb4': 4, '2012': 4, 'pipeline': 4, 'vision': 4, 'hardware': 4, 'over': 4, 're': 4, 'how': 4, 'power': 4, 'tensor': 4, 'under': 4, 'due': 4, 'train': 4, 'ioffe': 4, '81': 4, 'r13': 4, 'tend': 4, '25': 4, 'us': 4, 'not': 4, 'αβ2': 4, 'k3x3': 4, 'step': 4, 'comparison': 4, 'efﬁcientnetb3': 4, 'cubuk': 4, '64m': 4, '87x': 4, 'birdsnap': 4, 'food101': 4, 'ngiam': 4, '92': 4, 'visual': 4, 'iclr': 4, 'residual': 4, 'girshick': 4, 'wang': 4, 'shlens': 4, 'commonly': 3, 'lead': 3, 'based': 3, 'effective': 3, 'go': 3, 'obtain': 3, 'called': 3, '971': 3, 'top5': 3, 'being': 3, 'ac': 3, '40': 3, '100': 3, '82': 3, 'densenet201': 3, 'resnext101': 3, 'amoebanetc': 3, '811': 3, '2018827': 3, 'vs': 3, 'ways': 3, 'do': 3, 'their': 3, 'increasingly': 3, 'one': 3, 'often': 3, 'principled': 3, 'unlike': 3, 'times': 3, 'computational': 3, 'grid': 3, 'between': 3, 'intuitively': 3, 'patterns': 3, 'raghu': 3, 'lu': 3, 'empirically': 3, 'compared': 3, 'improves': 3, 'become': 3, 'about': 3, '827': 3, 'trained': 3, 'specialized': 3, 'studies': 3, 'computer': 3, 'detection': 3, 'although': 3, 'applications': 3, 'compression': 3, 'mobilesize': 3, 'ma': 3, 'cai': 3, 'convolution': 3, 'apply': 3, 'help': 3, 'expressive': 3, 'output': 3, 'shape': 3, 'representative': 3, 'focus': 3, 'optimization': 3, 'ˆci': 3, 'suggesting': 3, 'quickly': 3, 'single': 3, '224x224': 3, 'improve': 3, 'φ': 3, 'γ2': 3, 't': 3, 'k5x5': 3, '7': 3, 'reduction': 3, '41b': 3, '35b': 3, '13b': 3, '962': 3, '47x': 3, '23b': 3, 'efﬁcientnetb5': 3, 'efﬁcientnetb6': 3, 'pretrained': 3, 'mobilenetv1': 3, 'd4': 3, 'activation': 3, '96x': 3, 'average': 3, 'cifar10': 3, 'stanford': 3, '985': 3, 'fgvc': 3, 'oxfordiiit': 3, '02': 3, 'cars': 3, 'aircraft': 3, 'runs': 3, 'zhou': 3, 'krause': 3, 'zisserman': 3, 'pang': 3, 'van': 3, 'zhu': 3, 'f': 3, 'recognition': 3, 'o': 3, 'vanhoucke': 3, 'mingxing': 2, 'developed': 2, 'at': 2, 'identify': 2, 'carefully': 2, 'balancing': 2, 'yet': 2, 'highly': 2, 'family': 2, 'curacy': 2, '988': 2, 'resnet18': 2, 'resnet200': 2, 'achieved': 2, 'ima': 2, 'genet': 2, 'four': 2, 'process': 2, 'research': 2, 'brain': 2, 'team': 2, 'view': 2, '36': 2, 'international': 2, 'machine': 2, '97': 2, 'millions': 2, '74': 2, '84imagenet': 2, 'top': 2, 'resnet34': 2, 'inceptionv2': 2, 'amoebaneta': 2, 'b0': 2, 'b3': 2, 'b4': 2, 'b5': 2, '2017809': 2, 'signiﬁcantly': 2, '76x': 2, 'most': 2, 'popular': 2, 'though': 2, 'possible': 2, 'two': 2, 'arbitrarily': 2, 'arbitrary': 2, 'tedious': 2, 'manual': 2, 'want': 2, 'question': 2, 'simply': 2, 'them': 2, 'constant': 2, 'practice': 2, 'set': 2, 'determined': 2, 'original': 2, 'illustrates': 2, 'needs': 2, 'fact': 2, 'show': 2, 'relationship': 2, 'develop': 2, '763': 2, 'reducing': 2, '21x': 2, 'accurate': 2, 'going': 2, 'winner': 2, '748': 2, 'validation': 2, '557m': 2, 'parallelism': 2, 'library': 2, 'tasks': 2, 'already': 2, 'memory': 2, 'yang': 2, 'mo': 2, 'iandola': 2, 'gholami': 2, 'becomes': 2, 'techniques': 2, 'expensive': 2, 'surpass': 2, 'goal': 2, 'vnet': 2, 'eg': 2, 'prior': 2, 'jegelka': 2, 'sharir': 2, 'shashua': 2, 'important': 2, 'remains': 2, 'effectively': 2, 'cally': 2, 'section': 2, 'function': 2, 'yi': 2, 'operator': 2, 'spatial': 2, 'ci': 2, 'channel': 2, 'list': 2, 'multiple': 2, 'stages': 2, 'ﬁve': 2, 'except': 2, 'i1s': 2, 'fli': 2, 'omit': 2, 'initial': 2, 'regular': 2, 'mostly': 2, 'ﬁnd': 2, 'ex': 2, 'without': 2, 'changing': 2, 'predeﬁned': 2, 'reduce': 2, 'dwr': 2, 'ndwr': 2, 'st': 2, 'ˆhi': 2, 'ˆwi': 2, 'pa': 2, 'rameters': 2, '32': 2, 'main': 2, 'difﬁculty': 2, 'values': 2, 'change': 2, 'eral': 2, 'connections': 2, 'normalization': 2, 'diminishes': 2, 'resnet101': 2, 'wide': 2, 'saturates': 2, 'when': 2, 'starting': 2, '299x299': 2, 'indeed': 2, 'high': 2, 'should': 2, 'pixels': 2, '15': 2, 'works': 2, 'last': 2, 'intuitions': 2, 'suggest': 2, 'need': 2, 'rather': 2, 'singledimension': 2, 'validate': 2, 'compare': 2, 'second': 2, 'few': 2, 'constants': 2, 'doubling': 2, 'total': 2, 'constraint': 2, 'ˆfi': 2, 'evaluate': 2, 'here': 2, 'ˆli': 2, '112112': 2, '24': 2, '2828': 2, 'ble': 2, 'inverted': 2, 'san': 2, 'dler': 2, 'squeezeandexcitation': 2, 'ﬁx': 2, 'searching': 2, 'issue': 2, 'general': 2, 'russakovsky': 2, 'consistently': 2, 'parameter': 2, '16x': 2, 'params': 2, '932': 2, '760': 2, '930': 2, 'densenet169': 2, '944': 2, '778': 2, '11b': 2, 'inceptionv3': 2, '30x': 2, 'chollet': 2, '945': 2, '92m': 2, '52x': 2, '13x': 2, '18x': 2, 'polynet': 2, '77x': 2, '42b': 2, '833': 2, '99b': 2, '66m': 2, '37b': 2, '970': 2, 'mahajan': 2, '22b': 2, '12b': 2, 'cpu': 2, 'speedup': 2, 'settings': 2, 'decay': 2, '09': 2, 'momentum': 2, 'param': 2, 'paramratio': 2, '980': 2, '85m': 2, '4m': 2, '875': 2, '28m': 2, '10m': 2, '41xdat': 2, '948': 2, '929': 2, '17m': 2, 'giant': 2, 'dat': 2, 'domain': 2, 'adaptive': 2, 'nasnet': 2, '00': 2, '04': 2, '06': 2, '08': 2, '86': 2, '88': 2, '90': 2, '94': 2, 'pets': 2, 'ramachandran': 2, 'elfwing': 2, 'stochastic': 2, 'monly': 2, 'dropout': 2, 'srivastava': 2, '05': 2, 'curve': 2, 'report': 2, 'map': 2, 'relevant': 2, 'regions': 2, 'dataset': 2, 'classes': 2, '200950000': 2, '10000': 2, 'berg': 2, 'nilsback': 2, 'maji': 2, 'parkhi': 2, 'bossard': 2, 'including': 2, 'data': 2, 'compares': 2, 'inception': 2, 'aggarwal': 2, 'lee': 2, 'largescale': 2, 'discriminative': 2, 'convolutions': 2, 'linear': 2, 'reinforcement': 2, 'keutzer': 2, 'workshop': 2, 'dally': 2, 'lj': 2, 'devices': 2, 'adam': 2, 'weinberger': 2, 'der': 2, 'maaten': 2, 'icml': 2, 'deng': 2, 'feifei': 2, 'finegrained': 2, '2013': 2, 'sutskever': 2, 'classiﬁcation': 2, 'neurips': 2, 'dollar': 2, 'shufﬂenet': 2, 'vedaldi': 2, 'aaai': 2, 'khosla': 2, 'journal': 2, 'architectures': 2, 'platformaware': 2, 'tan1': 1, 'quoc': 1, 'le1': 1, 'abstract': 1, 'budget': 1, 'sys': 1, 'tematically': 1, 'res': 1, 'olution': 1, 'depthwidthresolution': 1, 'nets': 1, 'efﬁ': 1, 'ciency': 1, '917': 1, 'introduction': 1, 'time': 1, 'never': 1, 'been': 1, 'understood': 1, 'currently': 1, '1google': 1, 'mountain': 1, 'ca': 1, 'corre': 1, 'spondence': 1, 'tanmingxinggooglecom': 1, 'proceedings': 1, 'th': 1, 'conference': 1, 'long': 1, 'beach': 1, 'california': 1, 'pmlr': 1, 'copyright': 1, 'authors': 1, '60': 1, '120': 1, '140': 1, '160': 1, '180': 1, 'b6': 1, 'paramsresnet152': 1, '2016778': 1, '60mefﬁcientnetb1': 1, '78mresnext101': 1, '84mefﬁcientnetb3': 1, '12msenet': 1, '146mnasneta': 1, '89mefﬁcientnetb4': 1, '19mgpipe': 1, '556mefﬁcientnetb7': 1, '66mnot': 1, 'plotted': 1, '1model': 1, 'numbers': 1, 'singlecrop': 1, 'singlemodel': 1, 'perform': 1, 'another': 1, 'less': 1, 'requires': 1, 'yields': 1, 'suboptimal': 1, 'rethink': 1, 'investigate': 1, 'central': 1, 'widthdepthresolution': 1, 'surpris': 1, 'ingly': 1, 'fac': 1, 'tors': 1, '2n': 1, 'layer_i': 1, 'hxw': 1, '2model': 1, 'bd': 1, 'increases': 1, 'αn': 1, 'βn': 1, 'γn': 1, 'difference': 1, 'makes': 1, 'sense': 1, 'cause': 1, 'receptive': 1, 'ﬁeld': 1, 'theoretical': 1, 'exists': 1, 'certain': 1, 'knowledge': 1, 'quantify': 1, 'among': 1, 'exist': 1, 'heavily': 1, 'depends': 1, 'calledefﬁ': 1, 'cientnets': 1, 'summarizes': 1, 'outperform': 1, 'surpasses': 1, 'running': 1, 'besides': 1, 'stateof': 1, 'theart': 1, 'related': 1, 'alexnet': 1, 'won': 1, 'competition': 1, 'googlenet': 1, '68m': 1, '145m': 1, 'pushes': 1, 'so': 1, 'big': 1, 'partitioning': 1, 'spreading': 1, 'part': 1, 'accelera': 1, 'tor': 1, 'mainly': 1, 'designed': 1, 'recent': 1, 'per': 1, 'form': 1, 'across': 1, 'variety': 1, 'hit': 1, 'limit': 1, 'thus': 1, 'parameterized': 1, 'duce': 1, 'trading': 1, 'bile': 1, 'phones': 1, 'ubiquitous': 1, 'hand': 1, 'craft': 1, 'squeezenets': 1, 'shufﬂenets': 1, 'archi': 1, 'tecture': 1, 'designing': 1, 'handcrafted': 1, 'extensively': 1, 'kernel': 1, 'types': 1, 'sizes': 1, 'unclear': 1, 'aim': 1, 'super': 1, 'resort': 1, 'down': 1, 'adjusting': 1, 'wideresnet': 1, 'bilenets': 1, 'wellrecognized': 1, 'overhead': 1, 'open': 1, 'systemati': 1, 'formulate': 1, 'approaches': 1, '31': 1, 'formulation': 1, 'deﬁned': 1, 'fixi': 1, 'xi': 1, 'shapehiwici1': 1, 'hiand': 1, 'wi': 1, 'ncan': 1, 'represented': 1, 'composed': 1, 'lay': 1, 'ers': 1, 'fkf1': 1, 'f1x1': 1, 'j1kfjx1': 1, 'partitioned': 1, 'into': 1, 'share': 1, 'architec': 1, 'ture': 1, 'type': 1, 'performs': 1, 'downsampling': 1, 'therefore': 1, 'deﬁne': 1, 'xhiwici': 1, 'repeated': 1, 'hiwicidenotes': 1, '2a': 1, 'illustrate': 1, 'gradually': 1, 'shrunk': 1, '1for': 1, 'sake': 1, 'simplicity': 1, 'expanded': 1, '2242243to': 1, 'ﬁnal': 1, '77512': 1, 'designs': 1, 'tries': 1, 'pand': 1, 'length': 1, 'andor': 1, 'hiwi': 1, 'ﬁxing': 1, 'simpliﬁes': 1, 'explore': 1, 'licihiwi': 1, 'restrict': 1, 'must': 1, 'stant': 1, 'maximize': 1, 'given': 1, 'formulated': 1, 'max': 1, 'ˆfdˆli': 1, 'xrˆhirˆwiwˆci': 1, 'memoryn': 1, 'flopsn': 1, 'ﬂops': 1, 'wdr': 1, 'ˆfiˆli': 1, 'see': 1, 'optimal': 1, 'depend': 1, 'ddd': 1, 'intuition': 1, 'richer': 1, 'complex': 1, 'generalize': 1, 'difﬁcult': 1, 'vanishing': 1, 'gradient': 1, 'sev': 1, 'skip': 1, 'alleviate': 1, 'resnet1000': 1, 'middle': 1, 'diminishing': 1, 'return': 1, 'www': 1, '81imagenet': 1, 'w10': 1, 'w14': 1, 'w18': 1, 'w26': 1, 'w38': 1, 'w50': 1, 'd30d40': 1, 'd60': 1, 'd80': 1, 'r15': 1, 'r17': 1, 'r19': 1, 'r22': 1, 'r25': 1, '3scaling': 1, 'saturate': 1, 'after': 1, 'reaching': 1, 'demonstrating': 1, 'limitation': 1, 'described': 1, 'discussed': 1, 'ko': 1, 'modakis': 1, 'able': 1, 'easier': 1, 'extremely': 1, 'shallow': 1, 'difﬁcul': 1, 'ties': 1, 'capturing': 1, 'level': 1, 'left': 1, 'rrr': 1, 'potentially': 1, 'early': 1, 'modern': 1, '331x331': 1, '480x480': 1, '600x600': 1, 'right': 1, 'dimin': 1, 'ishes': 1, '560x560': 1, 'above': 1, 'analyses': 1, '33': 1, 'observe': 1, 'independent': 1, 'ceptive': 1, 'ﬁelds': 1, 'include': 1, 'correspondingly': 1, '2in': 1, 'some': 1, 'literature': 1, 'multiplier': 1, 'means': 1, '82imagenet': 1, '4scaling': 1, 'dot': 1, 'line': 1, '18': 1, 'coordinate': 1, 'depths': 1, 'r20': 1, 'pursue': 1, 'during': 1, 'tried': 1, 'they': 1, 'require': 1, 'newcompound': 1, 'αφ': 1, 'βφ': 1, 'γφ': 1, 'α1β': 1, '1γ': 1, 'φis': 1, 'userspeciﬁed': 1, 'coefﬁ': 1, 'cient': 1, 'controls': 1, 'specify': 1, 'assign': 1, 'extra': 1, 'spectively': 1, 'op': 1, 'proportional': 1, 'ie': 1, 'double': 1, 'resolu': 1, 'tion': 1, 'ops': 1, 'usually': 1, 'dominate': 1, 'computation': 1, 'approximately': 1, 'crease': 1, 'γ2φ': 1, 'approximately3': 1, '2φ': 1, 'does': 1, 'operators': 1, 'having': 1, 'good': 1, 'inspired': 1, 'leveraging': 1, 'multiobjective': 1, 'optimizes': 1, 'speciﬁ': 1, 'accmflops': 1, 'mtw': 1, 'accm': 1, 'denote': 1, 'w007': 1, 'hyperparameter': 1, 'controlling': 1, 'tradeoff': 1, 'optimize': 1, 'la': 1, 'tency': 1, 'targeting': 1, 'speciﬁc': 1, 'de': 1, 'vice': 1, 'produces': 1, 'name': 1, 'mnas': 1, '3flops': 1, 'may': 1, 'differ': 1, 'theocratic': 1, 'value': 1, 'rounding': 1, '1efﬁcientnetb0': 1, 'row': 1, 'describes': 1, 'iwith': 1, 'notations': 1, 'adopted': 1, 'resolutionchannelslayers': 1, 'ˆhiˆwi': 1, 'conv3x3': 1, '224224': 1, 'mbconv1': 1, '16': 1, '5656': 1, '112': 1, '1414': 1, '192': 1, '320': 1, '9': 1, 'conv1x1': 1, 'pooling': 1, 'fc77': 1, '1280': 1, 'slightly': 1, '400m': 1, 'ta': 1, 'its': 1, 'building': 1, 'block': 1, 'bottleneck': 1, 'mbconv': 1, 'add': 1, 'steps': 1, 'assuming': 1, 'twice': 1, 'sources': 1, 'α': 1, '12β': 1, '11γ': 1, '115': 1, 'φusing': 1, 'b7': 1, 'directly': 1, 'around': 1, 'prohibitively': 1, 'solves': 1, 'doing': 1, 'once': 1, 'experiments': 1, '51': 1, 'resnets': 1, 'proof': 1, 'concept': 1, 'widelyused': 1, 'φin': 1, 'top1top5': 1, 'grouped': 1, 'together': 1, 'ratiotoefﬁcientnetflops': 1, 'ratiotoefﬁcientnet': 1, '53m': 1, '039b': 1, '26m': 1, '49x': 1, '11x': 1, '762': 1, '14m': 1, '26x': 1, '89x': 1, '78m': 1, '070b': 1, '938': 1, '60m': 1, 'densenet264': 1, '779': 1, '939': 1, '34m': 1, '43x': 1, '60b': 1, '86x': 1, '24m': 1, '57b': 1, '81x': 1, '790': 1, '23m': 1, '84b': 1, '12x': 1, 'efﬁcientnetb2': 1, '798': 1, '949': 1, '10b': 1, '800': 1, '950': 1, '48m': 1, '2017801': 1, '951': 1, '56m': 1, '955': 1, '12m': 1, '18b': 1, '809': 1, '956': 1, '84m': 1, '70x': 1, '32b': 1, '813': 1, '958': 1, '19x': 1, '963': 1, '19m': 1, '146m': 1, '10x': 1, '89m': 1, '24b': 1, '828': 1, '961': 1, '87m': 1, '46x': 1, '55x': 1, 'pnasnet': 1, '829': 1, '86m': 1, '45x': 1, '60x': 1, '967': 1, '30m': 1, '835': 1, '965': 1, '155m': 1, '41x': 1, '840': 1, '969': 1, '43m': 1, '19b': 1, 'ensemble': 1, 'multicrop': 1, 'instagram': 1, '201706b': 1, '706': 1, '742': 1, '727': 1, 'ddd14www12rrr13': 1, '756': 1, '201803b': 1, '720': 1, '768': 1, '764': 1, '774': 1, '162b': 1, '781': 1, '147b': 1, '777': 1, '164b': 1, '775': 1, '167b': 1, 'measured': 1, 'core': 1, 'intel': 1, 'xeon': 1, 'e52690': 1, '0554s': 1, '190s': 1, '0098sefﬁcientnetb7': 1, '31s': 1, '30': 1, '35': 1, '45': 1, 'ameobaneta': 1, 'flopsresnet152': 1, '2017778': 1, '11befﬁcientnetb1': 1, '07bresnext101': 1, '32befﬁcientnetb3': 1, '18bsenet': 1, '42bnasneta': 1, '2018807': 1, '24befﬁcientnetb4': 1, '42bameobanetc': 1, '2019835': 1, '41befﬁcientnetb5': 1, '5flops': 1, '52': 1, 'simi': 1, 'lar': 1, 'rmsprop': 1, 'optimizer': 1, 'norm': 1, '099': 1, 'weight': 1, '1e5': 1, 'rate': 1, '0256': 1, 'decays': 1, '097': 1, 'every': 1, 'epochs': 1, 'swish': 1, 'stateofthe': 1, 'art': 1, 'publicavailable': 1, 'reported': 1, '981': 1, '21xgpipe990': 1, '989': 1, '881': 1, '21xgpipe': 1, '913': 1, 'efﬁcientnetb7917': 1, '818': 1, '820': 1, '15xgpipe': 1, '836': 1, 'efﬁcientnetb7843': 1, 'carsinceptionv4': 1, '934': 1, '936': 1, '947': 1, '15xdat': 1, '977': 1, 'aircraftinceptionv4': 1, '909': 1, '907': 1, 'petsresnet152': 1, '58m': 1, '56xgpipe': 1, '959': 1, '954': 1, '14x': 1, '908': 1, '915': 1, '24xgpipe': 1, 'efﬁcientnetb7930': 1, 'geomean': 1, 'trains': 1, 'imagenetbased': 1, 'logscale': 1, '98': 1, '99accuracy': 1, '84': 1, '70': 1, '85': 1, '91': 1, '93': 1, '975': 1, '825': 1, '850': 1, '900': 1, '925': 1, '96': 1, 'inceptionv1': 1, '6model': 1, 'ﬁnetuned': 1, 'au': 1, 'toaugment': 1, 'policy': 1, 'drop': 1, 'connect': 1, '03': 1, 'known': 1, 'regularization': 1, 'linearly': 1, 'generally': 1, 'but84x': 1, 'parametersaccuracy': 1, 'flopsaccuracy': 1, 'cheaper': 1, 'resnext': 1, 'mea': 1, 'sured': 1, 'fast': 1, '53': 1, 'evaluated': 1, 'borrow': 1, 'take': 1, 'checkpoints': 1, 'ﬁnetune': 1, 'bakeshop': 1, 'maze': 1, '7class': 1, 'cam': 1, 'allows': 1, 'column': 1, 'test': 1, '201447386': 1, '2443': 1, '500': 1, '20138144': 1, '8041': 1, '196': 1, '20082040': 1, '6149': 1, '20136667': 1, '3333': 1, '20123680': 1, '3369': 1, '37': 1, '201475750': 1, '25250': 1, 'pared': 1, 'public': 1, 'ef': 1, 'ﬁcientnet': 1, 'state': 1, 'oftheart': 1, 'dynamically': 1, 'synthesizes': 1, 'paral': 1, 'lelism': 1, 'accuracyparameters': 1, 'va': 1, 'riety': 1, 'densenet': 1, 'discussion': 1, 'disentangle': 1, 'contribution': 1, 'pares': 1, 'meth': 1, 'ods': 1, 'gen': 1, 'importance': 1, '83imagenet': 1, '8scaling': 1, 'understand': 1, 'why': 1, 'others': 1, 'class': 1, 'tivation': 1, '4x': 1, 'randomly': 1, 'picked': 1, 'ﬁgure': 1, 'pound': 1, 'tends': 1, 'either': 1, 'lack': 1, 'unable': 1, 'objects': 1, 'conclusion': 1, 'systematically': 1, 'missing': 1, 'piece': 1, 'preventing': 1, 'address': 1, 'enables': 1, 'easily': 1, 'maintaining': 1, 'powered': 1, 'surpassing': 1, 'magni': 1, 'tude': 1, 'acknowledgements': 1, 'thank': 1, 'ruoming': 1, 'vijay': 1, 'alok': 1, 'barret': 1, 'hongkun': 1, 'yu': 1, 'xiaodan': 1, 'song': 1, 'samy': 1, 'bengio': 1, 'jeff': 1, 'dean': 1, 'google': 1, 'references': 1, 'woo': 1, 'alexander': 1, 'jacobs': 1, 'belhumeur': 1, 'categorization': 1, 'birds': 1, '20112018': 1, 'guillaumin': 1, 'gool': 1, 'mining': 1, 'components': 1, 'random': 1, 'forests': 1, '446461': 1, 'proxylessnas': 1, 'direct': 1, 'task': 1, 'depthwise': 1, 'separa': 1, '161002357': 1, 'mane': 1, 'autoaugment': 1, 'augmentation': 1, 'policies': 1, 'uchibe': 1, 'doya': 1, 'sigmoidweighted': 1, 'units': 1, 'approximation': 1, '107311': 1, 'kwon': 1, 'wu': 1, 'tai': 1, 'yue': 1, 'jin': 1, 'zhao': 1, 'squeezenext': 1, 'hardwareaware': 1, 'ecv': 1, 'cvpr18': 1, 'mao': 1, 'compressing': 1, 'pruning': 1, 'quantization': 1, 'huffman': 1, 'coding': 1, 'ren': 1, '770778': 1, 'gkioxari': 1, 'doll': 1, 'ar': 1, 'mask': 1, 'rcnn': 1, 'iccv': 1, '29802988': 1, 'amc': 1, 'automl': 1, 'acceleration': 1, 'kalenichenko': 1, 'weyand': 1, 'andreetto': 1, 'arxiv170404861': 1, 'shen': 1, 'sedra': 1, '646661': 1, 'densely': 1, 'connected': 1, 'cheng': 1, 'parallelismarxiv': 1, 'arxiv180807233': 1, 'moskewicz': 1, 'ashraf': 1, 'squeezenet': 1, 'alexnetlevel': 1, '50x': 1, 'mb': 1, 'arxiv160207360': 1, 'accelerating': 1, 'internal': 1, 'covariate': 1, 'shift': 1, '448456': 1, 'stark': 1, 'collecting': 1, 'categorizatio': 1, 'tiny': 1, 'technical': 1, '2009': 1, 'nips': 1, '10971105': 1, 'oneneuron': 1, 'hidden': 1, 'universal': 1, 'approximator': 1, '6172': 1, '6181': 1, 'ty': 1, 'hariharan': 1, 'belongie': 1, 'feature': 1, 'pyramid': 1, 'hua': 1, 'yuille': 1, 'murphy': 1, 'progressive': 1, 'pu': 1, 'expres': 1, 'sive': 1, 'zheng': 1, 'ht': 1, 'v2': 1, 'practical': 1, 'guidelines': 1, 'cnn': 1, 'ramanathan': 1, 'paluri': 1, 'bharambe': 1, 'explor': 1, 'limits': 1, 'weakly': 1, 'supervised': 1, 'pretraining': 1, 'arxiv180500932': 1, 'rahtu': 1, 'kannala': 1, 'blaschko': 1, 'arxiv13065151': 1, 'peng': 1, 'spe': 1, 'cialist': 1, 'arxiv181107056': 1, 'me': 1, 'automated': 1, 'ﬂower': 1, 'clas': 1, 'siﬁcation': 1, 'icvgip': 1, '722729': 1, '2008': 1, 'jawahar': 1, 'cats': 1, 'dogs': 1, '34983505': 1, 'poole': 1, 'kleinberg': 1, 'ganguli': 1, 'sohl': 1, 'dickstein': 1, 'functions': 1, 'arxiv171005941': 1, 'regu': 1, 'larized': 1, 'evolution': 1, 'classiﬁer': 1, 'su': 1, 'satheesh': 1, 'karpathy': 1, 'bernstein': 1, 'chal': 1, 'lenge': 1, '1153': 1, '211252': 1, 'zhmoginov': 1, 'lc': 1, 'residuals': 1, 'bottlenecks': 1, 'overlapping': 1, 'salakhutdinov': 1, 'prevent': 1, 'overﬁtting': 1, '15119291958': 1, 'jia': 1, 'sermanet': 1, 'reed': 1, 'anguelov': 1, 'erhan': 1, 'rabinovich': 1, '19': 1, 'wojna': 1, '28182826': 1, 'alemi': 1, 'inceptionresnet': 1, 'impact': 1, '412': 1, 'mnasnet': 1, 'tu': 1, 'aggre': 1, 'gated': 1, 'transformations': 1, '59875995': 1, 'tj': 1, 'sze': 1, 'netadapt': 1, 'adaptation': 1, 'bmvc': 1, 'loy': 1, 'pursuit': 1, 'structural': 1, 'diversity': 1, '39003908': 1, 'tremely': 1, 'lapedriza': 1, 'oliva': 1, 'torralba': 1, 'localization': 1, '29212929': 1, 'transferable': 1, 'scalable': 1})}, {'file_name': 'pubmedbert', 'word_counts': Counter({'the': 406, 'and': 207, 'of': 191, 'for': 175, 'in': 157, 'a': 148, 'to': 131, 'pretraining': 116, 'on': 109, 'language': 94, 'we': 93, 'is': 84, 'as': 76, 'biomedical': 73, 'with': 60, 'text': 58, 'from': 56, 'bert': 52, 'models': 51, 'model': 50, 'by': 48, 'that': 46, 'entity': 44, 'are': 43, '1': 43, 'pubmed': 43, 'using': 41, 'relation': 36, 'such': 35, 'tasks': 32, 'use': 32, 'domainspecific': 31, 'this': 31, 'standard': 31, 'processing': 30, '3': 30, 'natural': 29, 'article': 29, 'task': 29, 'classification': 29, '2': 28, 'learning': 28, '2021': 27, 'work': 27, 'vocabulary': 27, 'an': 27, 'nlp': 26, 'blurb': 26, 'acm': 26, 'which': 26, 'or': 26, 'no': 26, 'layer': 26, 'original': 26, 'extraction': 26, 'neural': 25, 'taskspecific': 25, 'computing': 25, 'vol': 25, 'ner': 25, 'token': 25, 'healthcare': 24, 'october': 24, 'publication': 24, 'transactions': 24, 'abstracts': 24, 'date': 23, '2019': 23, 'training': 22, 'pubmedbert': 22, 'et': 21, 'al': 21, 'question': 21, 'not': 20, 'prior': 20, 'it': 19, 'more': 19, 'each': 19, 'methods': 19, 'evaluation': 18, 'all': 18, 'pico': 18, 'our': 17, 'performance': 17, 'applications': 17, 'encoding': 17, 'cls': 17, 'finetuning': 16, 'pretrained': 16, 'corpus': 16, 'comparison': 16, 'table': 16, 'results': 15, 'continual': 15, 'be': 15, 'representation': 15, 'e': 15, 'biobert': 15, 'chemprot': 15, 'sentence': 15, 'dataset': 15, 'f1': 15, 'has': 14, 'words': 14, 'y': 14, 'datasets': 14, 'input': 14, 'used': 14, 'gu': 13, 'benchmark': 13, 'word': 13, 'ddi': 13, 'association': 13, 'computational': 13, 'inproceedings': 13, 'can': 12, 'scratch': 12, 'tagging': 12, 'transfer': 12, 'first': 12, 'its': 12, 'mixeddomain': 12, 'may': 12, 'span': 12, 'same': 12, 'disease': 12, 'gene': 12, 'jnlpba': 12, 'biosses': 12, 'pubmedqa': 12, 'similarity': 12, 'score': 12, 'conference': 12, 'domain': 11, 'choices': 11, 'given': 11, 'sequence': 11, 'eg': 11, '5': 11, '8': 11, 'bc5disease': 11, 'bc2gm': 11, 'ebm': 11, 'hoc': 11, 'test': 11, 'books': 11, 'however': 10, 'focus': 10, 'r': 10, 'indomain': 10, 'approach': 10, 'data': 10, 'impact': 10, 'modeling': 10, 'mentions': 10, 'tokens': 10, '34': 10, 'clinical': 10, 'scibert': 10, 'bc5chem': 10, 'gad': 10, 'bioasq': 10, 'contains': 10, 'linear': 10, 'wiki': 10, '2017': 10, 'general': 9, 'generaldomain': 9, '4': 9, 'set': 9, 'downstream': 9, 'then': 9, 'two': 9, 'headtohead': 9, 'also': 9, 'adversarial': 9, '45': 9, 'into': 9, 'ncbidisease': 9, 'answering': 9, 'development': 9, '14': 9, 'regression': 9, 'mention': 9, '8236': 9, 'liu': 8, 'most': 8, 'even': 8, 'domains': 8, 'biomedicine': 8, 'both': 8, 'have': 8, 'at': 8, 'example': 8, 'full': 8, 'show': 8, 'thus': 8, 'their': 8, 'uses': 8, 'than': 8, 'long': 8, 'contextual': 8, 'information': 8, 'various': 8, 'if': 8, 'drug': 8, 'medical': 8, 'l': 8, 'future': 8, 'document': 8, 'entities': 8, 'workshop': 8, 'was': 8, '8562': 8, '7910': 8, '2018': 8, 'linguistics': 8, '2014': 8, 'research': 7, 'assumption': 7, 'facilitate': 7, 'representations': 7, 'top': 7, 'different': 7, 'study': 7, 'better': 7, 's': 7, 'special': 7, 'among': 7, 'but': 7, 'note': 7, 'compared': 7, 'similar': 7, 'entitylevel': 7, 'there': 7, 'c': 7, 'final': 7, '10': 7, 'm': 7, 'hallmarks': 7, 'cancer': 7, 'transforminput': 7, 'n': 7, 'dummification': 7, 'pmc': 7, 'cased': 7, '9333': 7, '7724': 7, '2011': 7, 'international': 7, 'wang': 7, 'xiaodong': 6, 'web': 6, 'stateoftheart': 6, 'common': 6, '23': 6, 'specific': 6, 'typically': 6, 'papers': 6, 'based': 6, 'multitask': 6, 'conduct': 6, 'experiments': 6, 'sentences': 6, 'subwords': 6, 'case': 6, 'uncased': 6, 'generally': 6, 'transformer': 6, '16': 6, '25': 6, 'semantic': 6, 'outdomain': 6, 'other': 6, '11': 6, 'overview': 6, 'train': 6, 'micro': 6, 'average': 6, 'version': 6, 'labels': 6, 'annotation': 6, 'shared': 6, 'i': 6, 'sequential': 6, 'returns': 6, 'featurizer': 6, 'markers': 6, 'longer': 6, 'bluebert': 6, 'time': 6, 'steps': 6, 'proceedings': 6, '2015': 6, 'peter': 6, 'robert': 5, 'jianfeng': 5, 'gao': 5, 'pre': 5, 'comprehensive': 5, 'across': 5, 'recognition': 5, 'accelerate': 5, 'leaderboard': 5, 'understanding': 5, 'additional': 5, 'reference': 5, 'h': 5, 'target': 5, 'when': 5, 'out': 5, 'tags': 5, 'io': 5, 't': 5, '32': 5, 'size': 5, 'architecture': 5, 'following': 5, 'mlm': 5, 'next': 5, '24': 5, 'roberta': 5, '20': 5, '37': 5, 'only': 5, 'one': 5, 'been': 5, 'here': 5, 'million': 5, 'u': 5, 'range': 5, 'compute': 5, 'interactions': 5, 'chemical': 5, 'crichton': 5, 'annotated': 5, 'type': 5, 'they': 5, 'were': 5, 'cpr': 5, '6': 5, 'parameters': 5, 'problem': 5, 'significant': 5, 'gb': 5, 'clinicalbert': 5, '2020': 5, 'wwm': 5, '8782': 5, '8452': 5, '7338': 5, '8396': 5, '9230': 5, '8116': 5, '8232': 5, '5584': 5, '8756': 5, 'column': 5, 'texts': 5, 'g': 5, 'biocreative': 5, 'empirical': 5, '2016': 5, 'j': 5, 'human': 5, 'technologies': 5, 'lu': 5, 'hoifung': 4, 'poon': 4, 'many': 4, 'leading': 4, 'new': 4, 'some': 4, 'complex': 4, 'schemes': 4, 'help': 4, 'released': 4, 'short': 4, 'make': 4, 'advantage': 4, 'requires': 4, 'existing': 4, 'literature': 4, '13': 4, 'variants': 4, 'label': 4, 'comparable': 4, 'o': 4, '21': 4, '55': 4, '39': 4, 'embedding': 4, 'layers': 4, 'generated': 4, 'whole': 4, 'preceding': 4, 'strategies': 4, 'instead': 4, 'loss': 4, 'makes': 4, 'random': 4, 'considered': 4, 'whether': 4, 'notes': 4, 'term': 4, 'will': 4, 'contain': 4, 'trained': 4, 'studies': 4, '27': 4, '44': 4, '31': 4, 'tends': 4, 'about': 4, 'evidencebased': 4, 'list': 4, 'instances': 4, 'types': 4, 'often': 4, 'report': 4, 'labeled': 4, 'joint': 4, 'protein': 4, 'abstract': 4, 'p': 4, 'pio': 4, 'slightly': 4, 'drugdrug': 4, 'traindevtest': 4, 'split': 4, 'peng': 4, 'any': 4, 'prediction': 4, 'fix': 4, 'shows': 4, 'instance': 4, 'labeling': 4, 'scheme': 4, 'lstm': 4, 'sep': 4, 'beginning': 4, 'leads': 4, 'attention': 4, '9': 4, 'conducted': 4, '33': 4, 'start': 4, 'eds': 4, '1997': 4, 'bionlp': 4, 'deep': 4, 'american': 4, 'volume': 4, '2013': 4, 'li': 4, 'cheng': 3, 'michael': 3, 'tristan': 3, 'gains': 3, 'newswire': 3, 'prevailing': 3, 'benefit': 3, 'challenge': 3, 'abundant': 3, 'substantial': 3, 'available': 3, 'board': 3, 'conducting': 3, 'named': 3, 'community': 3, 'created': 3, 'featuring': 3, 'strategy': 3, 'hard': 3, 'copies': 3, 'provided': 3, '22': 3, 'helpful': 3, 'conducts': 3, 'outperform': 3, 'form': 3, 'where': 3, 'rich': 3, 'bio': 3, 'find': 3, 'simply': 3, 'd': 3, 'consists': 3, 'concatenating': 3, 'recent': 3, 'processed': 3, 'multiple': 3, 'selfsupervision': 3, 'predict': 3, 'ngram': 3, 'between': 3, 'ones': 3, '15': 3, 'chosen': 3, 'include': 3, 'formulation': 3, 'masked': 3, 'relatively': 3, 'whereas': 3, 'purely': 3, 'process': 3, '26': 3, 'computer': 3, 'oropharyngeal': 3, 'cardiomyocyte': 3, 'cell': 3, 'chloramphenicol': 3, 'reca': 3, 'acetyltransferase': 3, 'clonidine': 3, 'naloxone': 3, 'corresponding': 3, '30': 3, 'pieces': 3, 'latter': 3, '45b': 3, 'b': 3, '35': 3, '18': 3, '54': 3, '42': 3, 'goal': 3, 'improve': 3, 'shown': 3, 'care': 3, 'blue': 3, 'create': 3, 'six': 3, 'perform': 3, 'selection': 3, 'provides': 3, 'macro': 3, '50': 3, '500': 3, 'elements': 3, 'summary': 3, 'relevant': 3, 'well': 3, 'three': 3, 'splits': 3, 'preprocessed': 3, 'very': 3, 'intervention': 3, 'con': 3, 'level': 3, 'element': 3, 'participant': 3, 'within': 3, 'consider': 3, 'sentencelevel': 3, 'authors': 3, 'pairs': 3, 'do': 3, 'archive': 3, 'examples': 3, 'system': 3, '100': 3, 'binary': 3, 'hallmark': 3, 'questions': 3, 'yesno': 3, 'generate': 3, 'method': 3, 'module': 3, 'either': 3, 'vary': 3, 'formulated': 3, 'sophisticated': 3, 'bioul': 3, 'f': 3, 'pair': 3, 'settings': 3, 'adam': 3, 'machine': 3, 'rate': 3, 'bertbase': 3, '12': 3, 'runs': 3, 'vs': 3, '8470': 3, 'ablation': 3, 'derived': 3, 'correct': 3, '7': 3, 'piece': 3, 'half': 3, 'amount': 3, '60': 3, 'corresponds': 3, 'v': 3, 'john': 3, 'roberts': 3, 'andrew': 3, 'simon': 3, 'he': 3, 'anna': 3, 'korhonen': 3, 'scientific': 3, 'jeffrey': 3, 'august': 3, 'zhiyong': 3, 'chen': 3, 'kim': 3, 'wei': 3, 'systems': 3, 'yu': 2, 'tinn': 2, 'hao': 2, 'naumann': 2, 'large': 2, 'led': 2, 'efforts': 2, 'corpora': 2, 'unlabeled': 2, 'over': 2, 'compile': 2, 'publicly': 2, 'further': 2, 'thorough': 2, 'mod': 2, 'eling': 2, 'discover': 2, 'practices': 2, 'httpsakamsblurb': 2, 'key': 2, 'httpsdoiorg1011453458754': 2, 'introduction': 2, 'successful': 2, 'encoder': 2, 'transformers': 2, 'way': 2, 'permission': 2, 'fee': 2, 'distributed': 2, 'otherwise': 2, '1500': 2, 'still': 2, 'bottom': 2, 'observe': 2, '38': 2, '59': 2, 'fact': 2, 'negative': 2, 'actually': 2, 'running': 2, 'support': 2, 'indepth': 2, 'comparisons': 2, 'several': 2, '211': 2, 'spans': 2, 'subword': 2, 'wordpiece': 2, 'bpe': 2, 'small': 2, 'rather': 2, '212': 2, 'success': 2, 'shortterm': 2, 'position': 2, 'ie': 2, 'belongs': 2, 'passed': 2, 'nonlinear': 2, '213': 2, 'eu': 2, 'generative': 2, 'conditional': 2, 'probability': 2, '43': 2, 'subset': 2, 'mask': 2, 'them': 2, 'objective': 2, 'crossentropy': 2, 'every': 2, '214': 2, 'techniques': 2, 'observed': 2, 'masking': 2, 'because': 2, 'dependencies': 2, 'others': 2, '221': 2, 'mimiciii': 2, 'science': 2, '222': 2, 'terms': 2, 'vocabularies': 2, 'ours': 2, 'insulin': 2, 'dna': 2, 'hypertension': 2, 'nephropathy': 2, 'lymphoma': 2, 'lidocaine': 2, 'indicates': 2, 'appears': 2, 'broken': 2, 'hypothesize': 2, 'result': 2, 'divided': 2, 'four': 2, 'lo': 2, 'ran': 2, 'another': 2, 'potentially': 2, 'although': 2, 'mentioned': 2, 'earlier': 2, 'network': 2, '52': 2, '53': 2, 'confers': 2, 'ad': 2, 'wide': 2, 'assess': 2, 'knowledge': 2, 'consequently': 2, 'advantageous': 2, 'separate': 2, 'these': 2, 'highvalue': 2, 'verticals': 2, 'effort': 2, 'explore': 2, 'addition': 2, 'answer': 2, 'dev': 2, '8662': 2, 'pearson': 2, '450': 2, 'accuracy': 2, '140': 2, 'release': 2, 'detailed': 2, 'descriptions': 2, '28': 2, 'trials': 2, 'patients': 2, 'comparator': 2, 'control': 2, 'respectively': 2, 'sets': 2, 'preprocessing': 2, 'interaction': 2, 'chemicalprotein': 2, 'highlevel': 2, 'classifying': 2, 'agonist': 2, 'provide': 2, 'particular': 2, 'genetic': 2, 'database': 2, 'specifically': 2, 'genedisease': 2, '29': 2, 'estimation': 2, 'discussion': 2, 'finegrained': 2, 'toplevel': 2, 'since': 2, 'yesmaybeno': 2, 'leave': 2, 'inclusion': 2, 'containing': 2, 'unifying': 2, 'vectors': 2, 'output': 2, 'application': 2, 'adopts': 2, 'underlying': 2, 'review': 2, 'fig': 2, 'goes': 2, 'lead': 2, 'published': 2, 'fed': 2, 'sequences': 2, 'usually': 2, 'presents': 2, 'tag': 2, 'stands': 2, 'last': 2, 'simple': 2, 'schemebio': 2, 'layerl': 2, 'rl': 2, 'relations': 2, 'prevent': 2, 'startend': 2, 'dummy': 2, 'marker': 2, 'crawl': 2, 'cs': 2, 'mimic': 2, 's2': 2, 'namely': 2, 'prepended': 2, 'length': 2, 'collection': 2, 'billion': 2, 'less': 2, 'noise': 2, 'batch': 2, 'comes': 2, 'versions': 2, 'exploration': 2, 'triangular': 2, 'tuning': 2, 'hyperparameters': 2, 'ical': 2, 'evaluate': 2, 'options': 2, 'compare': 2, '1m': 2, '9285': 2, '8913': 2, '8804': 2, '8090': 2, '8382': 2, '8336': 2, '7855': 2, '7851': 2, '7318': 2, '7614': 2, '8088': 2, '8048': 2, '8952': 2, '8154': 2, '6024': 2, '8414': 2, '7886': 2, '8034': 2, 'see': 2, 'worst': 2, 'worse': 2, 'consistent': 2, 'high': 2, 'paper': 2, 'does': 2, 'effect': 2, 'significantly': 2, 'helps': 2, 'errors': 2, 'second': 2, '8242': 2, '8174': 2, '7916': 2, 'vocab': 2, '359': 2, '280': 2, 'except': 2, 'slight': 2, 'attained': 2, 'third': 2, 'gain': 2, 'how': 2, 'pubmedpmc': 2, 'diverse': 2, 'crf': 2, 'recurrent': 2, 'networks': 2, 'particularly': 2, 'already': 2, 'indeed': 2, 'bidirectional': 2, 'bilstm': 2, 'distinguishing': 2, 'implications': 2, 'overfitting': 2, 'would': 2, 'need': 2, 'supervision': 2, 'dummify': 2, 'risk': 2, 'remains': 2, 'supervised': 2, 'marking': 2, 'direction': 2, '7722': 2, 'appended': 2, 'william': 2, 'mohammad': 2, 'steven': 2, 'bethard': 2, 'marine': 2, 'carpuat': 2, 'cecilia': 2, 'arighi': 2, 'chatraryamontri': 2, 'via': 2, 'baker': 2, 'imran': 2, 'ali': 2, 'ilona': 2, 'silins': 2, 'sampo': 2, 'pyysalo': 2, 'yufan': 2, 'guo': 2, 'johan': 2, 'högberg': 2, 'ulla': 2, 'stenius': 2, 'cancerbioinformatics': 2, 'tom': 2, 'annual': 2, 'meeting': 2, 'emnlpijcnlp19': 2, 'david': 2, 'semevalnaaclhlt': 2, 'usa': 2, 'june': 2, 'analysis': 2, '1992': 2, 'kevin': 2, 'cohen': 2, '18th': 2, 'lee': 2, 'north': 2, 'chapter': 2, 'rezarta': 2, 'islamaj': 2, 'leaman': 2, 'concept': 2, 'du': 2, 'yifan': 2, 'yang': 2, 'multilabel': 2, 'informatics': 2, 'association26': 2, '2000': 2, 'robin': 2, 'w': 2, 'johnson': 2, 'shen': 2, 'jindong': 2, 'sun': 2, 'mike': 2, 'pérez': 2, 'probabilistic': 2, '36': 2, 'ma': 2, 'omer': 2, 'levy': 2, 'httpsdoiorg1018653': 2, 'alec': 2, 'radford': 2, 'ilya': 2, 'sutskever': 2, 'noam': 2, 'shazeer': 2, 'zhou': 2, 'xu': 2, 'inadvances': 2, 'zhang': 2, '2domainspecific': 1, 'lucas': 1, 'naoto': 1, 'usuyama': 1, 'microsoft': 1, 'impressive': 1, 'starting': 1, 'showing': 1, 'investigation': 1, 'datasetsourexperimentsshowthatdomainspecificpretrainingservesasasolidfoundationforawiderangeofbiomedical': 1, 'unnecessary': 1, 'reasoning': 1, 'ccs': 1, 'concepts': 1, 'methodologies': 1, 'applied': 1, 'bioinformatics': 1, 'phrases': 1, 'format': 1, 'yuguroberttinnhaochengmichaellucasnaotousuyamaxiaodongliutristannaumannjianfenggaoandhoifung': 1, 'poon2021domainspecificlanguagemodelpretrainingforbiomedicalnaturallanguageprocessing': 1, 'trans': 1, 'comput': 1, 'pages': 1, 'innaturallanguageprocessingnlp': 1, 'pretraininglargeneurallanguagemodelsonunlabeledtexthasproven': 1, 'prime': 1, 'isbidirectional': 1, 'bert16': 1, 'become': 1, 'building': 1, 'block': 1, 'modelsexistingpretrainingworktypicallyfocusesonthenewswireandwebdomainsforexampletheoriginal': 1, 'contributed': 1, 'equally': 1, 'authorsaddressygurtinnhchengmlucasnusuyamaxliutnaumannjgaoandhpoonmicrosoftresearchonemicrosoft': 1, 'redmond': 1, 'wa': 1, '98052': 1, 'emails': 1, 'aidengu': 1, 'roberttinn': 1, 'chehao': 1, 'michaellucas': 1, 'naotous': 1, 'xiaodl': 1, 'jfgao': 1, 'hoifungmicrosoftcom': 1, 'digital': 1, 'part': 1, 'personal': 1, 'classroom': 1, 'granted': 1, 'without': 1, 'made': 1, 'profit': 1, 'commercial': 1, 'bear': 1, 'notice': 1, 'citation': 1, 'page': 1, 'copyrightsforcomponentsofthisworkownedbyothersthantheauthorsmustbehonoredabstractingwithcreditispermittedtocopy': 1, 'republish': 1, 'post': 1, 'servers': 1, 'redistribute': 1, 'lists': 1, 'andor': 1, 'request': 1, 'permissions': 1, 'frompermissionsacmorg': 1, 'copyright': 1, 'held': 1, 'ownerauthors': 1, 'rights': 1, 'licensed': 1, '26378051202110art2': 1, 'fig1': 1, 'twoparadigmsforneurallanguagemodelpretrainingtoptheprevailingmixeddomainparadigmassumesthatout': 1, 'initializes': 1, 'inherits': 1, 'derives': 1, 'solely': 1, 'substantially': 1, 'conventional': 1, 'bertmodelwastrainedonwikipedia': 1, 'andbookcorpus': 1, '62andsubsequenteffortshavefocusedoncrawling': 1, 'power': 1, 'largerscale': 1, '3950': 1, 'inspecializeddomainslikebiomedicinepastworkhasshownthatusingindomaintextcanprovideadditional': 1, 'gainsovergeneraldomainlanguagemodels': 1, '83445howeveraprevailingassumptionisthatoutdomaintext': 1, 'isstillhelpfulandpreviousworktypicallyadoptsamixeddomainapproachsuchasbystartingdomainspecific': 1, 'figure1': 1, 'viewed': 1, 'itself': 1, 'source': 1, 'specialized': 1, 'occurs': 1, 'scarce': 1, 'sourcedomainishighlyrelevanttothetargetonefordomainswithabundantunlabeledtextsuchasbiomedicine': 1, 'unclear': 1, 'majority': 1, 'substantively': 1, 'raising': 1, 'prospect': 1, 'hinders': 1, '1httpwikipediaorg': 1, 'rigorous': 1, 'examplewe': 1, 'substantiallyoutperformscontinualpretrainingofgenericlanguagemodelsthusdemonstratingthattheprevailing': 1, 'always': 1, 'applicablefigure1': 1, 'publiclyavailable': 1, 'scratchcanprovideasolidfoundationforbiomedicalnlpleadingtonewstateoftheartperformanceacrossa': 1, 'widerange': 1, 'oftasksadditionallywe': 1, 'thattheuseoftransformerbasedmodels': 1, 'likebertnecessitates': 1, 'rethinking': 1, 'fornamed': 1, 'suffices': 1, '2m': 1, 'inthissectionweprovideabriefoverviewofneurallanguagemodelpretrainingusingbert': 1, '16asarunning': 1, 'assume': 1, 'separated': 1, 'tokensseptoaddresstheproblemofoutofvocabularywordsneurallanguagemodelsgenerateavocabulary': 1, 'units': 1, 'usingbytepair': 1, 'bpe51': 1, 'essentially': 1, 'algorithm': 1, 'tries': 1, 'greedily': 1, 'identify': 1, 'compactly': 1, 'corpusitdoesthisbyfirstshatteringallwordsinthecorpusandinitializingthevocabularywithcharactersand': 1, 'delimiters': 1, 'iteratively': 1, 'augments': 1, 'frequent': 1, 'formed': 1, 'until': 1, 'reaches': 1, 'prespecified': 1, 'eg30000instandardbertmodelsor50000inroberta': 1, '39inthisarticleweusethewordpiecealgorithm': 1, 'variant': 1, 'likelihood': 1, 'unigram': 1, 'frequency': 1, 'choosing': 1, 'concatenate': 1, 'preserve': 1, 'casedo': 1, 'convert': 1, 'characters': 1, 'lowercase': 1, 'archi': 1, 'tectures': 1, 'introduces': 1, 'multilayer': 1, 'multiheadselfattentionmechanismwhichhasdemonstratedsuperiorityinleveraginggpubasedparallelcom': 1, 'putationandmodelinglongrangedependenciesintextscomparedtorecurrentneuralnetworkssuchas': 1, 'memory': 1, 'lstm22': 1, 'lexical': 1, 'combines': 1, 'segment': 1, 'elementwise': 1, 'summation': 1, 'modules': 1, 'summing': 1, 'transformation': 1, 'weighted': 1, 'tentionscomputedusingthegiventokensrepresentationinthepriorlayerasthequerythefinallayeroutputs': 1, 'combine': 1, 'innovation': 1, '16i': 1, 'st': 1, 'eo': 1, 'famasked': 1, 'selfsupervised': 1, 'traditional': 1, 'tokensfor': 1, 'represent': 1, 'multinomial': 1, 'smoothing': 1, 'handle': 1, 'rare': 1, 'occurrences': 1, 'randomly': 1, 'replaces': 1, 'asks': 1, 'predicted': 1, 'random80arereplacedby': 1, 'mask10areleftunchangedand10arerandomlyreplacedbyatokenfromthe': 1, 'vocabularyinsteadofusingaconstantmaskingrateof15astandardapproachistograduallyincreaseitfrom': 1, 'increment': 1, 'epochs': 1, 'stable': 1, 'originalbertalgorithmalsouses': 1, 'nextsentencepredictionnsp': 1, 'whichdeterminesforagivensentencepair': 1, 'whetheronesentencefollowstheotherintheoriginaltexttheutilityofnsphasbeencalledintoquestion': 1, 'enable': 1, 'advanced': 1, 'selects': 1, 'partially': 1, 'easy': 1, 'portion': 1, 'contrastwholeword': 1, 'wwmenforces': 1, 'must': 1, 'adopted': 1, 'forces': 1, 'capture': 1, 'inthisarticlewealsoexploreadversarialpretraininganditsimpactondownstreamapplicationsmotivatedby': 1, 'successesincounteringadversarialattacksincomputervisionadversarialpretrainingintroducesperturbations': 1, 'intheinputembeddinglayerthatmaximizetheadversariallossthusforcingthemodeltonotonlyoptimizethe': 1, 'minimize': 1, 'inthisarticlewewillusebiomedicineasarunningexampleinourstudyofdomainspecificpretraininginother': 1, 'regarded': 1, 'intuitively': 1, 'indomaintextinpretrainingshouldhelpwithdomainspecificapplicationsindeedpriorworkhasshownthat': 1, 'pretrainingwithpubmedtextleadstobetterperformanceinbiomedicalnlptasks': 1, '83445themainquestion': 1, 'should': 1, 'canalwaysbenefitfrommoretextincludingoutdomaintextinfactnoneofthepriorbiomedicalrelatedbert': 1, 'thatdomainspecific': 1, 'scratchcan': 1, 'superior': 1, 'tomixeddomain': 1, 'pretrainingfor': 1, 'continualpretraining': 1, 'ofageneraldomainpretrainedmodelasexemplifiedbybiobert': 1, '34specificallythisap': 1, 'proachwouldinitializewiththestandardbertmodel': 1, '16pretrainedusingwikipediaandbookcorpusitthen': 1, 'continues': 1, 'nsp': 1, 'pretrainingisconductedusingpubmedabstractsand': 1, 'pubmedcentralpmc': 1, 'fulltextarticlesbluebert': 1, 'deidentified': 1, 'thiscasetheonegeneratedfromwikipediaandbookcorpusalthoughconvenientthisisamajordisadvantage': 1, 'representative': 1, 'comparedtotheotherbiomedicalrelatedpretrainingeffortsscibert': 1, '8isanotableexceptionasitgenerates': 1, 'thevocabularyandpretrainsfromscratchusingbiomedicineandcomputerscienceasrepresentativesforscien': 1, 'tificliteraturehoweverfromtheperspectiveofbiomedicalapplicationsscibertstilladoptsthemixeddomain': 1, 'clearly': 1, 'scratchthe': 1, 'sense': 1, 'targetapplicationdomainhaslittletextofitsownandcantherebybenefitfrompretrainingusingrelateddomains': 1, 'category': 1, 'diabetes': 1, 'leukemia': 1, 'lithium': 1, 'promoter': 1, 'organ': 1, 'piecesseparatedbyahyphenthesewordpiecesoftenhavenobiomedicalrelevanceandmayhinderlearningindownstream': 1, 'adds': 1, 'year': 1, 'major': 1, 'stems': 1, 'having': 1, 'compares': 1, 'trainingare': 1, 'stuckwith': 1, 'theoriginal': 1, 'vocabularyfrom': 1, 'thegeneraldomain': 1, 'corporawhichdoesnot': 1, 'commonbiomedicaltermsevenforscibertwhichgeneratesitsvocabularypartiallyfrombiomedicaltextthe': 1, 'deficiency': 1, 'forcedtodivertparameterizationcapacityandtrainingbandwidthtomodelbiomedicaltermsusingfragmented': 1, 'examplenaloxone': 1, 'na': 1, 'xon': 1, 'andacetyltransferase': 1, 'shattered': 1, 'seven': 1, 'ace': 1, 'ty': 1, 'lt': 1, 'sf': 1, 'eras': 1, 'bert2': 1, 'appear': 1, 'balance': 1, 'optimizing': 1, 'unlikely': 1, 'beneficial': 1, 'recover': 1, 'completely': 1, 'aside': 1, 'issue': 1, 'nonconvex': 1, 'optimization': 1, 'means': 1, 'thatcontinualpretrainingmaynotbeabletocompletelyundosuboptimalinitializationfromthegeneraldomain': 1, '2prior': 1, 'shattering': 1, '8b': 1, 'clear': 1, 'vantagesovermixeddomainpretrainingbeitcontinualpretrainingofgeneraldomainlanguagemodelsorpre': 1, 'ultimate': 1, 'applicationsingeneraldomainnlpthecreationofcomprehensivebenchmarkssuchasglue': 1, '5657greatly': 1, 'accelerates': 1, 'advances': 1, 'enabling': 1, 'contrast': 1, 'table2': 1, 'best': 1, 'attempt': 1, 'aim': 1, 'design': 1, 'addressing': 1, 'limitations': 1, 'limited': 1, 'coverage': 1, 'biomedicallanguagemodelsasshownintable': 1, '2forexampleitdoesnotincludeanyquestionansweringtask': 1, 'importantly': 1, 'mixes': 1, 'pubmedbased': 1, 'bc5': 1, 'mimicbased': 1, 'i2b2': 1, 'mednli': 1, 'differ': 1, 'substantiallyfrombiomedicalliteraturetotheextentthatweobservebertmodelspretrainedonclinicalnotes': 1, 'poorly': 1, 'benchmarks': 1, 'investigations': 1, 'progress': 1, 'biomed': 1, 'icalnlpwecreateanewbenchmark': 1, 'blurbbiomedicallanguageunderstandingreasoningbench': 1, 'markwefocusonpubmedbasedbiomedicalapplicationsandleavetheexplorationoftheclinicaldomainand': 1, 'tractable': 1, 'prioritize': 1, 'blurbiscomprisedofacomprehensivesetofbiomedicalnlptasksfrompubliclyavailabledatasetsincluding': 1, 'table3': 1, 'swering': 1, 'metrics': 1, '5203': 1, '5347': 1, '5385': 1, '4182': 1, '4244': 1, '4424': 1, '5134': 1, '787': 1, '960': 1, '15197': 1, '3061': 1, '6325': 1, '46750': 1, '4551': 1, '339167': 1, '85321': 1, '16364': 1, 'wordlevel': 1, '18035': 1, '11268': 1, '15745': 1, '25296': 1, '2496': 1, '5716': 1, '4261': 1, '535': 1, '534': 1, '64': 1, '1295': 1, '186': 1, '371': 1, '670': 1, '75': 1, 'numbers': 1, 'require': 1, 'ditional': 1, 'engineering': 1, 'evaluating': 1, 'simplicity': 1, 'ontheclassificationtaskssuchasyesnoquestionansweringinblurbandleavetheinclusionofmorecomplex': 1, 'simplest': 1, 'place': 1, 'undue': 1, 'emphasis': 1, 'simpler': 1, 'datasetsthereforewegroupthedatasetsbytheirtasktypescomputetheaveragescoreforeachtasktypeand': 1, 'athttpakamsblurb': 1, '231': 1, 'bc5chemicalbc5disease': 1, 'thebiocreativevchemicaldiseaserelationcorpus': 1, '35wascreatedforeval': 1, 'uating': 1, 'drugdisease': 1, 'frequently': 1, 'detect': 1, 'ing': 1, 'discard': 1, 'bc5chemical': 1, 'separately': 1, 'center': 1, 'biotechnology': 1, '793': 1, '6892': 1, 'linked': 1, '790': 1, 'distinct': 1, 'thebiocreativeiigenementioncorpus': 1, '53consistsofsentencesfrompubmedabstractswithman': 1, 'ually': 1, 'alternative': 1, '15000': 1, '5000': 1, 'carves': 1, '2500': 1, 'molecular': 1, 'biology': 1, 'rna': 1, 'line': 1, 'distinctions': 1, 'meaningful': 1, 'exampleagenementionoftenreferstoboththednaandgeneproductssuchasthernaandproteinfollowing': 1, 'evaluates': 1, 'ignore': 1, 'distinction': 1, 'detecting': 1, '232': 1, 'picothe': 1, 'medicine': 1, 'participants': 1, 'egdiabetic': 1, 'egplacebo': 1, 'outcome': 1, 'egblood': 1, 'glucose': 1, 'levels': 1, 'omitted': 1, 'placebo': 1, 'passive': 1, 'active': 1, 'trol': 1, '4300': 1, '200': 1, 'amazon': 1, 'mechanical': 1, 'turkers': 1, 'upwork': 1, 'contributors': 1, 'tally': 1, 'theaverageamongpioelementsinthedatasetoccasionallytwopicoelementsmightoverlapwitheachother': 1, 'might': 1, 'overlap': 1, 'along': 1, 'remove': 1, 'overlapping': 1, 'larger': 1, 'scripts': 1, '233': 1, 'organized': 1, 'hierarchy': 1, 'includingnone': 1, 'single': 1, 'follow': 1, 'suggestions': 1, 'five': 1, 'interactionsupregulator': 1, 'downregulator': 1, 'antagonist': 1, 'substrate': 1, '9as': 1, 'thingelse': 1, 'falsethechemprotannotationisnotexhaustiveforallchemicalproteinpairsfollowingprevious': 1, 'expand': 1, 'assigning': 1, 'afalse': 1, 'occur': 1, 'explicit': 1, 'expansion': 1, 'instructions': 1, 'reproducing': 1, 'thedrugdruginteractioncorpus': 1, '21wascreatedtofacilitateresearchonpharmaceuticalinformation': 1, 'pharmacovigilance': 1, 'interactionsonpubmedabstractsnotethatsomepriorwork': 1, '4561discarded90trainingfilesthattheauthors': 1, 'conducive': 1, 'produce': 1, '62490191': 1, 'files': 1, 'semiautomatically': 1, 'sociation': 1, 'archive3': 1, 'associations': 1, 'reporting': 1, 'bravo': 1, '11u': 1, 'dab': 1, 'ln': 1, 'tooltoidentifygeneanddiseasementionsandcreatethepositiveexamplesfromtheannotatedsentencesinthe': 1, '3httpgeneticassociationdbnihgov': 1, 'cooccurrences': 1, 'useanexistingpreprocessedversionofgad': 1, 'anditscorrespondingtraindevtestsplitcreatedbyleeetal': 1, '234': 1, 'pubmedsentenceseachofwhichisannotatedbyfiveexpertlevelannotatorswithanestimatedsimilarityscore': 1, '0': 1, 'equivalent': 1, 'meanings': 1, 'correlation': 1, '235': 1, 'motivated': 1, 'pioneering': 1, 'signifies': 1, 'grouped': 1, 'predicting': 1, '1499': 1, 'expanded': 1, '1852': 1, 'discarded': 1, '272': 1, 'discuss': 1, 'false': 1, 'adopt': 1, 'followthecommonpracticeandevaluateontheabstractlevel': 1, '1960wecreatethetraindevtestsplitasthey': 1, 'previously4': 1, '236': 1, 'qa': 1, 'thepubmedqadataset': 1, '25containsasetofresearchquestionseachwithareferencetextfrom': 1, 'apubmedabstractaswellasanannotatedlabelofwhetherthetextcontainstheanswertotheresearchquestion': 1, 'thebioasqcorpus': 1, '42containsmultiplequestionansweringtasksannotatedbybiomedicalexperts': 1, 'includingyesnofactoidlistandsummaryquestionspertainingtoourobjectiveofcomparingneurallanguage': 1, '7b': 1, 'paired': 1, 'official': 1, '67075140': 1, 'foundation': 1, 'produces': 1, 'layered': 1, 'learn': 1, 'refine': 1, 'gradient': 1, 'descent': 1, 'backpropagation': 1, 'difficult': 1, 'understand': 1, 'section': 1, 'primary': 1, 'investi': 1, 'gation': 1, 'comparing': 1, 'identifed': 1, 'subse': 1, 'quentlywestartwiththesamepretrainedbertmodelandconductadditionalinvestigationontheimpactforthe': 1, '4the': 1, 'cross': 1, 'validation': 1, '210': 1, 'relationextraction': 1, 'through': 1, 'tokenization': 1, 'meth': 1, 'ods': 1, '241': 1, 'modelsfigure': 1, 'modulethatperformstaskspecifictransformationssuchasappendingspecialinstancemarker': 1, 'dummifying': 1, 'transformed': 1, 'tokenized': 1, 'afeaturizer': 1, 'thepredict': 1, 'apply': 1, 'procedure': 1, 'mean': 1, 'square': 1, 'error': 1, 'weconducthyperparametersearchusingthedevelopmentsetbasedontaskspecificmetricssimilartoprevious': 1, 'jointly': 1, 'finetune': 1, '242': 1, 'taskspecificproblemformulationandmodelingchoices': 1, 'manynlpapplicationscanbeformulatedas': 1, 'wherein': 1, 'individual': 1, 'aspects': 1, 'table4': 1, 'highlight': 1, 'asterisk': 1, 'seeks': 1, 'recognize': 1, 'interest': 1, 'assigned': 1, 'signify': 1, 'primarily': 1, 'clas': 1, 'sificationmethod': 1, 'isthestandardtaggingschemethatclassifieseachtokenasthebeginningofanentity': 1, 'formulations': 1, 'entityrelation': 1, 'questiontext': 1, 'insideanentity': 1, 'ioroutside': 1, 'othenertasksinblurbareonlyconcernedaboutoneentitytypeinjnlpba': 1, 'allthetypesaremergedintooneinthecasewhentherearemultipleentitytypesthe': 1, 'bi': 1, 'tagswouldbefurther': 1, 'asbioulw': 1, 'andl': 1, 'singleword': 1, 'siderthesimpler': 1, 'schemethatonlydifferentiatesbetweeninandoutofanentityclassificationisdoneusing': 1, 'orconditional': 1, 'field': 1, 'crf33': 1, 'conceptuallyevidencebasedmedicalinformationextractionisakintoslotfillingasittriestoidentify': 1, 'describing': 1, 'trial': 1, 'like': 1, 'belonging': 1, 'belong': 1, 'extractionexisting': 1, 'determine': 1, 'variations': 1, 'overfit': 1, 'ting': 1, 'memorizing': 1, 'augmented': 1, 'replaced': 1, 'featurization': 1, 'represented': 1, 'representationisusuallyproducedbypoolingthoseofindividualtokensmaxoraverageforcomputationalef': 1, 'ficiencyweusepaddingortruncationtosettheinputlengthto128tokensforgadand256tokensforchemprot': 1, 'pooling': 1, 'concatenation': 1, 'men': 1, 'tion': 1, 'classifiers': 1, 'mlp': 1, 'similaritythe': 1, 'normalized': 1, 'scoreforasentencepairbydefaultaspecial': 1, 'septokenisinsertedtoseparatethetwosentencesandaspecial': 1, 'details': 1, '33b': 1, 'words16': 1, '160': 1, '32b': 1, '05b': 1, 'words37': 1, '31b': 1, 'words21': 1, 'statistics': 1, 'taken': 1, 'publications': 1, 'bit': 1, 'depending': 1, 'downloading': 1, 'filtering': 1, 'empty': 1, 'clstokenisprependedtothebeginningtorepresentthepairthebertencodingof': 1, 'clsisusedtocompute': 1, 'clss1': 1, 'seps2': 1, 'pairs1': 1, 'classificationfor': 1, 'categoryan': 1, 'abstractand': 1, 'classifywhetherthe': 1, 'categoryby': 1, 'default': 1, 'appendedto': 1, 'thefeaturizer': 1, 'clsd': 1, 'documentd': 1, 'answeringfor': 1, 'twoway': 1, 'threeway': 1, 'questionanswering': 1, 'en': 1, 'coding': 1, 'followed': 1, 'efficiency': 1, 'padding': 1, 'truncation': 1, '512': 1, 'clsq': 1, 'sept': 1, 'questionq': 1, 'textt': 1, 'experimental': 1, 'forbiomedicaldomainspecificpretrainingwegeneratethevocabularyandconductpretrainingusingthelatest': 1, 'pubmed5': 1, 'filter': 1, '128': 1, 'reduce': 1, 'wefollowthestandardpretrainingprocedurebasedonthetensorflowimplementationreleasedbynvidia': 1, '30fortheoptimizerusingastandardslantedtriangularlearningrateschedulewithwarmupin': 1, '10ofstepsandcooldownin90ofstepsspecificallythelearningrateincreaseslinearlyfromzerotothepeak': 1, '5httpspubmedncbinlmnihgov': 1, 'downloaded': 1, 'february': 1, '6httpsgithubcomnvidiadeeplearningexamples': 1, 'rateof6': 1, '104': 1, 'inthefirst10ofstepsthendecayslinearlytozerointheremaining90ofstepstrainingisdone': 1, '62500': 1, '8192': 1, 'computation': 1, 'previous': 1, 'pretraining7': 1, 'takes': 1, 'days': 1, 'dgx2': 1, 'v100': 1, 'gpus': 1, 'preliminary': 1, 'denote': 1, 'resulting': 1, 'public': 1, 'releases': 1, 'clini': 1, 'calbert': 1, 'table5': 1, 'share': 1, 'v11': 1, 'performed': 1, 'bioclinical': 1, 'explored': 1, 'bertlarge': 1, '300': 1, 'pub': 1, 'medbertbertlargeappearstoyieldimprovedperformanceinsomepreliminaryexperimentsweleavean': 1, 'slanted': 1, 'schedule': 1, 'warmup': 1, 'cooldown': 1, 'remaining': 1, '90': 1, 'dropout': 1, '01duetorandominitializationofthetaskspecificmodelanddropouttheperformancemayvaryfordifferent': 1, 'randomseedsespeciallyforsmalldatasetslikebiossesbioasqandpubmedqawereporttheaveragescores': 1, 'rate1e53e55e5batchsize1632andepochnumber260ideallywewouldconductseparatehyperpa': 1, 'rametertuningforeachmodeloneachdatasethoweverthiswouldincuraprohibitiveamountofcomputation': 1, 'enumerate': 1, 'combinations': 1, 'averagingovermultiplerunswithdifferentrandomizationinpracticeweobservethatthedevelopmentperfor': 1, 'mance': 1, 'sensitive': 1, 'hyperparameter': 1, 'ballpark': 1, 'wefocusonhyperparametertuningusingasubsetofrepresentativemodelssuchasbertandbiobertanduse': 1, 'acommonsetofhyperparametersforeachdatasetthatworkwellforbothoutdomainandindomainlanguage': 1, '3r': 1, 'inthissectionweconductathoroughevaluationtoassesstheimpactofdomainspecificpretraininginbiomed': 1, 'finally': 1, 'applying': 1, 'specified': 1, 'section24t': 1, 'e6': 1, 'byconductingdomainspecificpretrainingfromscratchpubmedbertconsistentlyoutperformsalltheother': 1, 'bertmodelsinmostbiomedicalnlptasksoftenbyasignificantmarginthegainsaremostsubstantialagainst': 1, '7for': 1, 'started': 1, '256': 1, '8925': 1, '8999': 1, '8943': 1, '9249': 1, '9251': 1, '9080': 1, '9119': 1, '8144': 1, '7992': 1, '8065': 1, '8454': 1, '8304': 1, '8369': 1, '8567': 1, '8587': 1, '8810': 1, '8825': 1, '8632': 1, '8123': 1, '8171': 1, '8187': 1, '7769': 1, '7751': 1, '7786': 1, '7868': 1, '7807': 1, '7771': 1, '7234': 1, '7170': 1, '7302': 1, '7312': 1, '7306': 1, '7206': 1, '7254': 1, '7186': 1, '7154': 1, '7298': 1, '7524': 1, '7500': 1, '7204': 1, '7146': 1, '8004': 1, '7934': 1, '7952': 1, '8106': 1, '8122': 1, '7820': 1, '7778': 1, '8041': 1, '7961': 1, '8063': 1, '8238': 1, '8134': 1, '7915': 1, '8268': 1, '8140': 1, '8125': 1, '8625': 1, '8715': 1, '9123': 1, '8538': 1, '8020': 1, '8012': 1, '7966': 1, '8066': 1, '8074': 1, '5162': 1, '4996': 1, '5284': 1, '5738': 1, '5140': 1, '4908': 1, '4844': 1, '7036': 1, '7444': 1, '7520': 1, '7422': 1, '6850': 1, '6871': 1, '7611': 1, '7586': 1, '7646': 1, '7814': 1, '7729': 1, '7627': 1, 'table3for': 1, 'metric': 1, 'bertmodelstrainedusingoutdomaintextnotablyalthoughthepretrainingcorpusisthelargestforroberta': 1, 'mixing': 1, 'gen': 1, 'erally': 1, 'though': 1, 'domainthangeneraldomaintextaddingthemdoesnotconferanyadvantageasisevidentbytheresultsofclin': 1, 'icalbert': 1, 'surprisingly': 1, 'closest': 1, 'including': 1, 'able': 1, 'obtain': 1, 'notable': 1, 'exception': 1, 'variances': 1, 'seeds': 1, 'evaluated': 1, 'table7': 1, 'assessing': 1, 'wikipedia': 1, 'bookcorpus': 1, 'ally': 1, 'improvement': 1, 'regardless': 1, 'advantageinusinganindomainvocabularyisthattheinputwillbeshorterindownstreamtasksasshowninta': 1, 'ble8whichmakeslearningeasierfigure': 1, '3showsexamplesofhowdomainspecificpretrainingwithindomain': 1, 'furthermore': 1, 'found': 1, 'domainvocabularytable': 1, '9thefirstcolumnintable': 1, '9correspondstobiobertwhichconductedpretraining': 1, '215': 1, '9320': 1, '9331': 1, '9296': 1, '8500': 1, '8528': 1, '8472': 1, '8839': 1, '8853': 1, '8726': 1, '8365': 1, '8393': 1, '8319': 1, '7883': 1, '7877': 1, '7863': 1, '7330': 1, '7352': 1, '7344': 1, '7504': 1, '7670': 1, '7572': 1, '8130': 1, '8260': 1, '8084': 1, '8302': 1, '9136': 1, '9179': 1, '9245': 1, '8176': 1, '8038': 1, '5220': 1, '5592': 1, '5476': 1, '7369': 1, '7641': 1, '7996': 1, '7962': 1, '342': 1, '274': 1, '385': 1, '305': 1, '337': 1, '260': 1, '307': 1, '251': 1, '754': 1, '555': 1, '1060': 1, '759': 1, '470': 1, '357': 1, '807': 1, '616': 1, '406': 1, '310': 1, '3431': 1, '2930': 1, '7024': 1, '5414': 1, 'degra': 1, 'dation': 1, 'per': 1, 'formance': 1, 'fourth': 1, 'sum': 1, 'inourstandardpubmedbertpretrainingweusedpubmedabstractsonlywealsotriedaddingfulltextarti': 1, 'clesfrompmc': 1, 'withthetotalpretrainingtextincreasedsubstantiallyto168billionwords107gbsurprisingly': 1, '8httpswwwncbinlmnihgovpmc': 1, '216': 1, 'epithelialrestricted': 1, 'serine': 1, 'box': 1, 'abbreviation': 1, 'esx': 1, 'dummified': 1, 'cases': 1, 'aggregate': 1, 'penultimate': 1, 'informative': 1, 'shatter': 1, 'inheriting': 1, 'thegeneraldomainvocabularythedomainspecificvocabularyenablespubmedberttolearnbetterattentionpatternsand': 1, 'predictions': 1, 'degradation': 1, 'extending': 1, '100k': 1, 'total': 1, 'overall': 1, 'bertusingonlyabstractstheimprovementissomewhatmixedacrossthetaskswithsomegainingandothers': 1, 'losingwehypothesizethatthereasonforthisbehavioristwofoldfirstpmcinclusionisinfluencedbyfunding': 1, 'policy': 1, 'differs': 1, 'distribution': 1, 'moreover': 1, 'cycles': 1, 'extra': 1, '217': 1, '9341': 1, '9305': 1, '8543': 1, '8502': 1, '8760': 1, '8777': 1, '8403': 1, '8411': 1, '7901': 1, '7898': 1, '7380': 1, '7374': 1, '7705': 1, '7669': 1, '8121': 1, '8196': 1, '8247': 1, '8280': 1, '8993': 1, '9212': 1, '8314': 1, '8213': 1, '5484': 1, '5528': 1, '7900': 1, '7943': 1, '8003': 1, '8023': 1, 'columns': 1, 'twice': 1, '9336': 1, '9334': 1, '8576': 1, '8834': 1, '8439': 1, '8437': 1, '7890': 1, '7364': 1, '7372': 1, '7696': 1, '7680': 1, '8356': 1, '8206': 1, '8234': 1, '8224': 1, '8158': 1, '9039': 1, '9231': 1, '8216': 1, '8262': 1, '6102': 1, '6002': 1, '8343': 1, '8720': 1, '8107': 1, '8091': 1, '8142': 1, 'highly': 1, 'effective': 1, 'boosting': 1, 'withstandardpretrainingtable': 1, '11surprisinglyadversarialpretraininggenerallyleadstoaslightdegradation': 1, '218': 1, '9317': 1, '8548': 1, '8799': 1, '8407': 1, '7918': 1, '7292': 1, '7704': 1, '8362': 1, '8354': 1, '9411': 1, '8220': 1, '5330': 1, '8271': 1, '8077': 1, 'exceptions': 1, 'reason': 1, 'what': 1, 'useful': 1, 'de': 1, 'scribed': 1, 'section24': 1, 'fixing': 1, 'theunderlyingpretrainedlanguagemodeltoourstandardpubmedbertwwmpubmedvocabularypretrained': 1, 'current': 1, 'approaches': 1, 'dominated': 1, 'recently': 1, 'popular': 1, 'withtheadventofbertmodelsandtheselfattentionmechanismtheutilityofexplicitsequentialmodeling': 1, 'becomes': 1, 'questionable': 1, 'captures': 1, 'entire': 1, 'therefore': 1, 'conceivable': 1, 'competitively': 1, 'table12': 1, 'wealsoinvestigatethetaggingschemeusedinnerthestandardtaggingschemedistinguisheswordsbytheir': 1, 'positions': 1, 'withinanentityispotentiallyadvantageouscomparedtotheminimalioschemethatonlydistinguishesbetween': 1, 'theinsideandoutsideofentitiesbutforbertmodelsonceagaintheutilityofmorecomplextaggingschemes': 1, 'diminished': 1, 'table13': 1, 'difference': 1, 'minuscule': 1, 'suggesting': 1, 'selfattention': 1, 'nature': 1, 'essential': 1, 'subtle': 1, 'previously': 1, 'relationextractionwasgenerallyframedasaclassificationproblemwithmanuallycraftedfeaturetemplatesto': 1, 'enhance': 1, 'generalization': 1, 'feature': 1, 'templates': 1, 'avoid': 1, '219': 1, 'fine': 1, '9312': 1, '8564': 1, '7540': 1, '8170': 1, '8342': 1, '9337': 1, '9311': 1, '8559': 1, '8563': 1, '7902': 1, '7905': 1, 'handcrafted': 1, 'features': 1, 'textspanincludingtheentitiesthemselvesthisintroducesapotentialriskthattheneuralnetworkmaysimply': 1, 'memorize': 1, 'combination': 1, 'pronounced': 1, 'distant': 1, 'positive': 1, 'tuples': 1, 'known': 1, 'resultit': 1, 'isa': 1, 'practiceto': 1, 'entitiesiereplacean': 1, 'entitywitha': 1, 'generictagsuchasdrugor': 1, '2458': 1, 'setting': 1, 'comprise': 1, 'systematic': 1, 'add': 1, 'startandendtagstoentitiesinquestionforrelationencodingweconsiderthreeschemesinthe': 1, 'clsencoding': 1, 'introduced': 1, 'anditscontextualrepresentationatthetoplayerisusedastheinputinthefinalclassificationanotherstandard': 1, 'approachconcatenatesthebertencodingofthegivenentitymentionseachobtainedbyapplyingmaxpooling': 1, 'tothecorrespondingtokenrepresentationsfinallyfollowingpriorworkwealsoconsidersimplyconcatenating': 1, 'exposes': 1, 'choice': 1, 'reliable': 1, 'works': 1, 'interestingly': 1, 'resultsinbothdatasetsasitappearstopreventoverfittingwhilepreservingusefulentityinformationweleave': 1, 'generalize': 1, 'expensive': 1, 'consuming': 1, 'annotate': 1, 'longstanding': 1, 'alleviating': 1, 'bottleneck': 1, '220': 1, '8208': 1, '5052': 1, '3700': 1, '7548': 1, '7942': 1, '7772': 1, '8222': 1, '7758': 1, '8218': 1, 'anonymized': 1, 'end': 1, 'early': 1, 'focused': 1, 'clustering': 1, 'related': 1, 'brownclusters': 1, '1236withtherevivalofneuralapproachesneuralembeddinghasbecomethenewstaplefor': 1, 'transferlearningfromunlabeledtextthisstartswithsimplestandalonewordembeddings': 1, '4146andevolves': 1, 'intomoresophisticatedpretrainedlanguagemodelsfromlstminulmfit': 1, '23andelmo': 1, '47totransformer': 1, 'basedmodelsingpt': 1, '4849andbert': 1, '1639theirsuccessisfueledbyaccesstolargetextcorporaadvanced': 1, 'hardwaresuchasgpusandaculminationofadvancesinoptimizationmethodssuchasadam': 1, '30andslanted': 1, 'ventures': 1, 'beyond': 1, 'begins': 1, 'value': 1, 'kind': 1, 'brought': 1, 'play': 1, 'combining': 1, 'applicable': 1, 'sufficient': 1, 'comparingclinicalbertswithpubmedbertonbiomedicalnlptasksshowthatevenrelatedtextsuchasclinical': 1, 'weshoulddistinguishdifferenttypesoftransferlearningandseparatelyassesstheirutilityinvarioussituations': 1, 'plethora': 1, 'especially': 1, 'biocre': 1, 'ative3294053bionlp': 1, '1528semeval': 1, '291017': 1, 'dbi': 1, 'sq42thefocushase': 1, 'olv': 1, 'edfr': 1, 'omsimple': 1, 'taskshavebeenproposedforemergingapplicationscenariossuchasevidencebasedmedicalinformationextrac': 1, 'tion44howeveralthoughcomprehensivebenchmarksandleaderboardsareavailableforthegeneraldomains': 1, 'glue': 1, '57a': 1, 'ds': 1, 'e56': 1, 'rarity': 1, 'inspired': 1, 'toward': 1, 'blurba': 1, 'conclusion': 1, 'inthisarticlewechallengeaprevailingassumptioninpretrainingneurallanguagemodelsandshowthatdomain': 1, 'biomedicalnlpapplicationstofacilitatethisstudywecreateblurbacomprehensivebenchmarkforbiomed': 1, 'up': 1, 'directions': 1, 'incorporating': 1, 'extension': 1, 'references': 1, 'emily': 1, 'alsentzer': 1, 'murphy': 1, 'boag': 1, 'weihung': 1, 'weng': 1, 'di': 1, 'jindi': 1, 'matthew': 1, 'mcdermott': 1, 'availableclinicalbertembeddingsin': 1, 'ofthe2ndclinical': 1, 'workshop7278': 1, 'httpsdoiorg10': 1, '18653v1w191909': 1, 'marianna': 1, 'apidianaki': 1, 'saif': 1, 'jonathan': 1, 'ekaterina': 1, 'shutova': 1, '2018pro': 1, 'ceedingsofthe12thinternationalworkshoponsemanticevaluationsemevalnaaclhlt2018neworleanslouisianausajune56': 1, 'linguisticshttpswwwaclweborganthologyvolumess181': 1, 'phoebe': 1, 'shashank': 1, 'agarwal': 1, 'sanmitra': 1, 'bhattacharya': 1, 'gianni': 1, 'cesareni': 1, 'clematide': 1, 'iii': 1, 'interactive': 1, 'overviewbmc': 1, 'bioinformatics12': 1, 'oct': 1, 's4httpsdoiorg101186': 1, '1471210512s8s4': 1, 'amittai': 1, 'axelrod': 1, 'adaptation': 1, 'pseudo': 1, '355362httpswwwaclweborganthologyd111033': 1, 'analytics': 1, 'tool': 1, 'chat': 1, 'mining': 1, 'organize': 1, '39733981': 1, 'automatic': 1, 'classifi': 1, 'cation': 1, 'according': 1, '432440': 1, 'livio': 1, 'baldini': 1, 'soares': 1, 'nicholas': 1, 'fitzgerald': 1, 'ling': 1, 'kwiatkowski': 1, 'matching': 1, 'blanks': 1, 'distributional': 1, '57th': 1, '28952905httpsdoiorg': 1, '1018653v1p191279': 1, 'iz': 1, 'beltagy': 1, 'kyle': 1, 'arman': 1, 'cohan': 1, 'ference': 1, '9th': 1, '36153620httpsdoiorg1018653v1d191371': 1, 'stevenbethardmarinecarpuatmariannaapidianakisaifmmohammaddanielmceranddavidjurgenseds2017': 1, '11th': 1, 'semevalacl': 1, 'vancouver': 1, 'canada': 1, 'linguisticshttpswwwaclweborganthologyvolumess172': 1, 'daniel': 1, 'cer': 1, 'jurgens': 1, 'preslav': 1, 'nakov': 1, 'torsten': 1, 'zesch': 1, '2016proceedings': 1, '10th': 1, 'san': 1, 'diego': 1, 'ca': 1, '1617': 1, 'linguisticshttpswwwaclweborganthologyvolumess161': 1, 'àlexbravojanetpiñeronúriaqueraltrosinachmichaelrautschkaandlauraifurlong2015extractionofrelationsbetweengenes': 1, 'diseases': 1, 'largescale': 1, 'translational': 1, 'researchbmc': 1, 'bioinformatics16': 1, 'brown': 1, 'vincent': 1, 'della': 1, 'pietra': 1, 'desouza': 1, 'jennifer': 1, 'lai': 1, 'mercer': 1, 'classbased': 1, 'languagecomputational': 1, 'linguistics18': 1, '467480': 1, 'caruana': 1, 'learningmachine': 1, 'learning28': 1, '4175': 1, 'gamal': 1, 'billy': 1, 'chiu': 1, 'recognitionbmc': 1, 'bioinformatics18': 1, '368': 1, 'dina': 1, 'demnerfushman': 1, 'bretonnel': 1, 'sophia': 1, 'ananiadou': 1, 'junichi': 1, 'tsujii': 1, '2019proceedings': 1, 'bionlpacl': 1, 'florence': 1, 'italy': 1, 'linguisticshttpswww': 1, 'aclweborganthologyvolumesw1950': 1, 'jacob': 1, 'devlin': 1, 'mingwei': 1, 'chang': 1, 'kenton': 1, 'kristina': 1, 'toutanova': 1, '41714186': 1, '17': 1, 'mona': 1, 'diab': 1, 'timothy': 1, 'baldwin': 1, 'marco': 1, 'baroni': 1, '2013proceedings': 1, '7th': 1, 'atlanta': 1, 'georgia': 1, '1415': 1, 'linguisticshttpswwwaclweb': 1, 'organthologyvolumess132': 1, 'doğan': 1, 'ncbi': 1, 'resource': 1, 'name': 1, 'normalizationjournal': 1, 'informatics47': 1, '110': 1, '19': 1, 'jingcheng': 1, 'qingyu': 1, 'xiang': 1, 'cui': 1, 'tao': 1, 'mlnet': 1, 'networksjournal': 1, '12791285httpsdoiorg10': 1, '1093jamiaocz085': 1, 'douglas': 1, 'hanahan': 1, 'weinberg': 1, 'cancercell': 1, '5770': 1, 'maría': 1, 'herrerozazo': 1, 'isabel': 1, 'segurabedmar': 1, 'paloma': 1, 'martínez': 1, 'thierry': 1, 'declerck': 1, 'pharmacological': 1, 'substances': 1, 'interactionsjournal': 1, 'informatics46': 1, '914920': 1, 'sepp': 1, 'hochreiter': 1, 'jürgen': 1, 'schmidhuber': 1, 'memoryneural': 1, 'computation9': 1, '17351780': 1, 'jeremyhowardandsebastianruder2018universallanguagemodelfinetuningfortextclassificationin': 1, 'proceedingsofthe56thannual': 1, '328339httpsdoiorg1018653v1p181031': 1, 'jia': 1, 'cliff': 1, 'wong': 1, 'documentlevelnary': 1, 'multiscale': 1, '17th': 1, 'naaclhlt19': 1, 'qiao': 1, 'jin': 1, 'bhuwan': 1, 'dhingra': 1, 'zhengping': 1, 'xinghua': 1, 'questionansweringin': 1, 'proceedingsofthe2019conferenceonempiricalmethodsinnaturallanguageprocessingandthe9thinternational': 1, '25672577httpsdoiorg1018653v1d191259': 1, 'alistair': 1, 'pollard': 1, 'liwei': 1, 'lehman': 1, 'mengling': 1, 'feng': 1, 'ghassemi': 1, 'benjamin': 1, 'moody': 1, 'szolovits': 1, 'leo': 1, 'anthony': 1, 'celi': 1, 'roger': 1, 'mark': 1, 'freely': 1, 'accessible': 1, 'critical': 1, 'databasescientific': 1, 'data3': 1, '160035httpsdoiorg101038sdata201635': 1, 'tomoko': 1, 'ohta': 1, 'yoshimasa': 1, 'tsuruoka': 1, 'yuka': 1, 'tateisi': 1, 'nigel': 1, 'collier': 1, '2004': 1, 'bioentity': 1, 'nlpbabionlp04': 1, '7378httpswwwaclweborganthologyw041213': 1, 'yue': 1, 'toshihisa': 1, 'takagi': 1, 'akinori': 1, 'yonezawa': 1, 'genia': 1, 'event': 1, 'task11': 1, '715': 1, 'dogan': 1, 'tyers': 1, 'wilbur': 1, 'donald': 1, 'comeau': 1, 'bioc': 1, 'track': 1, 'fifth': 1, 'sevilla': 1, 'spain1': 1, 'diederikpkingmaandjimmyba2015adamamethodforstochasticoptimizationin': 1, 'proceedingsofthe3rdinternationalconference': 1, 'iclr15': 1, 'httparxivorgabs14126980': 1, 'martin': 1, 'krallinger': 1, 'obdulia': 1, 'rabal': 1, 'saber': 1, 'akhondi': 1, 'martın': 1, 'jesús': 1, 'santamaría': 1, 'rodríguez': 1, 'tsatsaronis': 1, 'overviewofthebiocreativevichemicalproteininteractiontrackin': 1, 'proceedingsofthe6thbiocreativechallengeevaluationworkshop': 1, '141146': 1, 'takukudoandjohnrichardson2018sentencepieceasimpleandlanguageindependentsubwordtokenizeranddetokenizerforneu': 1, 'ral': 1, 'demonstrations': 1, '6671httpsdoiorg1018653v1d182012': 1, 'lafferty': 1, 'mccallum': 1, 'fernando': 1, 'pereira': 1, '2001': 1, 'fields': 1, 'segmenting': 1, '282289': 1, 'jinhyukleewonjinyoonsungdongkimdonghyeonkimsunkyukimchanhosoandjaewookang2019biobertapretrained': 1, 'miningbioinformatics': 1, '12341240httpsdoiorg101093': 1, 'bioinformaticsbtz682': 1, 'jiao': 1, 'yueping': 1, 'daniela': 1, 'sciaky': 1, 'chihhsuan': 1, 'allan': 1, 'davis': 1, 'carolyn': 1, 'mattingly': 1, 'thomascwiegersandzhiyonglu2016biocreativevcdrtaskcorpusaresourceforchemicaldiseaserelationextraction': 1, 'online': 1, 'percy': 1, 'liang': 1, '2005semisupervised': 1, 'phd': 1, 'dissertation': 1, 'massachusetts': 1, 'institute': 1, 'technology': 1, 'cam': 1, 'bridge': 1, 'pengcheng': 1, 'weizhu': 1, 'arxiv200408994': 1, 'deng': 1, 'duh': 1, 'yeyi': 1, 'neuralnetworksforsemanticclassificationandinformationretrievalin': 1, 'proceedingsofthe2015conferenceofthenorthamericanchapter': 1, '912921': 1, 'yinhan': 1, 'myle': 1, 'ott': 1, 'naman': 1, 'goyal': 1, 'jingfei': 1, 'mandar': 1, 'joshi': 1, 'danqi': 1, 'lewis': 1, 'luke': 1, 'zettlemoyer': 1, 'veselin': 1, 'stoyanov': 1, 'robustly': 1, 'optimized': 1, 'arxiv190711692': 1, '40': 1, 'yuqing': 1, 'mao': 1, 'kimberly': 1, 'van': 1, 'auken': 1, 'donghui': 1, 'mcquilton': 1, 'thomas': 1, 'hayman': 1, 'susan': 1, 'tweedie': 1, 'ontology': 1, 'ivdatabaseonline': 1, '41': 1, 'tomasmikolovkaichengregcorradoandjeffreydean2013efficientestimationofwordrepresentationsinvectorspacearxiv1301': 1, '3781': 1, 'anastasios': 1, 'nentidis': 1, 'konstantinos': 1, 'bougiatiotis': 1, 'anastasia': 1, 'krithara': 1, 'georgios': 1, 'paliouras': 1, 'seventh': 1, 'edition': 1, 'bioasqchallengein': 1, 'proceedingsofthejointeuropeanconferenceonmachinelearningandknowledgediscoveryindatabases': 1, '553568': 1, 'hermannneyuteessenandreinhardkneser1994onstructuringprobabilisticdependencesinstochasticlanguagemodelling': 1, 'com': 1, 'puter': 1, 'speech': 1, 'language8': 1, '1994': 1, '138httpsdoiorg101006csla19941001': 1, '223': 1, 'benjaminnyejunyijessyliromapatelyinfeiyangiainjmarshallaninenkovaandbyroncwallace2018acorpuswithmulti': 1, 'annotations': 1, 'interventions': 1, 'outcomes': 1, '197': 1, 'shankai': 1, 'yan': 1, 'andelmoontenbenchmarkingdatasetsin': 1, 'ofthe18thbionlpworkshopandsharedtask': 1, '5865': 1, 'v1w195006': 1, '46': 1, 'pennington': 1, 'richard': 1, 'socher': 1, 'christopher': 1, 'manning': 1, 'glove': 1, 'global': 1, 'emnlp14': 1, '15321543': 1, '47': 1, 'matthewpetersmarkneumannmohitiyyermattgardnerchristopherclarkkentonleeandlukezettlemoyer2018deepcontex': 1, 'tualizedwordrepresentationsin': 1, 'proceedingsofthe2018conferenceofthenorthamericanchapteroftheassociationforcomputational': 1, '22272237httpsdoiorg1018653v1n181202': 1, '48': 1, 'karthik': 1, 'narasimhan': 1, 'tim': 1, 'salimans': 1, 'improving': 1, 'traininghttpswwwcsubccaamuham01ling530papersradford2018improvingpdf': 1, '49': 1, 'wu': 1, 'rewon': 1, 'child': 1, 'luan': 1, 'dario': 1, 'amodei': 1, 'unsupervised': 1, 'learnersopenai': 1, 'blog1': 1, 'colin': 1, 'raffel': 1, 'katherine': 1, 'sharan': 1, 'narang': 1, 'matena': 1, 'yanqi': 1, 'exploring': 1, 'limits': 1, 'unified': 1, 'texttotext': 1, 'transformerjournal': 1, 'research21': 1, '167': 1, 'httpjmlrorgpapersv2120074html': 1, '51': 1, 'ricosennrichbarryhaddowandalexandrabirch2016neuralmachinetranslationofrarewordswithsubwordunitsin': 1, 'ofthe54thannualmeetingoftheassociationforcomputationallinguisticsvolume1longpapers': 1, '17151725': 1, 'v1p161162': 1, 'yuqi': 1, 'si': 1, 'jingqi': 1, 'hua': 1, 'kirk': 1, 'enhancing': 1, 'embeddingsjournal': 1, '12971304': 1, 'larrysmithlorrainektanaberiejohnsonneeandochengjukuoifangchungchunnanhsuyushilinetal2008overview': 1, 'ii': 1, 'recognitiongenome': 1, 'biology9': 1, '2008': 1, 'gizem': 1, 'soğancıoğlu': 1, 'hakime': 1, 'öztürk': 1, 'arzucan': 1, 'özgür': 1, 'domainbioinformatics': 1, 'i49i58': 1, 'ashish': 1, 'vaswani': 1, 'niki': 1, 'parmar': 1, 'jakob': 1, 'uszkoreit': 1, 'llion': 1, 'jones': 1, 'aidan': 1, 'gomez': 1, 'łukasz': 1, 'kaiser': 1, 'illia': 1, 'polosukhin': 1, 'you': 1, '59986008': 1, '56': 1, 'alex': 1, 'yada': 1, 'pruksachatkun': 1, 'nikita': 1, 'nangia': 1, 'amanpreet': 1, 'singh': 1, 'julian': 1, 'felix': 1, 'hill': 1, 'samuel': 1, 'bowman': 1, 'superglue': 1, 'stickier': 1, 'generalpurpose': 1, '32663280': 1, '57': 1, 'alexwangamanpreetsinghjulianmichaelfelixhillomerlevyandsamuelrbowman2019glueamultitaskbenchmarkand': 1, 'platform': 1, 'iclr19': 1, '58': 1, 'hai': 1, 'logic': 1, 'framework': 1, 'indirect': 1, 'emnlp18': 1, 'yichong': 1, 'yelong': 1, 'jingjing': 1, 'sample': 1, 'reweighting': 1, 'chinereadingcomprehension': 1, 'the2019': 1, 'conferenceof': 1, 'thenorthamerican': 1, 'chapteroftheassociation': 1, '26442655httpsdoiorg1018653v1n191271': 1, 'z': 1, 'algorithmsieee': 1, 'engineering26': 1, '18191837httpsdoiorg101109tkde201339': 1, '61': 1, 'yijia': 1, 'zheng': 1, 'hongfei': 1, 'lin': 1, 'jian': 1, 'zhihao': 1, 'michel': 1, 'dumontier': 1, 'hierarchical': 1, 'rnns': 1, 'shortest': 1, 'dependency': 1, 'pathsbioinformatics': 1, '828835': 1, '62': 1, 'yukun': 1, 'zhu': 1, 'ryan': 1, 'kiros': 1, 'zemel': 1, 'ruslan': 1, 'salakhutdinov': 1, 'raquel': 1, 'urtasun': 1, 'antonio': 1, 'torralba': 1, 'sanja': 1, 'fidler': 1, 'aligning': 1, 'andmoviestowardsstorylikevisualexplanationsbywatchingmoviesandreadingbooksin': 1, 'proceedingsofthe2015ieeeinternational': 1, 'vision': 1, 'iccv15': 1, 'received': 1, 'july': 1, 'revised': 1, 'january': 1, 'accepted': 1, 'march': 1})}, {'file_name': 'clip', 'word_counts': Counter({'the': 318, 'a': 297, 'and': 288, 'of': 261, 'in': 183, 'to': 164, 'on': 136, 'et': 113, 'al': 113, 'j': 101, 'for': 94, 'learning': 93, 'we': 89, 'is': 86, 'clip': 86, 'm': 73, 'models': 72, 'image': 72, 'with': 67, 'zeroshot': 66, 'vision': 65, 't': 61, 'from': 60, 'this': 60, '2019': 60, 'as': 57, 'language': 56, '2020': 56, 'd': 55, 'computer': 53, 's': 52, 'visual': 51, 'pp': 51, 'preprint': 51, 'text': 49, 'that': 49, 'arxiv': 48, 'c': 45, 'model': 43, 'imagenet': 43, 'r': 43, 'natural': 42, 'by': 41, 'n': 41, 'performance': 40, 'datasets': 40, 'recognition': 39, 'i': 38, 'l': 38, 'are': 37, 'dataset': 37, 'conference': 37, '1': 36, 'y': 36, 'k': 36, 'h': 32, 'p': 32, '2018': 30, 'ieee': 30, 'supervision': 29, 'tasks': 29, 'training': 28, 'b': 28, 'an': 26, 'proceedings': 26, 'systems': 25, 'transfer': 25, 'trained': 24, 'which': 24, 'pretraining': 24, 'classiﬁcation': 24, 'at': 24, 'neural': 24, 'e': 24, 'deep': 22, '2017': 21, 'li': 20, 'our': 19, '2': 19, 'all': 19, 'x': 19, 'g': 19, 'accuracy': 18, 'has': 18, 'these': 18, 'more': 18, 'zhang': 18, '3': 18, 'linear': 18, 'representation': 18, 'distribution': 18, 'data': 17, 'work': 17, '2016': 17, 'encoder': 17, 'also': 17, 'w': 17, 'robustness': 17, 'v': 17, 'transferable': 16, 'supervised': 16, 'it': 16, 'images': 16, 'information': 16, 'evaluation': 16, 'pattern': 16, 'learn': 15, 'resnet50': 15, 'use': 15, 'while': 15, 'only': 15, 'networks': 15, 'representations': 14, 'over': 14, 'many': 14, '2015': 14, 'similar': 14, 'contrastive': 14, 'figure': 14, 'be': 14, 'using': 14, 'chen': 14, 'wang': 14, 'f': 14, 'task': 13, 'or': 13, 'classiﬁer': 13, 'best': 13, 'transformer': 13, 'not': 13, 'he': 13, 'suite': 13, 'advances': 13, 'processing': 13, 'ing': 12, 'study': 12, 'such': 12, 'international': 12, 'ngrams': 12, 'large': 12, 'outperforms': 12, 'than': 12, 'z': 12, 'other': 11, 'baseline': 11, 'across': 11, 'results': 11, 'scale': 11, 'tion': 11, 'compute': 11, 'research': 11, 'analysis': 11, 'fewshot': 11, 'shift': 11, 'set': 10, 'object': 10, 'their': 10, 'new': 10, 'methods': 10, 'words': 10, 'can': 10, 'approach': 10, 'train': 10, 'features': 10, 'when': 10, 'url': 10, 'radford': 9, 'predict': 9, 'used': 9, 'examples': 9, 'was': 9, 'machine': 9, 'however': 9, 'including': 9, 'found': 9, 'objective': 9, 'average': 9, '27': 9, 'benchmark': 9, 'lee': 9, 'since': 8, 'im': 8, 'much': 8, 'efﬁcient': 8, 'million': 8, '2009': 8, 'embedding': 8, 'classes': 8, 'existing': 8, '2014': 8, 'comparison': 8, 'possible': 8, 'class': 8, 'its': 8, 'well': 8, 'multimodal': 8, 'clips': 8, 'unsupervised': 8, 'studied': 8, 'towards': 8, 'kornblith': 8, 'overlap': 8, '2011': 8, 'taori': 8, 'largescale': 8, 'gender': 8, '2012': 8, 'van': 8, 'wu': 8, 'sota': 7, 'pairs': 7, '30': 7, 'action': 7, 'fully': 7, 'have': 7, 'le': 7, 'standard': 7, 'demonstrated': 7, 'kolesnikov': 7, 'mahajan': 7, 'ﬁnd': 7, 'feature': 7, 'perform': 7, 'same': 7, '5': 7, '20': 7, 'space': 7, 'transformers': 7, 'probe': 7, 'classiﬁers': 7, 'score': 7, 'capabilities': 7, 'how': 7, 're': 7, 'overall': 7, '7': 7, 'liu': 7, '2013': 7, 'zisserman': 7, 'convolutional': 7, 'o': 7, 'european': 7, 'pre': 6, 'form': 6, 'any': 6, 'directly': 6, 'broader': 6, 'simple': 6, 'concepts': 6, 'competitive': 6, 'need': 6, 'pretrained': 6, 'development': 6, 'like': 6, 'batch': 6, 'descriptions': 6, 'available': 6, 'each': 6, '100': 6, 'scaling': 6, '10': 6, 'improvement': 6, 'resnet': 6, 'single': 6, 'layer': 6, 'between': 6, '75': 6, 'socher': 6, 'lu': 6, 'q': 6, 'der': 6, 'con': 5, 'scratch': 5, 'learned': 5, 'different': 5, 'often': 5, 'speciﬁc': 5, 'no': 5, 'still': 5, 'joulin': 5, 'recent': 5, 'one': 5, 'publicly': 5, 'robust': 5, 'paired': 5, 'photos': 5, '35': 5, 'but': 5, 'search': 5, 'word': 5, 'above': 5, 'see': 5, 'improves': 5, 'efﬁciency': 5, 'explored': 5, 'loss': 5, 'ﬁrst': 5, 'labels': 5, 'due': 5, 'dosovitskiy': 5, 'width': 5, 'tan': 5, '12': 5, 'higher': 5, 'supplementary': 5, 'material': 5, 'sun': 5, 'measuring': 5, 'selfsupervised': 5, 'matches': 5, '2008': 5, 'logistic': 5, 'bitm': 5, 'both': 5, 'through': 5, 'bias': 5, 'human': 5, '2020b': 5, 'understanding': 5, 'under': 5, 'effective': 5, 'video': 5, 'jb': 5, 'scene': 5, 'categories': 4, 'additional': 4, 'about': 4, 'caption': 4, 'videos': 4, 'geolocalization': 4, 'original': 4, 'years': 4, 'raffel': 4, 'architectures': 4, 'zero': 4, 'shot': 4, 'crowdlabeled': 4, 'prior': 4, 'com': 4, 'then': 4, 'system': 4, 'virtex': 4, 'johnson': 4, 'modeling': 4, 'age': 4, 'big': 4, 'trans': 4, 'weakly': 4, 'two': 4, 'gap': 4, 'version': 4, 'pepper': 4, 'aussie': 4, 'pup': 4, 'jointly': 4, 'pairings': 4, 'learns': 4, 'during': 4, 'three': 4, 'yfcc100m': 4, 'approximately': 4, 'up': 4, '6': 4, 'internet': 4, 'per': 4, 'query': 4, 'encoding': 4, 'tian': 4, 'volume': 4, '0': 4, 'although': 4, 'further': 4, 'do': 4, 'embeddings': 4, 'scores': 4, 'details': 4, 'compared': 4, 'instead': 4, 'joint': 4, 'pooling': 4, 'attention': 4, 'described': 4, 'into': 4, 'depth': 4, 'resolution': 4, 'reported': 4, '762': 4, 'knowledge': 4, '95': 4, 'setting': 4, 'were': 4, '40': 4, '16': 4, 'unseen': 4, 'larochelle': 4, 'complex': 4, 'trafﬁc': 4, '70': 4, 'simclrv2': 4, 'better': 4, 'been': 4, 'zhai': 4, '80': 4, 'xie': 4, '2020a': 4, 'recht': 4, 'shifts': 4, 'improve': 4, 'people': 4, 'surveillance': 4, 'machines': 4, 'miech': 4, 'yu': 4, 'alayrac': 4, 'intelligence': 4, 'learners': 4, 'girshick': 4, 'ng': 4, 'feifei': 4, 'journal': 4, 'yang': 4, 'maaten': 4, 'springer': 4, 'acm': 4, 'manning': 4, 'askell': 3, 'clark': 3, 'sutskever': 3, 'limits': 3, 'labeled': 3, 'specify': 3, 'scalable': 3, 'way': 3, 'after': 3, 'downstream': 3, 'most': 3, 'without': 3, 'weights': 3, 'nlp': 3, 'taskagnostic': 3, 'brown': 3, 'modern': 3, 'within': 3, 'deng': 3, 'could': 3, 'web': 3, 'cnns': 3, 'captions': 3, 'ex': 3, 'predicting': 3, 'ability': 3, 'cation': 3, 'convirt': 3, 'recently': 3, 'days': 3, 'method': 3, 'prediction': 3, 'test': 3, 'names': 3, 'wide': 3, 'being': 3, 'core': 3, 'idea': 3, 'lin': 3, 'genome': 3, 'they': 3, 'english': 3, '15': 3, 'variety': 3, 'wit': 3, 'parameter': 3, 'uses': 3, 'bagofwords': 3, 'predictive': 3, 'ﬁnding': 3, 'base': 3, 'list': 3, 'least': 3, 'pair': 3, 'certain': 3, 'finally': 3, 'miller': 3, '25': 3, '3x': 3, 'bag': 3, 'another': 3, '4x': 3, 'observed': 3, 'given': 3, 'cosine': 3, 'optimize': 3, 'include': 3, 'pseudocode': 3, 'adapted': 3, 'en': 3, 'aligned': 3, 'd_e': 3, 'axis1': 3, 'scaled': 3, 'function': 3, 'choosing': 3, '2016a': 3, 'architecture': 3, 'global': 3, 'vit': 3, 'normalization': 3, 'slightly': 3, '8': 3, 'left': 3, 'previous': 3, 'increasing': 3, 'tional': 3, 'resnets': 3, 'resnet101': 3, 'largest': 3, '18': 3, 'v100': 3, 'touvron': 3, 'paper': 3, 'capability': 3, 'motivate': 3, 'classi': 3, 'amount': 3, 'differences': 3, 'potential': 3, 'order': 3, 'agnostic': 3, 'compare': 3, 'concept': 3, 'szegedy': 3, 'kitti': 3, 'eval': 3, '32': 3, 'lampert': 3, 'performing': 3, 'ﬁeld': 3, 'studying': 3, '50': 3, '4': 3, 'designed': 3, 'regression': 3, 'general': 3, 'wider': 3, 'several': 3, 'detection': 3, 'objects': 3, 'related': 3, 'german': 3, 'sign': 3, 'contrast': 3, 'signiﬁcant': 3, 'almost': 3, 'hu': 3, 'comparing': 3, '55': 3, '65': 3, 'imagenet21k': 3, '16shot': 3, 'via': 3, 'allows': 3, 'there': 3, 'noisy': 3, 'ﬁndings': 3, 'selection': 3, 'concerns': 3, 'facial': 3, 'perfor': 3, 'mance': 3, 'evaluated': 3, '101': 3, '85': 3, 'should': 3, 'relative': 3, 'top1': 3, 'line': 3, 'detected': 3, 'impacts': 3, 'design': 3, 'own': 3, 'taskspeciﬁc': 3, 'race': 3, 'child': 3, 'celebrity': 3, 'challenges': 3, 'improving': 3, 'semisupervised': 3, 'fergus': 3, 'ramanathan': 3, 'answering': 3, 'social': 3, 'wed': 3, 'thank': 3, 'harris': 3, 'scipy': 3, 'virtanen': 3, 'pandas': 3, 'pedregosa': 3, 'luo': 3, 'tenenbaum': 3, 'artiﬁcial': 3, 'doi': 3, 'man': 3, 'mann': 3, 'annotations': 3, 'cheng': 3, 'universal': 3, 'pro': 3, 'lj': 3, 'database': 3, 'textual': 3, 'lan': 3, 'tang': 3, 'hays': 3, 'confer': 3, 'ence': 3, 'ren': 3, 'residual': 3, 'ieeecvf': 3, 'krishnan': 3, 'jaderberg': 3, 'eccv': 3, 'vedaldi': 3, 'singh': 3, 'memes': 3, 'jiang': 3, 'crossmodal': 3, 'yin': 3, 'multitask': 3, 'laptev': 3, 'sivic': 3, 'schmidt': 3, 'algorithms': 3, 'you': 3, 'alec': 2, 'kim': 2, 'sastry': 2, 'krueger': 2, 'abstract': 2, 'dict': 2, 'needed': 2, 'cept': 2, 'raw': 2, 'alternative': 2, '400': 2, 'collected': 2, 'inter': 2, 'ocr': 2, 'ﬁnegrained': 2, 'match': 2, '128': 2, 'release': 2, 'introduction': 2, 'dai': 2, 'peters': 2, 'howard': 2, 'ruder': 2, 'devlin': 2, 'texttotext': 2, 'standardized': 2, 'contribution': 2, 'pmlr': 2, '2021': 2, 'interface': 2, 'mccann': 2, 'enabled': 2, 'now': 2, 'suggest': 2, 'webscale': 2, 'practice': 2, 'pretrain': 2, 'individual': 2, 'adopting': 2, 'desai': 2, 'icmlm': 2, 'bulent': 2, 'sariyildiz': 2, 'transformerbased': 2, 'masked': 2, 'objectives': 2, 'aforementioned': 2, 'underperform': 2, 'fer': 2, 'resnext': 2, 'millions': 2, 'ac': 2, 'hundred': 2, 'behaviors': 2, 'simpliﬁed': 2, 'ext': 2, 'label': 2, 'car': 2, 'dog': 2, 'photo': 2, 'some': 2, 'recogni': 2, 'equivalent': 2, 'creating': 2, 'sufﬁciently': 2, 'mscoco': 2, 'krishna': 2, 'thomee': 2, 'high': 2, 'quality': 2, 'small': 2, 'varying': 2, 'generated': 2, 'titles': 2, 'contain': 2, 'size': 2, 'major': 2, 'cover': 2, 'broad': 2, 'construction': 2, 'includes': 2, 'total': 2, '22': 2, 'initial': 2, 'cnn': 2, 'ties': 2, 'show': 2, 'already': 2, 'times': 2, 'slower': 2, 'predicts': 2, 'wikipedia': 2, 'mutual': 2, 'church': 2, 'hanks': 2, '1990': 2, 'wordnet': 2, '1995': 2, 'relatively': 2, 'weak': 2, 'here': 2, 'proxy': 2, 'rate': 2, 'ity': 2, 'similarity': 2, 'symmetric': 2, 'introduced': 2, 'multiclass': 2, 'npair': 2, 'sohn': 2, 'domain': 2, 'medical': 2, 'concern': 2, 'remove': 2, 'projection': 2, 'minibatch': 2, 'proj': 2, 'embed': 2, 'temperature': 2, 'logits': 2, 'loss_i': 2, 'cross_entropy_losslogits': 2, 'sentence': 2, 'formation': 2, 'random': 2, 'hyperparameter': 2, 'make': 2, 'modiﬁcations': 2, 'improvements': 2, 'where': 2, 'experiment': 2, 'follow': 2, 'implementation': 2, 'combined': 2, 'initialization': 2, 'vaswani': 2, 'sennrich': 2, 'sequence': 2, 'eos': 2, 'add': 2, 'future': 2, 'allocating': 2, 'addi': 2, 'equally': 2, 'increase': 2, 'rn50x64': 2, 'vitl14': 2, 'took': 2, '592': 2, 'gpus': 2, 'boost': 2, 'vitl14336px': 2, 'speciﬁed': 2, 'full': 2, '115': 2, 'table': 2, 'ﬁcation': 2, 'opposed': 2, 'providing': 2, 'help': 2, 'focuses': 2, 'investigate': 2, 'common': 2, 'protocol': 2, 'manner': 2, 'despite': 2, 'none': 2, 'additionally': 2, 'top5': 2, 'inceptionv4': 2, 'strong': 2, 'ﬂexible': 2, 'direct': 2, 'because': 2, 'δ': 2, 'generalization': 2, 'zerodata': 2, 'spe': 2, 'primarily': 2, 'benchmarks': 2, 'guide': 2, 'rather': 2, 'comprehensive': 2, 'detailed': 2, 'ﬁtting': 2, 'gression': 2, 'limited': 2, 'underperforms': 2, 'cifar10': 2, 'kinetics700': 2, 'ucf101': 2, 'notably': 2, 'satellite': 2, 'eurosat': 2, 'lymph': 2, 'node': 2, 'tumor': 2, 'counting': 2, 'dis': 2, 'room': 2, 'whether': 2, 'difﬁcult': 2, 'learner': 2, 'tasklearning': 2, '45': 2, '60': 2, '4shot': 2, 'gray': 2, 'lines': 2, 'oneshot': 2, 'ference': 2, 'able': 2, 'exploit': 2, 'resnet152x2': 2, 'would': 2, 'even': 2, 'performs': 2, 'somewhat': 2, 'section': 2, '33': 2, 'student': 2, 'efﬁcientnetl2': 2, 'minimal': 2, 'please': 2, 'raise': 2, 'very': 2, '26': 2, 'endtoend': 2, '2019s': 2, 'address': 2, 'measure': 2, 'rep': 2, 'stallkamp': 2, 'terms': 2, 'increases': 2, '34': 2, 'repeatedly': 2, 'dodge': 2, 'karam': 2, 'geirhos': 2, 'testing': 2, '102': 2, 'forwardpass': 2, 'gflopsimage': 2, 'instagrampretrained': 2, 'byol': 2, 'moco': 2, 'efﬁcientnet': 2, 'grill': 2, '2016b': 2, 'averaged': 2, 'right': 2, 'distributions': 2, 'ﬁnetuned': 2, 'barbu': 2, 'outofdistribution': 2, 'techniques': 2, 'yogatama': 2, 'linzen': 2, 'true': 2, '90': 2, 'ideal': 2, 'objectnet': 2, 'bootstrap': 2, 'visualizing': 2, 'validation': 2, 'out': 2, 'ment': 2, 'biases': 2, 'fairface': 2, 'karkkainen': 2, 'joo': 2, 'egregious': 2, 'impact': 2, 'thoughtful': 2, 'does': 2, 'may': 2, 'explore': 2, 'celeba': 2, 'wild': 2, 'production': 2, 'poses': 2, 'limitations': 2, 'just': 2, 'hardware': 2, 'computational': 2, 'oliver': 2, 'issue': 2, 'report': 2, 'collection': 2, 'main': 2, 'undeniably': 2, 'back': 2, 'top': 2, 'mori': 2, '1999': 2, 'based': 2, '2007': 2, 'weight': 2, 'srivastava': 2, 'salakhutdinov': 2, 'boltzmann': 2, 'investigated': 2, '2005': 2, 'sults': 2, 'anything': 2, 'weblysupervised': 2, 'divvala': 2, '2013a': 2, 'connecting': 2, 'frome': 2, 'elhoseiny': 2, 'lei': 2, 'ba': 2, 'beyond': 2, 'hermann': 2, 'bansal': 2, 'question': 2, 'openai': 2, 'error': 2, 'solaiman': 2, 'brundage': 2, 'software': 2, 'project': 2, 'numpy': 2, 'ftfy': 2, 'speer': 2, 'abadi': 2, 'pytorch': 2, 'paszke': 2, 'team': 2, 'scikitlearn': 2, 'davis': 2, 'dean': 2, '12th': 2, 'smaira': 2, 'gong': 2, 'pose': 2, 'bowker': 2, 'httpsdoiorg10': 2, 'chang': 2, 'classiﬁca': 2, 'press': 2, 'kaplan': 2, 'buolamwini': 2, 'gebru': 2, 'commercial': 2, 'swersky': 2, 'fan': 2, 'improved': 2, 'gan': 2, 'han': 2, 'remote': 2, 'sensing': 2, 'proceed': 2, 'ings': 2, 'httpswww': 2, 'coates': 2, 'nips': 2, 'ali': 2, 'dong': 2, 'fei': 2, 'hierarchical': 2, 'berg': 2, 'su': 2, 'bert': 2, 'guage': 2, 'beyer': 2, 'gelly': 2, 'perona': 2, 'shlens': 2, 'bengio': 2, 'semantic': 2, 'infor': 2, 'mation': 2, 'zhu': 2, 'making': 2, 'cvpr': 2, 'erhan': 2, 'google': 2, 'millman': 2, 'walt': 2, 'gommers': 2, 'cournapeau': 2, 'kern': 2, 'brett': 2, 'peterson': 2, 'reddy': 2, 'weckesser': 2, 'oliphant': 2, 'nature': 2, 'im2gps': 2, '770778': 2, 'borth': 2, 'land': 2, 'coding': 2, 'hendrycks': 2, 'units': 2, 'song': 2, 'hill': 2, '3d': 2, 'narang': 2, 'zhou': 2, 'weyand': 2, 'ioffe': 2, 'simonyan': 2, 'zitnick': 2, 'jabri': 2, 'amodei': 2, 'humancomputer': 2, 'interaction': 2, 'hateful': 2, 'puigcerver': 2, 'shamma': 2, 'face': 2, 'attribute': 2, 'lecun': 2, 'digits': 2, 'fang': 2, 'accelerate': 2, 'linguistic': 2, 'batra': 2, 'parikh': 2, 'xiong': 2, 'jawahar': 2, 'geolo': 2, 'group': 2, 'event': 2, 'good': 2, 'cessing': 2, 'massa': 2, 'bai': 2, 'vanderplas': 2, 'python': 2, 'empirical': 2, 'salimans': 2, 'shazeer': 2, 'shankar': 2, 'shah': 2, 'rethinking': 2, 'isola': 2, 'workshops': 2, 'jones': 2, 'jong': 1, 'wook': 1, 'chris': 1, 'hallacy': 1, 'aditya': 1, 'ramesh': 1, 'gabriel': 1, 'goh': 1, 'sandhini': 1, 'agarwal': 1, 'girish': 1, 'amanda': 1, 'pamela': 1, 'mishkin': 1, 'jack': 1, 'gretchen': 1, 'ilya': 1, 'ﬁxed': 1, 'predetermined': 1, 'restricted': 1, 'generality': 1, 'usability': 1, 'ages': 1, 'promising': 1, 'leverages': 1, 'source': 1, 'demon': 1, 'strate': 1, 'goes': 1, 'net': 1, 'reference': 1, 'describe': 1, 'ones': 1, 'enabling': 1, 'spanning': 1, 'types': 1, 'transfers': 1, 'non': 1, 'trivially': 1, 'instance': 1, 'needing': 1, 'code': 1, 'httpsgithubcomopenaiclip': 1, 'motivating': 1, 'revolutionized': 1, 'last': 1, 'few': 1, 'inputoutput': 1, 'equal': 1, '1openai': 1, 'san': 1, 'francisco': 1, 'ca': 1, '94110': 1, 'usa': 1, 'correspondence': 1, 'jongwookopenaicom': 1, '38': 1, 'th': 1, '139': 1, 'copyright': 1, 'authors': 1, 'flagship': 1, 'gpt3': 1, 'bespoke': 1, 'requiring': 1, 'little': 1, 'aggregate': 1, 'acces': 1, 'sible': 1, 'col': 1, 'lections': 1, 'surpasses': 1, 'highquality': 1, 'ﬁelds': 1, 'result': 1, 'breakthrough': 1, 'encouraging': 1, 'petitive': 1, 'tended': 1, 'phrase': 1, 'ad': 1, 'dition': 1, 'classiﬁ': 1, 'approaches': 1, 'po': 1, 'tential': 1, 'current': 1, 'crucial': 1, 'difference': 1, 'accelerator': 1, 'billions': 1, 'celerator': 1, 'thousand': 1, 'close': 1, 'demonstrate': 1, 'call': 1, 'create': 1, 'plane': 1, 'bird': 1, '1summary': 1, 'extractor': 1, 'trains': 1, 'correct': 1, 'time': 1, 'synthesizes': 1, 'target': 1, 'computationally': 1, 'perception': 1, 'contained': 1, 'following': 1, 'subsections': 1, 'detail': 1, '21': 1, 'mainly': 1, 'standards': 1, '100000': 1, 'billion': 1, 'instagram': 1, 'metadata': 1, 'sparse': 1, 'automati': 1, 'cally': 1, 'ﬁlenames': 1, '20160716': 1, '113957jpg': 1, 'camera': 1, 'exposure': 1, 'settings': 1, 'ﬁltering': 1, 'keep': 1, 'andor': 1, 'shrunk': 1, 'factor': 1, 'motivation': 1, 'quantities': 1, 'constructed': 1, 'sources': 1, 'attempt': 1, 'part': 1, 'process': 1, 'whose': 1, '500000': 1, 'queries': 1, 'balance': 1, '20000': 1, 'resulting': 1, 'count': 1, 'webtext': 1, 'gpt2': 1, 'refer': 1, 'webimagetext': 1, 'selecting': 1, 'encountered': 1, 'difﬁcul': 1, 'efﬁciently': 1, '63': 1, 'twice': 1, 'recognize': 1, 'outperform': 1, 'noting': 1, '1the': 1, 'occurring': 1, 'augmented': 1, 'bigrams': 1, 'pointwise': 1, 'articles': 1, 'synsets': 1, 'added': 1, '2m': 1, '33m': 1, '67m': 1, '134m': 1, '268m': 1, '400m': 1, 'processed': 1, '40zeroshot': 1, 'efficiency4x': 1, 'efficiency': 1, '2clip': 1, 'highly': 1, 'expressive': 1, 'bow': 1, 'swapping': 1, 'solve': 1, 'potentially': 1, 'eas': 1, 'ier': 1, 'whole': 1, 'exact': 1, 'starting': 1, 'swapped': 1, 'actually': 1, 'occurred': 1, 'maximize': 1, 'real': 1, 'minimizing': 1, 'n2': 1, 'incorrect': 1, 'cross': 1, 'entropy': 1, 'imple': 1, 'mentation': 1, 'technique': 1, 'imaging': 1, 'overﬁtting': 1, 'initializing': 1, 'nonlinear': 1, 'tween': 1, 'map': 1, 'coders': 1, 'image_encoder': 1, 'text_encoder': 1, 'cbow': 1, 'tn': 1, 'texts': 1, 'w_id_i': 1, 'w_td_t': 1, 'extract': 1, 'modality': 1, 'i_f': 1, 'image_encoderi': 1, 'd_i': 1, 't_f': 1, 'text_encodert': 1, 'd_t': 1, 'i_e': 1, 'l2_normalizenpdoti_f': 1, 'w_i': 1, 't_e': 1, 'l2_normalizenpdott_f': 1, 'w_t': 1, 'pairwise': 1, 'similarities': 1, 'npdoti_e': 1, 't_et': 1, 'npexpt': 1, 'nparangen': 1, 'axis0': 1, 'loss_t': 1, 'loss_t2': 1, '3numpylike': 1, 'implementa': 1, 'transformation': 1, 'functiontu': 1, 'samples': 1, 'uniform': 1, 'simplify': 1, 'tv': 1, 'square': 1, 'crop': 1, 'resized': 1, 'augmentation': 1, 'controls': 1, 'range': 1, 'softmax': 1, 'τ': 1, 'optimized': 1, 'logparameterized': 1, 'multiplicative': 1, 'scalar': 1, 'avoid': 1, 'turning': 1, '23': 1, 'consider': 1, 'coder': 1, 'widespread': 1, 'adoption': 1, 'proven': 1, 'sev': 1, 'eral': 1, 'resnetd': 1, 'antialiased': 1, 'rect2': 1, 'blur': 1, 'replace': 1, 'mechanism': 1, 'implemented': 1, 'sin': 1, 'gle': 1, 'transformerstyle': 1, 'multihead': 1, 'qkv': 1, 'conditioned': 1, 'averagepooled': 1, 'second': 1, 'closely': 1, 'minor': 1, 'modiﬁcation': 1, 'adding': 1, 'patch': 1, 'position': 1, 'before': 1, 'scheme': 1, '12layer': 1, '512wide': 1, 'heads': 1, 'operates': 1, 'lowercased': 1, 'byte': 1, 'bpe': 1, 'bracketed': 1, 'sos': 1, 'tokens': 1, 'activations': 1, 'highest': 1, 'token': 1, 'normalized': 1, 'linearly': 1, 'projected': 1, 'selfattention': 1, 'preserve': 1, 'auxiliary': 1, 'though': 1, 'exploration': 1, 'isolation': 1, 'encoders': 1, 'adapt': 1, 'dimension': 1, 'variant': 1, 'allocates': 1, 'proportional': 1, 'calculated': 1, 'less': 1, 'sensitive': 1, '24': 1, 'series': 1, 'efﬁcientnetstyle': 1, '16x': 1, '64x': 1, 'denoted': 1, 'rn50x4': 1, 'rn50x16': 1, 'respectively': 1, 'vitb32': 1, 'vitb16': 1, '256': 1, '336': 1, 'pixel': 1, 'epoch': 1, 'fixres': 1, 'denote': 1, 'unless': 1, 'otherwise': 1, 'hyperparameters': 1, 'if': 1, 'snip': 1, 'pet': 1, 'together': 1, 'apply': 1, 'down': 1, 'stream': 1, 'reuse': 1, 'ayahoo': 1, '724': 1, '230': 1, '984': 1, '585': 1, '1comparing': 1, 'reﬂects': 1, 'itsrepresentation': 1, 'learningcapability': 1, 'probable': 1, 'according': 1, 'tionally': 1, 'prompts': 1, 'ensembling': 1, 'multiple': 1, 'templates': 1, 'vast': 1, 'majority': 1, '31': 1, 'aware': 1, 'proof': 1, 'exam': 1, 'ples': 1, 'noticeably': 1, 'matching': 1, 'abil': 1, 'suggests': 1, 'signiﬁ': 1, 'cant': 1, 'step': 1, 'practical': 1, 'controlled': 1, 'closer': 1, 'vi': 1, 'sual': 1, 'matched': 1, 'gpu': 1, 'day': 1, 'initialized': 1, 'vs': 1, 'eurosat371': 1, 'distance340': 1, 'patchcamelyon195': 1, 'gtsrb184': 1, 'clevrcounts182': 1, 'dtd166': 1, 'flowers102125': 1, 'resisc45119': 1, 'fgvcaircraft113': 1, 'mnist100': 1, 'birdsnap32': 1, '05pascalvoc2007': 1, '11oxfordpets': 1, '19imagenet': 1, '20caltech101': 1, '28fer2013': 1, '30stl10': 1, '30cifar100': 1, '39cifar10': 1, '67hatefulmemes': 1, '77ucf101': 1, '78sun397': 1, '124sst2': 1, '145kinetics700': 1, '225food101': 1, '232country211': 1, '289stanfordcars': 1, '4zeroshot': 1, 'super': 1, 'vised': 1, 'ﬁtted': 1, 'usually': 1, 'refers': 1, 'generalizing': 1, 'term': 1, 'sense': 1, 'un': 1, 'seen': 1, 'aspired': 1, 'view': 1, 'evaluates': 1, 'ciﬁc': 1, 'popular': 1, 'created': 1, 'community': 1, 'generic': 1, 'conduct': 1, 'implement': 1, 'larger': 1, 'expand': 1, 'contextualize': 1, 'start': 1, 'look': 1, 'clas': 1, 'siﬁers': 1, 'offtheshelf': 1, 'regularized': 1, 'canonical': 1, 'wins': 1, 'proves': 1, 'stl10': 1, 'courage': 1, 'containing': 1, 'number': 1, 'achieves': 1, '993': 1, 'appears': 1, 'observe': 1, 'spread': 1, 'stanford': 1, 'cars': 1, 'food101': 1, 'flowers102': 1, 'fgvcaircraft': 1, 'suspect': 1, 'amounts': 1, 'pertask': 1, 'ob': 1, 'ject': 1, 'pascalvoc2007': 1, 'slight': 1, 'advantage': 1, 'sig': 1, 'niﬁcantly': 1, 'measur': 1, '145': 1, 'resnet50s': 1, '77': 1, 'speculate': 1, 'involving': 1, 'verbs': 1, 'nouncentric': 1, 'looking': 1, 'quite': 1, 'cialized': 1, 'resisc45': 1, 'patchcamelyon': 1, 'synthetic': 1, 'scenes': 1, 'clevrcounts': 1, 'selfdriving': 1, 'gtsrb': 1, 'recognizing': 1, 'tance': 1, 'nearest': 1, 'distance': 1, 'highlight': 1, 'poor': 1, 'nonexpert': 1, 'humans': 1, 'robustly': 1, 'suggesting': 1, 'caution': 1, 'unclear': 1, 'meaningful': 1, 'experience': 1, 'mans': 1, 'possibly': 1, 'contextualizes': 1, 'limit': 1, 'visualize': 1, 'compares': 1, '75average': 1, '5zeroshot': 1, 'probes': 1, 'nearly': 1, 'highlighted': 1, 'light': 1, 'pub': 1, 'licly': 1, 'itself': 1, 'might': 1, 'expect': 1, 'likely': 1, 'key': 1, 'dif': 1, 'first': 1, 'communicated': 1, 'normal': 1, 'must': 1, 'infer': 1, 'indirectly': 1, 'contextless': 1, 'examplebased': 1, 'drawback': 1, 'hypotheses': 1, 'consistent': 1, 'especially': 1, 'case': 1, 'tains': 1, 'capable': 1, 'cues': 1, 'heuristics': 1, 'assuming': 1, 'primary': 1, 'guarantee': 1, 'roughly': 1, 'fea': 1, 'tures': 1, 'bitl': 1, 'jft300m': 1, 'released': 1, 'surprising': 1, 'analyzed': 1, 'focused': 1, 'capa': 1, 'bilities': 1, 'mon': 1, 'requires': 1, 'tuning': 1, 'ized': 1, 'procedures': 1, 'summarizes': 1, 'minimize': 1, 'effects': 1, 'conﬁrmation': 1, 'reporting': 1, 'evalua': 1, 'stu': 1, 'dent': 1, 'bud': 1, 'get': 1, 'replicate': 1, 'convnets': 1, 'previously': 1, 'optical': 1, 'character': 1, 'emo': 1, 'measured': 1, 'argued': 1, 'uation': 1, 'appendix': 1, 'resenting': 1, 'signs': 1, 'vtab': 1, 'beneﬁts': 1, 'clear': 1, 'regardless': 1, 'outper': 1, 'announced': 1, 'ceeded': 1, 'subsequent': 1, 'mistakes': 1, 'corn': 1, 'lower': 1, '90average': 1, 'als': 1, '85average': 1, 'clipvit': 1, 'clipresnet': 1, 'efficientnetnoisystudent': 1, 'efficientnet': 1, 'bits': 1, '6linear': 1, 'bit': 1, 'dotted': 1, 'indicate': 1, 'higherresolution': 1, 'cent': 1, 'moving': 1, 'quantifying': 1, 'change': 1, 'predictably': 1, 'modeled': 1, 'logittransformed': 1, 'propose': 1, 'distinguish': 1, 'measures': 1, 'what': 1, 'predicted': 1, 'documented': 1, 'relationship': 1, 'indistribution': 1, 'captures': 1, 'argue': 1, 'aim': 1, 'prove': 1, 'adapting': 1, 'cause': 1, 'intuitively': 1, 'spurious': 1, 'correlations': 1, 'patterns': 1, 'hold': 1, 'thus': 1, 'exhibit': 1, 'reduce': 1, 'trace': 1, 'completely': 1, 'distinct': 1, 'frontier': 1, '204': 1, 'reorientation': 1, 'advocated': 1, 'promotes': 1, 'provides': 1, 'accurate': 1, 'assessment': 1, 'subsampled': 1, '100average': 1, 'exisiting': 1, 'imagenetv2': 1, 'imageneta': 1, 'imagenetr': 1, 'sketch': 1, '643': 1, '701': 1, '771': 1, '377': 1, '889': 1, '326': 1, '723': 1, '252': 1, '602': 1, '58': 1, '744': 1, '512': 1, '397': 1, '350': 1, '7zeroshot': 1, 'dashed': 1, 'shrink': 1, 'ﬁts': 1, 'logit': 1, 'transformed': 1, 'values': 1, 'shown': 1, 'estimated': 1, 'conﬁdence': 1, 'intervals': 1, 'bananas': 1, 'shared': 1, 'unintentional': 1, 'evals': 1, 'ducted': 1, 'deduplication': 1, '9': 1, 'median': 1, 'rarely': 1, 'shifted': 1, '01': 1, 'threshold': 1, 'statistically': 1, 'bonferroni': 1, 'correction': 1, 'max': 1, '06': 1, 'birdsnap': 1, 'echos': 1, 'duplicate': 1, 'rates': 1, 'changes': 1, 'moves': 1, 'heavily': 1, 'inﬂuences': 1, 'formance': 1, 'example': 1, 'handful': 1, 'criminal': 1, 'animal': 1, 'tends': 1, 'classify': 1, 'aged': 1, '020': 1, 'category': 1, '323': 1, 'behaviour': 1, 'drops': 1, '87': 1, 'discrepancies': 1, 'peo': 1, 'ple': 1, 'categorized': 1, 'crime': 1, 'nonhuman': 1, 'highlighting': 1, 'disparate': 1, 'extreme': 1, 'care': 1, 'taken': 1, 'unlock': 1, 'niche': 1, 'greater': 1, 'ease': 1, 'privacy': 1, 'risks': 1, 'identiﬁcation': 1, 'candidates': 1, '433': 1, '1000': 1, 'choices': 1, 'noteworthy': 1, 'achieve': 1, 'widely': 1, 'level': 1, 'supplemental': 1, 'materials': 1, 'hope': 1, 'motivates': 1, 'characterization': 1, 'shortcomings': 1, 'compet': 1, 'itive': 1, 'below': 1, 'es': 1, 'timate': 1, 'around': 1, '1000x': 1, 'required': 1, 'reach': 1, 'infeasible': 1, 'cur': 1, 'rent': 1, 'upon': 1, 'will': 1, 'necessary': 1, 'emphasis': 1, 'queried': 1, 'sets': 1, 'develop': 1, 'unrealistic': 1, 'scenarios': 1, 'raised': 1, 'haphazard': 1, 'co': 1, 'evaluate': 1, 'capabili': 1, 'emphasize': 1, 'specifying': 1, 'nat': 1, 'ural': 1, 'actual': 1, 'useful': 1, 'fall': 1, 'counterintuitive': 1, 'drop': 1, 'transitioning': 1, 'means': 1, 'behavior': 1, 'ago': 1, 'content': 1, 'retrieval': 1, 'nouns': 1, 'adjectives': 1, 'quat': 1, 'toni': 1, 'manifold': 1, 'lowlevel': 1, 'tag': 1, 'inspiring': 1, 'collections': 1, 'commonly': 1, 'webly': 1, 'demonstrating': 1, 'puter': 1, 'treating': 1, 'engine': 1, 'every': 1, 'thing': 1, 'ambi': 1, 'goal': 1, 'developments': 1, 'essential': 1, 'proved': 1, 'generating': 1, 'dates': 1, 'standing': 1, 'inforcement': 1, 'burst': 1, 'those': 1, 'conclusion': 1, 'success': 1, 'formula': 1, 'emerging': 1, 'discuss': 1, 'implications': 1, 'leveraged': 1, 'prompting': 1, 'enable': 1, 'sufﬁcient': 1, 'acknowledgments': 1, 'involved': 1, 'susan': 1, 'her': 1, 'conditional': 1, 'ishaan': 1, 'gulrajani': 1, 'catching': 1, 'irene': 1, 'miles': 1, 'gillian': 1, 'hadﬁeld': 1, 'feedback': 1, 'grateful': 1, 'acceleration': 1, 'supercomputing': 1, 'teams': 1, 'critical': 1, 'infrastructure': 1, 'developers': 1, 'packages': 1, 'throughout': 1, 'tensor': 1, 'flow': 1, 'references': 1, 'barham': 1, 'devin': 1, 'ghemawat': 1, 'irving': 1, 'isard': 1, 'tensorﬂow': 1, 'usenixsymposium': 1, 'operating': 1, 'osdi16': 1, '265283': 1, 'recasens': 1, 'schneider': 1, 'arandjelovic': 1, 'ramapuram': 1, 'de': 1, 'fauw': 1, 'dieleman': 1, 'versatile': 1, 'arxiv200616228': 1, 'alcorn': 1, 'mai': 1, 'ku': 1, 'nguyen': 1, 'strike': 1, 'easily': 1, 'fooled': 1, 'strange': 1, 'familiar': 1, '48454854': 1, 'assiri': 1, 'stochastic': 1, 'optimization': 1, 'plain': 1, 'arxiv200108856': 1, 'mayo': 1, 'alverio': 1, 'gut': 1, 'freund': 1, 'katz': 1, 'biascontrolled': 1, 'pushing': 1, 'lim': 1, '94539463': 1, 'bechmann': 1, 'name': 1, 'hidden': 1, 'layers': 1, 'media': 1, 'society': 1, '61205395171881956': 1, 'january': 1, '101177': 1, '2053951718819569': 1, '11772053951718819569': 1, 'blaise': 1, 'aguera': 1, 'arcas': 1, 'todorov': 1, 'physiognomys': 1, 'clothes': 1, 'httpsmediumcomblaisea': 1, 'physiognomysnewclothesf2d4b59fdd6a': 1, 'bolukbasi': 1, 'kw': 1, 'zou': 1, 'saligrama': 1, 'kalai': 1, 'programmer': 1, 'woman': 1, 'homemaker': 1, 'debiasing': 1, '2943494357': 1, 'star': 1, 'sorting': 1, 'things': 1, 'consequences': 1, 'mit': 1, '2000': 1, 'ryder': 1, 'subbiah': 1, 'dhariwal': 1, 'neelakantan': 1, 'shyam': 1, 'arxiv200514165': 1, 'browne': 1, 'dark': 1, 'matters': 1, 'blackness': 1, 'duke': 1, 'university': 1, 'perez': 1, 'larlus': 1, 'eprints': 1, 'arxiv2008': 1, 'shades': 1, 'intersec': 1, 'disparities': 1, 'fairness': 1, 'accountability': 1, 'transparency': 1, '7791': 1, 'carreira': 1, 'noland': 1, 'hillier': 1, 'short': 1, 'note': 1, 'datasetarxiv': 1, 'arxiv190706987': 1, 'norouzi': 1, 'hinton': 1, 'semi': 1, 'arxiv200610029': 1, 'baselines': 1, 'momentum': 1, 'arxiv200304297': 1, 'kholy': 1, 'ahmed': 1, 'uniter': 1, 'arxiv190911740': 1, 'state': 1, 'art': 1, '1051018651883': 1, 'association': 1, 'norms': 1, 'lexicography': 1, 'linguistics': 1, '1612229': 1, 'aclweborganthologyj901003': 1, 'ceedings': 1, 'fourteenth': 1, 'statistics': 1, '215223': 1, 'crawford': 1, 'trouble': 1, 'keynote': 1, 'httpswwwyoutubecom': 1, 'watchvfmym_bkwqzk': 1, '30793087': 1, 'damour': 1, 'heller': 1, 'moldovan': 1, 'adlam': 1, 'panahi': 1, 'beutel': 1, 'deaton': 1, 'eisenstein': 1, 'hoffman': 1, 'underspeciﬁcation': 1, 'presents': 1, 'credibility': 1, 'arxiv201103395': 1, 'cvpr09': 1, 'satheesh': 1, 'khosla': 1, 'ilsvrc': 1, 'httpwww': 1, 'imagenetorgchallengeslsvrc2012': 1, 'resentations': 1, 'arxiv200606666': 1, 'mw': 1, 'toutanova': 1, 'bidirectional': 1, 'arxiv181004805': 1, 'farhadi': 1, 'guestrin': 1, 'everything': 1, '3270': 1, '3277': 1, 'distortions': 1, '26th': 1, 'communication': 1, 'icccn': 1, 'weissenborn': 1, 'unterthiner': 1, 'dehghani': 1, 'minderer': 1, 'heigold': 1, 'worth': 1, '16x16': 1, 'scalearxiv': 1, 'arxiv201011929': 1, 'saleh': 1, 'elgammal': 1, 'write': 1, 'ﬁer': 1, 'purely': 1, '25842591': 1, 'googles': 1, 'tenth': 1, 'iccv05': 1, '18161823': 1, 'corrado': 1, 'ranzato': 1, 'mikolov': 1, 'devise': 1, '21212129': 1, 'adversarial': 1, 'visionandlanguage': 1, 'arxiv200606195': 1, 'gao': 1, 'fisch': 1, 'arxiv201215723': 1, 'garvie': 1, 'flawedfacedatacom': 1, 'geiger': 1, 'lenz': 1, 'urtasun': 1, 'ready': 1, 'autonomous': 1, 'driving': 1, 'rubisch': 1, 'michaelis': 1, 'bethge': 1, 'wich': 1, 'brendel': 1, 'imagenettrained': 1, 'biased': 1, 'texture': 1, 'shape': 1, 'curacy': 1, 'arxiv181112231': 1, 'goodfellow': 1, 'carrier': 1, 'courville': 1, 'mirza': 1, 'hamner': 1, 'cukierski': 1, 'thaler': 1, 'dh': 1, 'contests': 1, '645963': 1, 'cloud': 1, 'api': 1, 'httpscloudgooglecomvisiondocs': 1, 'celebrityrecognition': 1, 'strub': 1, 'altch': 1, 'tallec': 1, 'richemond': 1, 'buchatskaya': 1, 'doersch': 1, 'pires': 1, 'guo': 1, 'azar': 1, 'your': 1, 'latent': 1, 'arxiv200607733': 1, 'wieser': 1, 'taylor': 1, 'smith': 1, 'picus': 1, 'hoyer': 1, 'kerkwijk': 1, 'haldane': 1, 'fernandez': 1, 'del': 1, 'rıo': 1, 'wiebe': 1, 'erardmarchant': 1, 'sheppard': 1, 'abbasi': 1, 'gohlke': 1, 'array': 1, 'programming': 1, '585357362': 1, '101038': 1, 's4158602026492': 1, 'efros': 1, 'estimating': 1, 'geographic': 1, 'delving': 1, 'rectiﬁers': 1, 'surpassing': 1, 'humanlevel': 1, 'national': 1, '10261034': 1, 'mo': 1, 'mentum': 1, '9729': 1, '9738': 1, 'tricks': 1, '558': 1, '567': 1, 'helber': 1, 'bischke': 1, 'dengel': 1, 'novel': 1, 'se': 1, 'lected': 1, 'topics': 1, 'applied': 1, 'earth': 1, 'observations': 1, '12722172226': 1, 'henaff': 1, 'dataefﬁcient': 1, 'ma': 1, 'chine': 1, '41824192': 1, 'gimpel': 1, 'gaussian': 1, 'gelus': 1, 'arxiv160608415': 1, 'wallace': 1, 'dziedzic': 1, 'outof': 1, 'arxiv200406100': 1, 'green': 1, 'faulkner': 1, 'soyer': 1, 'szepesvari': 1, 'czarnecki': 1, 'teplyashin': 1, 'grounded': 1, 'simulated': 1, 'world': 1, 'arxiv170606551': 1, 'hestness': 1, 'ardalani': 1, 'diamos': 1, 'jun': 1, 'kianinejad': 1, 'patwary': 1, 'predictable': 1, 'empiricallyarxiv': 1, 'arxiv171200409': 1, 'hongsuck': 1, 'seo': 1, 'sim': 1, 'cplanet': 1, 'enhancing': 1, 'combinatorial': 1, 'parti': 1, 'tioning': 1, 'maps': 1, '536551': 1, 'ﬁnetuning': 1, 'arxiv180106146': 1, 'accelerating': 1, 'network': 1, 'reducing': 1, 'internal': 1, 'covariate': 1, 'arxiv150203167': 1, 'structured': 1, 'output': 1, 'unconstrained': 1, 'arxiv14125903': 1, 'spatial': 1, '2820172025': 1, 'hariharan': 1, 'lawrence': 1, 'clevr': 1, 'diag': 1, 'nostic': 1, 'compositional': 1, 'elementary': 1, 'reasoning': 1, '29012910': 1, 'vasilache': 1, '6784': 1, 'kalfaoglu': 1, 'kalkan': 1, 'alatan': 1, 'late': 1, 'temporal': 1, 'arxiv200801232': 1, 'mccandlish': 1, 'henighan': 1, 'chess': 1, 'laws': 1, 'arxiv200108361': 1, 'keyes': 1, 'misgendering': 1, 'transhci': 1, 'implica': 1, 'tions': 1, 'automatic': 1, '2cscw122': 1, 'kiela': 1, 'firooz': 1, 'mohan': 1, 'goswami': 1, 'ringshia': 1, 'testuggine': 1, 'challenge': 1, 'detecting': 1, 'hate': 1, 'speech': 1, 'arxiv200504790': 1, 'yung': 1, 'houlsby': 1, 'arxiv191211370': 1, '26612671': 1, 'groth': 1, 'hata': 1, 'kravitz': 1, 'kalantidis': 1, 'crowdsourced': 1, 'dense': 1, 'ternational': 1, '12313273': 1, 'balanced': 1, 'lake': 1, 'ullman': 1, 'gersh': 1, 'building': 1, 'think': 1, 'nickisch': 1, 'harmeling': 1, 'detect': 1, 'betweenclass': 1, '951958': 1, 'mnist': 1, 'handwritten': 1, 'httpyann': 1, 'comexdbmnist': 1, 'fidler': 1, '42474255': 1, '41834192': 1, 'duan': 1, 'unicodervl': 1, 'wei': 1, 'oscar': 1, 'semantics': 1, 'visionlanguage': 1, 'arxiv200406165': 1, 'ty': 1, 'maire': 1, 'belongie': 1, 'ra': 1, 'manan': 1, 'dollar': 1, 'microsoft': 1, 'coco': 1, 'context': 1, '740755': 1, 'progress': 1, 'humanlike': 1, 'arxiv200500955': 1, 'lippe': 1, 'holla': 1, 'chandra': 1, 'rajamanickam': 1, 'toniou': 1, 'shutova': 1, 'yannakoudakis': 1, 'mul': 1, 'timodal': 1, 'framework': 1, 'arxiv201212871': 1, 'celeb': 1, 'faces': 1, 'attributes': 1, 'retrieved': 1, 'august': 1, '201811': 1, 'vilbert': 1, 'visiolinguistic': 1, 'andlanguage': 1, '1323': 1, 'stroud': 1, 'ross': 1, 'leveraging': 1, 'httpswwwyoutube': 1, 'comwatchvkoqfxbpploet1390s': 1, 'paluri': 1, 'bharambe': 1, 'ploring': 1, '181196': 1, 'keskar': 1, 'decathlon': 1, 'ques': 1, 'arxiv180608730': 1, 'zhukov': 1, 'tapaswi': 1, 'howto100m': 1, 'textvideo': 1, 'em': 1, 'bedding': 1, 'watching': 1, 'narrated': 1, '26302640': 1, 'zisser': 1, 'rareact': 1, 'unusual': 1, 'interactions': 1, 'arxiv200801018': 1, 'represen': 1, 'tations': 1, 'uncurated': 1, 'instructional': 1, '98799889': 1, 'lexical': 1, 'munications': 1, '38113941': 1, 'krauth': 1, 'effect': 1, 'arxiv200414444': 1, 'mishra': 1, 'alahari': 1, 'priors': 1, 'takahashi': 1, 'oka': 1, 'imagetoword': 1, 'dividing': 1, 'vector': 1, 'quantizing': 1, 'citeseer': 1, 'mullerbudack': 1, 'pustuiren': 1, 'ewerth': 1, 'estimation': 1, '563579': 1, 'netzer': 1, 'bissacco': 1, 'reading': 1, 'noble': 1, 'u': 1, 'oppression': 1, 'engines': 1, 'reinforce': 1, 'racism': 1, 'nosek': 1, 'banaji': 1, 'greenwald': 1, 'harvest': 1, 'implicit': 1, 'attitudes': 1, 'beliefs': 1, 'demonstra': 1, 'site': 1, 'dynamics': 1, 'theory': 1, '61101': 1, '2002': 1, 'oh': 1, 'hoogs': 1, 'perera': 1, 'cuntoor': 1, 'cc': 1, 'mukherjee': 1, 'aggarwal': 1, '31533160': 1, 'odena': 1, 'cubuk': 1, 'fellow': 1, 'realistic': 1, '3132353246': 1, 'pandasdevpandas': 1, 'pan': 1, 'das': 1, 'february': 1, '5281zenodo3509134': 1, 'parkhi': 1, 'cats': 1, 'dogs': 1, 'inieee': 1, 'gross': 1, 'lerer': 1, 'bradbury': 1, 'chanan': 1, 'killeen': 1, 'gimelshein': 1, 'antiga': 1, 'desmaison': 1, 'kopf': 1, 'devito': 1, 'raison': 1, 'tejani': 1, 'chilamkurthy': 1, 'steiner': 1, 'chintala': 1, 'imperative': 1, 'style': 1, 'highperformance': 1, 'library': 1, 'wallach': 1, 'beygelzimer': 1, 'dalchebuc': 1, 'fox': 1, 'garnett': 1, 'eds': 1, '80248035': 1, 'varoquaux': 1, 'gramfort': 1, 'michel': 1, 'thirion': 1, 'grisel': 1, 'blondel': 1, 'prettenhofer': 1, 'weiss': 1, 'dubourg': 1, 'passos': 1, 'cour': 1, 'napeau': 1, 'brucher': 1, 'perrot': 1, 'duchesnay': 1, '1228252830': 1, 'pennington': 1, 'glove': 1, 'vectors': 1, 'emnlp': 1, '15321543': 1, 'neumann': 1, 'iyyer': 1, 'gardner': 1, 'zettlemoyer': 1, 'contextualized': 1, 'arxiv180205365': 1, 'qi': 1, 'cui': 1, 'bharti': 1, 'sacheti': 1, 'imagebert': 1, 'weaksupervised': 1, 'imagetext': 1, 'arxiv200107966': 1, 'quattoni': 1, 'collins': 1, 'darrell': 1, 'in2007': 1, 'narasimhan': 1, 'generative': 1, 'luan': 1, 'roberts': 1, 'matena': 1, 'exploring': 1, 'uniﬁed': 1, 'arxiv191010683': 1, 'raji': 1, 'mitchell': 1, 'denton': 1, 'saving': 1, 'investigating': 1, 'ethical': 1, 'auditing': 1, 'liang': 1, '905912': 1, 'roelofs': 1, 'agenet': 1, 'generalize': 1, 'arxiv190210811': 1, 'kingma': 1, 'reparameterization': 1, '901909': 1, 'scheuerman': 1, 'paul': 1, 'brubaker': 1, 'computers': 1, 'services': 1, '3cscw': 1, '133': 1, 'schwemmer': 1, 'knight': 1, 'bellopardo': 1, 'oklobdzija': 1, 'schoonvelde': 1, 'lockhart': 1, 'diagnosing': 1, 'socius': 1, '2378023120967171': 1, 'haddow': 1, 'birch': 1, 'translation': 1, 'rare': 1, 'subword': 1, 'arxiv150807909': 1, 'natarajan': 1, 'rohrbach': 1, 'vqa': 1, 'read': 1, '83178326': 1, 'ganjoo': 1, '935943': 1, 'perelygin': 1, 'chuang': 1, 'potts': 1, 'recursive': 1, 'compositionality': 1, 'sentiment': 1, 'treebank': 1, '16311642': 1, '2013b': 1, 'metric': 1, '18571865': 1, 'herbert': 1, 'oss': 1, 'kreps': 1, 'mccain': 1, 'newhouse': 1, 'blazakis': 1, 'mcgufﬁe': 1, 'strategies': 1, 'soomro': 1, 'zamir': 1, 'actions': 1, 'arxiv12120402': 1, 'zenodo': 1, 'httpsdoiorg': 1, '105281zenodo2591652': 1, 'schlipsing': 1, 'salmen': 1, 'igel': 1, 'multi': 1, 'competition': 1, '14531460': 1, 'vanhoucke': 1, 'alemi': 1, 'inceptionresnet': 1, 'connections': 1, 'arxiv160207261': 1, 'lxmert': 1, 'crossmodality': 1, 'transformersarxiv': 1, 'arxiv190807490': 1, 'arxiv190511946': 1, 'dave': 1, 'carlini': 1, 'tribution': 1, 'arxiv200700644': 1, 'friedland': 1, 'elizalde': 1, 'ni': 1, 'poland': 1, 'multimedia': 1, 'communications': 1, '5926473': 1, 'multiview': 1, 'arxiv190605849': 1, 'arxiv200311539': 1, 'douze': 1, 'egou': 1, 'fix': 1, 'traintest': 1, 'discrepancy': 1, '82528262': 1, 'varadarajan': 1, 'odobez': 1, 'jm': 1, 'topic': 1, 'abnormality': 1, 'iccv': 1, '13381345': 1, 'parmar': 1, 'uszkoreit': 1, 'gomez': 1, 'kaiser': 1, 'ł': 1, 'polosukhin': 1, 'atten': 1, '59986008': 1, 'veeling': 1, 'linmans': 1, 'winkens': 1, 'cohen': 1, 'welling': 1, 'rotation': 1, 'equivariant': 1, 'digital': 1, 'pathol': 1, 'ogy': 1, 'june': 1, 'haberland': 1, 'burovski': 1, 'bright': 1, 'wilson': 1, 'mayorov': 1, 'nelson': 1, 'larson': 1, 'carey': 1, 'polat': 1, 'feng': 1, 'moore': 1, 'laxalde': 1, 'perktold': 1, 'cimrman': 1, 'henriksen': 1, 'quintero': 1, 'archibald': 1, 'ribeiro': 1, 'mulbregt': 1, 'contributors': 1, 'fundamental': 1, 'scientiﬁc': 1, 'computing': 1, '17261272': 1, '101038s4159201906862': 1, 'jacobs': 1, 'revisiting': 1, 'era': 1, 'interna': 1, '26212630': 1, 'michael': 1, 'levy': 1, 'bowman': 1, 'glue': 1, 'anal': 1, 'ysis': 1, 'platform': 1, 'arxiv180407461': 1, 'xu': 1, 'boundary': 1, 'ward': 1, 'arbitraryshaped': 1, 'spotting': 1, 'aaai': 1, '1216012167': 1, 'kostrikov': 1, 'philbin': 1, 'planetphoto': 1, '3755': 1, 'kirillov': 1, 'lo': 1, 'wy': 1, 'gir': 1, 'shick': 1, 'detectron2': 1, 'httpsgithubcom': 1, 'facebookresearchdetectron2': 1, 'luong': 1, 'mt': 1, 'hovy': 1, 'selftraining': 1, '1068710698': 1, 'florencio': 1, 'tap': 1, 'textaware': 1, 'textvqa': 1, 'textcaption': 1, 'arxiv201204638': 1, 'dautume': 1, 'connor': 1, 'kocisky': 1, 'chrzanowski': 1, 'kong': 1, 'lazaridou': 1, 'ling': 1, 'dyer': 1, 'evaluating': 1, 'arxiv190111373': 1, 'ernievil': 1, 'enhanced': 1, 'graph': 1, 'arxiv200616934': 1, 'zeiler': 1, 'understand': 1, '818833': 1, 'ruyssen': 1, 'riquelme': 1, 'lucic': 1, 'djolonga': 1, 'pinto': 1, 'neu': 1, 'adaptation': 1, 'arxiv191004867': 1, 'shiftinvariant': 1, 'again': 1, 'arxiv190411486': 1, 'miura': 1, 'glotz': 1, 'repre': 1, 'sentations': 1, 'arxiv201000747': 1, 'zuboff': 1, 'capitalism': 1, 'prospects': 1, 'civilization': 1, 'technology': 1, '3017589': 1})}, {'file_name': 'clam', 'word_counts': Counter({'the': 935, 'of': 531, 'and': 465, 'to': 371, 'for': 349, 'a': 267, 'in': 246, 'we': 148, 'data': 142, 'on': 128, 'is': 124, 'are': 121, 'each': 114, 'from': 113, 'that': 113, 'as': 110, 'training': 104, 'with': 103, 'model': 100, 'slides': 99, 'clam': 99, 'using': 97, 'patches': 89, 'slide': 87, 'attention': 84, 'test': 82, '1': 82, '2': 77, 'by': 76, 'used': 74, 'be': 68, 'all': 68, 'set': 67, 'slidelevel': 65, 'models': 63, 'our': 63, 'auc': 62, 'tissue': 58, 'which': 58, 'were': 58, '0': 55, 'wsis': 52, 'can': 52, 'regions': 51, 'rcc': 50, 'positive': 50, 'learning': 48, 'or': 48, 'not': 48, 'class': 48, 'wsi': 46, 'cases': 46, '3': 44, '100': 44, 'nsclc': 43, 'classification': 42, 'an': 42, 'feature': 41, 'performance': 40, '10': 40, 'supervised': 39, 'at': 39, 'i': 39, 'prediction': 38, 'train': 38, 'trained': 38, 'fig': 37, 'et': 37, 'al': 37, 'negative': 36, 'subtyping': 35, 'pathology': 34, 'cancer': 34, 'weakly': 34, 'mil': 34, 'was': 34, 'when': 34, 'biomedical': 33, 'patch': 33, 'tumour': 33, 'also': 33, 'nature': 33, 'into': 33, 'scores': 33, 'dataset': 33, 'engineering': 32, 'this': 31, 'available': 31, 'independent': 31, 'number': 31, 'biopsy': 31, 'clustering': 31, 'during': 31, 'b': 31, '50': 30, 'labels': 29, 'label': 29, 'lymph': 29, 'heatmaps': 29, 'n': 29, 'datasets': 28, 'pc1': 28, '75': 28, 'node': 27, 'evidence': 27, 'supplementary': 27, 'different': 26, 'image': 26, 'images': 25, 'both': 25, 'diagnostic': 25, '5': 25, 'use': 25, 'deep': 24, '25': 24, 'sl': 24, 'example': 23, 'it': 23, 'detection': 23, 'network': 23, 'y': 22, 'computational': 22, 'if': 22, 'space': 22, 'average': 22, 'loss': 22, 'crcc': 22, 'size': 22, 'sampled': 21, 'multiclass': 21, 'research': 21, 'metastasis': 21, 'high': 21, 'ccrcc': 21, 'lusc': 21, 'only': 20, 'resection': 20, 'study': 20, 'score': 20, 'public': 20, '20': 20, 'luad': 20, '08': 20, 'pc20': 20, '06': 20, '02': 20, '2021': 19, 'given': 19, 'these': 19, 'same': 19, 'binary': 19, 'their': 19, 'probability': 19, 'validation': 19, 'bwh': 19, 'deeplearning': 18, 'single': 18, 'classes': 18, 'corresponding': 18, 'mean': 18, 'prcc': 18, 'extracted': 18, 's': 18, '2019': 18, 'm': 18, 'diagnosis': 17, 'has': 17, 'problems': 17, 'patchlevel': 17, 'smartphone': 17, 'j': 17, 'per': 17, 'analysis': 17, 'total': 17, 'no': 17, 'f': 16, 'features': 16, 'large': 16, 'limited': 16, 'methods': 16, 'normal': 16, 'cohorts': 16, 'cell': 16, 'without': 16, 'vol': 16, 'june': 16, '555570': 16, 'wwwnaturecom': 16, 'between': 16, 'other': 16, 'layer': 16, 'groundtruth': 16, 'predicted': 16, 'after': 16, 'tcga': 16, 'c': 16, 'inhouse': 16, '¼': 16, 'nat': 16, 'k': 15, 'annotation': 15, 'based': 15, 'small': 15, 'over': 15, 'instancelevel': 15, 'one': 15, 'its': 15, 'segmentation': 15, '04': 15, 'med': 15, 'tasks': 14, 'every': 14, 'interpretability': 14, 'clinical': 14, 'make': 14, 'addition': 14, 'labelled': 14, 'representation': 14, 'further': 14, 'multiple': 14, 'they': 14, 'magnification': 14, '2020': 14, 'wholeslide': 13, 'ing': 13, 'require': 13, 'may': 13, 'framework': 13, 'publicly': 13, 'processing': 13, 'any': 13, 'representations': 13, 'branches': 13, 'attended': 13, 'then': 13, 'additional': 13, 'information': 13, '4': 13, 'more': 13, 'via': 13, 'have': 12, 'although': 12, 'results': 12, 'content': 12, 'time': 12, 'patient': 12, 'directly': 12, 'branch': 12, 'observed': 12, 'will': 12, 'overlap': 12, 'version': 12, 'most': 11, 'well': 11, 'where': 11, 'predictions': 11, 'function': 11, 'rois': 11, 'tion': 11, 'do': 11, 'across': 11, 'while': 11, 'whole': 11, 'lung': 11, 'general': 11, 'neural': 11, 'under': 11, 'algorithm': 11, 'inference': 11, 'randomly': 11, 'case': 11, 'd': 11, 'heatmap': 11, 'g': 11, '075': 11, 'trainingset': 11, 'visualization': 11, 'than': 11, 'table': 11, '95': 11, 'morphological': 10, 'fully': 10, 'histology': 10, 'maxpooling': 10, '15': 10, 'attentionbased': 10, 'microscopy': 10, 'first': 10, 'relative': 10, 'crossvalidation': 10, '050': 10, '080': 10, 'specificity': 10, 'compared': 10, 'mpp': 10, 'collected': 10, 'cluster': 10, 'consists': 10, 'p': 10, 'articles': 9, 'medical': 9, 'roi': 9, 'however': 9, 'such': 9, 'shown': 9, 'extraction': 9, 'three': 9, 'adaptability': 9, 'region': 9, 'pooling': 9, 'highly': 9, 'task': 9, 'false': 9, 'morphology': 9, 'instead': 9, 'smaller': 9, '256': 9, '10fold': 9, 'achieved': 9, 'correct': 9, 'respectively': 9, 'investigate': 9, 'experiments': 9, 'hardware': 9, 'human': 9, 'histopathology': 9, 'cpis': 9, 'highest': 9, '512': 9, '2018': 9, 'l': 9, 'achieve': 8, 'aggregation': 8, 'sources': 8, 'interpretable': 8, 'method': 8, 'natbiomedeng': 8, 'parameters': 8, 'despite': 8, 'remaining': 8, 'sets': 8, 'following': 8, 'found': 8, 'efficiency': 8, 'cptac': 8, 'camelyon17': 8, 'top': 8, 'view': 8, 'whereas': 8, 'evaluation': 8, 'similarly': 8, 'selected': 8, 'two': 8, '05': 8, 'low': 8, 'cells': 8, 'should': 8, 'pathologist': 8, 'generated': 8, 'did': 8, 'biopsies': 8, 'intheclass': 8, 'svm': 8, 'raw': 8, 'institute': 7, 'authors': 7, 'potential': 7, 'many': 7, 'imaging': 7, 'been': 7, 'noisy': 7, 'thousands': 7, 'selection': 7, 'there': 7, 'generally': 7, 'consumergrade': 7, 'uses': 7, 'identify': 7, 'value': 7, 'pixellevel': 7, 'cnn': 7, 'able': 7, 'importance': 7, 'instances': 7, 'would': 7, 'final': 7, 'technique': 7, 'distinct': 7, 'often': 7, 'exclusive': 7, 'compute': 7, 'within': 7, 'adapt': 7, 'articlesnature': 7, 'see': 7, 'camelyon16': 7, 'e': 7, 'zoomedin': 7, '09': 7, 'details': 7, 'because': 7, 'perform': 7, '131': 7, '8': 7, 'ten': 7, 'scanner': 7, 'contain': 7, 'fovs': 7, 'problem': 7, 'connected': 7, 'vector': 7, 'þ': 7, 'computed': 7, 'outoftheclass': 7, 'smooth': 7, 'gigapixel': 6, 'domain': 6, 'reported': 6, 'assigning': 6, 'generalize': 6, 'present': 6, 'rare': 6, 'versus': 6, 'supervision': 6, 'sampling': 6, 'show': 6, 'carcinoma': 6, 'wellknown': 6, 'standard': 6, 'algorithms': 6, 'possible': 6, 'through': 6, 'annotations': 6, 'capable': 6, 'interactive': 6, 'demo': 6, 'known': 6, 'instance': 6, 'might': 6, 'annotated': 6, 'them': 6, 'points': 6, 'respective': 6, 'designed': 6, 'strong': 6, 'namely': 6, 'mutually': 6, 'learn': 6, 'pixels': 6, 'lowdimensional': 6, 'gpus': 6, 'fold': 6, 'area': 6, 'sd': 6, 'subtype': 6, 'µm': 6, 'left': 6, 'right': 6, 'layers': 6, '419': 6, 'h': 6, '040': 6, 'percentage': 6, 'incorrect': 6, 'aucbalanced': 6, 'errorconfidence': 6, 'confidence': 6, 'values': 6, 'range': 6, 'sizes': 6, '095': 6, 'implemented': 6, 'difference': 6, 'note': 6, 'whether': 6, 'due': 6, '46': 6, 'testing': 6, 'especially': 6, 'milmmil': 6, 'reduced': 6, 'produced': 6, 'before': 6, 'distribution': 6, 'highresolution': 6, 'artefacts': 6, '16': 6, 'networks': 6, 'entire': 6, 'expected': 6, 'aik': 6, 'processed': 6, 'breast': 6, '2017': 6, 'reporting': 6, 'national': 6, 'sciences': 6, 'usa': 5, 'chen': 5, 'artificial': 5, 'intelligence': 5, 'several': 5, 'tions': 5, 'multipleinstance': 5, 'predict': 5, 'cohort': 5, 'patients': 5, 'addi': 5, 'tested': 5, 'need': 5, 'summary': 5, 'efficient': 5, 'wholeslidelevel': 5, 'above': 5, 'demonstrate': 5, 'representative': 5, 'spatial': 5, 'pretrained': 5, 'contribution': 5, 'viewed': 5, 'challenge': 5, 'up': 5, 'hundreds': 5, 'gradient': 5, 'level': 5, 'hand': 5, 'generate': 5, 'pseudo': 5, 'clusters': 5, 'assumption': 5, 'computing': 5, 'pipeline': 5, 'so': 5, 'required': 5, 'crossvalidated': 5, 'evaluated': 5, 'once': 5, 'macroaveraged': 5, 'oneversusrest': 5, 'papillary': 5, 'chromophobe': 5, 'tumor': 5, '2b': 5, 'axillary': 5, '40': 5, 'classifier': 5, 'bottom': 5, 'supervise': 5, '289': 5, 'sensitivity': 5, '090': 5, 'sensitivity04': 5, 'analysed': 5, 'indicate': 5, 'made': 5, 'learned': 5, 'pca': 5, 'created': 5, 'denote': 5, 'mmil': 5, 'studies': 5, 'disease': 5, 'camelyon': 5, '7': 5, 'official': 5, 'provided': 5, 'generalization': 5, 'therefore': 5, 'stain': 5, '135': 5, 'full': 5, 'less': 5, '0011': 5, 'new': 5, 'dimensions': 5, 'but': 5, 'resolution': 5, 'digitized': 5, 'original': 5, 'presence': 5, 'simply': 5, 'include': 5, 'specifically': 5, 'automated': 5, 'fov': 5, '0025': 5, 'cpi': 5, 'resections': 5, '110': 5, 'w1': 5, 'r512': 5, 'crossentropy': 5, 'batch': 5, 'c1': 5, '32': 5, 'availability': 5, 'internal': 5, 'sample': 5, 'requests': 5, 'systems': 5, 'brigham': 4, 'ma': 4, 'digital': 4, 'analyse': 4, 'response': 4, 'unique': 4, 'approaches': 4, 'either': 4, 'manual': 4, 'settings': 4, 'pixel': 4, 'approach': 4, 'applicable': 4, 'micrometastasis': 4, 'subset': 4, 'work': 4, 'classifiers': 4, 'important': 4, 'heterogeneity': 4, 'trials': 4, 'adaptation': 4, 'adaptable': 4, 'applied': 4, 'dataefficient': 4, 'typically': 4, 'accurately': 4, 'varying': 4, 'transfer': 4, 'tional': 4, 'encoder': 4, 'pathologists': 4, 'python': 4, 'automatically': 4, 'simple': 4, 'result': 4, 'rule': 4, 'widely': 4, 'solve': 4, 'classspecific': 4, 'means': 4, 'separable': 4, 'leastattended': 4, 'correspond': 4, 'reduce': 4, 'enables': 4, 'sections': 4, '80': 4, 'out': 4, 'just': 4, 'curve': 4, 'clear': 4, 'patching': 4, 'strongly': 4, 'additionally': 4, 'interpret': 4, '347': 4, '172': 4, '710': 4, 'pc21': 4, '523': 4, '853': 4, '1577': 4, '1244': 4, '189': 4, '719': 4, '619': 4, '0016': 4, '008': 4, '014': 4, '020': 4, '065': 4, '030': 4, '004': 4, '0951': 4, '010': 4, '016': 4, '025': 4, 'comparative': 4, 'ai': 4, 'insets': 4, 'curves': 4, 'middle': 4, 'classified': 4, 'tables': 4, '13': 4, 'some': 4, 'approximately': 4, 'criteria': 4, 'denoted': 4, 'fewer': 4, 'reasonable': 4, 'probably': 4, 'poorly': 4, 'assess': 4, 'future': 4, '6': 4, 'institutional': 4, 'validate': 4, 'realworld': 4, '43': 4, '133': 4, '0015': 4, 'select': 4, 'observe': 4, 'improvement': 4, '3dhistech': 4, '150': 4, 'evaluating': 4, 'scanning': 4, 'ability': 4, 'diverse': 4, 'accurate': 4, 'normalized': 4, 'finegrained': 4, 'inform': 4, 'concordance': 4, 'highlighted': 4, 'quantitative': 4, 'exhibit': 4, '512dimensional': 4, 'he': 4, 'imaged': 4, 'ensure': 4, '48': 4, '0921': 4, '55': 4, 'kidney': 4, '26': 4, 'finetuning': 4, 'endtoend': 4, 'addressed': 4, 'pc2': 4, 'about': 4, '1024': 4, 'equation': 4, 'unnormalized': 4, 'softmax': 4, 'weight': 4, 'corresponds': 4, 'refer': 4, 'cannot': 4, 'does': 4, 'ðhk': 4, '108': 4, 'epochs': 4, 'early': 4, 'stopping': 4, 'saved': 4, 'performed': 4, 'nvidia': 4, 'local': 4, 'r': 4, 'archives': 4, 'requested': 4, 'excluded': 4, 'read': 4, 'converted': 4, 'foreground': 4, 'followed': 4, 'procedure': 4, 'prostate': 4, 'nih': 4, 'genomic': 4, 'material': 4, 'oncol': 4, 'ieee': 4, 'trans': 4, 'anal': 4, 'international': 4, 'eds': 4, 'grant': 4, 'my': 4, 'materials': 4, 'your': 4, 'harvard': 3, 'boston': 3, 'objective': 3, 'prognosis': 3, 'clini': 3, 'cal': 3, 'demonstrated': 3, 'variety': 3, 'resistance': 3, 'deeplearningbased': 3, 'setting': 3, 'roilevel': 3, 'annota': 3, 'localize': 3, 'furthermore': 3, 'provide': 3, 'recent': 3, 'difficult': 3, 'produce': 3, 'commonly': 3, 'fixed': 3, 'averaging': 3, 'suffer': 3, 'predictive': 3, 'still': 3, 'propose': 3, 'highthroughput': 3, 'key': 3, 'taken': 3, '14': 3, 'poor': 3, 'requires': 3, 'applying': 3, 'studied': 3, 'cally': 3, 'allow': 3, 'visualize': 3, 'developing': 3, 'highperformance': 3, 'having': 3, 'access': 3, 'thus': 3, 'aggregate': 3, 'atten': 3, 'impor': 3, 'specific': 3, 'com': 3, 'together': 3, 'examined': 3, 'knowledge': 3, 'subtypes': 3, 'assumed': 3, 'mutual': 3, 'exclusivity': 3, 'module': 3, 'highattention': 3, 'serve': 3, 'con': 3, 'mod': 3, 'units': 3, 'breastcancer': 3, 'metas': 3, 'tasis': 3, 'monte': 3, 'carlo': 3, 'genome': 3, 'adenocarcinoma': 3, 'squamous': 3, 'combined': 3, '0020': 3, 'nodes': 3, 'metrics': 3, 'backbone': 3, 'mm': 3, 'architecture': 3, 'red': 3, 'blue': 3, 'visualized': 3, '002': 3, '026': 3, '084': 3, '078': 3, '082': 3, '076': 3, '070': 3, 'ac': 3, 'df': 3, 'terms': 3, 'aucs': 3, 'plots': 3, 'boxes': 3, 'quartile': 3, 'whiskers': 3, 'extend': 3, 'interquartile': 3, 'represent': 3, 'conventional': 3, 'context': 3, 'findings': 3, 'subsets': 3, 'equal': 3, 'avoid': 3, 'variable': 3, 'represents': 3, 'sufficient': 3, 'lymphnodemetastasis': 3, 'variant': 3, 'sub': 3, 'typing': 3, 'areas': 3, 'noise': 3, 'overall': 3, 'conducted': 3, 'dif': 3, 'ferent': 3, 'partitions': 3, 'trainvalidatetest': 3, 'larger': 3, 'enable': 3, 'best': 3, 'ci': 3, 'encouraging': 3, 'divided': 3, 'robust': 3, 'encountered': 3, 'scanned': 3, '63': 3, 'much': 3, 'random': 3, '0008': 3, '3df': 3, 'confident': 3, '085': 3, 'must': 3, 'had': 3, 'embedded': 3, 'pro': 3, 'develop': 3, 'basis': 3, 'makes': 3, 'visual': 3, 'property': 3, 'prominent': 3, 'intuitively': 3, 'researchers': 3, 'agreement': 3, 'including': 3, 'cytoplasm': 3, 'nuclei': 3, 'nuclear': 3, 'background': 3, 'tiled': 3, 'second': 3, 'border': 3, 'highlight': 3, 'captured': 3, 'camera': 3, 'manually': 3, 'conditions': 3, 'adapting': 3, 'cellphone': 3, 'wider': 3, 'telepathology': 3, 'part': 3, 'collectively': 3, 'necessary': 3, 'relevant': 3, '0873': 3, '0023': 3, 'techniques': 3, 'normalization': 3, 'adversarial': 3, 'characteristics': 3, 'applicability': 3, 'accordingly': 3, '92': 3, 'contains': 3, 'specimens': 3, 'lungbiopsy': 3, 'five': 3, '17': 3, '0902': 3, 'differentiated': 3, 'extremely': 3, 'being': 3, 'tool': 3, 'associated': 3, 'anatomic': 3, 'flexibility': 3, 'help': 3, 'augmentation': 3, 'another': 3, 'those': 3, 'views': 3, 'consistent': 3, 'covering': 3, 'bag': 3, 'least': 3, 'design': 3, 'kth': 3, 'zk': 3, 'hk': 3, 'bias': 3, 'ith': 3, 'exp': 3, 'dropout': 3, 'r2': 3, 'assignment': 3, 'list': 3, 'take': 3, 'lowest': 3, 'assign': 3, 'hence': 3, 'below': 3, 'ai1': 3, 'else': 3, 'nclass': 3, 'specified': 3, 'margin': 3, 'α': 3, 'scaling': 3, 'τ': 3, 'apply': 3, 'against': 3, '9': 3, 'adam': 3, 'optimizer': 3, 'rate': 3, '104': 3, 'decay': 3, '105': 3, 'default': 3, 'β1': 3, 'β2': 3, '0999': 3, 'ε': 3, 'software': 3, 'files': 3, '2080': 3, 'ti': 3, 'library': 3, 'included': 3, 'queried': 3, '20162019': 3, 'development': 3, 'folds': 3, 'few': 3, '2016': 3, 'memory': 3, 'rgb': 3, 'closing': 3, '1024dimensional': 3, 'gpu': 3, 'visualizing': 3, 'displayed': 3, 'inside': 3, 'hidden': 3, 'w2': 3, 'website': 3, 'author': 3, 'code': 3, '23': 3, 'lancet': 3, 't': 3, 'realtime': 3, 'system': 3, '21': 3, 'cellular': 3, 'sci': 3, 'biomed': 3, 'eng': 3, 'litjens': 3, 'microscope': 3, 'conference': 3, 'o': 3, 'springer': 3, 'experimental': 3, 'participants': 3, 'population': 3, 'womens': 2, 'hospital': 2, 'school': 2, 'program': 2, 'science': 2, 'drew': 2, 'williamson': 2, 'tiffany': 2, 'identifying': 2, 'morphologies': 2, 'chal': 2, 'lenges': 2, 'tiny': 2, 'saliently': 2, 'useful': 2, 'clinicalgrade': 2, 'stratification': 2, 'comparable': 2, 'capturing': 2, 'diversity': 2, 'diagnoses': 2, 'examples': 2, 'suitable': 2, 'subtyp': 2, 'capability': 2, 'determinations': 2, 'article': 2, 'clusteringconstrainedattention': 2, 'address': 2, 'challenges': 2, 'photomicrographs': 2, '123': 2, 'mahmood': 2, 'here': 2, 'subregions': 2, 'classify': 2, 'identified': 2, 'constrain': 2, 'refine': 2, 'renal': 2, 'smart': 2, 'phone': 2, 'requiring': 2, 'benefit': 2, 'drasti': 2, 'speed': 2, 'predic': 2, 'adjacent': 2, 'easytouse': 2, 'package': 2, 'maps': 2, 'formulation': 2, 'usage': 2, 'empirically': 2, 'relatively': 2, 'treating': 2, 'maximizes': 2, 'ranks': 2, 'interpretation': 2, 'reflected': 2, 'unlike': 2, 'positivenegative': 2, 'obtain': 2, 'beyond': 2, 'explored': 2, 'existing': 2, 'algo': 2, 'rithms': 2, 'increase': 2, 'signals': 2, 'sion': 2, 'adopted': 2, 'supervising': 2, 'assume': 2, 'always': 2, 'source': 2, 'toolbox': 2, 'inputs': 2, 'volume': 2, 'decreased': 2, 'drastically': 2, 'els': 2, 'working': 2, 'hours': 2, 'workstations': 2, 'mentioned': 2, 'monitored': 2, 'held': 2, 'referred': 2, '0991': 2, '0004': 2, 'threeclass': 2, '0956': 2, '0953': 2, '0029': 2, '800': 2, 'scoring': 2, 'passed': 2, 'vectors': 2, '70': 2, '098': 2, '092': 2, '086': 2, '074': 2, '138': 2, '022': 2, '028': 2, '094': 2, 'adg': 2, 'beh': 2, 'cfi': 2, 'receiveroperatingcharacteristic': 2, 'balanced': 2, 'error': 2, 'box': 2, 'correctly': 2, 'incorrectly': 2, 'gi': 2, 'principal': 2, 'component': 2, 'di': 2, 'parentheses': 2, 'largest': 2, 'moderate': 2, '884': 2, '1967': 2, '899': 2, 'clas': 2, 'sification': 2, 'types': 2, 'feasible': 2, 'light': 2, 'sequentially': 2, 'cor': 2, 'responding': 2, 'kept': 2, 'amount': 2, 'introduction': 2, 'varies': 2, 'depending': 2, 'finally': 2, 'naively': 2, 'consistently': 2, 'performs': 2, 'maxpoolingbased': 2, 'demonstrates': 2, 'sparse': 2, 'location': 2, 'increased': 2, 'alone': 2, 'allows': 2, 'comparisons': 2, 'related': 2, '270': 2, '85': 2, '129': 2, 'protocols': 2, 'vary': 2, 'dataspecific': 2, 'variables': 2, 'scanners': 2, '68': 2, '66': 2, '67': 2, 'ate': 2, 'heldout': 2, 'bestperforming': 2, 'developed': 2, 'splits': 2, '0972': 2, '0975': 2, '0007': 2, '0940': 2, 'even': 2, 'respectable': 2, 'improved': 2, 'outperformed': 2, '142': 2, '577': 2, '081': 2, '0013': 2, 'become': 2, 'overly': 2, 'aperio': 2, 'scan': 2, 'digitize': 2, 'equivalent': 2, 'close': 2, 'mechanism': 2, 'approximate': 2, 'introduced': 2, 'lungresection': 2, 'miraxscan': 2, 'comparison': 2, 'proposed': 2, 'curated': 2, 'computeraided': 2, 'ensemble': 2, 'selecting': 2, 'computationally': 2, 'tuning': 2, 'mapping': 2, 'discussion': 2, 'degrees': 2, 'explicitly': 2, 'boundary': 2, '4ac': 2, 'towards': 2, 'purposes': 2, 'gen': 2, 'highlights': 2, 'intercellular': 2, 'bridges': 2, 'expertise': 2, 'challenging': 2, 'clearly': 2, 'epithelioid': 2, 'isolated': 2, 'masks': 2, 'others': 2, 'intuitive': 2, 'assigned': 2, 'separated': 2, 'characteristic': 2, 'capture': 2, 'wide': 2, 'array': 2, 'dense': 2, 'pattern': 2, 'atypia': 2, 'lymphocytes': 2, 'column': 2, 'overlaid': 2, 'third': 2, 'black': 2, 'among': 2, 'http': 2, 'clammahmoodlaborg': 2, 'resourceconstrained': 2, 'consult': 2, 'attached': 2, 'prob': 2, 'process': 2, 'underlying': 2, 'patientspecific': 2, 'fields': 2, 'x': 2, 'cover': 2, 'drop': 2, 'focus': 2, 'colour': 2, 'potentially': 2, 'imageprocessing': 2, 'convolutional': 2, 'attempt': 2, 'cost': 2, 'attends': 2, 'visibly': 2, 'substantially': 2, 'crush': 2, 'tant': 2, 'solely': 2, '53': 2, 'ranging': 2, 'kidneybiopsy': 2, 'input': 2, 'tumours': 2, 'immunohistochemistry': 2, 'lead': 2, 'showed': 2, 'treatment': 2, 'interactions': 2, 'contextaware': 2, 'improve': 2, 'involves': 2, 'contrast': 2, 'us': 2, 'detailed': 2, 'extensive': 2, 'overfitting': 2, 'survival': 2, '071': 2, '005': 2, '011': 2, '017': 2, '024': 2, '023': 2, '029': 2, '089': 2, 'adapted': 2, 'bc': 2, 'noted': 2, 'decrease': 2, 'near': 2, 'otherwise': 2, 'weak': 2, 'valuable': 2, '35': 2, 'lower': 2, '820': 2, '24714': 2, 'remains': 2, 'coverage': 2, 'mm2': 2, 'routine': 2, 'improving': 2, 'point': 2, 'functions': 2, 'operator': 2, 'built': 2, 'considered': 2, 'embedding': 2, 'r256': 2, 'parallel': 2, 'r1': 2, 'hslidei': 2, 'wai': 2, 'tanh': 2, 'sigm': 2, 'pk': 2, 'sslidei': 2, 'regularization': 2, 'sslide': 2, 'pik': 2, 'outputs': 2, '2y': 2, 'yyb': 2, 'linearly': 2, 'summarized': 2, 'ðeh1': 2, 'þ¼': 2, 'sortascending': 2, 'ððh1': 2, 'aikþ': 2, 'ib': 2, 'pib': 2, 'winstiehkbþb': 2, 'return': 2, 'top1': 2, 'greater': 2, 'temperature': 2, 'create': 2, 'yþ¼': 2, 'max': 2, 'sj': 2, 'sy': 2, 'weights': 2, 'ltotal': 2, 'lslide': 2, 'lpatch': 2, 'optional': 2, 'c2': 2, 'tuned': 2, '128': 2, '07': 2, 'met': 2, 'previous': 2, 'consecutive': 2, 'parallelization': 2, 'google': 2, 'cloud': 2, 'takes': 2, 'openslide': 2, 'pytorch': 2, 'performing': 2, 'workstation': 2, 'nonoverlapping': 2, 'generating': 2, 'includes': 2, 'extract': 2, 'algorithmic': 2, 'implementation': 2, 'proc': 2, 'received': 2, 'markings': 2, 'damaged': 2, 'differ': 2, 'repository': 2, 'projects': 2, 'condition': 2, 'comprises': 2, 'university': 2, 'center': 2, 'netherlands': 2, '500': 2, 'mask': 2, 'thresholding': 2, 'contours': 2, 'threshold': 2, 'along': 2, 'containing': 2, 'easily': 2, 'exhaustively': 2, 'patched': 2, 'resnet50': 2, 'convert': 2, 'fit': 2, 'simultaneously': 2, 'percentile': 2, 'zero': 2, 'colours': 2, 'colourmap': 2, '8000': 2, 'similar': 2, 'classagnostic': 2, 'adjust': 2, 'opening': 2, 'cavities': 2, 'pixelislands': 2, 'exclude': 2, 'decision': 2, 'making': 2, 'recurrentneuralnetworkbased': 2, 'skin': 2, 'previously': 2, 'relu': 2, 'activation': 2, 'sk': 2, 'rn': 2, 'optimized': 2, 'stochastic': 2, 'descent': 2, 'retrieved': 2, 'guide': 2, 'ethics': 2, 'approved': 2, 'mass': 2, 'irb': 2, 'protocol': 2, '2020p000233': 2, 'metastaticlymphnode': 2, 'links': 2, 'academic': 2, 'promptly': 2, 'reviewed': 2, 'determine': 2, 'request': 2, 'subject': 2, 'intellectual': 2, 'patientconfidentiality': 2, 'obligations': 2, 'departmental': 2, 'guidelines': 2, '22': 2, 'published': 2, 'online': 2, 'madabhushi': 2, 'clin': 2, 'kather': 2, 'w': 2, 'gleason': 2, 'grading': 2, 'cytometry': 2, '39': 2, '58': 2, '115': 2, 'commun': 2, 'transl': 2, 'integrated': 2, 'pancancer': 2, 'pathol': 2, 'res': 2, '2015': 2, '24': 2, 'esteva': 2, 'retinal': 2, 'fundus': 2, 'mach': 2, '33': 2, 'wang': 2, '37': 2, 'advances': 2, 'citeseer': 2, 'social': 2, 'z': 2, 'database': 2, 'supported': 2, 'rjc': 2, 'manuscript': 2, 'competing': 2, 'interests': 2, 'peer': 2, 'review': 2, 'summaryapril': 2, 'you': 2, 'sure': 2, 'appropriate': 2, 'life': 2, 'n46': 2, 'n55': 2, 'replication': 2, 'blinding': 2, 'na': 2, 'involved': 2, 'hospitals': 2, 'recruitment': 2, 'approval': 2, 'https': 1, 'doiorg101038s4155102000682w': 1, '1department': 1, '2cancer': 1, 'broad': 1, 'mit': 1, 'cambridge': 1, '3cancer': 1, 'danafarber': 1, '4department': 1, 'informatics': 1, '5these': 1, 'contributed': 1, 'equally': 1, 'email': 1, 'faisalmahmoodbwhharvardedu': 1, 'dvances': 1, 'presented': 1, 'therapeuticresponse': 1, 'prediction12': 1, 'apart': 1, 'immediate': 1, 'benefits36': 1, 'promise': 1, 'quantifying': 1, 'microenviron': 1, 'ment712': 1, 'conducting': 1, 'integrative': 1, 'imageomic': 1, 'analysis1319': 1, 'prognostic': 1, 'relevance': 1, 'associating': 1, 'treatment22': 1, '2324': 1, 'revolutionized': 1, 'imag': 1, 'solving': 1, '2530': 1, 'complex': 1, 'super': 1, 'vised': 1, 'corre': 1, 'spond': 1, 'relied': 1, 'regionofinterest': 1, 'needles': 1, 'haystack3134': 1, 'promising': 1, 'wsi35': 1, 'suffers': 1, 'dem': 1, 'onstrated': 1, 'exceptional': 1, 'setting36': 1, 'variants': 1, 'methodology': 1, 'beneficial': 1, 'immense': 1, 'curate': 1, 'handful': 1, 'exist': 1, 'outcome': 1, 'moreover': 1, 'predefined': 1, 'devices3536': 1, 'broader': 1, 'pixelpatchlevel': 1, 'ling': 1, 'naive': 1, 'aims': 1, 'outlined': 1, 'separate': 1, 'analyses': 1, 'renalcellcarcinoma': 1, 'nonsmallcelllungcancer': 1, 'systematically': 1, 'decreasing': 1, 'showing': 1, 'ming': 1, 'lu': 1, 'richard': 1, 'matteo': 1, 'barbieri12': 1, 'faisal': 1, 'report': 1, 'named': 1, 'nonsmallcell': 1, 'overperforms': 1, '555': 1, 'presents': 1, 'extends': 1, 'aggregation37': 1, 'convolu': 1, 'dimensionality': 1, 'reduction': 1, 'increasing': 1, 'clinicians': 1, 'iden': 1, 'tifying': 1, 'distinguishing': 1, 'tis': 1, 'sue': 1, 'github': 1, 'httpsgithubcommahmoodlabclam': 1, 'httpclam': 1, 'mahmoodlaborg': 1, 'enabling': 1, 'represen': 1, 'tative': 1, 'paradigm': 1, 'major': 1, 'machinelearning': 1, 'suboptimal': 1, 'signal': 1, 'update': 1, 'draw': 1, 'back': 1, 'partly': 1, 'explain': 1, 'why': 1, 'observation': 1, 'tasks36': 1, 'fea': 1, 'tures': 1, 'examines': 1, 'informs': 1, 'tance': 1, 'collective': 1, 'putes': 1, 'weighted': 1, 'algorithm3638': 1, 'generic': 1, 'paral': 1, 'lel': 1, 'calculate': 1, 'determined': 1, '1bc': 1, 'adopting': 1, 'pooling37': 1, 'favour': 1, 'inefficiency': 1, 'supervisory': 1, 'learns': 1, 'separating': 1, 'incorporate': 1, 'add': 1, 'supervi': 1, 'practice': 1, 'choose': 1, 'simpler': 1, 'ers': 1, 'readily': 1, 'adopt': 1, 'utilize': 1, 'dedicated': 1, 'open': 1, 'segments': 1, 'divides': 1, 'direct': 1, '1a': 1, 'next': 1, 'vert': 1, 'embed': 1, 'dings': 1, '1b': 1, 'occur': 1, 'highdimensional': 1, 'nearly': 1, '200fold': 1, 'subse': 1, 'quent': 1, 'computation': 1, 'gigapixelsized': 1, 'modern': 1, 'graphics': 1, 'proceeding': 1, 'putational': 1, 'datasetsize': 1, 'dependent': 1, 'partitioned': 1, 'stratified': 1, 'event': 1, 'plete': 1, 'evaluate': 1, 'atlas': 1, 'natbiomedeng556': 1, '2a': 1, 'twoclass': 1, 'proteomic': 1, 'consortium': 1, 'breastcancermetastasis': 1, 'mag': 1, 'nification': 1, '2c': 1, '89000': 1, '79000': 1, 'evidencepositive': 1, 'expxi': 1, 'σj': 1, 'expxj': 1, 'overview': 1, 'conceptual': 1, 'encoded': 1, 'descriptive': 1, 'assigns': 1, 'weighs': 1, 'summarizes': 1, 'samples': 1, 'rich': 1, '557': 1, '0984': 1, '0010': 1, '0971': 1, '096': 1, '072': 1, '0019': 1, '0941': 1, '088': 1, '003': 1, '0930': 1, '0044': 1, '0929': 1, '0028': 1, '86': 1, '196': 1, '89': 1, 'band': 1, 'shows': 1, 'averaged': 1, 'datasetsizedependent': 1, 'various': 1, 'plotted': 1, 'pc': 1, 'natbiomedeng558': 1, '510': 1, 'proprietary': 1, 'works536': 1, 'indicates': 1, 'effectively': 1, 'positiveversusnegative': 1, 'acquire': 1, 'collect': 1, 'diseases': 1, 'unusual': 1, 'limitations': 1, 'subsampled': 1, 'dependency': 1, 'constant': 1, 'confounding': 1, 'satisfactory': 1, 'merely': 1, 'avail': 1, '170': 1, 'needed': 1, 'nslcc': 1, 'popular': 1, 'assuming': 1, 'technical': 1, '2df': 1, 'pronounced': 1, 'leads': 1, 'hun': 1, 'dred': 1, 'contributes': 1, 'ablation': 1, 'improves': 1, 'baglevel': 1, 'constraint': 1, '601030': 1, '401050': 1, '801010': 1, 'body': 1, 'works': 1, 'splitting': 1, '0936': 1, 'interval': 1, '0890': 1, '0983': 1, '81': 1, '0963': 1, '09370990': 1, 'differences': 1, 'standards': 1, 'preparation': 1, 'digitization': 1, 'greatly': 1, 'appear': 1, 'ance': 1, 'col': 1, 'lected': 1, 'evalu': 1, 'explained': 1, 'completely': 1, 'denom': 1, 'inations': 1, 'variance': 1, 'higher': 1, 'give': 1, 'illu': 1, 'inconsistent': 1, 'accommodate': 1, 'estimate': 1, '3ac': 1, 'mmilmil': 1, 'delivered': 1, 'constrained': 1, '292': 1, 'simi': 1, 'larly': 1, '732': 1, '166': 1, '297': 1, 'respec': 1, 'tive': 1, '911': 1, 'became': 1, '559': 1, 'heb': 1, '097': 1, '091': 1, '079': 1, '073': 1, '093': 1, '087': 1, '069': 1, '0973': 1, '0952': 1, '0970': 1, '0969': 1, '0009': 1, '0922': 1, '0014': 1, '0875': 1, 'outperform': 1, 'baselines': 1, 'consistency': 1, 'natbiomedeng560': 1, 'desirable': 1, 'inaccurate': 1, 'severely': 1, 'erroneously': 1, 'overfit': 1, 'micrometreperpixel': 1, 'hamamatsu': 1, 'ners': 1, 'vast': 1, 'majority': 1, '044': 1, 'standardize': 1, 'downscaling': 1, 'cessing': 1, '0979': 1, '0005': 1, 'impact': 1, 'variability': 1, 'ner': 1, 'duces': 1, '0328': 1, '0910': 1, '0022': 1, 'native': 1, 'dras': 1, 'tic': 1, 'izing': 1, 'scans': 1, '0965': 1, '0006': 1, 'rea': 1, 'sonably': 1, 'quite': 1, 'variation': 1, 'illustrates': 1, 'standardization': 1, 'supporting': 1, 'moder': 1, 'institutions': 1, 'sourcespecific': 1, 'vari': 1, 'deployment': 1, 'inexpensive': 1, 'accomplish': 1, '1214': 1, 'readable': 1, 'aligns': 1, 'patholo': 1, 'gists': 1, 'failure': 1, 'artificialintelligence': 1, 'assisted': 1, 'humanintheloop': 1, 'aggre': 1, 'gating': 1, 'ignoring': 1, 'rel': 1, 'evance': 1, 'converting': 1, 'percentiles': 1, 'ping': 1, 'overlapped': 1, 'quality': 1, 'never': 1, 'delineating': 1, 'httpclammah': 1, 'moodlaborg': 1, 'welcoming': 1, 'finding': 1, 'meaningful': 1, 'inter': 1, 'pretability': 1, 'collecting': 1, 'erally': 1, 'already': 1, 'established': 1, 'recog': 1, 'nized': 1, 'keratiniza': 1, '4b': 1, 'cytokeratin': 1, 'ae1ae3': 1, 'immunohistochemical': 1, 'staining': 1, 'misclassified': 1, 'failed': 1, 'differen': 1, 'tiation': 1, 'contextual': 1, 'cues': 1, 'delineate': 1, 'typi': 1, 'histiocytes': 1, 'mimic': 1, 'degree': 1, 'negatives': 1, 'tended': 1, 'micrometastases': 1, 'practical': 1, 'usefulness': 1, 'caution': 1, 'rely': 1, 'expectation': 1, 'pixelperfect': 1, 'deter': 1, 'mining': 1, 'nonetheless': 1, 'insight': 1, 'patterns': 1, 'driving': 1, 'investigation': 1, 'enhanced': 1, 'investi': 1, 'gated': 1, 'agnostic': 1, 'immune': 1, 'populations': 1, '561': 1, 'eosinophilic': 1, 'pale': 1, 'plantcellwalllike': 1, 'cytoplasmic': 1, 'membrane': 1, 'raisinoid': 1, 'binucleiation': 1, 'voluminous': 1, 'alveolar': 1, 'acinarlike': 1, 'thinwalled': 1, 'chicken': 1, 'wire': 1, 'vasculature': 1, 'intracellular': 1, 'mucin': 1, 'droplets': 1, 'lumen': 1, 'formation': 1, 'nucleoli': 1, 'accumulation': 1, 'foamy': 1, 'macrophages': 1, 'cores': 1, 'lined': 1, 'neoplastic': 1, 'tubulopapillary': 1, 'pleomorphism': 1, 'keratinization': 1, 'atypical': 1, 'polygonal': 1, 'irregularity': 1, 'ccrccprccluadlusclymph': 1, 'ab': 1, 'who': 1, 'roughly': 1, 'parts': 1, 'onto': 1, 'squares': 1, 'lowattention': 1, 'green': 1, 'arrows': 1, 'textual': 1, 'description': 1, 'natbiomedeng562': 1, 'exclusively': 1, 'exper': 1, 'tise': 1, 'microscope39': 1, 'ably': 1, 'timeconsuming': 1, 'laborious': 1, 'curating': 1, 'pathological': 1, 'tissuesite': 1, 'appearances': 1, 'inherently': 1, 'deliver': 1, 'fore': 1, 'tremendous': 1, 'adoption': 1, 'iphone': 1, '5bc': 1, '5d': 1, 'attributed': 1, 'imperfect': 1, 'ditions': 1, 'nonuniform': 1, 'illumination': 1, 'vignetting': 1, 'shift': 1, 'magnifica': 1, 'changes': 1, 'adversities': 1, 'erative': 1, 'modelling40': 1, 'ize': 1, 'robustness': 1, 'keep': 1, '5ef': 1, '5gh': 1, 'instil': 1, 'regards': 1, 'resec': 1, 'resected': 1, 'coreneedlebiopsied': 1, 'distortion': 1, 'artefact': 1, 'pre': 1, 'dictions': 1, 'six': 1, 'vided': 1, 'evalua': 1, '6bc': 1, '1819': 1, 'contained': 1, 'impossible': 1, 'diagnose': 1, 'haematoxylin': 1, 'eosin': 1, 'stains': 1, 'auto': 1, 'mated': 1, 'exposing': 1, 'nontumour': 1, 'blood': 1, 'ves': 1, 'sels': 1, 'inflammation': 1, 'necrotic': 1, 'on35': 1, 'misclassification': 1, 'preprocessing': 1, '6de': 1, 'tinued': 1, 'similarity': 1, 'gener': 1, 'ally': 1, 'occupying': 1, 'altogether': 1, 'addresses': 1, 'classifi': 1, 'cation': 1, 'encouraged': 1, 'overcomes': 1, 'barrier': 1, 'timecostly': 1, 'effi': 1, 'cient': 1, 'achieves': 1, 'num': 1, 'ber': 1, 'applica': 1, 'ary': 1, 'opinion': 1, 'provides': 1, 'selectively': 1, 'aggregating': 1, 'limi': 1, 'tation': 1, 'milbased': 1, 'treat': 1, 'loca': 1, 'poten': 1, 'tial': 1, 'nonlinear': 1, 'line': 1, 'extending': 1, 'manner': 1, 'exten': 1, 'sive': 1, 'ment': 1, 'resources': 1, 'resourcehungry': 1, 'undertaking': 1, 'largescale': 1, 'experimentation': 1, 'conduct': 1, 'leaves': 1, 'room': 1, 'flexibly': 1, 'strike': 1, 'balance': 1, 'seeks': 1, 'maximize': 1, 'expressiveness': 1, 'curb': 1, '563': 1, 'last': 1, 'remain': 1, 'investigated': 1, 'stud': 1, 'ies': 1, 'mixed': 1, 'uncertainty': 1, 'estimates': 1, 'intheloop': 1, 'decisionmaking': 1, 'closer': 1, 'adaption': 1, 'patientlevel': 1, '190': 1, '0907': 1, '0900': 1, '0026': 1, '0850': 1, '0041': 1, '0837': 1, '0039': 1, 'modality': 1, '0102': 1, '0051': 1, 'performances': 1, 'ef': 1, 'highlighting': 1, 'discriminative': 1, 'largely': 1, 'ignores': 1, 'circularshaped': 1, 'cutout': 1, 'inevitably': 1, 'encapsulate': 1, 'mislead': 1, 'weaker': 1, 'boxed': 1, 'gh': 1, 'attentionpooled': 1, 'visible': 1, 'separation': 1, 'natbiomedeng564': 1, 'brought': 1, 'forth': 1, 'helps': 1, 'tradeoff': 1, 'assets': 1, 'distri': 1, 'bution': 1, 'essential': 1, 'indeed': 1, 'stratifying': 1, 'predominant': 1, 'look': 1, 'forward': 1, 'validating': 1, 'opti': 1, 'mistic': 1, 'utility': 1, 'applications': 1, 'luadlusc': 1, '0932': 1, '0882': 1, '0885': 1, 'sparsely': 1, 'distributed': 1, 'achieving': 1, 'de': 1, 'tend': 1, 'tumournormal': 1, 'boundaries': 1, 'fact': 1, 'fg': 1, '565': 1, 'form': 1, 'properties': 1, '11182': 1, 'dur': 1, 'later': 1, '156': 1, '1225': 1, 'remote': 1, 'brightfield': 1, 'microscopes': 1, 'hope': 1, 'ways': 1, 'lems': 1, 'thereby': 1, 'care': 1, 'facilitating': 1, 'discov': 1, 'ery': 1, 'empowered': 1, 'builds': 1, 'collection': 1, 'comprised': 1, 'restricts': 1, 'scope': 1, 'belongs': 1, 'rigid': 1, 'nontrainable': 1, 'rendering': 1, 'unsuitable': 1, 'intrinsic': 1, 'besides': 1, 'generalized': 1, 'logsumexp': 1, 'quantile': 1, 'noisyor': 1, 'noisyand4143': 1, 'offer': 1, 'around': 1, 'trainable': 1, 'function37': 1, 'predicts': 1, 'unambiguously': 1, 'noninformative': 1, 'absent': 1, 'classdefining': 1, 'summarize': 1, 'represented': 1, 'compresses': 1, 'r1024': 1, 'w1zk': 1, 'simplicity': 1, 'implied': 1, 'written': 1, 'stacked': 1, 'consider': 1, 'ua': 1, 'va': 1, 'shared': 1, 'wa1': 1, 'wan': 1, 'wc1': 1, 'wcn': 1, '137': 1, 'aggregated': 1, 'vahkð': 1, 'uahkð': 1, 'þðþ': 1, 'j¼1': 1, 'vahj': 1, 'uahj': 1, 'ð1þ': 1, 'xk': 1, 'k¼1': 1, 'aikhk': 1, 'ð2þ': 1, 'wci': 1, 'wcihslidei': 1, 'encourage': 1, 'introduce': 1, 'place': 1, 'winsti': 1, 'winstihk': 1, 'ð3þ': 1, 'iteration': 1, 'optimize': 1, 'let': 1, 'y¼f': 1, 'ng': 1, 'confusion': 1, 'way': 1, 'sorted': 1, 'ascending': 1, 'order': 1, 'ay1': 1, 'ayk': 1, 'receive': 1, 'interpreted': 1, 'constraining': 1, 'characterizing': 1, 'sense': 1, 'impose': 1, '8i': 1, 'ynfyg': 1, 'none': 1, 'hold': 1, 'know': 1, 'positives': 1, 'aforementioned': 1, 'notations': 1, 'clusterh1': 1, 'a1': 1, 'ak': 1, '¼ðehk': 1, 'þþ': 1, 'ibb': 1, 'winstiehb': 1, 'pibþb': 1, 'ðehk': 1, 'ai1þ': 1, 'aikþþ': 1, 'pass': 1, 'pn': 1, 'y1': 1, 'yn': 1, 'yy': 1, 'chose': 1, 'loss44': 1, 'wellestablished': 1, 'loss45': 1, 'output': 1, 'natbiomedeng566': 1, 'entry': 1, 'y¼': 1, 'nfg': 1, 'penalizes': 1, 'smoothed': 1, 'adds': 1, 'infinitely': 1, 'differentiable': 1, 'nonsparse': 1, 'gradients': 1, 'optimization': 1, 'efficiently44': 1, 'choices': 1, 'finite': 1, 'topattended': 1, 'necessarily': 1, 'guaranteed': 1, 'actual': 1, 'lðs': 1, 'j2ynfyg': 1, 'c8c9': 1, 'c26c27': 1, 'ð4þ': 1, 'l1τðs': 1, 'τlog': 1, 'j2y': 1, 'α1ðjyþþ': 1, 'ð5þ': 1, 'multinomial': 1, 'inversely': 1, 'proportional': 1, 'frequency': 1, 'underrepresented': 1, 'likely': 1, 'mitigate': 1, 'imbalance': 1, 'initialized': 1, 'rest': 1, 'sum': 1, 'scalar': 1, 'c1lslide': 1, 'c2lpatch': 1, 'ð6þ': 1, 'pseudocluster': 1, 'recall': 1, 'nonsubtyping': 1, 'unless': 1, '64': 1, '03': 1, 'chosen': 1, 'main': 1, 'drastic': 1, 'updated': 1, 'ℓ2': 1, 'coefficient': 1, 'running': 1, 'averages': 1, 'moment': 1, 'term': 1, 'numerical': 1, 'stability': 1, 'maximum': 1, '200': 1, 'criterion': 1, 'epoch': 1, 'hard': 1, 'drives': 1, 'store': 1, 'intel': 1, 'xeon': 1, 'cpus': 1, 'central': 1, 'accelerated': 1, 'p100': 1, 'consumer': 1, 'workstationgrade': 1, 'streaming': 1, 'fast': 1, 'solidstatedrive': 1, 'storage': 1, '375': 1, 'advantage': 1, 'libraries': 1, '341': 1, 'opencv': 1, '411': 1, 'pillow': 1, '621': 1, 'loading': 1, 'run': 1, 'streamlined': 1, '10626': 1, '4146': 1, '648': 1, 'saving': 1, '1565': 1, '442': 1, '1123': 1, 'generation': 1, 'highoverlap': 1, 'runs': 1, 'minibatches': 1, '5445': 1, '279': 1, 'overlapping': 1, 'productiongrade': 1, 'matplotlib': 1, '311': 1, 'seaborn': 1, 'estimated': 1, 'mannwhitney': 1, 'u': 1, 'statistic': 1, 'scikitlearn': 1, 'scientific': 1, '0221': 1, 'intervals': 1, 'true': 1, 'delongs': 1, '1162': 1, '361': 1, 'onsite': 1, 'substantial': 1, 'subsections': 1, 'exact': 1, 'brevity': 1, 'tcgakich': 1, 'tcgakirc': 1, 'tcgakirp': 1, '111': 1, '99': 1, '489': 1, '483': 1, '284': 1, '264': 1, '13907': 1, '20394': 1, '79': 1, '1709': 1, '993': 1, 'tcgalusc': 1, 'tcgaluad': 1, '507': 1, '444': 1, '486': 1, '452': 1, '1526': 1, 'tcia': 1, 'portal': 1, 'topological': 1, 'site': 1, '668': 1, '223': 1, '306': 1, '552': 1, '1175': 1, '667': 1, '792': 1, '560': 1, '9958': 1, '567': 1, 'comes': 1, '406': 1, 'ref': 1, 'radboud': 1, 'utrecht': 1, '1000': 1, 'centres': 1, 'yet': 1, 'portion': 1, '591': 1, '308': 1, '499': 1, '41802': 1, '51426': 1, 'begins': 1, 'downsampled': 1, 'downscale': 1, 'hsv': 1, 'saturation': 1, 'channel': 1, 'median': 1, 'blurring': 1, 'edges': 1, 'fill': 1, 'gaps': 1, 'holes': 1, 'detected': 1, 'objects': 1, 'filtered': 1, 'stored': 1, 'downstream': 1, 'inspection': 1, 'humanreadable': 1, 'textfile': 1, 'editable': 1, 'reliable': 1, 'edited': 1, 'individual': 1, 'user': 1, 'find': 1, 'unsatisfactory': 1, 'crops': 1, 'segmented': 1, 'userspecified': 1, 'stores': 1, 'stacks': 1, 'coordinates': 1, 'metadata': 1, 'hdf5': 1, 'hierarchical': 1, 'format': 1, 'imagenet47': 1, 'adaptive': 1, 'meanspatial': 1, 'residual': 1, 'block': 1, 'benefits': 1, 'faster': 1, 'matter': 1, '150000': 1, 'avoiding': 1, 'resulting': 1, 'twodimensional': 1, 'transformation': 1, 'shaded': 1, 'repeated': 1, 'interpreting': 1, 'corresponded': 1, 'scaled': 1, 'diverging': 1, 'locations': 1, 'visually': 1, 'calculated': 1, 'calculating': 1, 'transparency': 1, 'structures': 1, 'uniformly': 1, 'recorded': 1, 'words': 1, 'considers': 1, 'fashion': 1, 'intended': 1, 'possibility': 1, 'assistive': 1, 'annotator': 1, 'correctness': 1, 'dice': 1, 'intersection': 1, 'union': 1, 'cohens': 1, 'κ': 1, 'asked': 1, 'independently': 1, 'annotate': 1, 'platform': 1, 'asap': 1, 'constraints': 1, 'placed': 1, 'ae1': 1, 'ae3': 1, 'assist': 1, 'missed': 1, 'tiling': 1, 'dynamic': 1, 'probable': 1, 'scenario': 1, 'freely': 1, 'display': 1, 'desired': 1, 'contiguous': 1, 'thresholded': 1, 'assistance': 1, 'binarization': 1, 'post': 1, 'fragmentation': 1, 'suppress': 1, 'destroyed': 1, 'operations': 1, 'slightly': 1, 'dilated': 1, 'connect': 1, 'neighbouring': 1, 'fragments': 1, 'filter': 1, 'thoroughness': 1, 'detect': 1, 'cavity': 1, 'account': 1, 'natbiomedeng568': 1, 'study36': 1, 'passes': 1, 'ranked': 1, 'recurrent': 1, 'basal': 1, 'marginal': 1, 'baseline': 1, 'embeddings': 1, 'shape': 1, 'described': 1, 'featureextraction': 1, 'step': 1, 'rectified': 1, 'linear': 1, 'unit': 1, 'b1': 1, 'b2': 1, 'defined': 1, 'w2ð': 1, 'ðw1zk': 1, 'b1þþ': 1, 'þb2': 1, 'ð7þ': 1, 'according': 1, 'whose': 1, 'compare': 1, 'call': 1, 'dimension': 1, 'hyperparameters': 1, 'minibatch': 1, 'strategy': 1, 'normalizing': 1, 'maxpooled': 1, 'treated': 1, 'implies': 1, 'infer': 1, 'reasonably': 1, 'substantiated': 1, 'fraction': 1, 'responsible': 1, 'mislabelled': 1, 'purpose': 1, 'expect': 1, 'closely': 1, 'signaltonoise': 1, 'ratio': 1, 'consist': 1, 'stack': 1, 'get': 1, 'study35': 1, '100000': 1, 'checkpoint': 1, 'statement': 1, 'mgb': 1, 'office': 1, 'linked': 1, 'commons': 1, 'httpsportalgdccancer': 1, 'gov': 1, 'archive': 1, 'httpscancerimagingarchivenet': 1, 'datascopecptac': 1, 'httpscamelyon17': 1, 'grandchallengeorgdata': 1, 'primary': 1, 'complete': 1, 'httpsgithubcom': 1, 'mahmoodlabclam': 1, 'reproduce': 1, 'paper': 1, 'released': 1, 'gnu': 1, 'gplv3': 1, 'free': 1, 'license': 1, 'april': 1, 'accepted': 1, 'december': 1, 'march': 1, 'references': 1, 'bera': 1, 'schalper': 1, 'pathologynew': 1, 'tools': 1, 'precision': 1, 'oncology': 1, 'rev': 1, '703715': 1, 'niazi': 1, 'parwani': 1, 'v': 1, 'gurcan': 1, 'e253e261': 1, 'hollon': 1, 'intraoperative': 1, 'brain': 1, 'stimulated': 1, 'raman': 1, '5258': 1, 'microsatellite': 1, 'instability': 1, 'gastrointestinal': 1, '10541056': 1, 'bulten': 1, '233241': 1, 'ström': 1, 'populationbased': 1, '222232': 1, 'schapiro': 1, 'histocat': 1, 'phenotypes': 1, 'multiplex': 1, '873876': 1, 'moen': 1, '12331246': 1, 'multiorgan': 1, '32573267': 1, 'graham': 1, 'hovernet': 1, 'simultaneous': 1, 'multitissue': 1, '101563': 1, '11': 1, 'saltz': 1, 'organization': 1, 'molecular': 1, 'correlation': 1, 'tumorinfiltrating': 1, 'rep': 1, '181193': 1, '12': 1, 'javed': 1, 'community': 1, 'phenotyping': 1, 'colorectal': 1, '101696': 1, '569': 1, 'mobadersany': 1, 'predicting': 1, 'outcomes': 1, 'genomics': 1, 'natl': 1, 'acad': 1, 'e2970e2979': 1, 'heindl': 1, 'microenvironmental': 1, 'niche': 1, 'divergence': 1, 'shapes': 1, 'brca1dysregulated': 1, 'ovarian': 1, 'plasticity': 1, '3917': 1, 'yuan': 1, 'tumors': 1, 'complements': 1, 'profiling': 1, '157ra143': 1, '2012': 1, 'lazar': 1, 'comprehensive': 1, 'characterization': 1, 'adult': 1, 'soft': 1, 'sarcomas': 1, '171': 1, '950965': 1, 'fu': 1, 'reveals': 1, 'mutations': 1, 'composition': 1, '800810': 1, '18': 1, 'imagebased': 1, 'clinically': 1, 'actionable': 1, 'genetic': 1, 'alterations': 1, '789799': 1, '19': 1, 'pathomic': 1, 'fusion': 1, 'fusing': 1, 'httpsdoiorg101109tmi20203021387': 1, 'beck': 1, 'systematic': 1, 'uncovers': 1, 'stromal': 1, '108ra113': 1, '2011': 1, 'amamoto': 1, 'acquisition': 1, 'explainable': 1, 'unannotated': 1, '5642': 1, 'pell': 1, '8190': 1, 'lecun': 1, 'bengio': 1, 'hinton': 1, '521': 1, '436444': 1, 'healthcare': 1, '2429': 1, 'dermatologistlevel': 1, '542': 1, '115118': 1, 'poplin': 1, 'cardiovascular': 1, 'risk': 1, 'factors': 1, 'photographs': 1, '158164': 1, '27': 1, 'mckinney': 1, 'screening': 1, '8994': 1, '28': 1, 'mitani': 1, 'anaemia': 1, '1827': 1, '29': 1, 'shen': 1, 'zhao': 1, 'xing': 1, 'reconstruction': 1, 'volumetric': 1, 'tomography': 1, 'projection': 1, '880888': 1, '30': 1, 'tellez': 1, 'van': 1, 'der': 1, 'laak': 1, 'ciompi': 1, 'compression': 1, 'intell': 1, '567578': 1, '31': 1, 'bejnordi': 1, 'assessment': 1, 'metastases': 1, 'women': 1, 'jama': 1, '318': 1, '21992210': 1, 'augmented': 1, 'reality': 1, 'integration': 1, '14531457': 1, 'nagpal': 1, 'npj': 1, 'digit': 1, '34': 1, 'rmdl': 1, 'recalibrated': 1, 'multiinstance': 1, 'gastric': 1, '101549': 1, 'coudray': 1, 'mutation': 1, 'nonsmall': 1, '15591567': 1, '36': 1, 'campanella': 1, '13011309': 1, 'ilse': 1, 'tomczak': 1, 'welling': 1, 'machine': 1, 'lawrence': 1, 'reid': 1, '21322141': 1, 'pmlr': 1, '38': 1, 'maron': 1, 'lozanopérez': 1, 'jordan': 1, '570576': 1, '1998': 1, 'schaumberg': 1, 'multimodal': 1, 'pantissue': 1, 'pandisease': 1, 'search': 1, 'media': 1, '21692185': 1, 'bentaieb': 1, 'hamarneh': 1, '792802': 1, '41': 1, 'couture': 1, 'marron': 1, 'perou': 1, 'troester': 1, 'niethammer': 1, 'heterogeneous': 1, 'computerassisted': 1, 'intervention': 1, 'frangi': 1, '254262': 1, '42': 1, 'kraus': 1, 'ba': 1, 'frey': 1, 'classifying': 1, 'segmenting': 1, 'bioinformatics': 1, 'i52i59': 1, 'zhang': 1, 'platt': 1, 'viola': 1, 'boosting': 1, 'object': 1, 'weiss': 1, '14171424': 1, '2006': 1, '44': 1, 'berrada': 1, 'zisserman': 1, 'kumar': 1, 'topk': 1, '45': 1, 'crammer': 1, 'singer': 1, 'kernelbased': 1, 'machines': 1, '265292': 1, '2001': 1, '1399': 1, 'hestained': 1, 'sentinel': 1, 'gigascience': 1, 'giy065': 1, '47': 1, 'russakovsky': 1, 'imagenet': 1, 'scale': 1, 'recognition': 1, 'int': 1, 'comput': 1, 'vis': 1, '211252': 1, 'acknowledgements': 1, 'thank': 1, 'bruce': 1, 'bronstein': 1, 'cirelli': 1, 'sahai': 1, 'querying': 1, 'retrieving': 1, 'archival': 1, 'bragg': 1, 'zimmet': 1, 'mellen': 1, 'administrative': 1, 'support': 1, 'noor': 1, 'funds': 1, 'nigms': 1, 'r35gm138216a': 1, 'nsf': 1, 'graduate': 1, 'fellowship': 1, 'nhgri': 1, 't32hg002295': 1, 'responsibility': 1, 'reflect': 1, 'health': 1, 'foundation': 1, 'contributions': 1, 'conceived': 1, 'kw': 1, 'ty': 1, 'mb': 1, 'prepared': 1, 'declare': 1, 'httpsdoiorg101038s4155102000682w': 1, 'correspondence': 1, 'thanks': 1, 'anant': 1, 'geert': 1, 'anonymous': 1, 'reviewers': 1, 'reprints': 1, 'permissions': 1, 'wwwnaturecomreprints': 1, 'publishers': 1, 'neutral': 1, 'regard': 1, 'jurisdictional': 1, 'claims': 1, 'affiliations': 1, 'licence': 1, 'natbiomedeng570': 1, 'httpscamelyon17grandchallengeorgdata': 1, 'fieldspecific': 1, 'please': 1, 'behavioural': 1, 'ecological': 1, 'evolutionary': 1, 'environmental': 1, 'reference': 1, 'copy': 1, 'document': 1, 'naturecomdocumentsnrreportingsummaryflatpdf': 1, 'disclose': 1, 'disclosure': 1, 'statistical': 1, 'predetermine': 1, 'repositories': 1, 'tcgacptac': 1, 'split': 1, 'excluding': 1, 'n131': 1, 'n63': 1, 'n68': 1, 'n135': 1, 'n43': 1, 'brca': 1, 'mets': 1, 'n133': 1, 'n67': 1, 'n66': 1, 'n110': 1, 'n92': 1, 'n53': 1, 'n26': 1, 'n13': 1, 'since': 1, 'exclusions': 1, 'preestablished': 1, 'exclusion': 1, 'significant': 1, 'marking': 1, 'missing': 1, 'predominantly': 1, 'downloaded': 1, 'removed': 1, 'could': 1, 'owing': 1, 'file': 1, 'corruption': 1, 'absence': 1, 'lowresolution': 1, 'downsamples': 1, 'denomination': 1, 'times': 1, 'successful': 1, 'randomization': 1, 'groups': 1, 'covariates': 1, 'controlled': 1, 'listed': 1, 'item': 1, 'applies': 1, 'section': 1, 'antibodies': 1, 'eukaryotic': 1, 'lines': 1, 'palaeontology': 1, 'archaeology': 1, 'animals': 1, 'organisms': 1, 'dual': 1, 'concern': 1, 'chipseq': 1, 'flow': 1, 'mribased': 1, 'neuroimaging': 1, 'policy': 1, 'involving': 1, 'representing': 1, 'retrospectively': 1, 'oversight': 1, 'committee': 1})}, {'file_name': 'rosettafold', 'word_counts': Counter({'the': 291, 'of': 180, 'and': 171, 'a': 103, 'in': 94, 'to': 81, 'for': 66, 'with': 57, 'structure': 50, 'that': 43, 'from': 42, 'fig': 42, 'models': 40, 'e': 39, 'rosettafold': 39, 'by': 36, 'we': 35, 'are': 35, 'structures': 32, 'model': 32, 'protein': 31, 'is': 31, 'on': 31, 'network': 29, 't': 28, 'university': 28, 'n': 26, 'r': 26, 'at': 26, 'sequence': 26, 'al': 26, '2021': 26, 'c': 24, 'j': 24, 'research': 23, 's': 22, 'information': 21, 'o': 21, 'as': 20, 'i': 20, 'than': 19, 'h': 18, 'be': 18, 'et': 18, 'd': 17, 'm': 17, 'proteins': 16, 'more': 16, 'between': 16, 'or': 16, 'prediction': 15, 'an': 15, 'l': 15, 'can': 15, '3d': 14, 'into': 14, 'residues': 14, 'human': 14, 'b': 14, 'complex': 13, 'two': 13, 'casp14': 12, '2d': 12, 'available': 12, 'known': 12, 'final': 12, 'sequences': 12, 'science': 12, 'blue': 12, 'which': 11, '1d': 11, 'distance': 11, 'methods': 11, 'was': 11, 'all': 11, 'predicted': 11, 'å': 11, 'colored': 11, 'conserved': 11, 'could': 10, 'generated': 10, 'this': 10, 'ca': 10, 'were': 10, 'red': 10, 'terminus': 10, 'threetrack': 9, 'predictions': 9, 'modeling': 9, 'approaches': 9, 'coordinates': 9, 'used': 9, '20': 9, 'f': 9, '15': 9, 'experimental': 9, 'cryoem': 9, 'complexes': 9, 'accurate': 8, 'using': 8, 'map': 8, 'data': 8, 'pdb': 8, 'alphafold2': 8, 'twotrack': 8, 'these': 8, 'provide': 8, 'because': 8, 'not': 8, 'medical': 8, 'stanford': 8, '11': 8, 'top': 8, 'homologs': 8, 'shown': 8, 'performance': 7, 'unknown': 7, 'method': 7, 'have': 7, '3': 7, 'tion': 7, 'accuracy': 7, 'architecture': 7, 'back': 7, 'endtoend': 7, 'results': 7, 'generate': 7, 'institute': 7, 'molecular': 7, 'targets': 7, 'version': 7, 'g': 7, 'u': 7, 'determination': 7, 'no': 7, 'refined': 7, 'il12': 7, 'article': 6, 'v': 6, 'proteinprotein': 6, 'biological': 6, '2': 6, 'trrosetta': 6, 'baek': 6, '373': 6, '871': 6, '876': 6, 'august': 6, 'f6': 6, 'center': 6, 'graz': 6, 'downloaded': 6, 'httpswwwscienceorg': 6, 'chicago': 6, 'january': 6, '2025': 6, 'but': 6, 'three': 6, 'server': 6, 'chain': 6, 'chains': 6, 'p': 6, 'domain': 6, 'tango2': 6, '2020': 6, 'grant': 6, 'level': 5, 'solution': 5, 'currently': 5, 'has': 5, 'such': 5, 'multiple': 5, 'better': 5, 'structural': 5, 's1': 5, 'ing': 5, 'baker': 5, 'levels': 5, 'see': 5, 'processing': 5, 'each': 5, 'first': 5, 'pyrosetta': 5, 'biochemistry': 5, 'cambridge': 5, 'time': 5, '24': 5, 'out': 5, 'likely': 5, 'only': 5, 'it': 5, '7': 5, 'may': 5, 'error': 5, 'mr': 5, 'es': 5, 'carmsd': 5, 'function': 5, 'gray': 5, 'fold': 5, 'magenta': 5, 'prodomain': 5, 'pathogenic': 5, '2019': 5, 'fd': 5, 'deepmind': 4, 'critical': 4, 'architectures': 4, 'incorporate': 4, 'coordinate': 4, 'enables': 4, 'xray': 4, 'problems': 4, 'insights': 4, 'been': 4, 'struc': 4, '1': 4, 'large': 4, 'details': 4, 'work': 4, 'msas': 4, 'rather': 4, 'replacement': 4, 'attention': 4, 'use': 4, 'directly': 4, 'atomic': 4, 'parameters': 4, 'through': 4, 'input': 4, 'design': 4, '4': 4, 'absence': 4, 'published': 4, 'track': 4, 'bakerrosettaserver': 4, '1b': 4, 'after': 4, 'within': 4, 'training': 4, 'crops': 4, 'tures': 4, 'second': 4, 'washington': 4, 'texas': 4, 'southwestern': 4, 'dallas': 4, 'tx': 4, 'school': 4, 'medicine': 4, '94305': 4, 'berkeley': 4, 'layer': 4, '8': 4, 'full': 4, 'groups': 4, 'msa': 4, 'limitations': 4, 'side': 4, 'over': 4, 'predict': 4, 'new': 4, 'cameo': 4, 'ts': 4, 'further': 4, 'improve': 4, 'number': 4, 'will': 4, 'comparison': 4, 'key': 4, 'cases': 4, 'had': 4, 'slp': 4, 'lrbp': 4, '12': 4, 'nterminal': 4, 'density': 4, 'insight': 4, 'disease': 4, 'gpcr': 4, 'active': 4, 'there': 4, 'mutations': 4, '26': 4, 'diseases': 4, 'adam33': 4, 'cers1': 4, 'w': 4, 'pda': 4, 'mb': 4, 'db': 4, 'grl': 4, 'foundation': 4, 'rjr': 4, 'interactions': 3, 'k': 3, 'presented': 3, 'recent': 3, 'assessment': 3, 'those': 3, 'rapid': 3, 'challenging': 3, 'crystallography': 3, 'provides': 3, 'individual': 3, 'subunits': 3, 'docking': 3, 'make': 3, 'amino': 3, 'acid': 3, 'ture': 3, 'casp': 3, 'deeplearning': 3, 'pro': 3, 'whether': 3, 'com': 3, 'features': 3, 'along': 3, 'se3equivariant': 3, 'predic': 3, 'biology': 3, 'table': 3, 'distances': 3, 'backbone': 3, 'relationships': 3, 'hardware': 3, 'memory': 3, 'many': 3, 'orientation': 3, 'residue': 3, 'seattle': 3, 'wa': 3, '98195': 3, 'harvard': 3, 'hughes': 3, 'national': 3, 'focus': 3, 'department': 3, 'usa': 3, 'author': 3, 'inference': 3, 're': 3, 'central': 3, 'tested': 3, 'our': 3, 'case': 3, 'accu': 3, 'other': 3, 'intensive': 3, 'whereas': 3, 'single': 3, 'template': 3, 'requires': 3, '10': 3, 'gpu': 3, 'compared': 3, 'approach': 3, 'once': 3, 'sets': 3, 'similar': 3, 'tracks': 3, 'suggests': 3, 'gpcrs': 3, 'tests': 3, 'any': 3, 'ec': 3, 'during': 3, '19': 3, '9': 3, 'experimented': 3, 'templates': 3, 'tmscore': 3, 'based': 3, 'able': 3, 'very': 3, 'often': 3, 'us': 3, 'surface': 3, 'closest': 3, 'nt': 3, '13': 3, 'building': 3, '14': 3, 'em': 3, '30': 3, 'bottom': 3, 'providing': 3, 'close': 3, 'actual': 3, 'rms': 3, 'ranging': 3, 'greater': 3, 'atoms': 3, 'superimposed': 3, 'rainbow': 3, 'without': 3, 'membrane': 3, 'adopts': 3, 'ntn': 3, 'enzyme': 3, 'spheres': 3, 'metallopro': 3, 'tease': 3, '21': 3, 'metalloprotease': 3, '25': 3, 'barrel': 3, 'transmembrane': 3, 'ceramide': 3, 'asp213': 3, '31': 3, 'mutation': 3, 'site': 3, 'ortholog': 3, 'conservation': 3, 'variable': 3, 'q': 3, 'so': 3, 'receptor': 3, 'il23': 3, 'interaction': 3, 'il12p35': 3, 'il12rb2': 3, 'determined': 3, 'sci': 3, 'nucleic': 3, 'acids': 3, '2006': 3, 'tpk': 3, 'technology': 3, 'djo': 3, 'under': 3, 'cb': 3, 'hp': 3, 'kcg': 3, 'nvg': 3, 'analyzed': 3, 'materials': 3, 'deposited': 3, 'folding': 2, 'van': 2, '14th': 2, 'conference': 2, 'explored': 2, 'related': 2, 'obtained': 2, 'best': 2, 'transformed': 2, 'integrated': 2, 'accuracies': 2, 'microscopy': 2, 'also': 2, 'generation': 2, 'alone': 2, 'traditional': 2, 'scientific': 2, 'community': 2, 'extract': 2, 'httpspredictioncenterorgcasp14': 2, 'zscores_finalcgi': 2, 'learn': 2, 'achieved': 2, 'described': 2, 'ments': 2, 'mechanism': 2, 'distant': 2, 'iteratively': 2, 'forth': 2, 'net': 2, 'maps': 2, 'layers': 2, 'development': 2, 'ent': 2, 'different': 2, 'parts': 2, 'networks': 2, 'summarized': 2, 'producing': 2, 'parallel': 2, 'alignment': 2, 'con': 2, 'reasoned': 2, 'third': 2, 'oper': 2, 'ating': 2, 'residueresidue': 2, 'coor': 2, 'dinates': 2, 'depicted': 2, 'reason': 2, 'about': 2, 'contrast': 2, 'reasoning': 2, 'architec': 2, 'complete': 2, 'extent': 2, 'computer': 2, 'train': 2, 'averaged': 2, 'then': 2, 'fed': 2, 'sciences': 2, 'division': 2, 'ma': 2, '02138': 2, 'program': 2, 'biophysics': 2, 'south': 2, 'victoria': 2, 'bc': 2, '6': 2, 'bone': 2, 'ac': 2, 'curacy': 2, 'gpus': 2, '400': 2, 'resi': 2, 'dues': 2, 'gb': 2, 'sidechain': 2, 'cpu': 2, 'zhang': 2, 'racy': 2, 's2': 2, 'eo': 2, 'good': 2, 'made': 2, 'min': 2, 'rtx2080': 2, '5': 2, 'allatom': 2, 'poorer': 2, 'less': 2, 'simultaneously': 2, 'informa': 2, 'low': 2, 'computational': 2, 'public': 2, 'below': 2, 'blind': 2, 'y': 2, 'sa': 2, 'experiment': 2, 'servers': 2, 'they': 2, 'evaluated': 2, 'hard': 2, 'june': 2, 'including': 2, 'sampling': 2, 'average': 2, 'both': 2, 'benchmark': 2, 'range': 2, 'predictor': 2, 's3': 2, 'found': 2, 'combining': 2, 'discontin': 2, 'uous': 2, 'moreaccurate': 2, 'most': 2, 'relevant': 2, 'up': 2, 'uses': 2, 'required': 2, 'enabling': 2, 'considerable': 2, 'investigated': 2, 'electron': 2, 'quite': 2, 'much': 2, 'test': 2, 'previously': 2, 'datasets': 2, 'resolution': 2, 'glyat': 2, 'bacterial': 2, 'oxidoreductase': 2, '2a': 2, 's5c': 2, 'sufficient': 2, 'similarity': 2, 'deepaccnet': 2, 'solutions': 2, 'successful': 2, 'pre': 2, 'viously': 2, 'failed': 2, 'part': 2, 'cterminal': 2, 'hhsearch': 2, 'p101': 2, 'binding': 2, 'gbd': 2, 'pi3kg': 2, '40': 2, 'readily': 2, 'local': 2, '2c': 2, 'root': 2, 'mean': 2, 'square': 2, 'sheets': 2, 'states': 2, 'estimated': 2, '35': 2, 'open': 2, 'lead': 2, 'contain': 2, 'meth': 2, 'mod': 2, 'els': 2, 'domains': 2, 'illustrate': 2, 'golgi': 2, 'organization': 2, 'role': 2, '16': 2, '17': 2, '3a': 2, 'activesite': 2, 'orthologs': 2, 'superfamily': 2, 'suggest': 2, '18': 2, 'near': 2, 'adam': 2, 'disintegrin': 2, '22': 2, '23': 2, 'lipocalinlike': 2, 'includes': 2, 'inhibitors': 2, 'cysteine': 2, 'activity': 2, 'line': 2, 'interact': 2, 'synthase': 2, 'metab': 2, 'acyl': 2, '27': 2, 'tmh': 2, '28': 2, '29': 2, 'his182': 2, 'trp298': 2, 'well': 2, 'h183q': 2, 'neighboring': 2, 'direct': 2, 'generates': 2, 'possible': 2, 'rigidbody': 2, 'addition': 2, 'compute': 2, 'represented': 2, 'labeled': 2, 'sticks': 2, 'cavity': 2, 'construction': 2, 'dt': 2, '32': 2, '33': 2, 'when': 2, 'interleukin12': 2, 'il12r': 2, 'il12rb2a': 2, '34': 2, 'fits': 2, 'experimentally': 2, 'smallmolecule': 2, 'references': 2, 'techniques': 2, 'natl': 2, 'acad': 2, 'anishchenko': 2, 'ovchinnikov': 2, '2010': 2, 'haas': 2, '2018': 2, '87': 2, 'hiranuma': 2, 'am': 2, 'hum': 2, 'genet98': 2, '2016': 2, 'biol': 2, 'cell': 2, 'subunit': 2, '2004': 2, 'kim': 2, '36': 2, 'zenodo': 2, 'collection': 2, 'source': 2, 'acknowledge': 2, 'facilities': 2, 'support': 2, 'jhp': 2, 'ad': 2, 'avr': 2, 'supported': 2, 'energy': 2, 'office': 2, 'schmidt': 2, 'jw': 2, 'ia': 2, 'wellcome': 2, 'trust': 2, 'health': 2, 'numbers': 2, 'jeb': 2, 'cky': 2, 'fund': 2, 'aavd': 2, 'ace': 2, 'qc': 2, 'lnk': 2, 'crg': 2, 'manuscript': 2, 'authors': 2, 'interests': 2, 'code': 2, 'accepted': 2, 'july': 2, 'neural': 1, 'minkyung': 1, 'baek12': 1, 'frank': 1, 'dimaio12': 1, 'ivan': 1, 'anishchenko12': 1, 'justas': 1, 'dauparas12': 1, 'sergey': 1, 'ovchinnikov34': 1, 'gyu': 1, 'rie': 1, 'lee12': 1, 'jue': 1, 'wang12': 1, 'qian': 1, 'cong56': 1, 'lisa': 1, 'kinch7': 1, 'dustin': 1, 'schaeffer6': 1, 'claudia': 1, 'millán8': 1, 'hahnbeom': 1, 'park12': 1, 'carson': 1, 'adams12': 1, 'caleb': 1, 'glassman91011': 1, 'andy': 1, 'degiovanni12': 1, 'jose': 1, 'pereira12': 1, 'andria': 1, 'rodrigues12': 1, 'alberdina': 1, 'dijk13': 1, 'ana': 1, 'ebrecht13': 1, 'diederik': 1, 'opperman14': 1, 'theo': 1, 'sagmeister15': 1, 'christoph': 1, 'buhlheller1516': 1, 'tea': 1, 'pavkovkeller1517': 1, 'manoj': 1, 'rathinaswamy18': 1, 'udit': 1, 'dalwadi19': 1, 'calvin': 1, 'yip19': 1, 'john': 1, 'burke18': 1, 'christopher': 1, 'garcia9101120': 1, 'nick': 1, 'grishin6721': 1, 'paul': 1, 'adams1222': 1, 'randy': 1, 'read8': 1, 'david': 1, 'baker1223': 1, 'notably': 1, 'ideas': 1, 'onedimensional': 1, 'successively': 1, 'produces': 1, 'approaching': 1, 'cryoelectron': 1, 'functions': 1, 'shortcircuiting': 1, 'require': 1, 'followed': 1, 'speed': 1, 'he': 1, 'longstanding': 1, 'challenge': 1, 'biannual': 1, 'meetings': 1, 'demonstrated': 1, 'alphafold': 1, 'base': 1, 'bank': 1, 'outperform': 1, 'explicitly': 1, 'cess': 1, 'outstanding': 1, 'deepminds': 1, 'meeting': 1, 'leftthe': 1, 'eager': 1, 'beyond': 1, 'overall': 1, 'frame': 1, 'raised': 1, 'ques': 1, 'outside': 1, 'worldleading': 1, 'pany': 1, 'methodological': 1, 'advances': 1, 'cluded': 1, 'starting': 1, 'align': 1, 'moreprocessed': 1, 'inverse': 1, 'covariance': 1, 'matrices': 1, 'derived': 1, 'ii': 1, 'dimensional': 1, 'convolution': 1, 'represents': 1, 'iii': 1, 'passed': 1, 'iv': 1, 'transformer': 1, 'refine': 1, 'previous': 1, 'learning': 1, 'optimized': 1, 'propagation': 1, 'co': 1, 'ordinates': 1, 'intrigued': 1, 'goal': 1, 'increasing': 1, 'advancing': 1, 'differ': 1, 'combinations': 1, 'five': 1, 'properties': 1, 'experi': 1, 'mented': 1, 'wide': 1, 'variety': 1, 'passing': 1, 'succeeded': 1, 'flow': 1, 'matrix': 1, 'siderably': 1, 'nextbest': 1, 'extending': 1, 'space': 1, 'tighter': 1, 'connection': 1, 'orientations': 1, 'constructed': 1, 'augmented': 1, 'operating': 1, '1a': 1, 'flows': 1, 'formation': 1, 'allowing': 1, 'collect': 1, 'ively': 1, 'happens': 1, 'although': 1, 'does': 1, 'link': 1, 'some': 1, 'lim': 1, 'itations': 1, 'millions': 1, 'instead': 1, 'discontinu': 1, 'ous': 1, 'consisting': 1, 'discontinuous': 1, 'segments': 1, 'span': 1, 'ning': 1, 'total': 1, '260': 1, 'combined': 1, 'fea': 1, 'tions': 1, 'produced': 1, 'distributions': 1, '5t': 1, 'og': 1, 'ea': 1, '1o': 1, '1department': 1, 'usa2institute': 1, 'usa3faculty': 1, 'arts': 1, 'usa4john': 1, 'distinguished': 1, 'fellowship': 1, 'usa5eugene': 1, 'mcdermott': 1, 'growth': 1, 'usa6department': 1, 'usa7howard': 1, 'usa8department': 1, 'haematology': 1, 'uk9program': 1, 'immunology': 1, 'usa10department': 1, 'cellular': 1, 'physiology': 1, 'usa11department': 1, 'usa12molecular': 1, 'bioimaging': 1, 'lawrence': 1, 'laboratory': 1, 'usa13department': 1, 'area': 1, 'metabolomics': 1, 'northwest': 1, '2531': 1, 'potchefstroom': 1, 'africa14department': 1, 'biotechnology': 1, 'free': 1, 'state': 1, '205': 1, 'nelson': 1, 'mandela': 1, 'drive': 1, 'bloemfontein': 1, '9300': 1, 'africa15institute': 1, 'biosciences': 1, 'humboldtstrasse': 1, '50': 1, '8010': 1, 'austria16medical': 1, 'austria17biotechmedgraz': 1, 'austria18department': 1, 'microbiology': 1, 'canada19life': 1, 'british': 1, 'columbia': 1, 'vancouver': 1, 'canada20howard': 1, 'usa21department': 1, '22department': 1, 'bioengineering': 1, 'california': 1, '94720': 1, 'usa23howard': 1, 'corresponding': 1, 'email': 1, 'dabakeruwedu': 1, 'refer': 1, 'perresidue': 1, 'advantages': 1, 'requiring': 1, 'lowermemory': 1, 'graphics': 1, 'units': 1, 'gigabytes': 1, 'quires': 1, 'unit': 1, 'step': 1, 'infor': 1, 'mation': 1, 'flowing': 1, 'clearly': 1, 'performing': 1, 'group': 1, 'ranked': 1, 'among': 1, 'cor': 1, 'relation': 1, 'depth': 1, 'lower': 1, 'ep': 1, 'ft': 1, 'km': 1, 'still': 1, 'reflect': 1, 'limited': 1, 'size': 1, 'explore': 1, 'alternative': 1, 'loss': 1, 'formulations': 1, 'reported': 1, 'several': 1, 'days': 1, 'pass': 1, 'same': 1, 'manner': 1, 'would': 1, 'search': 1, 'hours': 1, 'coordi': 1, 'nates': 1, 'fewer': 1, 'calculations': 1, 'hour': 1, 'gen': 1, 'eration': 1, 'cores': 1, 'incomplete': 1, 'opti': 1, 'mization': 1, 'due': 1, 'neglect': 1, 'explain': 1, 'end': 1, 'toend': 1, 'latter': 1, 'incorporates': 1, 'relaxation': 1, 'stage': 1, 'main': 1, 'body': 1, 'added': 1, 'gain': 1, 'se3': 1, 'expect': 1, 'ulti': 1, 'mately': 1, 'least': 1, 'puter': 1, 'overcome': 1, 'incorporated': 1, 'improved': 1, 'identical': 1, 'attentionbased': 1, 'operations': 1, 'mode': 1, 'representations': 1, 'effectively': 1, 'sequencestructure': 1, 'relatively': 1, 'cost': 1, 'makes': 1, 'straightforward': 1, 'example': 1, 'proteincoupled': 1, 'receptors': 1, 'needed': 1, 'assess': 1, 'held': 1, 'every': 1, '2y': 1, 'evaluation': 1, 'blindly': 1, 'submitted': 1, 'since': 1, '69': 1, 'me': 1, 'dium': 1, 'released': 1, 'performed': 1, 'robetta': 1, 'intfold6': 1, 'bestsingletemplate': 1, 'swiss': 1, 'fur': 1, 'ther': 1, 'improving': 1, 'take': 1, '2o': 1, 'connections': 1, 'allow': 1, 'zhangserver': 1, 'completely': 1, 'automated': 1, 'medium': 1, 'values': 1, 'website': 1, 'httpscameo3dorg': 1, 'bars': 1, 'represent': 1, '95': 1, 'confidence': 1, 'interval': 1, 'coupling': 1, 'feeding': 1, 'random': 1, 'sub': 1, 'sample': 1, 'broader': 1, 'ensembles': 1, 'contained': 1, 'higheraccuracy': 1, 'consistently': 1, 'identify': 1, 'singlepass': 1, 'nevertheless': 1, 'suspect': 1, 'perform': 1, 'ance': 1, 'carrying': 1, 'investi': 1, 'gations': 1, 'lines': 1, 'developing': 1, 'predicting': 1, 'entire': 1, 's4a': 1, 'hypothesized': 1, 'arises': 1, 'selecting': 1, 'region': 1, 'aligned': 1, 's4b': 1, 'enable': 1, 'gion': 1, 'while': 1, 'keeping': 1, 'access': 1, 'memoryefficient': 1, 'way': 1, 'perceiver': 1, 'updating': 1, 'smallerseed': 1, '100': 1, 'extra': 1, 'thousands': 1, 'crossattention': 1, 's4c': 1, 'now': 1, '1000': 1, 'addi': 1, '10000': 1, 'initial': 1, 'promising': 1, 's4d': 1, 'rigorous': 1, 'progress': 1, 'tein': 1, 'question': 1, 'what': 1, 'utility': 1, 'facilitate': 1, 'cryo': 1, 'build': 1, 'sights': 1, 'rate': 1, 'higher': 1, 'prompted': 1, 'help': 1, 'solve': 1, 'unsolved': 1, 'chal': 1, 'lenging': 1, 'borderline': 1, 'fourrecent': 1, 'crystallographic': 1, 'limits': 1, 'eluded': 1, 'reanalyzed': 1, 'glycine': 1, 'nacyltransferase': 1, 'frombos': 1, 'taurus': 1, 's5a': 1, 's5b': 1, 'secreted': 1, 'fungus': 1, 'phanerochaete': 1, 'chrysosporiumfig': 1, '2b': 1, 'four': 1, 'true': 1, 'enabled': 1, 'per': 1, 'estimates': 1, 'allowed': 1, 'weighted': 1, 'heavily': 1, 'increased': 1, 'success': 1, 'didnot': 1, 'yield': 1, 'determine': 1, 'why': 1, 'where': 1, 'crystal': 1, 'images': 1, 's5': 1, 'show': 1, 'homolog': 1, 'fs': 1, 'yad': 1, 'tm': 1, 'covering': 1, '38': 1, 'portion': 1, 'detected': 1, 'assem': 1, 'blies': 1, 'gbg': 1, 'hetero': 1, 'dimeric': 1, 'hit': 1, 'statistically': 1, 'insignificant': 1, 'evalue': 1, 'covers': 1, '167': 1, 'fit': 1, 'despite': 1, 'res': 1, 'olution': 1, 'tf': 1, 'dw': 1, 'ht': 1, 'ai': 1, 's6': 1, 'de': 1, 'viation': 1, 'coreb': 1, 'vide': 1, 'func': 1, 'similarly': 1, 'focused': 1, 'set': 1, 'implicated': 1, 'deter': 1, 'mined': 1, 'showed': 1, 'inactive': 1, 'even': 1, 'current': 1, 'databases': 1, 's7': 1, 'quality': 1, 'measure': 1, 's7d': 1, '3o': 1, 'rosettafolda': 1, '067': 1, 'ninetyfive': 1, 'yielding': 1, '098': 1, '54': 1, '4l3a': 1, 'brown': 1, '169': 1, 'having': 1, 'omitted': 1, 'heterodimeric': 1, 'consistent': 1, 'allb': 1, 'topology': 1, 'clear': 1, 'correspondence': 1, 'theb': 1, 'figure': 1, 'prepared': 1, 'chimerax': 1, 'accompanying': 1, 'closed': 1, 'how': 1, 'iden': 1, 'tified': 1, 'causing': 1, 'haveb': 1, 'investigation': 1, 'ods': 1, '693': 1, 'onethird': 1, 'difference': 1, 'lddt': 1, '08': 1, 'corresponded': 1, 's8': 1, 'here': 1, 'examples': 1, 'ways': 1, 'mecha': 1, 'nisms': 1, 'deficiencies': 1, 'transport': 1, 'metabolic': 1, 'dis': 1, 'orders': 1, 'plays': 1, 'redistribution': 1, 'en': 1, 'doplasmic': 1, 'reticulum': 1, 'nu': 1, 'cleophile': 1, 'aminohydrolase': 1, 'wellaligned': 1, '3b': 1, 'members': 1, 'sim': 1, 'ilar': 1, 'hydrolyzes': 1, 'carbonnitrogen': 1, 'bond': 1, 'mem': 1, 'brane': 1, 'component': 1, 'cause': 1, 'act': 1, 'hindering': 1, 'cat': 1, 'alysis': 1, 'arg26lys': 1, 'r26k': 1, 'arg32gln': 1, 'r32q': 1, 'leu50pro': 1, 'l50p': 1, 'thea': 1, 'produce': 1, 'steric': 1, 'clashes': 1, 'gly154arg': 1, 'g154r': 1, 'hydrophobic': 1, 'core': 1, 'homology': 1, 'identity': 1, 'shifts': 1, 'misplace': 1, 's9': 1, 'adamts': 1, 'metal': 1, 'loproteinase': 1, 'thrombospondin': 1, 'motifs': 1, 'families': 1, 'metalloproteases': 1, 'encoded': 1, 'genes': 1, 'mediate': 1, 'cellcell': 1, 'cellmatrix': 1, 'involved': 1, 'includ': 1, 'cancer': 1, 'metastasis': 1, 'inflammatory': 1, 'disorders': 1, 'neurological': 1, 'asthma': 1, 'adams': 1, 'rosettafoldpredicted': 1, 'prodomainhas': 1, 'bbarrel': 1, '3c': 1, 'belongs': 1, 'ex': 1, 'tended': 1, 'extension': 1, 'following': 1, 'taken': 1, 'together': 1, 'consist': 1, 'inhibits': 1, 'switch': 1, 'one': 1, 'spanning': 1, 'sphingolipid': 1, 'olism': 1, 'coenzyme': 1, 'acylcoa': 1, 'ceramides': 1, 'various': 1, 'lengths': 1, 'regulate': 1, 'differentiation': 1, 'prolifer': 1, 'ation': 1, 'apoptosis': 1, 'enzymes': 1, 'their': 1, 'helices': 1, '98': 1, '304': 1, 'pfam': 1, 'tlc': 1, 'six': 1, 'traverse': 1, 'down': 1, 'arrange': 1, 'ment': 1, '3e': 1, 'crevice': 1, 'extends': 1, 'lined': 1, 'quired': 1, '30o': 1, 'mu': 1, 'tation': 1, 'his183gln': 1, 'progres': 1, 'sive': 1, 'myoclonus': 1, 'epilepsy': 1, 'dementia': 1, 'decreases': 1, 'composition': 1, 'his182a': 1, 'p213': 1, 'potentially': 1, 'ser212': 1, 'testable': 1, 'reaction': 1, 'mechanisms': 1, '3f': 1, 'seg': 1, 'break': 1, 'them': 1, 'seamlessly': 1, 'handle': 1, 'breaks': 1, 'might': 1, 'se': 1, 'quences': 1, 'output': 1, 'being': 1, 'thus': 1, 'plexes': 1, 'short': 1, 'circuiting': 1, 'standard': 1, 'procedure': 1, 'carry': 1, 'great': 1, 'reduction': 1, '4o': 1, 'functiona': 1, 'hydrolase': 1, 'sites': 1, 'stick': 1, 'format': 1, 'wildtype': 1, 'select': 1, 'rendering': 1, 'highlighting': 1, 'patch': 1, 'tmh2': 1, 'sphere': 1, 'zoomin': 1, 'contribute': 1, 'catalysis': 1, 'adjacent': 1, 'asp': 1, 'his': 1, 'gln': 1, 'arg': 1, 'trp': 1, 'ni': 1, '0m': 1, 'na2': 1, 'bt': 1, 'nr': 1, 'x': 1, 'implementsflexible': 1, 'almost': 1, 'context': 1, 'endto': 1, 'kn': 1, 'ko': 1, 'np': 1, 'ds': 1, 'alignments': 1, '4f': 1, 'rd': 1, 'containing': 1, '4a': 1, '4b': 1, 'resulting': 1, 'tem': 1, 'plate': 1, 'score': 1, '0': 1, 'coevolution': 1, 'paired': 1, 'contrib': 1, 'utes': 1, 'placement': 1, 'avail': 1, 's10': 1, 'trained': 1, 'monomeric': 1, 'yb': 1, 'tb': 1, 'si': 1, 'none': 1, 'application': 1, 'fourchain': 1, '4c': 1, 's11': 1, 'indicated': 1, 'topol': 1, 'ogy': 1, 'however': 1, 'observe': 1, 'detailed': 1, 'understanding': 1, 'im': 1, 'portant': 1, 'dissecting': 1, 'specific': 1, 'actions': 1, 'generating': 1, 'block': 1, 'affecting': 1, 'signaling': 1, 'identified': 1, 'shared': 1, 'tyr189': 1, 'gly115': 1, 'analogous': 1, 'packing': 1, 'tween': 1, 'trp156': 1, 'il23p19': 1, 'gly116': 1, 'il23r': 1, 'peptide': 1, 'p26': 1, 'nearby': 1, 'lys190': 1, 'lys194': 1, 'avenue': 1, 'specifically': 1, 'target': 1, 'conclusions': 1, 'rapidly': 1, 'multiprotein': 1, 'assemblies': 1, 'coupled': 1, 'existing': 1, 'binder': 1, 'odology': 1, 'discovery': 1, 'ligands': 1, 'interest': 1, 'simultaneous': 1, 'opens': 1, 'door': 1, 'straints': 1, 'notes': 1, 'senior': 1, 'nature': 1, '577': 1, '706710': 1, 'jumper': 1, 'fourteenth': 1, 'abstract': 1, 'book': 1, 'pp': 1, '2224': 1, 'yang': 1, 'proc': 1, 'usa117': 1, '14961503': 1, 'chidyausiku': 1, 'pellock': 1, 'biorxiv': 1, '20200722211482': 1, 'preprint': 1, 'httpsdoiorg10110120200722211482': 1, 'chaudhury': 1, 'lyskov': 1, 'graybioinformatics': 1, '689691': 1, 'fuchs': 1, 'worrall': 1, 'fischer': 1, 'welling': 1, 'arxiv200610503': 1, 'cslg': 1, '86': 1, '387398': 1, 'mcguffin': 1, 'res47': 1, 'w408w413': 1, '13781387': 1, 'waterhouseet': 1, 'res46w': 1, '6w303': 1, 'jaegle': 1, 'arxiv210303206': 1, 'cscv': 1, 'nat': 1, 'commun': 1, '1340': 1, 'steinegger': 1, 'bmc': 1, 'bioinformatics': 1, '473': 1, 'kooistraet': 1, 'res49': 1, 'd335d343': 1, 'bender': 1, 'marlow': 1, 'meilerplos': 1, 'comput': 1, 'biol16': 1, 'e1007597': 1, 'kremeret': 1, '358362': 1, 'rabouille': 1, 'kondylisgenome': 1, '213': 1, 'milevet': 1, 'inherit': 1, 'dis44': 1, '426437': 1, 'lalaniet': 1, '347357': 1, 'wolfsberg': 1, 'primakoff': 1, 'myles': 1, 'white': 1, 'biol131': 1, '275278': 1, '1995': 1, 'klein': 1, 'bischoffj': 1, 'proteome': 1, 'res101': 1, '733': 1, '2011': 1, '5o': 1, 'escherichia': 1, 'coli': 1, 'left': 1, 'right': 1, 'tmscores': 1, 'indicate': 1, 'cyan': 1, 'emd21645': 1, 'zhong': 1, 'khalilbiochem': 1, 'pharmacol': 1, '164': 1, '188204': 1, 'orthet': 1, 'mol': 1, 'biol335': 1, '129137': 1, 'takeda': 1, 'igarashi': 1, 'mori': 1, 'arakiembo': 1, '23882396': 1, 'flower': 1, 'north': 1, 'sansombiochim': 1, 'biophys': 1, 'acta': 1, '14829': 1, '2000': 1, 'wart': 1, 'birkedalhansenproc': 1, '55785582': 1, '1990': 1, 'levy': 1, 'futermaniubmb': 1, 'life': 1, '62': 1, '347356': 1, 'mestre': 1, 'sh': 1, 'shin': 1, 'futermancell': 1, 'signal': 1, '82': 1, '109958': 1, 'winter': 1, 'pontingtrends': 1, 'biochem': 1, 'sci27': 1, '381383': 1, '2002': 1, 'spassievaet': 1, 'chem281': 1, '3393133938': 1, 'vanni': 1, 'ann': 1, 'neurol': 1, '76': 1, '206212': 1, '2014': 1, 'cong': 1, 'bakerscience': 1, '365': 1, '185189': 1, 'skolnickproteins': 1, '57': 1, '702710': 1, 'glassmanet': 1, '184': 1, '983999e24': 1, 'pettersenet': 1, '307': 1, '082': 1, 'baeket': 1, 'release': 1, 'httpszenodoorgrecord5068265': 1, 'acknowledgments': 1, 'thank': 1, 'horvitz': 1, 'juergens': 1, 'mansoor': 1, 'tischer': 1, 'helpful': 1, 'discussions': 1, 'webserver': 1, 'goldschmidt': 1, 'computing': 1, 'resource': 1, 'management': 1, 'thanks': 1, 'nidetzky': 1, 'monschein': 1, 'samples': 1, 'crystallization': 1, 'acknowledges': 1, 'assistance': 1, 'scientists': 1, 'diamond': 1, 'light': 1, 'beamline': 1, 'i04': 1, 'proposal': 1, 'mx20303': 1, 'esrf': 1, 'id303': 1, 'grenoble': 1, 'france': 1, 'desy': 1, 'p11': 1, 'petraiii': 1, 'hamburg': 1, 'germany': 1, 'provision': 1, 'synchrotronradiation': 1, 'joint': 1, 'bioenergy': 1, 'environmental': 1, 'contract': 1, 'deac0205ch11231': 1, 'lbnl': 1, 'funding': 1, 'microsoft': 1, 'generous': 1, 'gifts': 1, 'azure': 1, 'expertise': 1, 'philanthropy': 1, 'recommendation': 1, 'futures': 1, 'cyberinfrastructure': 1, 'award': 1, 'dbi': 1, '1937533': 1, '209407z17z': 1, 'institutes': 1, 'p01gm063210': 1, 'dp5od026389': 1, 'ro1ai51321': 1, 'gm127390': 1, 'mathers': 1, 'canadian': 1, 'cihr': 1, 'project': 1, '168998': 1, '168907': 1, 'welch': 1, 'i1505': 1, 'global': 1, 'challenges': 1, 'gcrf': 1, 'council': 1, 'stfc': 1, 'str0027541': 1, 'synchrotron': 1, 'african': 1, 'start': 1, 'austrian': 1, 'fwf': 1, 'projects': 1, 'p29432': 1, 'doc50': 1, 'docfund': 1, 'metabolism': 1, 'tpkauthor': 1, 'contributionsmb': 1, 'designed': 1, 'jd': 1, 'developed': 1, 'rds': 1, 'il12ril12': 1, 'cm': 1, 'worked': 1, 'mkr': 1, 'ud': 1, 'provided': 1, 'wrote': 1, 'discussed': 1, 'commented': 1, 'manuscriptcompeting': 1, 'declare': 1, 'competing': 1, 'availabilitythe': 1, 'httpfilesipduw': 1, 'edupubrosettafoldall_human_gpcr_unknown_modelstargz': 1, 'httpfilesipduwedupubrosettafoldgpcr_benchmark_': 1, 'one_state_unknown_modelstargz': 1, 'structurally': 1, 'uncharacterized': 1, 'httpfilesipduwedupubrosettafoldhuman_prottargz': 1, 'pi3k': 1, 'accession': 1, '7mez': 1, 'httpfilesipduwedupubrosettafoldmr_modelstargz': 1, 'completed': 1, 'httpsrobetta': 1, 'bakerlaborg': 1, 'option': 1, 'httpsgithubcom': 1, 'rosettacommonsrosettafold': 1, 'funded': 1, 'whole': 1, '209407': 1, 'z17z': 1, 'coalition': 1, 'aam': 1, 'cc': 1, 'copyright': 1, 'license': 1, 'supplementary': 1, 'sciencesciencemagorgcontent3736557871suppldc1': 1, 'figs': 1, 's17': 1, 'tables': 1, 's4': 1, '3782': 1, 'mdar': 1, 'reproducibility': 1, 'checklist': 1, 'viewrequest': 1, 'protocol': 1, 'paper': 1, 'frombioprotocol': 1, 'online': 1, '101126scienceabj8754': 1, '6o': 1})}, {'file_name': 'alphafold', 'word_counts': Counter({'the': 650, 'of': 349, 'and': 282, 'a': 246, 'in': 178, 'for': 160, 'to': 157, 'structure': 111, 'protein': 104, 'is': 100, 'with': 94, 'we': 87, 'that': 76, 'on': 75, 'are': 72, 'from': 58, 'msa': 57, 'as': 53, 'j': 51, 'alphafold': 49, 'prediction': 48, 'structures': 48, 'this': 47, 'methods': 47, 'accuracy': 47, 'sequence': 44, 'network': 44, 'used': 44, 'by': 42, 'pdb': 42, 'representation': 42, 'data': 41, 'using': 40, 'supplementary': 36, 'or': 34, 'm': 33, 'fig': 32, 'an': 31, 'residues': 31, 'sequences': 31, 'each': 31, 'et': 30, 'al': 30, 'proteins': 29, 'training': 29, 'chains': 28, 't': 28, 'information': 27, 'at': 26, 'see': 26, 'attention': 26, 'pair': 26, '2021': 25, 'was': 25, 'which': 24, 'learning': 24, 'not': 24, '2020': 24, 'were': 24, 'all': 24, 'c': 24, '3d': 23, 'model': 22, 'details': 22, 'k': 22, 'single': 21, 'casp14': 21, 'use': 21, 'nature': 20, 'neural': 20, 'full': 20, 's': 20, 'r': 20, '2019': 20, 'residue': 19, 'set': 19, 'update': 19, 'accurate': 18, 'research': 18, 'than': 18, 'no': 18, 'our': 18, 'these': 18, 'b': 18, 'n': 18, 'number': 18, 'have': 17, 'confidence': 17, 'predicted': 17, 'search': 17, 'available': 16, 'backbone': 16, 'å': 16, '95': 16, 'template': 16, 'within': 16, 'final': 16, 'experimental': 15, 'but': 15, 'deep': 15, 'has': 15, 'chain': 15, 'models': 14, '40': 14, 'triangle': 14, 'frame': 14, 'bioinformatics': 13, 'more': 13, 'version': 13, 'also': 13, 'be': 13, 'loss': 13, 'd': 13, 'evoformer': 13, 'module': 13, 'e': 13, '30': 13, 'biol': 13, 'around': 12, 'structural': 12, 'able': 12, 'both': 12, 'ipa': 12, 'proc': 12, 'bfd': 12, 'other': 11, 'into': 11, 'provided': 11, 'different': 11, 'dataset': 11, 'new': 11, '0': 11, 'without': 11, 'atoms': 11, 'block': 11, 'identity': 11, '20': 11, 'further': 11, 'points': 11, 'nat': 11, 'downloaded': 11, 'can': 10, 'coverage': 10, 'recent': 10, 'first': 10, 'predict': 10, 'assessment': 10, 'two': 10, 'analysis': 10, 'casp': 10, 'rmsd95': 10, 'cα': 10, 'domains': 10, 'high': 10, '1': 10, '2': 10, '48': 10, 'blocks': 10, 'time': 10, 'edges': 10, '80': 10, 'networks': 10, 'one': 10, 'trained': 10, 'depth': 10, 'reporting': 10, 'code': 10, 'august': 9, 'highly': 9, 'their': 9, 'computational': 9, 'its': 9, 'evolutionary': 9, 'very': 9, 'predictions': 9, 'test': 9, 'median': 9, 'interval': 9, '14': 9, 'domain': 9, 'templates': 9, 'lddtcα': 9, 'global': 9, 'multiple': 9, 'input': 9, 'local': 9, 'then': 9, 'inference': 9, 'between': 9, '10': 9, 'size': 9, 'only': 9, 'computed': 9, 'conference': 9, '26': 8, 'article': 8, 'through': 8, 'approaches': 8, 'will': 8, 'when': 8, 'physical': 8, 'compared': 8, 'architecture': 8, '8': 8, 'database': 8, 'samples': 8, 'does': 8, 'iterative': 8, 'positions': 8, 'computer': 8, 'any': 8, 'error': 8, 'i': 8, 'gdt': 8, 'availability': 8, 'h': 8, 'mgnify': 8, 'p': 8, 'clusters': 8, 'clustering': 8, 'vol': 7, '596': 7, 'w': 7, 'function': 7, 'been': 7, 'represents': 7, 'method': 7, 'cases': 7, 'approach': 7, 'about': 7, 'design': 7, 'produce': 7, 'many': 7, 'study': 7, 'developed': 7, 'it': 7, '15': 7, 'authors': 7, 'had': 7, 'point': 7, 'atom': 7, 'perresidue': 7, 'large': 7, 'after': 7, 'show': 7, 'position': 7, 'msas': 7, '4': 7, 'recycling': 7, 'true': 7, 'correct': 7, 'nres': 7, 'refinement': 7, 'train': 7, 'databases': 7, 'description': 7, 'most': 7, 'representations': 7, '60': 7, '100': 7, 'if': 7, '5': 7, 'mean': 7, 'additional': 7, 'acids': 7, 'ij': 7, 'copy': 7, 'values': 7, 'vision': 7, 'gpu': 7, 'g': 7, '2018': 7, 'l': 7, 'plos': 7, 'clustered': 7, 'systems': 7, 'license': 7, 'memory': 7, 'eg': 7, 'predicting': 6, 'based': 6, 'amino': 6, 'even': 6, 'machine': 6, 'alignments': 6, 'solved': 6, 'pairwise': 6, 'techniques': 6, '11': 6, 'side': 6, 'sidechain': 6, 'difference': 6, 'associated': 6, 'endtoend': 6, '3': 6, 'tmscore': 6, 'rmsd': 6, 'rrc': 6, '87': 6, 'bootstrap': 6, 'target': 6, 'components': 6, 'main': 6, 'inputs': 6, 'including': 6, 'procedure': 6, 'work': 6, 'term': 6, 'key': 6, 'applied': 6, 'operation': 6, 'resolved': 6, 'some': 6, 'linear': 6, 'bias': 6, 'uses': 6, 'force': 6, 'f': 6, 'distance': 6, 'such': 6, 'alignment': 6, 'resulting': 6, 'few': 6, 't1024': 6, '13': 6, 'effect': 6, 'contacts': 6, 'human': 6, 'sci': 6, '7': 6, '16': 6, 'zhang': 6, '2017': 6, 'mol': 6, 'v': 6, 'removed': 6, 'v2018_08': 6, 'ensembling': 6, 'jj': 6, 're': 6, 'andrew': 5, 'small': 5, 'atomic': 5, 'challenging': 5, 'novel': 5, 'tion': 5, 'under': 5, 'may': 5, '12': 5, 'sciences': 5, 'seoul': 5, 'green': 5, 'long': 5, 'best': 5, '28': 5, 'measured': 5, 'estimates': 5, 'demonstrate': 5, 'sample': 5, 'back': 5, 'plddt': 5, 'corresponding': 5, 'score': 5, 'estimated': 5, 'geometric': 5, 'particular': 5, 'output': 5, 'experiment': 5, 'three': 5, '10000': 5, 'made': 5, 'described': 5, 'raw': 5, 'rotations': 5, 'same': 5, 'related': 5, 'while': 5, '06': 5, '08': 5, 'region': 5, 'ptm': 5, 'intervals': 5, 'covering': 5, 'filtered': 5, 'fit': 5, 'multiplicative': 5, 'highaccuracy': 5, 'during': 5, 'gas': 5, 'exact': 5, 'relaxation': 5, 'weights': 5, 'source': 5, '9': 5, 'o': 5, 't1064': 5, 'results': 5, 'auxiliary': 5, 'created': 5, 'applicable': 5, 'processing': 5, 'min': 5, 'z': 5, 'nucleic': 5, 'res': 5, 'y': 5, 'tests': 5, 'you': 5, 'material': 5, 'your': 5, 'against': 5, 'cluster': 5, 'uniclust30': 5, 'pdb70': 5, 'uniref90': 5, 'openmm': 5, 'reduced': 5, 'xla': 5, 'jax': 5, 'tensorflow': 5, 'dh': 5, 'life': 4, 'understanding': 4, 'enable': 4, 'acid': 4, 'open': 4, 'problem': 4, 'provide': 4, 'similar': 4, 'critical': 4, 'majority': 4, 'greatly': 4, 'biological': 4, 'algorithm': 4, 'development': 4, 'either': 4, 'history': 4, 'molecular': 4, 'statistical': 4, 'although': 4, 'constraints': 4, 'deposited': 4, 'interpret': 4, 'advances': 4, 'casp13': 4, 'out': 4, 'recently': 4, 'publicly': 4, 'so': 4, 'updates': 4, 'national': 4, 'pritzel': 4, 'competing': 4, 'comparison': 4, 'approximately': 4, 'improves': 4, 'over': 4, 'should': 4, 'table': 4, '6': 4, 'measure': 4, 'metric': 4, 'overall': 4, 'features': 4, 'equivariant': 4, 'times': 4, 'produces': 4, 'entries': 4, 'assigned': 4, 'example': 4, 'packing': 4, 'flow': 4, 'array': 4, 'shown': 4, 'masked': 4, 'coordinates': 4, 'construction': 4, 'below': 4, 'stages': 4, 'trunk': 4, 'layers': 4, 'frames': 4, 'section': 4, 'include': 4, '18': 4, 'figs': 4, 'graph': 4, 'individual': 4, 'note': 4, 'correlation': 4, '25': 4, 'must': 4, 'distances': 4, 'third': 4, 'geometry': 4, 'constrained': 4, 'bond': 4, 'application': 4, 'complex': 4, 'improve': 4, 'rk': 4, 'xi': 4, 'self': 4, 'node': 4, 'ik': 4, 'updated': 4, 'invariant': 4, 'remove': 4, 'value': 4, 'obtained': 4, 'make': 4, 'objective': 4, 'interpreting': 4, 'how': 4, 'parameters': 4, 'per': 4, 'figure': 4, 'contains': 4, 'detailed': 4, 'ablation': 4, 'baseline': 4, 'where': 4, 'less': 4, 'they': 4, 'image': 4, 'language': 4, 'unit': 4, '384': 4, 'steinegger': 4, 'söding': 4, 'improved': 4, 'li': 4, 'contact': 4, 'comput': 4, 'moult': 4, 'platform': 4, 'natl': 4, 'acad': 4, 'usa': 4, 'segmentation': 4, 'pattern': 4, 'recognition': 4, 'international': 4, 'uniclust': 4, 'creative': 4, 'commons': 4, 'appropriate': 4, 'sources': 4, 'reference': 4, 'following': 4, 'hhsuite': 4, 'distillation': 4, 'release': 4, 'date': 4, 'april': 4, '256': 4, 'tpu': 4, 'v100': 4, 'opensource': 4, 'httpsgithub': 4, 'v1': 4, 'sonnet': 4, 'hhblits': 4, 'httpsgithubcom': 4, 'tg': 4, 'aws': 4, 'materials': 4, 'whether': 4, 'software': 4, 'john': 3, 'demis': 3, 'essential': 3, 'largescale': 3, 'threedimensional': 3, 'folding': 3, 'important': 3, 'despite': 3, 'far': 3, 'knowledge': 3, 'interactions': 3, 'alternative': 3, 'rapid': 3, 'limited': 3, 'applications': 3, 'develop': 3, 'name': 3, 'httpsdoiorg101038s41586021038192': 3, 'published': 3, 'online': 3, 'university': 3, 'korea': 3, 'potapenko': 3, 'addition': 3, 'accu': 3, 'considerably': 3, 'strong': 3, '35': 3, 'precise': 3, 'reliability': 3, 'observe': 3, 'predicts': 3, 'modelling': 3, 'analyses': 3, 'collection': 3, 'jointly': 3, 'terminus': 3, 'src': 3, 'repr': 3, 'rc': 3, 'relative': 3, 'blue': 3, 'paper': 3, 'nseq': 3, 'text': 3, 'intermediate': 3, 'unlabelled': 3, 'selfdistillation': 3, 'directly': 3, 'primary': 3, 'ideas': 3, 'processes': 3, 'attentionbased': 3, 'early': 3, 'direct': 3, 'form': 3, 'outputs': 3, 'building': 3, 'space': 3, 'defined': 3, 'elements': 3, 'represent': 3, 'those': 3, 'appear': 3, 'operations': 3, 'product': 3, 'enables': 3, 'lddtcuni03b1': 3, '09': 3, 'average': 3, '90': 3, '02': 3, '04': 3, 'filtering': 3, 'bars': 3, 'errors': 3, 'observed': 3, 'resolution': 3, 'range': 3, 'units': 3, 'above': 3, 'student': 3, 'basis': 3, 'leastsquares': 3, 'pearsons': 3, '10795': 3, 'shaded': 3, 'companion': 3, 'paper39': 3, 'triangles': 3, 'axial': 3, 'missing': 3, 'edge': 3, 'project': 3, 'represented': 3, 'translations': 3, 'respect': 3, 'peptide': 3, 'finetuning': 3, 'field': 3, '33': 3, 'angles': 3, 'shared': 3, 'selfattention': 3, 'ki': 3, 'jk': 3, 'fape': 3, 'activations': 3, 'makes': 3, 'well': 3, 'com': 3, '4a': 3, 'uniclust3036': 3, 'separate': 3, 'trajectory': 3, 'likely': 3, 'trajectories': 3, 'until': 3, 'longer': 3, 'orf8': 3, 'sarscov2': 3, 'searches': 3, 'before': 3, 'lmrp': 3, 'targets': 3, 'sizes': 3, 'maps': 3, 'd1': 3, 'sets': 3, 'scored': 3, 'iterations': 3, 'changes': 3, '5a': 3, 'threshold': 3, 'give': 3, 'intertwined': 3, 'system': 3, 'studies': 3, 'community': 3, 'neff': 3, 'groups': 3, 'nongap': 3, 'weighting': 3, 'peer': 3, 'review': 3, 'interests': 3, 'macromolecular': 3, '2015': 3, 'nmr': 3, 'struct': 3, 'mirdita': 3, 'science': 3, 'wang': 3, 'fidelis': 3, 'schwede': 3, 'unified': 3, '2010': 3, 'sander': 3, 'marks': 3, 'variation': 3, '2012': 3, 'ieee': 3, '2016': 3, '31': 3, 'lddt': 3, 'cell': 3, 'alquraishi': 3, 'preprint': 3, 'bmc': 3, 'involved': 3, 'articles': 3, 'algorithms': 3, 'rep': 3, 'query': 3, 'projected': 3, 'standard': 3, 'families': 3, 'markov': 3, 'uniprot': 3, 'representative': 3, 'public': 3, 'datasets': 3, 'lookup': 3, 'v2020_01': 3, 'v2018_12': 3, 'v30beta3': 3, '14072017': 3, 'jackhmmer': 3, 'amber99sb': 3, 'removing': 3, 'maximum': 3, 'random': 3, 'require': 3, 'five': 3, 'timings': 3, 'searching': 3, 'central': 3, 'chosen': 3, 'computing': 3, 'summary': 3, 'constructing': 3, 'compiler': 3, 'mmseqs2': 3, 'famsa': 3, 'python': 3, 'numpy': 3, 'https': 3, 'wu': 3, 'deepmind': 3, 'support': 3, 'mf': 3, 'rb': 3, 'kk': 3, 'technical': 3, 'policy': 3, 'na': 3, 'statement': 3, 'making': 3, 'richard': 2, 'alexander': 2, 'tim': 2, 'michael': 2, 'olaf': 2, 'kathryn': 2, 'russ': 2, 'augustin': 2, 'anna': 2, 'alex': 2, 'clemens': 2, 'simon': 2, 'bernardino': 2, 'stanislav': 2, 'rishub': 2, 'david': 2, 'unique': 2, 'fraction': 2, 'known': 2, 'years': 2, 'needed': 2, 'solely': 2, 'component': 2, '50': 2, 'existing': 2, 'short': 2, 'homologous': 2, 'here': 2, 'entirely': 2, 'proceeded': 2, 'programme': 2, 'simulation': 2, 'pro': 2, 'tein': 2, 'context': 2, 'physics': 2, 'homology': 2, 'bank': 2, 'pdb5': 2, 'explosion': 2, 'genomic': 2, 'sequencing': 2, 'close': 2, 'utility': 2, 'entered': 2, 'team': 2, 'completely': 2, 'accepted': 2, 'july': 2, 'access': 2, 'south': 2, 'contributed': 2, 'jumper': 2, 'tunyasuvunakool': 2, 'hassabis': 2, '096': 2, 'deviation': 2, 'whereas': 2, 'allatom': 2, 'carbon': 2, 'rate': 2, 'alpha': 2, 'fold': 2, 'scalable': 2, '2180residue': 2, 'homologues': 2, 'finally': 2, 'extends': 2, 'released': 2, 'cutoff': 2, 'analysed': 2, 'furthermore': 2, 'bone': 2, 'localdistance': 2, 'find': 2, 'tmscore27': 2, '2d': 2, 'expected': 2, 'cuni03b1': 2, 'conf_idence': 2, 'genetic': 2, 'performance': 2, '146': 2, 'group': 2, 'numbers': 2, 'correspond': 2, 'four': 2, 'crystal': 2, 'bfactor': 2, 'outliers': 2, 'zinc': 2, 'ion': 2, 't1044': 2, 'arrows': 2, 'various': 2, 'parentheses': 2, '585': 2, 'losses': 2, 'achieve': 2, 'heavy': 2, 'given': 2, 'aligned': 2, '1e': 2, 'net': 2, 'initialized': 2, 'contain': 2, 'ing': 2, 'concrete': 2, 'hypothesis': 2, 'innovations': 2, 'mechanisms': 2, 'spatial': 2, 'relationships': 2, 'rotation': 2, 'rigid': 2, 'body': 2, 'origin': 2, 'rapidly': 2, 'breaking': 2, 'allow': 2, 'parts': 2, 'transformer': 2, 'implicitly': 2, 'substantial': 2, 'whole': 2, 'repeatedly': 2, 'applying': 2, 'contributes': 2, 'extra': 2, 'evo': 2, 'view': 2, 'encode': 2, 'define': 2, 'elementwise': 2, 'outer': 2, 'contrast': 2, 'evolving': 2, 'there': 2, 'inspired': 2, 'consistency': 2, '005': 2, '24': 2, '07': 2, 'redundancy': 2, 'excludes': 2, 'hmmsearch': 2, 'sensitive': 2, '70': 2, 'otherwise': 2, '117': 2, 'terms': 2, 'add': 2, 'combination': 2, 'logits': 2, 'stack': 2, 'loop': 2, 'providing': 2, 'prepare': 2, 'generation': 2, 'operates': 2, 'original': 2, 'row': 2, 'independent': 2, 'conversely': 2, 'constraint': 2, 'violation': 2, 'r3': 2, 'gated': 2, 'tran': 2, 'sition': 2, 'outgoing': 2, 'starting': 2, 'ending': 2, 'incoming': 2, 'kj': 2, 'ji': 2, 'shape': 2, 'directed': 2, 'circles': 2, 'illustrated': 2, 'χ': 2, 'tk': 2, 'gdt33': 2, 'violations': 2, 'augments': 2, 'queries': 2, 'keys': 2, 'produced': 2, 'projection': 2, 'aligning': 2, 'hence': 2, '193': 2, 'supervised': 2, 'noisy': 2, 'crop': 2, 'effective': 2, 'randomly': 2, 'bidirectional': 2, 'transformers': 2, 'learn': 2, 'statistic': 2, 'bert': 2, 'examples': 2, 'provides': 2, '192': 2, 'improvements': 2, 'secondary': 2, 'process': 2, 'ablations': 2, 'variety': 2, 'descriptions': 2, 'extended': 2, 'discussion': 2, 'keep': 2, 'head': 2, 'distogram': 2, 'd2': 2, 'reported': 2, 'seeds': 2, 'means': 2, 'refer': 2, 'obtain': 2, 'requires': 2, 'iteration': 2, 'crosschain': 2, 'across': 2, 'factors': 2, 'limit': 2, 'substantially': 2, 'con': 2, 'larger': 2, 'homomers': 2, 'future': 2, 'ment': 2, 'effectively': 2, 'improvement': 2, 'heuristic': 2, 'coordinate': 2, 'handcrafted': 2, 'taken': 2, 'hydrogen': 2, 'complexity': 2, 'handle': 2, 'unknown': 2, 'conditions': 2, 'depending': 2, 'length': 2, 'up': 2, 'determination': 2, 'counting': 2, 'eff': 2, 'taking': 2, '589': 2, 'hope': 2, 'genomics': 2, 'become': 2, 'tools': 2, 'biology': 2, 'acknowledgements': 2, 'author': 2, 'bai': 2, 'cryoem': 2, 'way': 2, '2001': 2, 'consortium': 2, '47': 2, 'assembly': 2, 'dill': 2, 'rev': 2, '37': 2, 'senior': 2, 'potentials': 2, 'sun': 2, 'xu': 2, 'map': 2, '68': 2, 'kryshtafovych': 2, 'topf': 2, 'abstract': 2, '17': 2, 'globular': 2, 'automated': 2, '21': 2, 'correlated': 2, 'mutations': 2, 'eng': 2, 'identification': 2, '2009': 2, '23': 2, '2011': 2, 'jones': 2, 'inverse': 2, 'estimation': 2, '27': 2, 'scoring': 2, '57': 2, 'x': 2, 'mach': 2, 'intell': 2, '32': 2, '29': 2, 'huang': 2, 'ieeecvf': 2, '65': 2, 'finding': 2, 'q': 2, '36': 2, '45': 2, 'technologies': 2, '39': 2, 'biotechnol': 2, 'yang': 2, 'interresidue': 2, 'residual': 2, 'differentiable': 2, '49': 2, 'ingraham': 2, 'universal': 2, 'need': 2, 'springer': 2, 'million': 2, '118': 2, 'distribution': 2, 'credit': 2, 'indicate': 2, 'included': 2, 'permitted': 2, 'tables': 2, 'repre': 2, 'computation': 2, 'invariance': 2, 'construct': 2, 'hhblits61': 2, 'avail': 2, 'matches': 2, 'several': 2, 'teams': 2, 'hidden': 2, 'hmms': 2, 'eukaryotic': 2, 'shorter': 2, 'second': 2, 'added': 2, 'them': 2, '61': 2, 'found': 2, 'effects': 2, 'inclusion': 2, 'having': 2, 'uniref': 2, 'regimen': 2, '128': 2, 'v3': 2, 'finetuned': 2, 'configuration': 2, 'initial': 2, 'stage': 2, 'takes': 2, 'binned': 2, '75': 2, 'encourage': 2, 'select': 2, 'run': 2, '2500': 2, 'faster': 2, 'exceed': 2, '2500residue': 2, 'cloud': 2, 'accuracies': 2, 'selecting': 2, 'february': 2, '1400': 2, 'legends': 2, 'num': 2, 'freely': 2, 'httpswwwwwpdborgftppdbftpsites': 2, 'accession': 2, '6y4f77': 2, '6yj178': 2, '6vr479': 2, '6sk080': 2, '6fes81': 2, '6w6w82': 2, '6t1z83': 2, '7jtl84': 2, 'script': 2, 'v0169': 2, 'httpsgithubcomgooglejax': 2, 'haiku': 2, 'v004': 2, 'httpsgithubcomdeepminddmhaiku': 2, 'bundled': 2, 'bfduniclust30': 2, 'hhsearch': 2, 'httpsgithubcomsoedinglabhhsuite': 2, 'hmmer': 2, 'v33': 2, 'httpeddylaborgsoftwarehmmer': 2, 'v731': 2, 'v125': 2, 'v36': 2, 'httpswwwpythonorg': 2, 'v1164': 2, 'httpsgithubcomnumpynumpy': 2, 'scipy': 2, 'v121': 2, 'seaborn': 2, 'v0111': 2, 'httpsgithubcommwaskomseaborn': 2, 'matplotlib': 2, 'v334': 2, 'httpsgithubcommatplotlibmatplotlib': 2, 'bokeh': 2, 'v140': 2, 'httpsgithubcombokehbokeh': 2, 'pandas': 2, 'v115': 2, 'plotnine': 2, 'v080': 2, 'statsmodels': 2, 'v0122': 2, 'colab': 2, 'tmalign': 2, 'v20190822': 2, 'tmscores': 2, 'visualizations': 2, 'pymol': 2, 'v230': 2, 'johnson': 2, 'eddy': 2, 'hmm': 2, 'huge': 2, 'fast': 2, 'comprehensive': 2, 'google': 2, 'hou': 2, 'phage': 2, 'contributions': 2, 'help': 2, 'ms': 2, 'led': 2, 'saak': 2, 'ov': 2, 'kt': 2, 'ab': 2, 'dr': 2, 'pk': 2, 'cm': 2, 'conceived': 2, 'patent': 2, 'summaryapril': 2, 'policies': 2, 'editorial': 2, 'statistics': 2, 'legend': 2, 'noted': 2, 'web': 2, 'manuscripts': 2, 'list': 2, 'figures': 2, 'sure': 2, 'read': 2, 'sections': 2, 'exclusions': 2, 'too': 2, '583': 1, 'jumper14': 1, 'evans14': 1, 'pritzel14': 1, 'green14': 1, 'figurnov14': 1, 'ronneberger14': 1, 'tunyasuvunakool14': 1, 'bates14': 1, 'žídek14': 1, 'potapenko14': 1, 'bridgland14': 1, 'meyer14': 1, 'kohl14': 1, 'ballard14': 1, 'cowie14': 1, 'romeraparedes14': 1, 'nikolov14': 1, 'jain14': 1, 'jonas': 1, 'adler1': 1, 'trevor': 1, 'back1': 1, 'stig': 1, 'petersen1': 1, 'reiman1': 1, 'ellen': 1, 'clancy1': 1, 'michal': 1, 'zielinski1': 1, 'martin': 1, 'steinegger23': 1, 'michalina': 1, 'pacholska1': 1, 'tamas': 1, 'berghammer1': 1, 'sebastian': 1, 'bodenstein1': 1, 'silver1': 1, 'oriol': 1, 'vinyals1': 1, 'senior1': 1, 'koray': 1, 'kavukcuoglu1': 1, 'pushmeet': 1, 'kohli1': 1, 'hassabis14': 1, 'facilitate': 1, 'mechanistic': 1, 'enormous': 1, 'effort14': 1, '100000': 1, 'determined5': 1, 'billions': 1, 'sequences67': 1, 'bottlenecked': 1, 'months': 1, 'painstaking': 1, 'effort': 1, 'required': 1, 'determine': 1, 'address': 1, 'gap': 1, 'adopt': 1, 'sequencethe': 1, 'problem8has': 1, 'years9': 1, 'progress1014': 1, 'fall': 1, 'especially': 1, 'regularly': 1, 'validated': 1, 'redesigned': 1, 'networkbased': 1, '14th': 1, 'casp1415': 1, 'demonstrating': 1, 'competitive': 1, 'outperforming': 1, 'underpinning': 1, 'latest': 1, 'incorporates': 1, 'leveraging': 1, 'multisequence': 1, 'along': 1, 'complementary': 1, 'paths': 1, 'focus': 1, 'interac': 1, 'heavily': 1, 'integrates': 1, 'driving': 1, 'forces': 1, 'thermodynamic': 1, 'kinetic': 1, 'physics16': 1, 'approximations': 1, 'thereof17': 1, 'theoreti': 1, 'cally': 1, 'appealing': 1, 'proved': 1, 'moderatesized': 1, 'due': 1, 'intractability': 1, 'dependence': 1, 'stability': 1, 'difficulty': 1, 'producing': 1, 'sufficiently': 1, 'derived': 1, 'structures1819': 1, 'correla': 1, 'tions2024': 1, 'benefited': 1, 'steady': 1, 'growth': 1, 'correlations': 1, 'contemporary': 1, 'evolutionaryhistorybased': 1, 'homologue': 1, 'experimentally': 1, 'capable': 1, 'near': 1, 'mayjuly': 1, 'alphafold2': 1, 'system10': 1, 'carried': 1, 'biennially': 1, 'disclosed': 1, 'blind': 1, 'received': 1, 'check': 1, '1deepmind': 1, 'london': 1, 'uk': 1, '2school': 1, '3artificial': 1, 'intelligence': 1, 'institute': 1, '4these': 1, 'equally': 1, 'evans': 1, 'figurnov': 1, 'ronneberger': 1, 'bates': 1, 'žídek': 1, 'bridgland': 1, 'meyer': 1, 'kohl': 1, 'ballard': 1, 'cowie': 1, 'romeraparedes': 1, 'nikolov': 1, 'jain': 1, 'email': 1, 'jumperdeepmindcom': 1, 'dhcontactdeepmindcom': 1, '584': 1, 'participating': 1, 'served': 1, 'goldstandard': 1, 'prediction2526': 1, 'vastly': 1, 'rootmeansquare': 1, '085116': 1, 'next': 1, 'performing': 1, '2740': 1, '1a': 1, 'width': 1, '1b': 1, '1c': 1, 'templatebased': 1, '1216': 1, '3142': 1, 'domainpacking': 1, '1d': 1, 'confident': 1, '2a': 1, 'dem': 1, 'onstrated': 1, '2b': 1, 'reliably': 1, '2c': 1, 'super': 1, 'rately': 1, 'validate': 1, 'transfers': 1, 'uncurated': 1, 'submissions': 1, 'would': 1, '115': 1, 'confirmation': 1, 'folds': 1, 'incorporating': 1, 'architectures': 1, 'proce': 1, 'dures': 1, 'embed': 1, 'g427': 1, 'g009': 1, 'g473': 1, 'g129': 1, 'g403': 1, 'g032': 1, 'g420': 1, 'g480': 1, 'g498': 1, 'g488': 1, 'g368': 1, 'g324': 1, 'g362': 1, 'g253': 1, 'g216': 1, '08å': 1, '093': 1, '059': 1, '8å': 1, 'zn': 1, '22å': 1, 'low': 1, 'uni2190': 1, 'pairing': 1, 'top': 1, 'entrants': 1, 't1049': 1, '6y4f': 1, 'depicted': 1, 't1056': 1, '6yj1': 1, 'wellpredicted': 1, 'zincbinding': 1, 'site': 1, 'though': 1, 'explicitly': 1, '6vr4a': 1, 'chainwas': 1, 'intervention': 1, 'among': 1, 'shapes': 1, 'channels': 1, 'selfestimates': 1, 'comprises': 1, 'repeated': 1, 'processed': 1, 'pairs': 1, '127': 1, 'handling': 1, 'nonattentionbased': 1, 'evidence': 1, 'arises': 1, 'continuously': 1, 'refined': 1, 'exchange': 1, 'reasoning': 1, 'followed': 1, 'introduces': 1, 'explicit': 1, 'transla': 1, 'trivial': 1, 'state': 1, 'refine': 1, 'simultaneous': 1, 'reason': 1, 'unrepresented': 1, 'places': 1, 'weight': 1, 'orientational': 1, 'correctness': 1, 'throughout': 1, 'reinforce': 1, 'notion': 1, 'feeding': 1, 'recursively': 1, 'modules': 1, 'vision2829': 1, 'markedly': 1, 'minor': 1, 'principle': 1, 'networknamed': 1, 'former': 1, '3ais': 1, 'proximity': 1, 'relation': 1, '3b': 1, 'columns': 1, 'rows': 1, 'framework': 1, 'series': 1, 'summed': 1, 'dimension': 1, 'previous': 1, 'work30': 1, 'every': 1, 'rather': 1, 'once': 1, 'continuous': 1, 'communication': 1, 'pat': 1, 'terns': 1, 'necessity': 1, '051': 1, '010': 1, '015': 1, '020': 1, '025': 1, '030fraction': 1, '05': 1, 'f1': 1, 'rotamers': 1, 'newer': 1, 'reduce': 1, 'histogram': 1, 'poisson': 1, 'identified': 1, '3144': 1, '140156': 1, 'uncertain': 1, 'better': 1, '5317': 1, 'rotamer': 1, 'classified': 1, 'torsion': 1, 'angle': 1, 'aggregates': 1, 'bin': 1, 'ttest': 1, '0997': 1, '076': 1, 'quantification': 1, '098': 1, '007': 1, '085': 1, '586': 1, 'representationfor': 1, 'satisfied': 1, 'inequality': 1, 'intuition': 1, 'arrange': 1, 'involving': 1, 'nodes': 1, '3c': 1, 'logit': 1, 'attention31': 1, 'nonattention': 1, '165': 1, 'multipli': 1, 'cative': 1, 'originally': 1, 'symmetric': 1, 'cheaper': 1, 'replacement': 1, 'however': 1, 'variant': 1, 'persequence': 1, 'closes': 1, 'ensuring': 1, 'fully': 1, 'mix': 1, '3e': 1, 'translationsrepresenting': 1, 'ncαc': 1, 'atomsprioritize': 1, 'orientation': 1, 'location': 1, 'unconstrained': 1, 'frequently': 1, 'violate': 1, 'mod': 1, 'ule': 1, 'solving': 1, 'closure': 1, 'problems': 1, 'satisfac': 1, 'encouraged': 1, 'enforcement': 1, 'achieved': 1, 'postprediction': 1, 'gradient': 1, 'descent': 1, 'amber32': 1, 'empirically': 1, 'relaxa': 1, 'initially': 1, 'compute': 1, 'weightsd': 1, 'rowwise': 1, 'column': 1, 'wise': 1, 'kjjk': 1, 'kjki': 1, 'architectural': 1, 'arrays': 1, 'interpreted': 1, 'diagram': 1, 'being': 1, 'freefloating': 1, 'grey': 1, '587': 1, 'lddtcα34': 1, 'distracting': 1, 'stereochemical': 1, 'iteratively': 1, 'geometryaware': 1, 'changing': 1, 'performed': 1, 'usual': 1, 'impose': 1, 'spatiallocality': 1, 'wellsuited': 1, 'transition': 1, 'computes': 1, 'translation': 1, 'end': 1, 'estimate': 1, 'framealigned': 1, '3f': 1, 'compares': 1, 'pute': 1, 'nframes': 1, 'natoms': 1, 'penalized': 1, 'clamped': 1, 'l1': 1, 'creates': 1, 'chirality': 1, 'labelled': 1, 'enhance': 1, 'selfdistillation35': 1, '350000': 1, 'diverse': 1, 'highconfidence': 1, 'subset': 1, 'again': 1, 'scratch': 1, 'mixture': 1, 'augmentations': 1, 'ping': 1, 'subsampling': 1, 'recapitulate': 1, 'previously': 1, 'additionally': 1, 'mask': 1, 'mutate': 1, 'encoder': 1, 'bertstyle37': 1, 'encourages': 1, 'phylogenetic': 1, 'covariation': 1, 'hardcoding': 1, 'normal': 1, 'pretrained': 1, 'work38': 1, 'understand': 1, 'keeping': 1, 'fro': 1, 'zen': 1, '114': 1, 'structuresone': 1, 'blockin': 1, 'belief': 1, 'surprisingly': 1, 'smooth': 1, 'constant': 1, 'incremental': 1, '4b': 1, 'illustrate': 1, 'role': 1, 'rearranges': 1, 'settling': 1, 'good': 1, 'finds': 1, 't1091': 1, 'clear': 1, 'difficulties': 1, 'videos': 1, '116': 1, 'contribute': 1, 'gradients': 1, 'heads': 1, 'biasing': 1, 'gating': 1, 'frequencies': 1, 'chainsa': 1, '96': 1, '144': 1, '100domain': 1, '2261': 1, 'percentile': 1, 'nearly': 1, 'reach': 1, 'comprise': 1, '588': 1, '113': 1, 'vast': 1, 'still': 1, 'affect': 1, 'applicability': 1, 'decreases': 1, 'lead': 1, 'gains': 1, 'hypothesize': 1, 'coarsely': 1, 'depend': 1, 'crucially': 1, 'limitation': 1, 'much': 1, 'weaker': 1, 'intrachain': 1, 'homotypic': 1, 'tacts': 1, 'heterotypic': 1, 'typically': 1, 'occurs': 1, 'bridging': 1, 'complexes': 1, 'almost': 1, 'often': 1, '5b': 1, 'expect': 1, 'readily': 1, 'heterocomplexes': 1, 'dif': 1, 'ficulty': 1, 'heterocontacts': 1, 'varied': 1, 'extensively': 1, 'covered': 1, 'reviews144043': 1, 'prediction144243': 1, 'come': 1, 'prediction10114445': 1, 'leverage': 1, 'puter': 1, 'systems46': 1, 'treating': 1, 'converting': 1, 'couplings2224': 1, 'matrix': 1, 'integrating': 1, 'directly4750': 1, 'match': 1, 'traditional': 1, 'pipelines51': 1, 'paral': 1, 'lel': 1, 'success': 1, 'processing52': 1, 'vision3153': 1, 'exploration': 1, 'sequences5456': 1, 'methodology': 1, 'designing': 1, 'combi': 1, 'nation': 1, 'inductive': 1, 'build': 1, 'minimal': 1, 'imposition': 1, 'builds': 1, 'bonds': 1, 'learns': 1, 'efficiently': 1, 'cope': 1, 'presence': 1, 'haem': 1, 'ability': 1, 'underspecified': 1, 'gen': 1, 'eral': 1, 'part': 1, 'stochiometry': 1, 'ligand': 1, 'predictable': 1, 'alone': 1, 'respects': 1, 'already': 1, 'demonstrated': 1, 'replacement57': 1, 'cryogenic': 1, 'electron': 1, 'microscopy': 1, 'maps58': 1, 'moreover': 1, 'because': 1, 'graphics': 1, 'minutes': 1, 'hours': 1, 'ute': 1, 'opens': 1, 'exciting': 1, 'possibility': 1, 'proteomescale': 1, 'beyondin': 1, 'entire': 1, 'proteome39': 1, 'revolutionized': 1, 'intrinsic': 1, 'challenge': 1, 'experi': 1, 'mental': 1, 'prevented': 1, 'expansion': 1, 'developing': 1, '101': 1, '102': 1, '103': 1, '104': 1, '100lddtcuni03b1': 1, '03': 1, 'redundancyreduced': 1, 'restricting': 1, 'longrange': 1, 'heteromer': 1, 'consider': 1, '6743': 1, '1596': 1, 'scheme': 1, 'curves': 1, 'gaussian': 1, 'kernel': 1, 'smoothing': 1, 'window': 1, 'log10neff': 1, 'area': 1, 'homotrimer': 1, '6sk0': 1, 'correctly': 1, 'stoichiometry': 1, 'weak': 1, 'coupled': 1, 'wellcurated': 1, 'assembled': 1, 'accelerate': 1, 'advancement': 1, 'pace': 1, 'revolution': 1, 'alphafoldand': 1, 'apply': 1, 'biophysical': 1, 'problemswill': 1, 'modern': 1, 'content': 1, 'references': 1, 'sum': 1, 'maries': 1, 'tributions': 1, 'statements': 1, 'thompson': 1, 'yeates': 1, 'rodriguez': 1, 'f1000res': 1, '667': 1, 'xc': 1, 'mcmullan': 1, 'scheres': 1, 'revolutionizing': 1, 'trends': 1, 'biochem': 1, '4957': 1, 'jaskolski': 1, 'dauter': 1, 'wlodawer': 1, 'brief': 1, 'crystallography': 1, 'family': 1, 'tree': 1, 'nobel': 1, 'fruits': 1, 'febs': 1, '281': 1, '39854009': 1, '2014': 1, 'wüthrich': 1, '923925': 1, 'wwpdb': 1, 'archive': 1, 'd520d528': 1, 'mitchell': 1, 'microbiome': 1, 'resource': 1, 'd570d578': 1, 'proteinlevel': 1, 'increases': 1, 'recovery': 1, 'metagenomic': 1, 'manyfold': 1, '603606': 1, 'ozkan': 1, 'shell': 1, 'weikl': 1, 'annu': 1, 'biophys': 1, '289316': 1, '2008': 1, 'anfinsen': 1, 'principles': 1, 'govern': 1, '181': 1, '223230': 1, '1973': 1, '577': 1, '706710': 1, 'de': 1, 'novo': 1, 'ultradeep': 1, 'e1005324': 1, 'zheng': 1, 'deeplearning': 1, 'contactmap': 1, 'guided': 1, '11491164': 1, 'abriata': 1, 'tamò': 1, 'dal': 1, 'peraro': 1, 'leap': 1, 'tertiary': 1, 'prompts': 1, 'routes': 1, 'assessments': 1, '11001112': 1, 'pearce': 1, 'significantly': 1, 'impacted': 1, 'curr': 1, 'opin': 1, '194207': 1, 'fourteenth': 1, 'round': 1, 'book': 1, 'httpswwwpredictioncenterorgcasp14doccasp14_abstractspdf': 1, 'brini': 1, 'simmerling': 1, 'storytelling': 1, '370': 1, 'eaaz3041': 1, 'sippl': 1, 'calculation': 1, 'conformational': 1, 'ensembles': 1, 'knowledgebased': 1, '213': 1, '859883': 1, '1990': 1, 'šali': 1, 'blundell': 1, 'comparative': 1, 'satisfaction': 1, 'restraints': 1, '234': 1, '779815': 1, '1993': 1, '19': 1, 'roy': 1, 'kucukural': 1, 'itasser': 1, 'protocols': 1, '725738': 1, 'altschuh': 1, 'lesk': 1, 'bloomer': 1, 'klug': 1, 'coordinated': 1, 'substitutions': 1, 'viruses': 1, 'tobacco': 1, 'mosaic': 1, 'virus': 1, '693707': 1, '1987': 1, 'shindyalov': 1, 'kolchanov': 1, '349358': 1, '1994': 1, '22': 1, 'weigt': 1, 'white': 1, 'szurmant': 1, 'hoch': 1, 'hwa': 1, 'proteinprotein': 1, 'interaction': 1, 'message': 1, 'passing': 1, '106': 1, '6772': 1, 'e28766': 1, 'buchan': 1, 'cozzetto': 1, 'pontil': 1, 'psicov': 1, 'sparse': 1, 'covariance': 1, '184190': 1, 'pedersen': 1, 'judson': 1, 'assess': 1, 'iiiv': 1, '1995': 1, 'caspround': 1, 'xiii': 1, '10111020': 1, 'skolnick': 1, 'quality': 1, '702710': 1, '2004': 1, 'tu': 1, 'autocontext': 1, 'highlevel': 1, 'tasks': 1, 'brain': 1, 'trans': 1, 'anal': 1, '17441757': 1, 'carreira': 1, 'agrawal': 1, 'fragkiadaki': 1, 'malik': 1, 'pose': 1, 'feedback': 1, '47334742': 1, 'mirabello': 1, 'wallner': 1, 'rawmsa': 1, 'e0220182': 1, 'ccnet': 1, 'crisscross': 1, 'semantic': 1, '603612': 1, 'hornak': 1, 'amber': 1, 'fields': 1, '712725': 1, '2006': 1, 'zemla': 1, 'lga': 1, 'similarities': 1, '33703374': 1, '2003': 1, '34': 1, 'mariani': 1, 'biasini': 1, 'barbato': 1, 'superpositionfree': 1, 'comparing': 1, '27222728': 1, '2013': 1, 'xie': 1, 'luong': 1, 'mt': 1, 'hovy': 1, 'le': 1, 'selftraining': 1, 'imagenet': 1, 'classification': 1, '1068710698': 1, 'deeply': 1, 'annotated': 1, 'd170d176': 1, 'devlin': 1, 'chang': 1, 'mw': 1, 'lee': 1, 'toutanova': 1, 'pretraining': 1, 'north': 1, 'american': 1, 'chapter': 1, 'association': 1, 'linguistics': 1, '41714186': 1, '38': 1, 'rao': 1, '38th': 1, 'pmlr': 1, '139': 1, '88448856': 1, 'proteome': 1, 'httpsdoiorg101038s41586021038281': 1, 'kuhlman': 1, 'bradley': 1, '681697': 1, '41': 1, 'hopf': 1, '10721080': 1, '42': 1, 'qian': 1, 'sejnowski': 1, '202': 1, '865884': 1, '1988': 1, '43': 1, 'fariselli': 1, 'olmea': 1, 'valencia': 1, 'casadio': 1, '835843': 1, '44': 1, 'orientations': 1, '14961503': 1, 'deducing': 1, 'contactmaps': 1, 'triplet': 1, 'coevolutionary': 1, 'matrices': 1, 'convolutional': 1, 'e1008865': 1, '46': 1, 'he': 1, 'ren': 1, '770778': 1, 'syst': 1, '292301': 1, '13th': 1, '11411148': 1, 'riesselman': 1, 'simulator': 1, 'transforming': 1, 'httpsarxivorg': 1, 'abs190800723': 1, '51': 1, 'mcpartlon': 1, 'irrespective': 1, 'coevolution': 1, '601609': 1, '52': 1, 'vaswani': 1, '59986008': 1, '53': 1, 'axialdeeplab': 1, 'standalone': 1, 'axialattention': 1, 'panoptic': 1, 'european': 1, '108126': 1, '54': 1, 'alley': 1, 'khimulya': 1, 'biswas': 1, 'church': 1, 'rational': 1, 'engineering': 1, 'sequencebased': 1, '13151322': 1, '55': 1, 'heinzinger': 1, 'modeling': 1, 'aspects': 1, 'transferlearning': 1, '723': 1, '56': 1, 'rives': 1, 'emerge': 1, 'scaling': 1, 'unsupervised': 1, '250': 1, 'e2016239118': 1, 'pereira': 1, 'httpsdoi': 1, 'org101002prot26171': 1, '58': 1, 'gupta': 1, 'ai': 1, 'reveal': 1, 'nsp2': 1, 'multifunctional': 1, 'host': 1, 'httpsdoiorg10110120210510': 1, '443524': 1, 'publishers': 1, 'remains': 1, 'neutral': 1, 'regard': 1, 'jurisdictional': 1, 'claims': 1, 'institutional': 1, 'affiliations': 1, 'licensed': 1, 'attribution': 1, 'permits': 1, 'sharing': 1, 'adaptation': 1, 'reproduction': 1, 'medium': 1, 'format': 1, 'link': 1, 'images': 1, 'party': 1, 'unless': 1, 'indicated': 1, 'line': 1, 'intended': 1, 'statutory': 1, 'regulation': 1, 'exceeds': 1, 'permission': 1, 'copyright': 1, 'holder': 1, 'visit': 1, 'httpcreativecommonsorglicensesby40': 1, 'extensive': 1, 'explanations': 1, 'motivations': 1, '11110': 1, 'pseudocode': 1, '132': 1, 'diagrams': 1, '111112': 1, 'combines': 1, 'sentation': 1, 'resentation': 1, 'affinities': 1, 'interact': 1, 'affinity': 1, 'squared': 1, 'transformations': 1, 'ensure': 1, '182': 1, 'proof': 1, 'multihead': 1, 'construc': 1, 'classic': 1, 'invariants': 1, 'fea': 1, 'tures': 1, 'place': 1, 'learned': 1, 'design59': 1, 'dot': 1, 'special': 1, 'controls': 1, 'lutionarily': 1, 'jackhmmer60': 1, 'tuned': 1, 'recall': 1, 'spurious': 1, 'probably': 1, 'condition': 1, 'big': 1, 'fantastic': 1, 'custommade': 1, 'largest': 1, 'collections': 1, 'consists': 1, '65983866': 1, '2204359010': 1, 'metage': 1, 'nomes': 1, 'metatranscriptomes': 1, 'built': 1, 'steps': 1, '2423213294': 1, 'collected': 1, 'swissprottrembl': 1, '20171162': 1, 'soil': 1, 'ence': 1, 'catalogue': 1, 'marine': 1, 'catalogue7': 1, 'enforcing': 1, 'align': 1, 'mmseqs2linclust63': 1, 'resulted': 1, '345159030': 1, 'efficiency': 1, 'members': 1, '61083719': 1, '166510624': 1, 'metaclust': 1, 'nr': 1, '201705': 1, 'discarding': 1, '150': 1, 'residues63': 1, 'resentatives': 1, 'mmseqs264': 1, 'fulfilled': 1, 'criteria': 1, 'remaining': 1, '25347429': 1, 'could': 1, 'separately': 1, 'famsa65': 1, 'protocol36': 1, 'versions': 1, 'pdb7066': 1, 'uniref9067': 1, 'mgnify6': 1, 'hhsearch66': 1, 'hmmer368': 1, 'v73169': 1, 'field32': 1, 'running': 1, 'ensorflow70': 1, 'sonnet71': 1, 'numpy72': 1, 'python73': 1, 'colab74': 1, 'quantify': 1, 'reran': 1, 'varying': 1, 'constructed': 1, 'case': 1, 'differences': 1, 'consistent': 1, 'relatively': 1, 'unimportant': 1, 'quite': 1, 'mostly': 1, 'overlapping': 1, 'least': 1, 'metagenomics': 1, 'classes': 1, 'poorly': 1, 'necessary': 1, 'sampled': 1, 'proportion': 1, 'assemble': 1, 'batches': 1, 'ensor': 1, 'batch': 1, 'core': 1, 'cores': 1, 'convergence': 1, 'crops': 1, '111': 1, 'week': 1, 'days': 1, 'auxil': 1, 'iary': 1, 'linearly': 1, 'crossentropy': 1, 'masking': 1, 'reconstruct': 1, 'regions': 1, 'bertlike': 1, 'loss37': 1, 'objectives': 1, '355993': 1, 'identical': 1, 'hyperparameters': 1, 'except': 1, 'sampling': 1, 'subsampled': 1, 'diversity': 1, 'predic': 1, 'tions': 1, '1121': 1, '197': 1, 'choices': 1, 'centres': 1, '1112': 1, 'varies': 1, 'sentative': 1, '92': 1, 'notably': 1, 'ran': 1, 'now': 1, 'compiler75': 1, 'since': 1, 'equal': 1, 'turn': 1, 'off': 1, 'inferencing': 1, 'easily': 1, 'gb': 1, '1300': 1, '384residue': 1, 'usage': 1, 'quadratic': 1, 'involves': 1, 'setup': 1, 'requested': 1, 'gpus': 1, 'sufficient': 1, 'take': 1, 'cpu': 1, 'do': 1, 'metrics': 1, 'metric34': 1, 'reports': 1, 'racy': 1, 'requiring': 1, 'focuses': 1, 'pen': 1, 'alty': 1, 'clashes': 1, 'superposition': 1, 'report': 1, 'cov': 1, 'erage': 1, 'perform': 1, 'currently': 1, 'lowest': 1, 'robust': 1, 'apparent': 1, 'originate': 1, 'artefacts': 1, 'genuine': 1, 'evaluation': 1, '2ad': 1, 'fil': 1, 'tered': 1, 'consisted': 1, 'ambiguous': 1, 'chemical': 1, 'duplicates': 1, 'subsequently': 1, 'nearduplicate': 1, 'highest': 1, 'selected': 1, 'fewer': 1, 'contained': 1, 'prior': 1, 'follows': 1, 'default': 1, 'seqres': 1, 'fasta': 1, 'emplate': 1, 'hits': 1, 'struc': 1, 'ture': 1, 'earlier': 1, 'hit': 1, 'normalized': 1, 'ber': 1, 'scheme76': 1, 'linked': 1, 'httpsftpwwpdb': 1, 'orgpubpdbderived_data': 1, 'httpscdnrcsborgresources': 1, 'sequenceclustersbc40out': 1, 'httpswwwusergwdg': 1, 'decompbioldatahhsuitedatabaseshhsuite_dbs': 1, 'bers': 1, 'httpsftpebiacukpubdatabasesuniprot': 1, 'previous_releasesrelease2020_01uniref': 1, 'httpsbfdmmseqs': 1, 'httpswwwusergwdgdecompbiol': 1, 'uniclust2018_08': 1, 'httpsftpebiacuk': 1, 'pubdatabasesmetagenomicspeptide_database2018_12': 1, 'comdeepmindalphafold': 1, 'ensorflow': 1, 'comtensorflowtensorflow': 1, 'httpsgithubcomdeep': 1, 'mindsonnet': 1, 'openmmopenmm': 1, 'v925af': 1, 'comsoedinglabmmseqs2': 1, 'refreshbiofamsa': 1, 'httpswww': 1, 'scipyorg': 1, 'githubcompandasdevpandas': 1, 'comhas2k1plotnine': 1, 'statsmodelsstatsmodels': 1, 'httpsresearchgooglecom': 1, 'colaboratory': 1, 'httpszhanglabdcmbmed': 1, 'umichedutmalign': 1, 'httpsgithubcomschro': 1, 'dingerpymolopensource': 1, '59': 1, 'garg': 1, 'barzilay': 1, 'jaakkola': 1, 'generative': 1, 'graphbased': 1, '33rd': 1, 'portugaly': 1, 'speed': 1, '431': 1, 'remmert': 1, 'biegert': 1, 'hauser': 1, 'lightningfast': 1, 'hmmhmm': 1, '173175': 1, '62': 1, 'knowledgebase': 1, 'd480d489': 1, '63': 1, 'commun': 1, '2542': 1, '64': 1, 'massive': 1, '10261028': 1, 'deorowicz': 1, 'debudajgrabysz': 1, 'gudyś': 1, '33964': 1, '66': 1, 'hhsuite3': 1, 'remote': 1, 'detection': 1, 'annotation': 1, '473': 1, '67': 1, 'suzek': 1, 'mcgarvey': 1, 'improving': 1, 'similarity': 1, '926932': 1, 'accelerated': 1, 'profile': 1, 'e1002195': 1, '69': 1, 'eastman': 1, 'dynamics': 1, 'e1005659': 1, 'ashish': 1, 'heterogeneous': 1, 'httpsarxivorgabs160304467': 1, '71': 1, 'reynolds': 1, 'sourcing': 1, 'library': 1, 'httpsdeepmindcomblogopensourcingsonnet': 1, '72': 1, 'harris': 1, 'programming': 1, '357362': 1, '73': 1, 'van': 1, 'rossum': 1, 'drake': 1, 'manual': 1, 'createspace': 1, '74': 1, 'bisong': 1, 'guide': 1, 'beginners': 1, '5964': 1, 'apress': 1, 'optimizing': 1, 'httpswwwtensorfloworgxla': 1, '76': 1, 'adhikari': 1, 'cheng': 1, 'influencing': 1, 'learningbased': 1, '10911098': 1, '77': 1, 'jiang': 1, 'mrph': 1, 'class': 1, 'metalbinding': 1, 'adhesin': 1, 'mediate': 1, 'biofilm': 1, 'formation': 1, 'pathog': 1, 'e1008707': 1, '78': 1, 'dunne': 1, 'ernst': 1, 'sobieraj': 1, 'pluckthun': 1, 'loessner': 1, 'm23': 1, 'peptidase': 1, 'staphylococcal': 1, '2638a': 1, 'endolysin': 1, 'httpsdoiorg102210': 1, 'pdb6yj1pdb': 1, '79': 1, 'drobysheva': 1, 'virion': 1, 'rna': 1, 'polymerase': 1, 'crasslike': 1, '306309': 1, 'flaugnatti': 1, 'loading': 1, 'inhibition': 1, 'bacterial': 1, 't6ss': 1, 'phospholipase': 1, 'effector': 1, 'vgrg': 1, 'spike': 1, 'embo': 1, 'e104129': 1, '81': 1, 'elgamacy': 1, 'interfacedriven': 1, 'strategy': 1, 'yields': 1, 'corrugated': 1, 'acs': 1, 'synth': 1, '22262235': 1, '82': 1, 'lim': 1, 'cst': 1, 'reveals': 1, 'decameric': 1, 'bound': 1, 'telomeric': 1, 'dna': 1, '368': 1, '10811085': 1, '83': 1, 'debruycker': 1, 'embedded': 1, 'lipid': 1, 'multidrug': 1, 'transporter': 1, 'suggests': 1, 'mechanism': 1, 'polyspecificity': 1, '829835': 1, '84': 1, 'flower': 1, 'immune': 1, 'evasion': 1, 'e2021785118': 1, 'thank': 1, 'rrustemi': 1, 'gu': 1, 'guseynov': 1, 'hechtman': 1, 'beattie': 1, 'donner': 1, 'parisotto': 1, 'elsen': 1, 'popovici': 1, 'necula': 1, 'maclean': 1, 'menick': 1, 'kirkpatrick': 1, 'molloy': 1, 'yim': 1, 'stanway': 1, 'simonyan': 1, 'sifre': 1, 'martens': 1, 'oneill': 1, 'antropova': 1, 'hadsell': 1, 'blackwell': 1, 'das': 1, 'gouws': 1, 'wheelwright': 1, 'hennigan': 1, 'ward': 1, 'ž': 1, 'avsec': 1, 'his': 1, 'piovesanforster': 1, 'nelson': 1, 'kemp': 1, 'managing': 1, 'enabling': 1, 'colleagues': 1, 'alphabet': 1, 'encouragement': 1, 'organizers': 1, 'experimentalists': 1, 'whose': 1, 'enabled': 1, 'acknowledges': 1, 'foundation': 1, 'grant': 1, '2019r1a6a1a10073437': 1, '2020m3a9g7103933': 1, 'creativepioneering': 1, 'researchers': 1, 'program': 1, 'brp': 1, 'ja': 1, 'mp': 1, 'berghammer': 1, 'až': 1, 'ajb': 1, 'ac': 1, 'sn': 1, 'rj': 1, 'mz': 1, 'sb': 1, 'analytics': 1, 'ec': 1, 'managed': 1, 'ds': 1, 'sp': 1, 'advice': 1, 'assistance': 1, 'wrote': 1, 'filed': 1, 'nonprovisional': 1, '16701070': 1, 'pctep2020084238': 1, 'provisional': 1, '63107362': 1, '63118917': 1, '63118918': 1, '63118921': 1, '63118919': 1, 'pending': 1, 'relating': 1, 'declare': 1, 'correspondence': 1, 'requests': 1, 'addressed': 1, 'thanks': 1, 'mohammed': 1, 'charlotte': 1, 'deane': 1, 'contribution': 1, 'reprints': 1, 'permissions': 1, 'httpwwwnaturecomreprints': 1, 'last': 1, 'jul': 1, 'wishes': 1, 'reproducibility': 1, 'publish': 1, 'transparency': 1, 'checklist': 1, 'confirm': 1, 'items': 1, 'present': 1, 'confirmed': 1, 'groupcondition': 1, 'discrete': 1, 'measurement': 1, 'measurements': 1, 'distinct': 1, 'twosided': 1, 'common': 1, 'describe': 1, 'covariates': 1, 'tested': 1, 'assumptions': 1, 'corrections': 1, 'normality': 1, 'adjustment': 1, 'comparisons': 1, 'tendency': 1, 'basic': 1, 'regression': 1, 'coefficient': 1, 'uncertainty': 1, 'null': 1, 'testing': 1, 'degrees': 1, 'freedom': 1, 'whenever': 1, 'suitable': 1, 'bayesian': 1, 'choice': 1, 'priors': 1, 'monte': 1, 'carlo': 1, 'settings': 1, 'hierarchical': 1, 'designs': 1, 'level': 1, 'outcomes': 1, 'cohens': 1, 'indicating': 1, 'calculated': 1, 'biologists': 1, 'githubcomdeepmind': 1, 'upon': 1, 'publication': 1, 'httpsgithubcomtensorflowtensorflow': 1, 'httpsgithubcomdeepmind': 1, 'httpsgithubcomopenmm': 1, '925af': 1, 'httpsgithubcomsoedinglabmmseqs2': 1, 'httpsgithubcomrefresh': 1, 'biofamsa': 1, 'wwwscipyorg': 1, 'httpsgithubcompandasdevpandas': 1, 'has2k1plotnine': 1, 'httpsgithubcomstatsmodelsstatsmodels': 1, 'httpsresearchgooglecomcolaboratory': 1, 'httpszhanglabdcmbmedumichedutmalign': 1, 'httpsgithubcomschrodingerpymolopensource': 1, 'utilizing': 1, 'custom': 1, 'yet': 1, 'literature': 1, 'editors': 1, 'reviewers': 1, 'strongly': 1, 'deposition': 1, 'repository': 1, 'github': 1, 'guidelines': 1, 'submitting': 1, 'codes': 1, 'identifiers': 1, 'links': 1, 'restrictions': 1, 'httpsftpwwpdborgpubpdbderived_data': 1, 'httpscdnrcsborgresourcessequenceclustersbc40out': 1, '28082019': 1, '14052020': 1, '13052020': 1, 'httpswwwusergwdgdecompbioldatahhsuitedatabaseshhsuite_dbs': 1, 'accessions': 1, 'httpsftpebiacukpubdatabasesuniprotprevious_releases': 1, 'release2020_01uniref': 1, 'httpsbfdmmseqscom': 1, 'httpswwwusergwdgdecompbioluniclust2018_08': 1, 'httpsftpebiacukpubdatabasesmetagenomicspeptide_database2018_12': 1, 'fieldspecific': 1, 'please': 1, 'selection': 1, 'behavioural': 1, 'social': 1, 'ecological': 1, 'environmental': 1, 'document': 1, 'naturecomdocumentsnrreportingsummaryflatpdf': 1, 'disclose': 1, 'disclosure': 1, 'negative': 1, 'evaluated': 1, 'benchmark': 1, 'subject': 1, 'briefly': 1, 'unknownambiguous': 1, 'representatives': 1, 'similaritybased': 1, 'filter': 1, 'replication': 1, 'randomization': 1, 'blinding': 1, 'specific': 1, 'types': 1, 'listed': 1, 'relevant': 1, 'item': 1, 'applies': 1, 'response': 1, 'antibodies': 1, 'lines': 1, 'palaeontology': 1, 'archaeology': 1, 'animals': 1, 'organisms': 1, 'participants': 1, 'clinical': 1, 'dual': 1, 'concern': 1, 'chipseq': 1, 'cytometry': 1, 'mribased': 1, 'neuroimaging': 1})}, {'file_name': 'esm-2', 'word_counts': Counter({'the': 335, 'of': 229, 'and': 148, 'a': 140, 'in': 136, 'to': 118, 'structure': 83, 'for': 71, 'that': 69, 'with': 68, 'on': 68, 'is': 63, 'model': 57, 'from': 52, 'are': 50, 'e': 50, 'language': 48, 'protein': 46, 'prediction': 45, 'al': 45, 'sequences': 44, 'structures': 43, 'we': 41, 'sequence': 40, 'proteins': 39, 'models': 38, 'this': 38, 'esmfold': 36, 'which': 35, 'esm2': 35, 't': 34, 'as': 32, 'predicted': 31, 'fig': 31, 'o': 30, 's': 30, 'r': 29, 'at': 29, 'metagenomic': 28, 'by': 28, 'd': 28, 'million': 27, 'n': 27, 'perplexity': 27, 'predictions': 27, 'tmscore': 27, 'c': 26, 'j': 25, 'alphafold2': 25, 'i': 24, 'l': 24, 'an': 23, 'be': 23, 'accuracy': 23, 'm': 23, 'have': 22, 'h': 21, 'scale': 21, 'et': 21, 'confidence': 20, 'pdb': 20, 'b': 20, '2023': 19, 'cameo': 18, '2022': 18, 'can': 17, 'training': 17, 'science': 17, 'high': 16, 'f': 16, 'research': 16, 'also': 16, 'these': 16, 'contact': 16, 'has': 15, '1': 15, 'test': 15, 'plddt': 15, 'parameters': 14, 'between': 14, 'set': 14, 'performance': 14, 'results': 13, 'u': 13, 'patterns': 13, 'low': 13, 'all': 13, 'used': 13, 'it': 13, 'casp14': 13, '2021': 13, 'structural': 12, 'been': 12, 'not': 12, 'lddt': 12, 'into': 11, 'g': 11, 'highconfidence': 11, 'trained': 11, 'when': 11, 'no': 11, 'information': 10, 'using': 10, 'large': 10, '15': 10, 'characterization': 10, 'function': 10, 'uniref90': 10, 'sm': 10, '17': 10, 'university': 10, '11': 10, 'each': 10, 'folding': 10, 'alphafold': 10, 'k': 10, 'up': 9, 'atomicresolution': 9, 'esm': 9, 'atlas': 9, 'across': 9, 'acids': 9, 'billion': 8, 'through': 8, 'data': 8, 'improvement': 8, 'lin': 8, '379': 8, '11231130': 8, 'march': 8, 'f8': 8, 'downloaded': 8, 'httpswwwscienceorg': 8, 'chicago': 8, 'january': 8, '2025': 8, 'y': 8, 'colored': 8, 'w': 8, 'experimental': 8, 'atomiclevel': 7, 'largescale': 7, 'diversity': 7, 'computational': 7, 'number': 7, 'pre': 7, 'search': 7, 'well': 7, '2': 7, 'determined': 7, 'struc': 7, 'table': 7, 'random': 7, 'average': 7, 'precision': 7, 'article': 7, 'similar': 7, 'rosettafold': 7, 'both': 7, 'example': 7, '05': 7, 'mean': 7, '3': 7, 'there': 7, 'p': 7, 'evolutionary': 6, 'representations': 6, 'highresolution': 6, 'tion': 6, 'developed': 6, 'shown': 6, 'because': 6, '8': 6, 'mil': 6, 'over': 6, 'modeling': 6, 'accurate': 6, 'stateoftheart': 6, 'speed': 6, 'pro': 6, 'two': 6, 'but': 6, '32': 6, 'larger': 6, 'out': 6, 'shows': 6, 'axis': 6, 'changes': 6, 'sets': 6, 'rmsd': 6, 'map': 6, 'attention': 6, 'use': 6, 'projection': 6, 'single': 6, 'score': 6, 'msas': 6, 'å': 6, '07': 6, 'sample': 6, 's2': 6, '2020': 6, 'achieves': 6, '2019': 6, 'zenodo': 6, 'recent': 5, 'learning': 5, 'emerges': 5, 'evolution': 5, '4': 5, '14': 5, 'their': 5, 'such': 5, 'show': 5, 'amino': 5, 'will': 5, 'under': 5, 'increase': 5, 'improve': 5, 'parameter': 5, 'understanding': 5, 'ing': 5, 'our': 5, 'architecture': 5, 'fold': 5, 'makes': 5, 'known': 5, 'were': 5, 'dictions': 5, 'most': 5, 'identity': 5, 'any': 5, 'match': 5, 'experimentally': 5, 'emergence': 5, 'improvements': 5, 's1': 5, 'masked': 5, 'unsupervised': 5, 'database': 5, '43': 5, '20': 5, 'right': 5, 'top': 5, 'left': 5, 'head': 5, '40': 5, 'trunk': 5, 'blocks': 5, 'module': 5, 'distribution': 5, 'ture': 5, 'mgnify': 5, 'similarity': 5, 'predic': 5, 'very': 5, 'biorxiv': 5, 'preprint': 5, 'nucleic': 5, 'thank': 5, 'zl': 5, 'predict': 4, 'predicting': 4, 'vast': 4, 'natural': 4, 'es': 4, 'sa': 4, 'its': 4, 'methods': 4, '12': 4, '18': 4, 'or': 4, 'trans': 4, 'without': 4, 'lion': 4, 'continues': 4, 'magnitude': 4, 'fast': 4, 'approach': 4, 'msa': 4, 'neural': 4, 'related': 4, '10': 4, '30': 4, 'one': 4, 'possible': 4, 'far': 4, 'processing': 4, 'existing': 4, 'tures': 4, 'scales': 4, 'work': 4, 'x': 4, 'em': 4, 'perplex': 4, 'ity': 4, 'held': 4, 'el': 4, 'within': 4, 'per': 4, 'longrange': 4, 'against': 4, 'change': 4, '7qqa': 4, 'target': 4, 'improves': 4, '26': 4, '44': 4, 'true': 4, 'only': 4, 'how': 4, 'rep': 4, 'dockq': 4, 'atomic': 4, 'degrades': 4, 'compare': 4, 'ground': 4, 'truth': 4, 'below': 4, 'subset': 4, 'lan': 4, 'guage': 4, 'project': 4, '51': 4, 'comparison': 4, 'evaluation': 4, 'calibration': 4, 'ptm': 4, 'obtained': 4, 'foldseek': 4, '53': 4, 'according': 4, 'same': 4, '57': 4, 'sc': 4, 'good': 4, 'rmsd95': 4, '59': 4, 'many': 4, 'available': 4, 'natl': 4, 'acad': 4, 'sci': 4, 'z': 4, '2018': 4, 'pp': 4, 'evolutionaryscale': 3, 'inference': 3, 'full': 3, 'enables': 3, '617': 3, '225': 3, 'view': 3, 'so': 3, 'ical': 3, 'selected': 3, '13': 3, 'insight': 3, '6': 3, 'deep': 3, 'potential': 3, 'learn': 3, 'evolu': 3, 'greater': 3, 'resolution': 3, 'text': 3, 'develop': 3, 'hundreds': 3, 'biological': 3, 'those': 3, 'orders': 3, 'strong': 3, 'directly': 3, 'current': 3, 'while': 3, '60': 3, 'teins': 3, 'min': 3, 'pipelines': 3, 'even': 3, 'means': 3, 'efforts': 3, 'knowledge': 3, '34': 3, 'uniprot': 3, '35': 3, 'source': 3, 'mgnify90': 3, 'able': 3, 'reveals': 3, 'regions': 3, '90': 3, '126': 3, 'do': 3, 'httpsesmatlascom': 3, 'open': 3, 'resource': 3, 'family': 3, 'transformer': 3, 'esm1b': 3, 'train': 3, 'increases': 3, 'supplementary': 3, 'pa': 3, 'york': 3, 'corresponding': 3, 'authors': 3, 'other': 3, 'randomly': 3, 'im': 3, 'mod': 3, 'objective': 3, 'millions': 3, 'linked': 3, '39': 3, 'observe': 3, 'fidelity': 3, 'measured': 3, 'ex': 3, 'evaluated': 3, 'scaling': 3, 'positive': 3, 'contacts': 3, 'length': 3, 'formance': 3, '150': 3, 'numbers': 3, 'smaller': 3, 'pink': 3, 'teal': 3, 'threshold': 3, 't1056': 3, '54': 3, '28': 3, '41': 3, '45': 3, '21': 3, '50': 3, 'result': 3, '19': 3, 'lowresolution': 3, 'con': 3, 'more': 3, 'depth': 3, 'often': 3, 'block': 3, 'recycling': 3, '7qym': 3, 'coordinates': 3, 'examples': 3, 'green': 3, 'gray': 3, 'chain': 3, 'scores': 3, 'substantially': 3, 'level': 3, 'indicates': 3, 'values': 3, 'singlesequence': 3, 'dataset': 3, 'representative': 3, 'expected': 3, 'released': 3, '2b': 3, 'correlation': 3, 'tm': 3, '7': 3, 'matches': 3, 'difference': 3, 'analysis': 3, 'complexes': 3, 'agreement': 3, 'light': 3, 'remote': 3, 'tools': 3, 'ns': 3, 'liu': 3, 'proc': 3, 'zhang': 3, '2017': 3, 'syst': 3, 'nature': 3, '16': 3, 'nat': 3, 'cs': 3, 'linet': 3, 'code': 3, 'ha': 3, 'zz': 3, 'wl': 3, 'rr': 3, 'bh': 3, 'ts': 3, 'materials': 3, 'archived': 3, 'advances': 2, 'machine': 2, 'multiple': 2, 'demonstrate': 2, 'direct': 2, 'primary': 2, 'picture': 2, 'learned': 2, 'capability': 2, 'including': 2, 'q': 2, 'tt': 2, 'eo': 2, 'contain': 2, 'biolog': 2, 'properties': 2, 'biol': 2, '5': 2, 'progress': 2, 'op': 2, 'aspects': 2, 'underlying': 2, 'capacity': 2, '22': 2, 'modern': 2, 'attentionbased': 2, 'despite': 2, 'filling': 2, 'missing': 2, 'next': 2, 'exhibit': 2, 'power': 2, 'tens': 2, 'billions': 2, 'math': 2, 'solving': 2, 'evo': 2, 'seen': 2, 'expect': 2, 'about': 2, 'give': 2, 'study': 2, 'discover': 2, 'correlations': 2, 'close': 2, 'link': 2, 'endtoend': 2, 'captured': 2, 'eliminates': 2, 'need': 2, 'greatly': 2, 'removing': 2, 'process': 2, 'entirely': 2, 'take': 2, 'highsensitivity': 2, 'expand': 2, 'contributed': 2, 'size': 2, 'bases': 2, 'recently': 2, 'completed': 2, 'human': 2, 'proteome': 2, 'extent': 2, 'frontier': 2, 'complete': 2, 'weeks': 2, 'hetero': 2, 'cluster': 2, '2000': 2, 'gpus': 2, 'databases': 2, 'made': 2, 'space': 2, 'distant': 2, '768': 2, 'lions': 2, 'relative': 2, 'previous': 2, 'outperforms': 2, 'comparable': 2, 'new': 2, 'ny': 2, 'usa': 2, 'tein': 2, 'generated': 2, 'iden': 2, 'tity': 2, 'lt': 2, 'uniref': 2, 'sampled': 2, 'uniref50': 2, 'clusters': 2, 'ranges': 2, 'performed': 2, 'steps': 2, 'section': 2, 'reaches': 2, 'bottom': 2, '3lyw': 2, 'if': 2, 'likely': 2, 'a21': 2, 'mmseqs': 2, 'hits': 2, 'better': 2, 'differ': 2, 'points': 2, 'combined': 2, 'casp': 2, '597': 2, '70': 2, '29': 2, '758': 2, '27': 2, '3b': 2, 'se': 2, 'correspond': 2, 'measures': 2, 'pattern': 2, 'ments': 2, 'thus': 2, '48': 2, 'lm': 2, 't1057': 2, '098': 2, '097': 2, '7lqm': 2, '23': 2, 't1074': 2, '064': 2, '166': 2, 'network': 2, '3d': 2, 'confidences': 2, 'similarly': 2, 'estimate': 2, 'testset': 2, 'coloring': 2, 'tetramer': 2, 'pairs': 2, 'unsuccessful': 2, 'does': 2, 'understand': 2, 'long': 2, 'overall': 2, 'toward': 2, 'gain': 2, 'correlated': 2, 'folded': 2, 'maps': 2, 'internal': 2, 'equivariant': 2, '194': 2, '42': 2, '0': 2, 'uses': 2, 'cutoff': 2, 'ft': 2, 'respectively': 2, 'increased': 2, 'rameters': 2, 'before': 2, 'after': 2, 'validation': 2, 'additionally': 2, 'esmfoldpredicted': 2, 'umap': 2, 'distance': 2, 'nearest': 2, 'see': 2, 'entry': 2, 'pic': 2, 'tween': 2, 'na': 2, 'dc': 2, 'findings': 2, 'accelerate': 2, 'external': 2, 'templates': 2, 'find': 2, 'time': 2, 'passed': 2, 'representation': 2, 'output': 2, 'er': 2, 'a12': 2, '142': 2, 'six': 2, 'than': 2, 'struct': 2, 'protocols': 2, 'reduced': 2, 'although': 2, 'total': 2, 'experimen': 2, 'tally': 2, 'further': 2, 'may': 2, 'predictive': 2, 'consists': 2, '083': 2, '068': 2, '085': 2, 'competitive': 2, 'design': 2, 'plexity': 2, 'suggests': 2, 'v': 2, 'provides': 2, 'some': 2, '2d': 2, 'tions': 2, '49': 2, 'reports': 2, 'form': 2, 's5': 2, 'median': 2, 'allatom': 2, 'backbone': 2, '09': 2, '52': 2, 'corresponds': 2, 'found': 2, '9': 2, 'showing': 2, 'figs': 2, 'subfigure': 2, 'displayed': 2, 'mgyp000936678158': 2, 'aligns': 2, 'bacterial': 2, 'brown': 2, 'mgyp004000959047': 2, 'binding': 2, 'ys': 2, 'limit': 2, 'reference': 2, '55': 2, 'discovered': 2, 'sequencing': 2, '56': 2, 'references': 2, 'notes': 2, 'comput': 2, '2009': 2, 'wang': 2, '596': 2, 'rives': 2, 'heinzinger': 2, 'ovchinnikov': 2, 'systems': 2, 'curran': 2, 'associates': 2, 'conference': 2, 'american': 2, 'association': 2, 'linguistics': 2, 'steinegger': 2, 'södingnat': 2, 'd1': 2, 'van': 2, '58': 2, 'v0': 2, 'httpsdoiorg': 2, 'feedback': 2, 'ar': 2, 'analyzed': 2, 'zeming': 1, 'lin12': 1, 'halil': 1, 'akin1': 1, 'roshan': 1, 'rao1': 1, 'brian': 1, 'hie13': 1, 'zhongkai': 1, 'zhu1': 1, 'wenting': 1, 'lu1': 1, 'nikita': 1, 'smetanin1': 1, 'robert': 1, 'verkuil1': 1, 'ori': 1, 'kabeli1': 1, 'yaniv': 1, 'shmueli1': 1, 'allan': 1, 'dos': 1, 'santos': 1, 'costa4': 1, 'maryam': 1, 'fazelzarandi1': 1, 'tom': 1, 'sercu1': 1, 'salvatore': 1, 'candido1': 1, 'alexander': 1, 'rives12': 1, 'leveraged': 1, 'alignments': 1, 'scaled': 1, 'orderofmagnitude': 1, 'acceleration': 1, 'apply': 1, 'construct': 1, 'gives': 1, 'breadth': 1, 'fp': 1, 'image': 1, 'constrain': 1, 'mutations': 1, 'recording': 1, 'ogy': 1, 'therefore': 1, 'ferred': 1, 'central': 1, 'starting': 1, 'classical': 1, '7t': 1, 'ht': 1, 'ei': 1, 'duction': 1, '811u': 1, 'pt': 1, 'highaccuracy': 1, 'idea': 1, 'motivates': 1, 'tionaryscale': 1, 'basic': 1, '1517': 1, 'reflect': 1, 'biology': 1, 'representational': 1, 'capture': 1, 'secondary': 1, 'ter': 1, 'tiary': 1, '1921': 1, 'beginning': 1, 'shannonsm': 1, 'lf': 1, 'rt': 1, 'entropy': 1, 'creasing': 1, 'complexity': 1, 'culminated': 1, 'architectures': 1, '2325': 1, 'simplicity': 1, 'objectives': 1, 'words': 1, 'word': 1, 'emergent': 1, 'capabilities': 1, 'increasing': 1, 'compu': 1, 'tational': 1, 'param': 1, 'eters': 1, 'containing': 1, 'abilities': 1, 'fewshot': 1, 'lation': 1, 'commonsense': 1, 'reasoning': 1, 'ematical': 1, 'problem': 1, 'explicit': 1, 'supervision': 1, '2629': 1, 'posit': 1, 'task': 1, 'lution': 1, 'require': 1, 'stand': 1, 'creates': 1, 'representa': 1, 'tional': 1, 'emerge': 1, 'proper': 1, 'ties': 1, 'rise': 1, 'observed': 1, 'kind': 1, 'emer': 1, 'gence': 1, 'four': 1, 'accu': 1, 'racy': 1, 'reveal': 1, 'enable': 1, 'diction': 1, 'leverages': 1, 'produce': 1, 'removes': 1, 'costly': 1, 'pipeline': 1, 'alignment': 1, 'simplifying': 1, 'forward': 1, 'pass': 1, 'alone': 1, 'alpha': 1, '12a': 1, 'dr': 1, 'd13a': 1, 'di': 1, 'sam': 1, 'ingful': 1, 'part': 1, 'cost': 1, 'lowersensitivity': 1, 'practice': 1, 'speedup': 1, 'advantage': 1, 'datasets': 1, 'past': 1, 'decade': 1, 'immense': 1, 'microbial': 1, 'earth': 1, 'sampling': 1, 'exponential': 1, 'growth': 1, 'now': 1, '3133': 1, 'character': 1, 'izations': 1, '20000': 1, '200': 1, 'cataloged': 1, 'represents': 1, 'challenge': 1, 'un': 1, 'knowl': 1, 'edge': 1, 'discoveries': 1, 'medicine': 1, 'biotechnology': 1, '3638': 1, 'present': 1, 'folds': 1, 'practically': 1, 'geneous': 1, 'graphics': 1, 'units': 1, 'demonstrates': 1, 'scalability': 1, 'characterizes': 1, 'separate': 1, '39b': 1, 'ya': 1, 'least': 1, 'accessed': 1, 'begin': 1, 'generation': 1, 'introduces': 1, 'resources': 1, 'mate': 1, 'rial': 1, 'sections': 1, 'a11': 1, 'a2': 1, 'resulting': 1, 'previously': 1, 'a650': 1, 'rameter': 1, '1o': 1, '1fair': 1, 'meta': 1, 'ai': 1, 'usa2new': 1, 'usa3stanford': 1, 'palo': 1, 'alto': 1, 'ca': 1, '4massachusetts': 1, 'institute': 1, 'technology': 1, 'cambridge': 1, 'ma': 1, 'author': 1, 'email': 1, 'arivesmetacom': 1, 'equally': 1, 'bench': 1, 'marks': 1, 'lmlm': 1, '¼c0': 1, 'log': 1, 'px': 1, 'ijxnm': 1, 'þð': 1, 'ð1þ': 1, 'where': 1, 'maskm': 1, 'includes': 1, 'positionsi': 1, 'sequencex': 1, 'tasked': 1, 'acidsxi': 1, 'mask': 1, 'surrounding': 1, 'contextxnm': 1, 'excluding': 1, 'positions': 1, 'eling': 1, '25c': 1, 'st': 1, 'ol': 1, 'dependencies': 1, 'though': 1, 'itself': 1, 'simple': 1, 'evolutionarily': 1, 'diverse': 1, 'requires': 1, 'internalize': 1, 'cause': 1, 'materialize': 1, 'during': 1, 'weighting': 1, '138': 1, 'course': 1, 'sees': 1, '65': 1, 'unique': 1, 'perfect': 1, 'intuitively': 1, 'describes': 1, 'choosing': 1, 'among': 1, 'position': 1, 'ematically': 1, 'defined': 1, 'ponential': 1, 'negative': 1, 'loglikelihood': 1, 'a22': 1, 'figure': 1, 'updates': 1, 'of500000': 1, 'comparisons': 1, '270000': 1, 'largest': 1, '8millionparameter': 1, '1045': 1, '637': 1, 'dicates': 1, 'standing': 1, '2o': 1, 'billionparameters': 1, 'probabilities': 1, 'actual': 1, 'pl': 1, 'binned': 1, 'searching': 1, 'perform': 1, 'levels': 1, 'millionparameter': 1, '650million': 1, 'trajectory': 1, 'ent': 1, 'lefttoright': 1, 'comparing': 1, 'moduleonly': 1, '1056': 1, '150millionparameter': 1, '15billionparameter': 1, '77': 1, '125': 1, '656': 1, '113': 1, '699': 1, '68': 1, '750': 1, '756': 1, '572': 1, '115': 1, '618': 1, '109': 1, '96': 1, '671': 1, '80': 1, '665': 1, '67': 1, '8m': 1, '35m': 1, '150m': 1, '650m': 1, '15b': 1, 'ab': 1, 'pdi': 1, '8å': 1, 'false': 1, 'esm2s': 1, 'informa': 1, 'develops': 1, 'must': 1, 'representing': 1, 'quences': 1, 'residueresidue': 1, 'examine': 1, 'linear': 1, 'extract': 1, 'atten': 1, 'tact': 1, 'correspondence': 1, 'tertiary': 1, '1a': 1, 'leads': 1, '1b': 1, 'varies': 1, 'tionarily': 1, 'steeper': 1, 'trajectories': 1, 'respect': 1, '1c': 1, 'ment': 1, 'saturates': 1, 'lower': 1, 'lutionary': 1, 'individual': 1, 'nonlinear': 1, '3o': 1, 'pair': 1, 'seq': 1, 'egrltvyctvq': 1, 'pretrained': 1, 'via': 1, '7m6b': 1, 'glucosamine6phosphate': 1, 'deaminase': 1, '091': 1, 'lasparaginase': 1, '7oc9': 1, '093': 1, 'esmfolda': 1, 'arrows': 1, 'flow': 1, 'outputs': 1, 'produces': 1, 'ablated': 1, 'scatterplots': 1, 'af': 1, 'versus': 1, 'wellcalibrated': 1, 'successful': 1, 'complex': 1, 'dimer': 1, 'id': 1, 'overlaid': 1, 'reported': 1, 'interactions': 1, 'case': 1, 'interacting': 1, 'meaning': 1, 'input': 1, 'plotting': 1, 'range': 1, 'transition': 1, 'higher': 1, 'shift': 1, '1d': 1, 'undergo': 1, 'undergoing': 1, 'going': 1, 'normalized': 1, 'discounted': 1, 'cumulative': 1, 'ndcg': 1, '087': 1, 'a26': 1, 'materialization': 1, 'identify': 1, 'spatial': 1, 'coordi': 1, 'nates': 1, 'atoms': 1, 'a33': 1, 'fitted': 1, 'bank': 1, 'value': 1, 'thresh': 1, 'old': 1, 'correctly': 1, 'temporal': 1, 'en': 1, 'sures': 1, 'testing': 1, 'fitting': 1, 'measure': 1, 'projected': 1, 'resentations': 1, 'e1': 1, '5b': 1, '072': 1, '055': 1, 'rel': 1, 'ative': 1, '1e': 1, 'undergoes': 1, 'root': 1, 'square': 1, 'deviation': 1, 'proves': 1, '1f': 1, 'jumps': 1, 'much': 1, 'cor': 1, 'relation': 1, 'of099': 1, 'and100': 1, 'connection': 1, '4o': 1, 'mapping': 1, 'spacea': 1, 'thex': 1, 'they': 1, 'density': 1, 'subsample': 1, 'of4000': 1, 'computed': 1, 'of617': 1, 'report': 1, 'visualized': 1, 'dimensions': 1, 'algorithm': 1, 'dark': 1, 'blue': 1, 'locations': 1, 'landscape': 1, 'provided': 1, 'additional': 1, 'plot': 1, 'plotted': 1, 'blastp': 1, 'extracted': 1, '096': 1, '099': 1, 'contactp': 1, 'connect': 1, 'accelerating': 1, 'stateof': 1, 'theart': 1, 'internalizes': 1, 'generates': 1, 'threedimensional': 1, 'ep': 1, 'fm': 1, 'ro': 1, 'maintaining': 1, 'fully': 1, 'predictor': 1, '2a': 1, 'inputted': 1, 'processed': 1, 'feedforward': 1, 'layers': 1, 'states': 1, 'begins': 1, 'series': 1, 'alternates': 1, 'updating': 1, 'pairwise': 1, 'former': 1, 'three': 1, 'outputting': 1, 'final': 1, 'a31': 1, 'resents': 1, 'major': 1, 'simplification': 1, 'deeply': 1, 'integrate': 1, 'mechanism': 1, 'operates': 1, 'considerable': 1, 'provement': 1, 'nvidia': 1, 'v100': 1, 'gpu': 1, '384': 1, 'residues': 1, 'times': 1, 'faster': 1, 'shorter': 1, 'creases': 1, 'to60': 1, 'required': 1, 'published': 1, 'versions': 1, 'sensitivity': 1, 'on25000': 1, 'clus': 1, 'ters': 1, 'covering': 1, 'of325000': 1, 'augmented': 1, 'losses': 1, 'evaluate': 1, 'date': 1, 'would': 1, 'regular': 1, 'usage': 1, 'kinds': 1, 'perimentalists': 1, 'depos': 1, 'ited': 1, 'ongoing': 1, 'assessment': 1, 'april': 1, 'june': 1, 'publicly': 1, 'dif': 1, 'ficulty': 1, 'biannual': 1, 'competition': 1, '088': 1, 'averages': 1, '082': 1, 'evaluat': 1, 'ablating': 1, 'falls': 1, 'arti': 1, 'ficial': 1, 'setting': 1, 'explicitly': 1, 'ever': 1, 'emerged': 1, 'important': 1, 'inputs': 1, 'de': 1, 'novo': 1, '4648': 1, 'ee': 1, 'db': 1, 'yt': 1, 'lp': 1, '3billionparameter': 1, '100': 1, 'pearson': 1, 'of052': 1, 'is071': 1, '003': 1, 'differences': 1, '01': 1, 'relationship': 1, 'translate': 1, 'hi': 1, 'tw': 1, 'ho': 1, 'sf': 1, 'mt': 1, 'quality': 1, 'ablation': 1, 'studies': 1, 'indicate': 1, 'lr': 1, 'ec': 1, 'oe': 1, 's3': 1, 'eight': 1, '074': 1, 'local': 1, 'baseline': 1, '058': 1, 'ie': 1, '066': 1, 'ablations': 1, 'turning': 1, 'off': 1, 'distillation': 1, 'targets': 1, 'triangular': 1, 'dates': 1, 'small': 1, 'degradations': 1, 'of001': 1, 'to004': 1, 'matching': 1, '005': 1, 'half': 1, 't1076': 1, '540': 1, 'res': 1, 'idues': 1, 'parts': 1, 'notably': 1, 'contained': 1, 'make': 1, 'components': 1, 'homo': 1, 'dimeric': 1, 'proteinprotein': 1, 'alphafoldmultimer': 1, '2978': 1, 'multimeric': 1, 'deposited': 1, 'qualitative': 1, '50c': 1, 'egorization': 1, '532': 1, 'being': 1, 's4': 1, 'calibrated': 1, 'dicted': 1, 'correlates': 1, 'compa': 1, 'rable': 1, '2c': 1, 'ap': 1, 'proach': 1, 'experimentallevel': 1, '95': 1, 'resi': 1, 'due': 1, 'coverage': 1, '191': 1, '133': 1, '094': 1, '5o': 1, 'given': 1, 'investigated': 1, 'concurrently': 1, 'ours': 1, 'appears': 1, 'metagenomics': 1, 'entirety': 1, '1024': 1, 'covers': 1, '99': 1, 'produces365': 1, 'and225': 1, 'to36': 1, 'a41': 1, 'crit': 1, 'distinguish': 1, 'wellpredicted': 1, 'poorly': 1, 'perimentally': 1, 'determinedstructures': 1, 'heldout': 1, 'assess': 1, '4000': 1, 'pearsonr': 1, 'dp': 1, 'ta': 1, 'dt': 1, 'oa': 1, '3a': 1, 'fidence': 1, 'ac': 1, 'curacy': 1, 'esmfolds': 1, 'provide': 1, 'indication': 1, 'the617': 1, 'structures113': 1, 'meet': 1, 'represented': 1, '767580': 1, 'dis': 1, 'tinct': 1, '3e': 1, '33521': 1, 'a42': 1, 'thresholds': 1, '254': 1, '253905': 1, '6o': 1, 'bc': 1, 'different': 1, 'foldseekdetermined': 1, 'nuclease': 1, '3h4r': 1, 'whereas': 1, 'sterol': 1, 'domain': 1, '6bym': 1, '125765': 1, 'respec': 1, 'tively': 1, '25664': 1, 'homolog': 1, 'basis': 1, 'subsampled': 1, 'timates': 1, '4a': 1, 'effectively': 1, 'characterize': 1, 'si': 1, 'tp': 1, 'oi': 1, 'ls': 1, 'ilarities': 1, 'absence': 1, 'sim': 1, 'ilarity': 1, 'homology': 1, 'extends': 1, 'beyond': 1, 'detectable': 1, 'quence': 1, 'jackhm': 1, 'mer': 1, 'conserved': 1, 'nucleases': 1, '5yet_b': 1, '3hr4_a': 1, '067': 1, '4b': 1, 'jackhmmer': 1, 'teome': 1, 'lipid': 1, 'domains': 1, '6bym_a': 1, '080': 1, '5yqp_b': 1, '078': 1, '4c': 1, 'ability': 1, 'detect': 1, 'similarities': 1, 'cannot': 1, 'bulk': 1, 'download': 1, 'application': 1, 'programming': 1, 'interface': 1, 'api': 1, 'web': 1, 'facilitate': 1, 'focused': 1, 'scope': 1, 'conclusions': 1, 'era': 1, 'gene': 1, 'experiments': 1, 'promise': 1, 'insights': 1, 'end': 1, 'distinct': 1, 'comes': 1, 'critical': 1, 'throughput': 1, 'limiting': 1, 'joint': 1, 'oo': 1, 'nb': 1, 'ea': 1, 'tar': 1, 'tf': 1, 'biochemistry': 1, 'active': 1, 'sites': 1, 'topology': 1, 'reliably': 1, 'sight': 1, 'relationships': 1, 'could': 1, 'otherwise': 1, 'detected': 1, 'encoded': 1, 'els': 1, 'computing': 1, 'principle': 1, 'applied': 1, 'timistic': 1, 'continue': 1, 'point': 1, 'direction': 1, 'advance': 1, 'practical': 1, 'terms': 1, 'puts': 1, 'reach': 1, 'accel': 1, 'erate': 1, 'discovery': 1, 'functions': 1, 'yanofsky': 1, 'horn': 1, 'thorpescience': 1, '146': 1, '15931594': 1, '1964': 1, 'altschuh': 1, 'vernet': 1, 'berti': 1, 'moras': 1, 'nagaiprotein': 1, 'eng': 1, '193199': 1, '1988': 1, 'göbel': 1, 'sander': 1, 'schneider': 1, 'valenciaproteins': 1, '309317': 1, '1994': 1, 'lapedes': 1, 'giraud': 1, 'stormolect': 1, 'monogr': 1, 'ser33': 1, '236256': 1, '1999': 1, 'thomas': 1, 'ramakrishnan': 1, 'baileykelloggieeeacm': 1, 'bioinform5': 1, '183197': 1, '2008': 1, 'weigt': 1, 'white': 1, 'szurmant': 1, 'hoch': 1, 'hwaproc': 1, 'usa1066': 1, '772': 1, 'morcos': 1, 'usa108': 1, 'e1293e1301': 1, '2011': 1, 'sun': 1, 'li': 1, 'xuplos': 1, 'biol13': 1, 'e1005324': 1, 'palmedo': 1, 'ye': 1, 'berger': 1, 'pengcell': 1, '6574e3': 1, 'senioret': 1, '577': 1, '706710': 1, 'yang': 1, 'usa117': 1, '14961503': 1, 'jumper': 1, '583589': 1, 'baek': 1, '373': 1, '871876': 1, 'usa118': 1, 'e2016239118': 1, 'bepler': 1, 'bergercell': 1, '654669e3': 1, 'alley': 1, 'khimulya': 1, 'biswas': 1, 'alquraishi': 1, 'church': 1, '13151322': 1, 'bmc': 1, 'bioinformatics': 1, '723': 1, 'elnaggar': 1, 'ieee': 1, 'anal': 1, 'mach': 1, 'intell14': 1, 'vig': 1, 'arxiv200615222': 1, 'qbio': 1, 'rao': 1, 'meier': 1, 'sercu': 1, '422761': 1, 'httpsdoiorg101101': 1, '20201215422761': 1, 'chowdhury': 1, 'biotechnol': 1, '16171623': 1, 'shannonbell': 1, 'tech': 1, 'j305': 1, '1951': 1, 'vaswaniet': 1, '59986008': 1, '24': 1, 'radford': 1, 'narasimhan': 1, 'salimans': 1, 'sutskever': 1, 'improving': 1, 'generative': 1, 'pretraining': 1, 'https': 1, 'cdnopenaicomresearchcoverslanguageunsupervised': 1, 'language_understanding_paperpdf': 1, '25': 1, 'devlin': 1, 'mw': 1, 'chang': 1, 'lee': 1, 'toutanova': 1, 'inproceedings': 1, 'north': 1, 'chapter': 1, 'technologies': 1, 'volume': 1, 'short': 1, 'papersassociation': 1, '41714186': 1, 'brownet': 1, 'inadvances': 1, '18771901': 1, 'wei': 1, 'arxiv210901652': 1, 'cscl': 1, 'weiet': 1, 'arxiv220111903': 1, 'chowdheryet': 1, 'arxiv220402311': 1, 'mirditaet': 1, '679682': 1, '31': 1, 'mirdita': 1, 'methods16': 1, '603606': 1, 'mitchellet': 1, 'res48': 1, 'd570d578': 1, '33': 1, 'mukherjeeet': 1, 'res49': 1, 'd723d733': 1, 'tunyasuvunakoolet': 1, '590596': 1, 'varadiet': 1, 'res50': 1, 'd439d444': 1, '36': 1, 'shimomura': 1, 'johnson': 1, 'saigaj': 1, 'cell': 1, 'comp': 1, 'physiol': 1, '223239': 1, '1962': 1, '37': 1, 'mulliset': 1, 'cold': 1, 'spring': 1, 'harb': 1, 'symp': 1, 'quant': 1, 'biol51': 1, '263273': 1, '1986': 1, '38': 1, 'jineket': 1, '337': 1, '816821': 1, '2012': 1, 'suzek': 1, 'huang': 1, 'mcgarvey': 1, 'wu': 1, 'consortiumbioinformatics319': 1, '6932': 1, '2015': 1, 'burleyet': 1, 'res47': 1, 'd464d474': 1, 'haas': 1, '86': 1, 'suppl': 1, '387398': 1, 'kryshtafovych': 1, 'schwede': 1, 'topf': 1, 'fidelis': 1, 'moult': 1, '89': 1, '16071617': 1, 'skolnickproteins': 1, '702710': 1, '2004': 1, 'raoet': 1, 'proceedings': 1, '38th': 1, 'international': 1, 'learningpmlr': 1, '88448856': 1, 'ahdritzet': 1, '517210': 1, 'httpsdoiorg10110120221120517210': 1, '46': 1, 'dauparaset': 1, '3784': 1, '956': 1, '47': 1, 'wanget': 1, '377': 1, '387394': 1, 'wickyet': 1, '3785': 1, '661': 1, 'evanset': 1, '463034': 1, 'httpsdoiorg10110120211004463034': 1, 'basu': 1, 'wallnerplos': 1, 'e0161879': 1, '2016': 1, 'weissenow': 1, 'roststructure30': 1, '11691177e4': 1, 'wuet': 1, '500999': 1, 'kempenet': 1, '479398': 1, 'httpsdoiorg10110120220207479398': 1, 'potteret': 1, 'res46': 1, 'w200w204': 1, 'biotechnol35': 1, '10261028': 1, 'zhangcurr': 1, 'opin': 1, 'biol191': 1, '5155': 1, 'esmfoldv0': 1, 'weights': 1, 'httpsdoiorg105281zenodo7566741': 1, '105281zenodo7623482': 1, '105281zenodo7623627': 1, 'altschul': 1, 'gish': 1, 'miller': 1, 'myers': 1, 'lipman': 1, 'mol': 1, 'biol215': 1, '403410': 1, '1990': 1, 'acknowledgments': 1, 'fair': 1, 'team': 1, 'members': 1, 'goyal': 1, 'lecun': 1, 'lerer': 1, 'der': 1, 'maaten': 1, 'sukhbaatar': 1, 'collaborators': 1, 'dauparas': 1, 'technical': 1, 'help': 1, 'discussions': 1, 'helped': 1, 'shape': 1, 'koonin': 1, 'rizvi': 1, 'shepard': 1, 'spisak': 1, 'program': 1, 'support': 1, 'gomez': 1, 'jain': 1, 'ngan': 1, 'seejoor': 1, 'website': 1, 'developers': 1, 'openfold': 1, 'fairseq': 1, 'pytorch': 1, 'mmseqs2': 1, 'pymol': 1, 'biotite': 1, 'others': 1, 'building': 1, 'invaluable': 1, 'opensource': 1, 'creators': 1, 'maintainers': 1, 'researchers': 1, 'whose': 1, 'included': 1, 'resourcesfunding': 1, '7o': 1, 'sources': 1, 'funding': 1, 'projectauthor': 1, 'contributions': 1, 'conceptualized': 1, 'initiated': 1, 'rv': 1, 'asc': 1, 'mfz': 1, 'produced': 1, 'ok': 1, 'wrote': 1, 'manuscript': 1, 'engineering': 1, 'leadership': 1, 'arcompeting': 1, 'intereststhe': 1, 'declare': 1, 'competing': 1, 'financial': 1, 'interests': 1, 'patent': 1, 'applications': 1, 'filed': 1, 'workdata': 1, 'availability': 1, 'httpsgithub': 1, 'comfacebookresearchesm': 1, 'permissive': 1, 'licenses': 1, 'mit': 1, 'license': 1, 'ccby': 1, 'structureslicense': 1, 'copyright': 1, 'rights': 1, 'reserved': 1, 'exclusive': 1, 'licensee': 1, 'advancement': 1, 'claim': 1, 'original': 1, 'us': 1, 'government': 1, 'works': 1, 'httpswww': 1, 'sciencemagorgaboutsciencelicensesjournalarticlereuse': 1, 'scienceorgdoi101126scienceade2574': 1, 's8': 1, 'tables': 1, '6174': 1, 'viewrequest': 1, 'protocol': 1, 'paper': 1, 'frombioprotocol': 1, 'submitted': 1, 'august': 1, 'resubmitted': 1, 'november': 1, 'accepted': 1, 'february': 1, '101126scienceade2574': 1, '8o': 1})}, {'file_name': 'conch', 'word_counts': Counter({'the': 874, 'of': 575, 'and': 575, 'for': 482, 'a': 358, 'in': 298, 'to': 286, 'we': 279, 'on': 182, 'data': 157, 'with': 156, 'image': 132, 'used': 125, 'model': 117, 'each': 116, 'as': 116, 'zeroshot': 106, 'from': 105, 'that': 104, 'conch': 103, 'by': 98, 'is': 94, 'tasks': 93, 'classification': 92, 'learning': 87, 'using': 86, 'performance': 85, 'training': 84, 'or': 76, 'set': 74, 'et': 74, 'al': 74, 'pretraining': 72, 'class': 71, 'dataset': 71, 'slides': 70, 'images': 68, 'text': 68, 'models': 67, 'was': 65, 'are': 63, 'an': 60, 'supervised': 58, 'all': 54, 'encoder': 54, 'were': 53, '1': 52, 'be': 52, 'tcga': 52, 'nature': 51, 'fig': 51, 'retrieval': 49, 'pretrained': 49, 'visuallanguage': 48, 'histopathology': 45, 'at': 43, 'plip': 43, 'biomedclip': 43, 'n': 43, 'tiles': 42, 'p': 42, 'pathology': 41, 'other': 41, 'labels': 41, 'our': 41, 'segmentation': 40, 'this': 40, 'language': 39, 'which': 38, '10': 38, 'captions': 36, 'see': 36, 'j': 35, 'can': 35, '5': 35, 'luad': 35, 'version': 35, 'wsis': 34, 'more': 34, 'test': 34, 'article': 33, 'supplementary': 33, 'medicine': 32, 'httpsdoiorg101038s41591024028564': 32, 'cancer': 32, 'foundation': 31, 'downstream': 31, 'examples': 31, 'not': 31, 'evaluation': 31, 'score': 31, 'study': 30, '2': 29, 'information': 29, 'vision': 29, 'c': 29, '2022': 29, 'captioning': 28, 'extended': 28, 'subtyping': 28, 'accuracy': 28, 'available': 28, 'conference': 28, 'task': 27, 'tissue': 27, 'slidelevel': 27, 'sicap': 27, 'b': 27, 'slide': 27, 'similarity': 27, 'average': 27, 'per': 27, 'pairs': 26, 'prompts': 26, 'regions': 26, '001': 26, 'both': 25, 'methods': 25, 'also': 25, 'source': 25, '30': 24, 'openaiclip': 24, 'number': 24, 'these': 24, 'caption': 24, 'corresponding': 24, 'embedding': 24, 'table': 24, 'recall': 24, 'it': 23, 'over': 23, 'while': 23, 'experiments': 23, 'after': 23, 'm': 23, 'deep': 22, '2023': 22, 'datasets': 22, 'scores': 22, 'visual': 22, 'their': 22, 't': 22, 'k': 21, 'label': 21, 'prediction': 21, 'results': 21, 'found': 21, 'into': 21, 'rcc': 21, 'between': 21, 'imagecaption': 20, 'recognition': 20, 'where': 20, 'prompt': 20, 'brca': 20, 'multimodal': 20, 'encoders': 20, 'performed': 20, 'fewshot': 20, 'diverse': 19, 'when': 19, 'given': 19, 'same': 19, 'wsi': 19, 'pattern': 19, 'transformer': 19, '025': 19, 'predicted': 19, 'proc': 19, 'consisted': 19, 'computational': 18, 'labeled': 18, 'such': 18, 'nsclc': 18, 'reporting': 18, 'split': 18, '16': 18, 'he': 18, '075': 18, 'research': 18, 'ieee': 18, '2024': 17, 'contrastive': 17, 'no': 17, 'details': 17, 'cell': 17, 'gleason': 17, 'balanced': 17, 'high': 17, 'computed': 17, 'than': 17, 'tumor': 17, '4': 17, 'additionally': 16, 'only': 16, 'machine': 16, 'classes': 16, 'cases': 16, 'crossmodal': 16, '3': 16, 'ing': 16, 'size': 16, 'validation': 16, 'med': 16, 'medical': 15, 'trained': 15, 'most': 15, 'including': 15, 'computer': 15, 'single': 15, 'level': 15, 'detailed': 15, 'report': 15, '32': 15, '20': 15, 'grading': 15, 'tile': 15, 'analysis': 15, 'nc': 15, '8': 15, 'nat': 15, 'eds': 15, 'across': 14, 'however': 14, 'texttoimage': 14, 'finetuning': 14, 'pathologist': 14, 'rare': 14, 'general': 14, 'different': 14, 'imagetext': 14, 'lung': 14, 'crc100k': 14, 'representation': 14, 'paired': 14, 'd': 14, 'within': 14, 'rois': 14, 'up': 14, 'o': 14, '2021': 14, 'r': 14, 'volume': 13, 'march': 13, 'diseases': 13, 'specific': 13, 'biomedical': 13, 'large': 13, 'baselines': 13, 'space': 13, 'example': 13, 'metric': 13, 'κ': 13, 'performing': 13, 'pubmed': 13, 'train': 13, 'embeddings': 13, 'achieved': 13, '100': 13, 'because': 13, 'weakly': 13, 'sampled': 13, 'shot': 13, 'pixels': 13, 's': 13, 'international': 13, 'processing': 13, 'full': 13, '863874': 12, 'y': 12, 'patient': 12, 'imagetotext': 12, 'process': 12, 'perform': 12, 'first': 12, 'similar': 12, 'tion': 12, 'breast': 12, 'dhmc': 12, 'central': 12, 'out': 12, '64': 12, '0': 12, 'block': 12, 'µm': 12, 'heatmap': 12, 'values': 12, 'official': 12, 'provided': 12, 'token': 12, '512': 12, 'zero': 12, 'extracted': 12, 'unimodal': 12, 'have': 11, 'million': 11, 'through': 11, 'evaluated': 11, 'stateoftheart': 11, 'primary': 11, 'prostate': 11, 'usa': 11, 'well': 11, 'multiple': 11, 'adenocarcinoma': 11, 'three': 11, 'roilevel': 11, 'wsss4luad': 11, 'following': 11, 'weighted': 11, 'metrics': 11, 'respectively': 11, 'cleaning': 11, 'cosinesimilarity': 11, '40': 11, 'tokens': 11, '050': 11, 'e': 11, 'terms': 11, 'retrieved': 11, 'precision': 11, 'randomly': 11, 'dimension': 11, 'range': 10, 'transfer': 10, 'may': 10, 'carcinoma': 10, 'cohens': 10, 'largescale': 10, 'small': 10, 'decoder': 10, 'open': 10, 'outperformed': 10, 'pleuropulmonary': 10, 'blastoma': 10, 'ctranspath': 10, 'low': 10, 'topk': 10, 'reported': 10, 'additional': 10, 'selfsupervised': 10, '2019': 10, 'pmlr': 10, 'testing': 10, 'f': 9, 'often': 9, '117': 9, 'further': 9, 'standard': 9, 'many': 9, 'under': 9, 'ma': 9, 'compared': 9, 'some': 9, 'previous': 9, 'tables': 9, 'capabilities': 9, 'sets': 9, 'based': 9, 'educational': 9, 'humanonly': 9, 'generated': 9, 'attention': 9, '12': 9, 'digestpath': 9, 'ebrains': 9, 'top': 9, 'confidence': 9, 'intervals': 9, 'annotated': 9, 'normal': 9, 'statistical': 9, 'still': 9, 'efficiency': 9, 'imagenet': 9, 'diagnostic': 9, 'sup': 9, 'show': 9, 'mean': 9, '05': 9, 'dice': 9, 'before': 9, 'portfolio': 9, 'code': 9, '2020': 9, 'whole': 9, 'association': 9, 'zhang': 8, 'digital': 8, 'limited': 8, 'sources': 8, 'diagnosis': 8, 'has': 8, 'practice': 8, 'authors': 8, '25': 8, 'then': 8, 'natural': 8, 'form': 8, 'setting': 8, 'distribution': 8, '46': 8, '15': 8, '34': 8, '6': 8, '50': 8, '150': 8, 'i': 8, 'stroma': 8, 'resnet50': 8, 'highest': 8, '95': 8, 'abmil': 8, 'left': 8, '35': 8, 'human': 8, 'potentially': 8, 'few': 8, 'backbone': 8, 'its': 8, 'versus': 8, 'next': 8, 'maximum': 8, 'five': 8, 'median': 8, 'could': 8, '7': 8, 'availability': 8, 'boxes': 8, 'indicate': 8, 'samples': 8, 'described': 8, 'work': 8, 'finetuned': 8, '9': 8, 'publicly': 8, 'visionlanguage': 8, 'ground': 8, 'bc': 8, 'detection': 8, 'h': 8, 'x': 8, 'w': 8, 'ieeecvf': 8, 'associated': 8, 'patients': 8, 'linguistics': 8, 'eg': 8, 'subset': 8, 'collected': 8, 'supported': 8, 'epochs': 8, '448': 8, '224': 8, 'curated': 8, 'myl': 8, 'sample': 8, 'domain': 7, 'benchmarks': 7, 'wide': 7, 'histology': 7, 'systems': 7, 'potential': 7, 'directly': 7, 'artificial': 7, 'intelligence': 7, 'current': 7, 'made': 7, 'annotation': 7, 'wholeslide': 7, 'harvard': 7, 'sciences': 7, 'during': 7, 'smaller': 7, 'total': 7, 'colorectal': 7, 'they': 7, '31': 7, 'been': 7, 'representations': 7, 'scale': 7, 'relatively': 7, 'alignment': 7, 'curation': 7, 'edu': 7, 'system': 7, 'cls': 7, 'pooler': 7, 'magnification': 7, 'divided': 7, 'represent': 7, 'linear': 7, 'right': 7, 'idc': 7, 'observed': 7, 'achieve': 7, 'section': 7, 'if': 7, 'remaining': 7, 'widely': 7, 'better': 7, 'rate': 7, 'generation': 7, 'weights': 7, 'shown': 7, 'query': 7, 'resources': 7, 'followed': 7, 'mean00': 7, 'queries': 7, 'pixellevel': 7, 'value': 7, 'endtoend': 7, 'neural': 7, 'one': 7, 'truth': 7, 'mask': 7, 'l': 7, 'via': 7, 'z': 7, 'challenge': 7, 'original': 7, 'loss': 7, 'meningioma': 7, 'experimental': 7, 'chen': 6, 'due': 6, 'about': 6, 'collection': 6, 'common': 6, 'science': 6, 'center': 6, 'biomed': 6, 'accurate': 6, 'clinical': 6, 'use': 6, 'lobular': 6, 'created': 6, 'investigated': 6, 'descriptions': 6, 'findings': 6, 'clip': 6, 'coca': 6, 'studies': 6, 'utility': 6, 'investigate': 6, 'access': 6, 'manually': 6, 'cleaned': 6, 'detected': 6, 'yielded': 6, 'comparison': 6, 'plot': 6, '23': 6, '200': 6, 'word': 6, 'pooling': 6, 'error': 6, 'bars': 6, 'find': 6, 'agreement': 6, 'outperforming': 6, 'addition': 6, 'computing': 6, 'included': 6, 'had': 6, 'tested': 6, 'sizes': 6, 'best': 6, 'finegrained': 6, 'brain': 6, 'quality': 6, 'even': 6, '256': 6, 'points': 6, 'search': 6, 'much': 6, 'recallk': 6, 'reports': 6, 'provide': 6, 'did': 6, '75': 6, 'articles': 6, 'any': 6, 'ensembling': 6, 'panel': 6, 'considered': 6, 'morphological': 6, 'anal': 6, 'li': 6, 'feature': 6, 'figure': 6, 'generative': 6, 'above': 6, 'architecture': 6, 'pixel': 6, 'hyperparameters': 6, 'inhouse': 6, 'f1': 6, 'splits': 6, 'meteor': 6, 'software': 6, 'libraries': 6, 'nih': 6, 'accessed': 6, 'whether': 6, 'fm': 6, 'dfkw': 6, 'border': 6, 'policy': 6, 'sex': 6, 'lu': 5, 'various': 5, 'disease': 5, 'how': 5, '14': 5, 'andor': 5, 'demonstrated': 5, 'among': 5, 'possible': 5, '19': 5, 'cambridge': 5, 'relative': 5, 'margin': 5, 'aligned': 5, 'capability': 5, 'new': 5, 'strong': 5, 'baseline': 5, 'names': 5, 'invasive': 5, 'ilc': 5, 'generally': 5, 'gigapixel': 5, 'individual': 5, 'refer': 5, 'demonstrate': 5, 'demonstrating': 5, 'learn': 5, 'applications': 5, 'framework': 5, 'pmc': 5, 'oa': 5, 'create': 5, 'filtering': 5, 'topic': 5, 'note': 5, 'instance': 5, '17': 5, '68': 5, '80': 5, '18': 5, '48': 5, 'tract': 5, 'peripheral': 5, 'tumors': 5, 'channel': 5, 't2i': 5, 'mm': 5, 'basis': 5, 'excellent': 5, 'significance': 5, 'global': 5, 'obtained': 5, 'showed': 5, 'boxplot': 5, 'very': 5, 'should': 5, 'classifier': 5, 'initialized': 5, '21': 5, 'ssl': 5, 'latent': 5, 'box': 5, 'extend': 5, 'case': 5, 'heldout': 5, 'complex': 5, 'overlap': 5, '29': 5, 'patterns': 5, 'relevant': 5, 'improve': 5, 'question': 5, 'red': 5, 'review': 5, 'oncol': 5, 'wang': 5, 'g': 5, '38': 5, '41': 5, 'splitting': 5, 'them': 5, 'figures': 5, 'vj': 5, 'attentional': 5, 'learned': 5, 'layer': 5, 'similarly': 5, 'method': 5, 'opensource': 5, 'rouge': 5, 'idh1mutant': 5, 'panda': 5, 'aggc': 5, 'python': 5, 'bwh': 5, 'rjc': 5, 'vs': 5, 'ie': 5, 'gender': 5, 'v': 4, 'development': 4, 'cohorts': 4, 'difficult': 4, 'notably': 4, 'taskagnostic': 4, 'involving': 4, 'achieving': 4, 'substantial': 4, 'requiring': 4, 'recent': 4, 'online': 4, 'hospital': 4, 'school': 4, 'boston': 4, 'massachusetts': 4, 'program': 4, 'outperforms': 4, 'without': 4, 'matching': 4, 'shared': 4, 'there': 4, 'therefore': 4, 'roi': 4, 'four': 4, 'atlas': 4, 'classifica': 4, 'community': 4, 'analyses': 4, 'pathological': 4, 'align': 4, 'imaging': 4, 'com': 4, 'taskspecific': 4, 'remain': 4, 'schematic': 4, 'object': 4, 'filtered': 4, 'topics': 4, 'pmcpath': 4, 'consists': 4, 'correct': 4, 'bos': 4, 'eos': 4, 'spanning': 4, '69': 4, '51': 4, '60': 4, '24': 4, 'head': 4, 'uniformly': 4, 'expanded': 4, 'septa': 4, 'focus': 4, 'algorithm': 4, 'sim': 4, 'ductal': 4, 'according': 4, '225': 4, 'quadratically': 4, 'probing': 4, 'highsimilarity': 4, 'morphology': 4, 'constituents': 4, 'twosided': 4, 'permutation': 4, 'lastly': 4, 'although': 4, 'noted': 4, 'predictions': 4, 'able': 4, 'challenging': 4, '2e': 4, 'middle': 4, 'involved': 4, 'displayed': 4, 'ability': 4, 'regression': 4, 'third': 4, 'residual': 4, '26': 4, 'but': 4, 'varied': 4, 'account': 4, 'variance': 4, 'yet': 4, 'studied': 4, 'national': 4, 'line': 4, '33': 4, 'capable': 4, 'likely': 4, 'quartile': 4, 'whiskers': 4, 'interquartile': 4, 'reference': 4, 'input': 4, 'papillary': 4, '165': 4, 'held': 4, 'oncotree': 4, 'types': 4, 'explored': 4, 'against': 4, 'groundtruth': 4, '28': 4, 'substantially': 4, 'social': 4, 'will': 4, 'healthcare': 4, 'network': 4, 'domainspecific': 4, 'larger': 4, 'removing': 4, 'ensuring': 4, 'templates': 4, 'sections': 4, 'inform': 4, 'content': 4, 'author': 4, 'oncology': 4, '2018': 4, 'pancancer': 4, 'npj': 4, 'response': 4, 'preprint': 4, '47': 4, 'springer': 4, '59': 4, '70': 4, 'maps': 4, 'institutional': 4, 'two': 4, 'panels': 4, 'subimages': 4, 'modeling': 4, 'along': 4, 'ui': 4, 'layers': 4, 'absolute': 4, 'last': 4, 'vit': 4, 'autoregressive': 4, 'discrete': 4, 'i1': 4, 't1': 4, 'gpus': 4, 'batch': 4, 'gpu': 4, 'consisting': 4, 'sampling': 4, 'chose': 4, 'averaging': 4, 'independently': 4, 'paper': 4, 'g3': 4, 'g4': 4, 'g5': 4, 'respective': 4, 'boardcertified': 4, 'evaluate': 4, 'auroc': 4, 'astrocytoma': 4, 'anaplastic': 4, 'summary': 4, 'website': 4, 'portal': 4, 'policies': 4, 'thank': 4, 'materials': 4, 'heatmaps': 4, 'statement': 4, 'tests': 4, 'description': 4, 'must': 4, 'your': 4, 'faisal': 3, 'mahmood': 3, 'advances': 3, 'array': 3, 'leverage': 3, 'contrast': 3, 'histopathologic': 3, 'workflows': 3, 'ai': 3, 'published': 3, 'brigham': 3, 'institute': 3, 'informatics': 3, 'applied': 3, 'university': 3, 'equally': 3, 'allows': 3, 'currently': 3, 'serve': 3, 'conventional': 3, 'especially': 3, 'represented': 3, 'category': 3, 'classified': 3, 'tilelevel': 3, 'renal': 3, 'dartmouth': 3, 'histologic': 3, 'quadratic': 3, 'typically': 3, 'higher': 3, 'readers': 3, 'structured': 3, 'tools': 3, '39': 3, 'pretrain': 3, 'mod': 3, 'broader': 3, 'xray': 3, 'meaningful': 3, 'develop': 3, 'histopathologyspecific': 3, 'underexplored': 3, 'need': 3, 'fusion': 3, 'objectives': 3, 'objective': 3, 'achieves': 3, 'referring': 3, 'cover': 3, 'wordclouds': 3, 'setup': 3, 'conditioned': 3, 'comparing': 3, '120': 3, '92': 3, '36': 3, '100000': 3, 'lower': 3, 'male': 3, 'female': 3, 'septal': 3, 'markings': 3, 'lesion': 3, 'lymphatic': 3, 'unfiltered': 3, 'i2t': 3, 'special': 3, 'cells': 3, 'adjacent': 3, 'pr3': 3, 'specified': 3, 'below': 3, '143': 3, 'chance': 3, '012': 3, 'enables': 3, 'suggest': 3, 'coarsegrained': 3, 'application': 3, 'laborious': 3, 'maximize': 3, 'benchmark': 3, 'either': 3, 'holding': 3, 'attentionbased': 3, 'logistic': 3, 'commonly': 3, 'truncated': 3, 'overall': 3, 'provides': 3, 'experiment': 3, 'runs': 3, 'fewer': 3, 'representing': 3, 'trend': 3, 'competitive': 3, 'sub': 3, 'crc': 3, '128': 3, 'good': 3, 'annual': 3, 'con': 3, '371': 3, 'random': 3, 'pre': 3, 'frozen': 3, 'combined': 3, 'specifically': 3, 'effective': 3, 'super': 3, 'vised': 3, 'around': 3, 'analogous': 3, 'embed': 3, 'retrieve': 3, 'concepts': 3, 'house': 3, 'significantly': 3, 'except': 3, '1000': 3, '797': 3, '1755': 3, 'texts': 3, 'bottom': 3, 'matched': 3, '65': 3, 'codes': 3, 'characteristics': 3, 'instead': 3, 'benign': 3, 'core': 3, 'approach': 3, 'masks': 3, 'attempted': 3, 'extract': 3, 'genomics': 3, 'present': 3, 'build': 3, 'applicable': 3, 'settings': 3, 'clinically': 3, 'equipped': 3, 'inputs': 3, 'select': 3, 'effect': 3, 'algorithms': 3, 'foundational': 3, 'limitation': 3, 'preparation': 3, 'written': 3, '250': 3, 'macro': 3, 'finetune': 3, 'mizero': 3, 'exclusive': 3, 'positive': 3, 'references': 3, 'peer': 3, 'song': 3, 'kather': 3, 'integration': 3, 'nonsmall': 3, 'eng': 3, 'lancet': 3, '83': 3, 'predicting': 3, 'sci': 3, '27': 3, 'huang': 3, 'bootstrapping': 3, 'adv': 3, 'inf': 3, 'syst': 3, 'transformers': 3, 'inference': 3, 'u': 3, 'health': 3, 'hierarchical': 3, '81': 3, 'zhou': 3, 'openreviewnet': 3, 'manuscript': 3, 'solely': 3, 'internal': 3, 'retrospective': 3, 'retrospectively': 3, 'raw': 3, 'second': 3, 'steps': 3, 'subcaptions': 3, 'generate': 3, 'gptstyle': 3, '1786362': 3, 'contained': 3, '1170647': 3, '457372': 3, 'parameterized': 3, 'θcontrast': 3, '768': 3, 'hidden': 3, 'designed': 3, 'local': 3, 'output': 3, 'eight': 3, 'nvidia': 3, 'a100': 3, '80gb': 3, 'resized': 3, 'ℓ2normalized': 3, 'vary': 3, 'choice': 3, 'support': 3, 'obtain': 3, '104': 3, 'increased': 3, 'kept': 3, 'vitl16': 3, 'automatic': 3, 'patience': 3, 'partitioned': 3, 'translation': 3, 'binary': 3, 'calculated': 3, 'width': 3, 'height': 3, 'ffpe': 3, 'missing': 3, 'excluding': 3, 'who': 3, 'appeared': 3, 'lusc': 3, 'ccrcc': 3, 'prcc': 3, 'chrcc': 3, '573': 3, 'follows': 3, '88': 3, '85': 3, 'extraction': 3, 'list': 3, 'difference': 3, '3813': 3, 'ibot': 3, 'implementation': 3, 'hugging': 3, 'face': 3, 'https': 3, 'numpy': 3, '1203': 3, 'pandas': 3, '153': 3, 'pillow': 3, '930': 3, 'grand': 3, 'requests': 3, 'subject': 3, 'you': 3, 'weaklysupervised': 3, 'td': 3, 'life': 3, 'material': 3, 'performs': 3, 'carcinomas': 3, 'black': 3, 'stereotypical': 3, 'legend': 3, 'summaryapril': 3, 'na': 3, 'covariates': 3, 'estimates': 3, 'appropriate': 3, 'web': 3, 'race': 3, 'ethnicity': 3, 'ming': 2, 'bowen': 2, 'drew': 2, 'williamson': 2, 'long': 2, 'andrew': 2, 'robust': 2, 'usage': 2, 'entities': 2, 'introduce': 2, 'developed': 2, 'transferred': 2, 'concurrent': 2, 'learningbased': 2, 'remains': 2, 'metastasis': 2, 'sur': 2, 'unknown': 2, 'origin': 2, 'strides': 2, 'paradigm': 2, 'developing': 2, 'targeting': 2, 'lymph': 2, 'node': 2, 'scalable': 2, 'openset': 2, 'problems': 2, 'thousands': 2, 'separate': 2, 'every': 2, 'accepted': 2, 'womens': 2, 'mit': 2, 'engineering': 2, 'technology': 2, 'oh': 2, 'figs': 2, 'contrastively': 2, 'do': 2, 'expect': 2, 'corre': 2, 'sponded': 2, 'representa': 2, 'ensemble': 2, 'boost': 2, 'perfor': 2, 'mance': 2, 'studies4454': 2, 'primarily': 2, 'focused': 2, 'classifi': 2, 'cation': 2, 'mizero56': 2, 'noma': 2, 'subjective': 2, '114': 2, 'include': 2, 'entity': 2, 'els': 2, 'variety': 2, 'putational': 2, 'readily': 2, 'practical': 2, 'poor': 2, 'combinations': 2, 'generalize': 2, 'understand': 2, 'limitations': 2, 'uses': 2, 'predict': 2, 'automated': 2, 'pipeline': 2, 'parts': 2, 'detector': 2, 'detect': 2, 'match': 2, '179': 2, 'discussion': 2, 'ablation': 2, 'estimated': 2, 'generating': 2, 'sentence': 2, 'end': 2, 'nextbestperforming': 2, '54': 2, '72': 2, 'genital': 2, 'muscle': 2, 'kidney': 2, 'nervous': 2, '098': 2, 'tokenizer': 2, 'characterization': 2, 'ihc': 2, 'fibrous': 2, 'protein': 2, 'antibodies': 2, '100supervised': 2, 'tiled': 2, 'rcctcga': 2, 'closest': 2, 'aggregated': 2, 'corresponds': 2, 'dashed': 2, 'lines': 2, 'correspond': 2, '7180': 2, '4693': 2, '2122': 2, '1519': 2, 'selected': 2, '867': 2, 'subtyp': 2, 'ranging': 2, 'nearly': 2, 'easy': 2, 'classifying': 2, 'map': 2, 'visualize': 2, 'highlighted': 2, 'closely': 2, 'simple': 2, 'interpretability': 2, 'visualized': 2, 'possibility': 2, 'formance': 2, 'known': 2, 'introduced': 2, '2d': 2, '107': 2, '873': 2, '938': 2, 'comparisons': 2, 'includ': 2, 'visiononly': 2, 'until': 2, 'reached': 2, 'highly': 2, 'typing': 2, 'fact': 2, 'evaluating': 2, 'effectiveness': 2, 'here': 2, 'categories': 2, 'cancers': 2, 'definition': 2, 'institutes': 2, '30class': 2, 'problem': 2, 'far': 2, 'surpassing': 2, 'base': 2, 'imageonly': 2, 'times': 2, 'ated': 2, 'useful': 2, 'identify': 2, 'morphologies': 2, 'direction': 2, 'stud': 2, 'nextbest': 2, '173': 2, '53': 2, 'slightly': 2, 'micropapillary': 2, 'acinar': 2, 'solid': 2, 'atypical': 2, '400': 2, '700': 2, 'database': 2, 'averages': 2, 'row': 2, 'topright': 2, 'corner': 2, 'what': 2, '1620': 2, '108': 2, 'quantify': 2, 'presented': 2, 'confirmed': 2, 'architectures': 2, 'making': 2, 'distinct': 2, 'interest': 2, 'needed': 2, 'posed': 2, 'averaged': 2, 'overlapped': 2, 'segmenta': 2, 'annotations': 2, 'macroaveraged': 2, '0601': 2, '0024': 2, 'reasonably': 2, 'ignored': 2, 'textual': 2, 'aspect': 2, 'approaches': 2, 'leave': 2, 'amount': 2, 'trainees': 2, 'world': 2, 'several': 2, 'media': 2, 'generalpurpose': 2, 'beyond': 2, 'est': 2, 'burden': 2, 'once': 2, 'implemented': 2, 'involve': 2, 'another': 2, 'suitable': 2, 'does': 2, 'convolutional': 2, 'kimianet': 2, 'ctranspath62': 2, 'improvement': 2, 'worse': 2, 'providing': 2, 'less': 2, 'continuous': 2, 'robustness': 2, 'detecting': 2, 'combina': 2, 'manual': 2, 'assessment': 2, 'future': 2, 'derived': 2, 'staining': 2, 'simply': 2, '01': 2, 'shows': 2, 'blue': 2, 'cancerassociated': 2, 'explicit': 2, 'benefit': 2, 'would': 2, 'assumptions': 2, 'guidelines': 2, 'scoring': 2, 'region': 2, 'identified': 2, 'function': 2, 'acknowledgements': 2, 'competing': 2, 'interests': 2, 'rev': 2, 'clin': 2, '2017': 2, 'mutation': 2, 'outcome': 2, '11': 2, 'metastatic': 2, '87': 2, '13': 2, 'archives': 2, 'digit': 2, '56': 2, 'precis': 2, '58': 2, 'bulten': 2, 'biopsies': 2, '22': 2, 'outcomes': 2, 'networks': 2, 'survival': 2, 'reveals': 2, 'prognosis': 2, 'therapy': 2, 'features': 2, 'multistain': 2, 'radiology': 2, 'supervision': 2, 'meila': 2, 'scaling': 2, 'noisy': 2, 'trans': 2, 'mach': 2, 'httpsopenreviewnet': 2, 'hoi': 2, 'unified': 2, '37': 2, 'fan': 2, 'empirical': 2, 'liu': 2, 'chest': 2, 'doshivelez': 2, '106': 2, 'scientific': 2, 'sun': 2, 'unpaired': 2, 'interpretable': 2, 'realtime': 2, 'look': 2, 'dual': 2, 'konukoglu': 2, '49': 2, 'histopathological': 2, 'patches': 2, 'answering': 2, 'meeting': 2, 'joint': 2, '55': 2, '57': 2, 'lin': 2, 'languageimage': 2, 'ed': 2, 'inc': 2, 'krause': 2, '2016': 2, '2009': 2, 'transformerbased': 2, 'int': 2, 'comput': 2, 'vis': 2, 'true': 2, '10th': 2, 'publishers': 2, 'publishing': 2, 'mass': 2, 'approved': 2, 'curate': 2, 'downloaded': 2, 'poses': 2, 'main': 2, 'challenges': 2, 'handling': 2, 'portion': 2, 'bounding': 2, 'avoid': 2, 'selecting': 2, 'tively': 2, 'adding': 2, 'causal': 2, 'separated': 2, 'require': 2, 'pair': 2, 'animal': 2, 'stains': 2, 'animals': 2, 'θ': 2, 'ϕ': 2, 'modules': 2, 'θcaption': 2, 'vitbase': 2, '3072': 2, 'positional': 2, 'added': 2, 'rgb': 2, 'multiheaded': 2, 'capture': 2, 'required': 2, 'final': 2, 'minibatch': 2, 'wi': 2, 'sequence': 2, 'vi': 2, '2m': 2, 'log': 2, 'expτut': 2, 'j1': 2, 'expτ': 2, 'vt': 2, 'negative': 2, 'loglikelihood': 2, 'prior': 2, 'unlabeled': 2, '20equivalent': 2, 'pro': 2, 'identifiers': 2, 'initialize': 2, 'parameters': 2, 'assessed': 2, 'name': 2, 'considerably': 2, 'measured': 2, 'analogously': 2, 'consequently': 2, 'evalu': 2, 'align31': 2, 'otherwise': 2, 'configuration': 2, 'aggregate': 2, 'mapped': 2, 'assigned': 2, 'preprocessing': 2, 'patch': 2, 'leakage': 2, 'fully': 2, 'connected': 2, 'unit': 2, 'interpreted': 2, 'regularization': 2, 'adamw': 2, 'optimizer': 2, 'probability': 2, 'epoch': 2, 'summarized': 2, 'conducted': 2, 'coefficient': 2, 'iterations': 2, '228482': 2, '189484': 2, '9959': 2, 'vitb16': 2, 'popular': 2, 'cnn': 2, '4622': 2, 'earlystopping': 2, 'techniques': 2, '90': 2, '180': 2, 'color': 2, 'saturation': 2, 'toplevel': 2, 'highquality': 2, 'decoding': 2, 'strategy': 2, 'area': 2, 'taking': 2, 'considers': 2, 'computes': 2, 'multipanel': 2, 'tis': 2, 'sue': 2, 'metadata': 2, 'squamous': 2, 'clear': 2, 'chromophobe': 2, 'col': 2, '693': 2, 'smooth': 2, 'sicap75': 2, 'digitized': 2, '124': 2, 'ensure': 2, 'idh1wildtype': 2, 'glioblastoma': 2, '1p19q': 2, 'codeleted': 2, 'oligodendroglioma': 2, 'diffuse': 2, 'ependymoma': 2, '10equivalent': 2, 'compare': 2, 'tiling': 2, 'clam': 2, 'contours': 2, 'covered': 2, 'keywords': 2, 'qualitatively': 2, 'differences': 2, 'null': 2, 'hypothesis': 2, 'cuda': 2, 'library': 2, '040': 2, 'rougescore': 2, 'nltk': 2, '367': 2, 'openslide': 2, '431': 2, 'openslidepython': 2, 'scikitlearn': 2, '121': 2, 'opencvpython': 2, 'basic': 2, 'matplotlib': 2, 'seaborn': 2, '0122': 2, 'listed': 2, 'design': 2, 'genomic': 2, 'commons': 2, 'cessed': 2, 'restrictions': 2, 'departmental': 2, 'determine': 2, 'requested': 2, 'intellectual': 2, 'property': 2, 'privacy': 2, 'obligations': 2, 'words': 2, 'measures': 2, 'git': 2, 'summarization': 2, 'package': 2, 'roetzerpejrimovsky': 2, 'tumour': 2, 'resource': 2, 'feedback': 2, 'presidents': 2, 'nigms': 2, 'award': 2, 'fellowship': 2, 'gg': 2, 'il': 2, 'lpl': 2, 'gj': 2, 'contains': 2, 'pathologistannotated': 2, 'dotted': 2, 'protocol': 2, 'n558': 2, 'editorial': 2, 'statistics': 2, 'exact': 2, 'manuscripts': 2, 'please': 2, 'participants': 2, 'unspecified': 2, 'groupings': 2, 'analyzed': 2, 'population': 2, 'recruitment': 2, 'necessary': 2, 'sure': 2, 'read': 2, 'exclusions': 2, 'excluded': 2, 'replication': 2, 'blinding': 2, '863': 1, '1234511': 1, 'chen1211': 1, '12311': 1, 'richard': 1, '12346': 1, 'ivy': 1, 'liang17': 1, 'tong': 1, 'ding17': 1, 'guillaume': 1, 'jaume1234': 1, 'igor': 1, 'odintsov1': 1, 'phi': 1, 'le2': 1, 'georg': 1, 'gerber': 1, 'anil': 1, 'parwani8': 1, '12349': 1, '123410': 1, 'accelerated': 1, 'adoption': 1, 'enabled': 1, 'scarcity': 1, 'stark': 1, 'humans': 1, 'teach': 1, 'reason': 1, 'suite': 1, 'represents': 1, 'leap': 1, 'facilitate': 1, 'minimal': 1, 'gold': 1, 'exami': 1, 'nation': 1, 'rise': 1, 'pathology14': 1, 'leverages': 1, 'solve': 1, 'prob': 1, 'lems': 1, 'considerable': 1, 'detection5': 1, 'subtyping67': 1, 'vival': 1, 'prediction810': 1, 'site': 1, 'prediction1112': 1, 'search1316': 1, 'molecular': 1, 'alterations1718': 1, 'tasks19': 1, 'field': 1, 'detection20': 1, 'grading2122': 1, 'labor': 1, 'intensive': 1, 'diagnoses': 1, 'step': 1, 'workflow': 1, 'untenable': 1, 'received': 1, 'august': 1, 'february': 1, 'check': 1, 'updates': 1, '1department': 1, '2department': 1, '3cancer': 1, 'broad': 1, '4cancer': 1, 'danafarber': 1, '5electrical': 1, '6department': 1, '7harvard': 1, 'john': 1, 'paulson': 1, '8department': 1, 'wexner': 1, 'ohio': 1, 'state': 1, 'columbus': 1, '9health': 1, 'harvardmit': 1, '10harvard': 1, 'initiative': 1, '11these': 1, 'contributed': 1, 'email': 1, 'faisalmahmoodbwhharvardedu': 1, '864': 1, '1d': 1, 'plip54': 1, 'clip44': 1, 'tissues': 1, 'off': 1, 'shelf': 1, 'arbitrary': 1, 'sufficiently': 1, 'prisingly': 1, 'scarce': 1, 'predetermined': 1, '2a': 1, 'ways': 1, 'phrase': 1, 'concept': 1, 'predictive': 1, 'regionofinterest': 1, 'leveraging': 1, 'divides': 1, 'subsequently': 1, 'aggre': 1, 'gates': 1, '2b': 1, 'genome': 1, 'carci': 1, 'nonsmallcell': 1, 'hitchcock': 1, 'pat': 1, 'tern': 1, 'accounted': 1, 'imbalance': 1, 'weighing': 1, 'regarded': 1, 'translates': 1, 'interrater': 1, 'variability': 1, 'omics': 1, 'refs': 1, '2326': 1, '2729': 1, 'communication': 1, 'make': 1, 'extensive': 1, 'prepares': 1, 'treating': 1, 'clini': 1, 'cian': 1, 'journal': 1, 'textbook': 1, 'chapter': 1, 'teaches': 1, 'residents': 1, 'immense': 1, 'infor': 1, 'mation': 1, 'representative': 1, 'others3338': 1, 'modelstaskagnostic': 1, 'leveraged': 1, 'generation4041': 1, 'classification4245': 1, 'retrieval4548': 1, 'others4953': 1, 'inte': 1, 'grating': 1, 'studies445458': 1, 'histo': 1, 'studies445456': 1, 'lack': 1, 'leading': 1, 'diversity': 1, 'difficulty': 1, 'acquiring': 1, 'exten': 1, 'sive': 1, '1ab': 1, 'address': 1, 'unfilled': 1, 'needs': 1, 'combination': 1, 'seek': 1, 'modalities': 1, 'learns': 1, '1c': 1, 'nonhumans': 1, 'investigating': 1, 'inset': 1, 'lengths': 1, 'maximizing': 1, 'maximizes': 1, 'likelihood': 1, 'previously': 1, 'beginning': 1, 'attn': 1, 'radar': 1, 'significant': 1, '865': 1, 'openai': 1, '2c': 1, '907': 1, '902': 1, '98': 1, '924623': 1, '4646': 1, '17345168': 1, '20000': 1, '40000': 1, '60000': 1, '80000': 1, '120000': 1, '140000': 1, 'eye': 1, 'urinary': 1, 'pancreas': 1, 'endocrine': 1, 'blood': 1, 'vessels': 1, 'heart': 1, 'neck': 1, 'nerve': 1, 'skeletal': 1, 'hematopathology': 1, 'liver': 1, 'biliary': 1, 'skin': 1, 'bones': 1, 'joints': 1, 'softtissue': 1, 'gastrointestinal': 1, '11569': 1, '12916': 1, '21963': 1, '26358': 1, '27709': 1, '29049': 1, '33668': 1, '53438': 1, '55665': 1, '61341': 1, '64992': 1, '83311': 1, '86163': 1, '87388': 1, '89494': 1, '90585': 1, '102751': 1, '111078': 1, '121209': 1, 'splitter': 1, 'matcher': 1, 'cysts': 1, 'appreciated': 1, 'multicystic': 1, '099': 1, 'crossattn': 1, 'pooled': 1, 'count': 1, 'density': 1, 'i2ttcga': 1, 'i2tsource': 1, '457373': 1, '713595': 1, 'type': 1, 'presenting': 1, 'cystic': 1, 'cambium': 1, 'layerlike': 1, 'concentration': 1, 'primitive': 1, 'proteinase3': 1, 'expressed': 1, 'brown': 1, 'wgm2': 1, '866': 1, 'tr': 1, '96': 1, 'imagetile': 1, 'luadtcga': 1, 'constructed': 1, 'whose': 1, 'processed': 1, 'cd': 1, 'centers': 1, 'power': 1, 'colored': 1, 'classic': 1, 'lowsimilarity': 1, '913': 1, 'nearrandom': 1, 'accuracies': 1, '507': 1, '553': 1, '0200': 1, '0055': 1, '0690': 1, '0140': 1, '791': 1, '719': 1, 'struggle': 1, 'heat': 1, 'deemed': 1, 'close': 1, 'matches': 1, 'diag': 1, 'nosis': 1, 'resembled': 1, 'delineated': 1, 'highlighting': 1, 'decisionmaking': 1, 'resolution': 1, 'user': 1, 'inspection': 1, 'quantitatively': 1, 'efficient': 1, 'expedited': 1, 'going': 1, 'processes': 1, 'sometimes': 1, 'desirable': 1, 'specialize': 1, 'ideally': 1, 'multipleinstance': 1, 'algorithm59': 1, 'addi': 1, 'popularity': 1, 'ref': 1, 'imagenet61': 1, 'ctranspath62a': 1, '942': 1, '933': 1, 'monly': 1, '0223': 1, '0033': 1, '914': 1, 'whereas': 1, '894': 1, '004': 1, 'identically': 1, '0833': 1, '0835': 1, '4058': 1, '00710128': 1, 'comparably': 1, 'translated': 1, 'nontrivial': 1, 'reduction': 1, 'sizea': 1, 'aside': 1, 'plipbased': 1, 'biomedclipbased': 1, 'trivialized': 1, 'evaluat': 1, 'hand': 1, 'easily': 1, 'surpassed': 1, 'just': 1, 'investigations': 1, 'narrow': 1, 'knowledge': 1, 'recognizing': 1, 'rarecare': 1, 'project63': 1, 'having': 1, 'crude': 1, 'incidence': 1, 'adopted': 1, 'surveillance': 1, 'epidemiology': 1, 'seer': 1, 'structed': 1, '868': 1, 'secondbestperforming': 1, '170': 1, 'suggests': 1, 'wild': 1, 'recog': 1, 'nition': 1, 'kimi': 1, 'anet64': 1, 'strongperforming': 1, '682': 1, '7a': 1, '144': 1, '178': 1, 'predominantly': 1, 'involves': 1, 'motivated': 1, 'superior': 1, 'comparable': 1, '7b': 1, '000': 1, 'ac': 1, 'surprisingly': 1, 'exceeding': 1, '869': 1, 'retrieving': 1, 'entry': 1, 'abbreviated': 1, 'vice': 1, 'versa': 1, 'abbrevi': 1, 'naturally': 1, 'lends': 1, 'itself': 1, 'inclusion': 1, 'trials': 1, 'assistance': 1, 'presentations': 1, 'collecting': 1, 'helping': 1, 'served': 1, '4b': 1, 'ies314454': 1, '440': 1, '4a': 1, '688': 1, '390': 1, 'secondbest': 1, '315': 1, '151': 1, '240': 1, '022': 1, 'gap': 1, '2227': 1, 'leipidic': 1, 'inflammation': 1, 'bronchus': 1, 'vessel': 1, 'necrosis': 1, 'crowded': 1, 'gland': 1, 'lined': 1, 'pale': 1, 'eosinophilic': 1, 'intraluminal': 1, 'secretions': 1, 'desmoplastic': 1, 'stromal': 1, 'reaction': 1, 'invasion': 1, 'detached': 1, 'cluster': 1, 'floating': 1, 'alveolar': 1, 'spaces': 1, 'top1': 1, '300': 1, 'recallimagetotext': 1, 'mean0': 1, '06139': 1, '04567': 1, '06173': 1, '05532': 1, '04706': 1, '030260199302038': 1, '03715': 1, 'columns': 1, 'rightmost': 1, 'column': 1, 'toimage': 1, 'luadrelevant': 1, '870': 1, 'exam': 1, 'ples': 1, 'related': 1, 'solidpattern': 1, '4c': 1, 'cribriform': 1, 'prostatic': 1, '321261': 1, 'gigapixels': 1, 'heterogene': 1, 'ous': 1, 'share': 1, 'conse': 1, 'quently': 1, 'reduce': 1, 'subslide': 1, 'expensive': 1, 'collect': 1, 'valuable': 1, 'capabili': 1, 'ties': 1, 'assigning': 1, '5a': 1, 'minimize': 1, 'sharp': 1, 'transition': 1, 'boundary': 1, 'neighboring': 1, 'smoother': 1, 'appearance': 1, 'malignant': 1, 'segmen': 1, 'tation': 1, 'specimens': 1, 'reader': 1, '5bc': 1, '0549': 1, '008': 1, '0484': 1, '0751': 1, '0644': 1, '0557': 1, '0672': 1, '0605': 1, '0536': 1, '0615': 1, '0426': 1, '0446': 1, '0709': 1, '0541': 1, '0663': 1, '0526': 1, '0581': 1, 'despite': 1, 'produce': 1, 'instances': 1, 'ized': 1, '5de': 1, 'discriminative': 1, 'signals': 1, 'huge': 1, 'exemplar': 1, 'real': 1, 'hindering': 1, 'working': 1, 'abilities': 1, 'heterogeneous': 1, 'larg': 1, 'highperformance': 1, 'relieve': 1, 'annotating': 1, 'rivaled': 1, 'ventional': 1, 'muchimproved': 1, 'empower': 1, 'physicians': 1, 'researchers': 1, 'accurately': 1, 'flexibly': 1, 'efficiently': 1, 'databases': 1, 'flexibility': 1, 'exploratory': 1, 'reasoning': 1, 'ever': 1, 'promising': 1, 'poorly': 1, 'counterparts': 1, 'observations': 1, 'way': 1, 'go': 1, 'goal': 1, 'building': 1, 'truly': 1, 'universal': 1, 'clas': 1, 'sification': 1, 'compatibility': 1, 'aimed': 1, 'singledisease': 1, 'efforts': 1, 'groups': 1, 'abun': 1, 'dance': 1, 'lead': 1, 'tangible': 1, 'benefits': 1, 'insight': 1, 'assembling': 1, '200000': 1, 'cer': 1, '871': 1, 'handful': 1, 'cnnbased': 1, 'vitbased': 1, 'hundreds': 1, 'sizeable': 1, 'vitlarge': 1, 'investigation': 1, 'scenario': 1, 'wsitowsi': 1, 'algorithm66': 1, 'unlocks': 1, 'neverthe': 1, 'highlight': 1, 'importance': 1, 'strengths': 1, 'key': 1, 'pales': 1, 'billionscale': 1, 'increasing': 1, 'quantity': 1, 'increas': 1, 'unintentional': 1, 'becomes': 1, 'increasingly': 1, 'domain4454': 1, 'duplicates': 1, 'nearduplicates': 1, 'relies': 1, 'heuristics': 1, 'suffi': 1, 'ciently': 1, 'serving': 1, 'minimized': 1, 'variations': 1, 'protocols': 1, 'scannerspecific': 1, 'profiles': 1, 'parameterefficient': 1, 'techniques6768': 1, 'already': 1, 'attempt': 1, 'explicitly': 1, 'engineer': 1, '04': 1, 'stitch': 1, 'illustrating': 1, 'stitched': 1, 'together': 1, 'de': 1, 'enlarged': 1, 'displays': 1, 'sensitivity': 1, 'specificity': 1, 'segments': 1, 'nontumor': 1, 'cancerous': 1, 'glands': 1, 'contain': 1, '872': 1, 'doing': 1, 'retaining': 1, 'needing': 1, 'longer': 1, 'strictly': 1, 'transfer6970': 1, 'moreover': 1, 'suited': 1, 'defining': 1, 'mutually': 1, 'includes': 1, 'secondary': 1, 'appropriately': 1, 'soon': 1, 'tumorcontaining': 1, 'adjusted': 1, 'suit': 1, 'imple': 1, 'mentation': 1, 'landscape': 1, 'focuses': 1, 'imagelevel': 1, 'recognize': 1, 'cel': 1, 'lular': 1, 'subcellular': 1, 'meaning': 1, 'important': 1, 'mitosis': 1, 'counting': 1, 'outside': 1, 'scope': 1, 'sum': 1, 'maries': 1, 'contri': 1, 'butions': 1, 'statements': 1, 'avail': 1, 'bioeng': 1, '930949': 1, 'bera': 1, 'schalper': 1, 'rimm': 1, 'velcheti': 1, 'madabhushi': 1, 'pathologynew': 1, '703715': 1, 'shmatko': 1, 'ghaffari': 1, 'laleh': 1, 'gerstung': 1, 'enhancing': 1, '10261038': 1, 'lipkova': 1, '10951110': 1, 'bejnordi': 1, 'metastases': 1, 'women': 1, 'jama': 1, '318': 1, '21992210': 1, 'coudray': 1, '15591567': 1, 'dataefficient': 1, '555570': 1, 'skrede': 1, 'oj': 1, 'discovery': 1, '395': 1, '350360': 1, 'integrative': 1, 'histologygenomic': 1, '865878': 1, 'courtiol': 1, 'mesothelioma': 1, 'improves': 1, '15191525': 1, 'aibased': 1, 'predicts': 1, 'origins': 1, '594': 1, '106110': 1, 'zhu': 1, 'bone': 1, 'ebiomedicine': 1, '104426': 1, 'kalra': 1, 'yottixelan': 1, 'engine': 1, '101757': 1, 'hegde': 1, 'smily': 1, 'retccl': 1, 'clusteringguided': 1, '102645': 1, 'fast': 1, '14201434': 1, 'imagebased': 1, 'actionable': 1, 'genetic': 1, 'alterations': 1, '789799': 1, 'saldanha': 1, 'graham': 1, 'hovernet': 1, 'simultaneous': 1, 'nuclei': 1, 'multitissue': 1, '101563': 1, 'campanella': 1, 'clinicalgrade': 1, '13011309': 1, 'deeplearning': 1, '233241': 1, 'nagpal': 1, 'improving': 1, 'mobadersany': 1, 'natl': 1, 'acad': 1, '115': 1, 'e2970e2979': 1, 'coattention': 1, '40154025': 1, 'fu': 1, 'mutations': 1, 'composition': 1, '800810': 1, 'sammut': 1, 'sj': 1, 'multiomic': 1, 'predictor': 1, '601': 1, '623629': 1, 'neoadjuvant': 1, 'chemotherapy': 1, 'responses': 1, 'foersch': 1, '430439': 1, 'vanguri': 1, 'pdl1': 1, 'blockade': 1, '11511164': 1, 'radford': 1, 'transferable': 1, '87488763': 1, 'jia': 1, '49044916': 1, 'yu': 1, 'captioners': 1, 'artif': 1, 'intell': 1, 'forumidee277p3ayc': 1, 'xiong': 1, 'blip': 1, 'understanding': 1, 'chaudhur': 1, '1288812900': 1, 'singh': 1, 'flava': 1, '1563815650': 1, 'uniperceiver': 1, 'v2': 1, 'generalist': 1, '26912700': 1, 'alayrac': 1, 'jb': 1, 'flamingo': 1, '2371623736': 1, 'hu': 1, 'feichtenhofer': 1, 'masking': 1, '2339023400': 1, 'foreign': 1, 'beit': 1, '1917519186': 1, 'schuhmann': 1, 'laion5b': 1, '2527825294': 1, 'chang': 1, 'th': 1, 'wan': 1, 'memorydriven': 1, 'emnlp': 1, 'webber': 1, '14391449': 1, 'httpsaclanthologyorg2020emnlpmain112': 1, '4th': 1, 'vol': 1, '249269': 1, '42': 1, 'tiu': 1, 'expertlevel': 1, 'pathologies': 1, 'unannotated': 1, '13991406': 1, '43': 1, 'sc': 1, 'shen': 1, 'lungren': 1, 'yeung': 1, 'gloria': 1, 'globallocal': 1, 'labelefficient': 1, '39423951': 1, '44': 1, 'fifteen': 1, 'httpsdoiorg1048550arxiv230300915': 1, '45': 1, 'wu': 1, 'agarwal': 1, 'medclip': 1, 'che': 1, 'shutova': 1, '38763887': 1, 'schaumberg': 1, 'pantissue': 1, 'pandisease': 1, 'pathol': 1, '21692185': 1, 'maleki': 1, 'tizhoosh': 1, 'lile': 1, 'indepth': 1, 'looking': 1, 'elsewherea': 1, '879894': 1, 'jiang': 1, 'miura': 1, 'manning': 1, 'langlotz': 1, 'lipton': 1, 'pathnarratives': 1, 'humanai': 1, 'collaborative': 1, 'front': 1, '1070072': 1, 'tsuneki': 1, 'kanavati': 1, '12351250': 1, 'weber': 1, 'grossman': 1, 'khan': 1, 'interpreting': 1, '418435': 1, '52': 1, 'naseem': 1, 'khushi': 1, 'kim': 1, '16811690': 1, 'towards': 1, '59th': 1, '11th': 1, 'short': 1, 'papers': 1, 'zong': 1, '708718': 1, 'bianchi': 1, 'yuksekgonul': 1, 'montine': 1, 'zou': 1, 'twitter': 1, '23072316': 1, 'gamper': 1, 'rajpoot': 1, 'textbooks': 1, '1654916559': 1, 'cvf': 1, '1976419775': 1, 'pmcclip': 1, 'documents': 1, 'assisted': 1, 'interventionmiccai': 1, 'greenspan': 1, '525536': 1, 'ikezogwo': 1, 'quilt1m': 1, '3799538017': 1, 'curran': 1, 'associates': 1, 'ilse': 1, 'tomczak': 1, 'welling': 1, 'dy': 1, '21272136': 1, 'ren': 1, '770778': 1, '61': 1, 'deng': 1, '248255': 1, '62': 1, 'unsupervised': 1, '102559': 1, '63': 1, 'gatta': 1, 'centralised': 1, 'treatment': 1, 'europe': 1, 'tumours': 1, 'rarecareneta': 1, 'populationbased': 1, '10221039': 1, 'riasatian': 1, 'densenet': 1, '102032': 1, 'kundra': 1, 'jco': 1, '221230': 1, '66': 1, 'alfasly': 1, 'httpsdoiorg1048550arxiv230911510': 1, '67': 1, 'yang': 1, 'loy': 1, '130': 1, '23372348': 1, 'gao': 1, 'clipadapter': 1, 'adapters': 1, '132': 1, '581595': 1, 'perez': 1, 'kiela': 1, 'cho': 1, '1105411070': 1, 'sanh': 1, 'multitask': 1, 'prompted': 1, 'generalization': 1, 'httpsopenreviewnetforumid9vrb9d0wi4': 1, '874': 1, 'neutral': 1, 'regard': 1, 'jurisdictional': 1, 'claims': 1, 'affiliations': 1, 'licensor': 1, 'society': 1, 'partner': 1, 'holds': 1, 'rights': 1, 'rightsholders': 1, 'selfarchiving': 1, 'governed': 1, 'law': 1, 'licence': 1, 'america': 1, 'insti': 1, 'tutional': 1, 'board': 1, 'electronic': 1, 'records': 1, 'emrs': 1, 'deidentified': 1, 'ment': 1, 'recruited': 1, 'informed': 1, 'consent': 1, 'waived': 1, 'analyzing': 1, 'archival': 1, 'largesttodate': 1, 'automate': 1, 'iteratively': 1, 'notes': 1, 'httpsncbinlmnihgovpmctoolsopenftlist': 1, 'comprised': 1, 'nonhistopathology': 1, 'stantial': 1, 'arranged': 1, 'addressing': 1, 'light': 1, 'infeasible': 1, 'referred': 1, 'aligning': 1, 'yolov571': 1, 'extracting': 1, 'labeling': 1, 'synthetic': 1, 'singlepanel': 1, 'arranging': 1, 'itera': 1, 'refined': 1, 'validating': 1, 'incorrectly': 1, 'text72': 1, 'take': 1, 'keyword': 1, 'model30': 1, 'u0': 1, 'u1': 1, 'um': 1, 'v0': 1, 'v1': 1, 'vn': 1, 'sij': 1, 'ut': 1, 'applying': 1, 'pathologyspecific': 1, 'unfil': 1, 'tered': 1, 'nonhematoxylin': 1, 'eosin': 1, 'immunohistochemistry': 1, 'massons': 1, 'trichrome': 1, 'congo': 1, 'etc': 1, 'concerned': 1, 'wanted': 1, 'assess': 1, 'affect': 1, 'parsed': 1, 'exclude': 1, 'referencing': 1, 'nonhuman': 1, 'forming': 1, 'filter': 1, '10a': 1, 'equalweighted': 1, 'coca32': 1, 'generaldomain': 1, 'ψ': 1, 'vit73': 1, 'heads': 1, 'transformed': 1, 'redgreenblue': 1, 'dense': 1, 'semantically': 1, 'rich': 1, 'responsible': 1, 'fixed': 1, 'denoted': 1, 'enabling': 1, 'fcontrast': 1, 'ncontrast': 1, 'compute': 1, 'repre': 1, 'sentation': 1, 'fcaption': 1, 'ncaption': 1, 'lefttoright': 1, 'mapping': 1, 'appended': 1, 'tokenized': 1, 'context': 1, 'inserted': 1, 'crossattention': 1, 'selfattention': 1, 'incorporate': 1, 'vocabulary': 1, 'xiwi': 1, 'wi1': 1, 'wit': 1, 'ith': 1, 'let': 1, 'position': 1, 'ℓ2normalization': 1, 'complete': 1, 'ℒ': 1, 'uj': 1, 'logpwitwi0t1xiθϕψ': 1, 'pairings': 1, 'term': 1, 'seeks': 1, 'jointly': 1, 'distributed': 1, 'graphics': 1, 'units': 1, 'gradient': 1, 'accumulation': 1, '1536': 1, 'shorter': 1, 'edge': 1, 'centercropped': 1, 'zeropadded': 1, 'optimization': 1, 'work56': 1, 'ibot74': 1, '256sized': 1, '21442': 1, '350': 1, 'subtypes': 1, 'system65': 1, 'vided': 1, 'built': 1, 'corpus': 1, 'pathologyrelevant': 1, '550000': 1, 'surgical': 1, '400000': 1, 'histopathologyrelevant': 1, 'abstracts': 1, 'regex': 1, 'deidentify': 1, 'replac': 1, 'physician': 1, 'specimen': 1, 'record': 1, 'numbers': 1, 'dates': 1, 'vocabu': 1, 'lary': 1, '24layer': 1, 'nextword': 1, 'w1': 1, 'wt': 1, 'maximized': 1, 'ξ': 1, 'ℒclmξ': 1, 'logpwtw0t1ξ': 1, 'textonly': 1, 'scheme': 1, 'ran': 1, 'domly': 1, 'clip30': 1, 'template': 1, 'depending': 1, 'spread': 1, 'subsets': 1, 'pathologistcurated': 1, 'alternatively': 1, 'ding': 1, 'parison': 1, 'yi': 1, 'argmax': 1, 'tvj': 1, 'imbalanced': 1, 'grading75': 1, 'errors': 1, 'penalized': 1, 'ate': 1, 'percentage': 1, 'result': 1, 'unless': 1, 'enforced': 1, 'resizing': 1, 'cropping': 1, 'ours': 1, 'processor': 1, 'default': 1, 'extending': 1, 'obtaining': 1, 'operator': 1, 'aggregating': 1, 'spatial': 1, 'locations': 1, 'finally': 1, 'producing': 1, 'score76': 1, 'breakdown': 1, 'namely': 1, 'those': 1, 'resulted': 1, 'unfairly': 1, 'inflated': 1, 'standardized': 1, 'lowdimensional': 1, 'clipbased': 1, 'ope': 1, 'naiclip': 1, '1024dimensional': 1, 'abmil59': 1, 'rectified': 1, 'relu': 1, 'nonlinearity': 1, 'twolayer': 1, 'gated': 1, 'vari': 1, 'ant': 1, '384': 1, 'attentionpooled': 1, 'logits': 1, 'probabilities': 1, 'softmax': 1, 'normalization': 1, 'dropout': 1, 'intermediate': 1, 'cosine': 1, 'scheduler': 1, 'sampler': 1, 'minority': 1, 'saw': 1, 'dings': 1, 'recommended': 1, 'community77': 1, 'ℓ2': 1, 'λ': 1, 'mc': 1, 'limitedmemory': 1, 'broydenfletchergoldfarbshanno': 1, 'lbfgs': 1, 'solver': 1, 'maxi': 1, 'mum': 1, '800': 1, 'remained': 1, 'fourclass': 1, 'bench': 1, 'mark': 1, '29039': 1, 'individually': 1, 'covering': 1, 'strategies': 1, 'param': 1, 'eters': 1, 'vitb': 1, 'architec': 1, 'ture': 1, 'swin': 1, 'compu': 1, 'tational': 1, 'kimianet64': 1, 'lightweight': 1, 'densenet121': 1, 'subsampling': 1, '19304': 1, '462': 1, '1864': 1, 'nvdia': 1, 'memory': 1, 'constraints': 1, 'warmed': 1, 'β': 1, '09': 1, '0999': 1, 'fp16': 1, 'mixed': 1, 'swept': 1, '105': 1, '103': 1, '102': 1, 'moni': 1, 'tored': 1, 'early': 1, 'stopping': 1, 'bestperforming': 1, 'respec': 1, 'augmentation': 1, 'horizontal': 1, 'vertical': 1, 'flips': 1, 'angle': 1, 'rotation': 1, 'θrot': 1, '270': 1, 'jittering': 1, 'brightness': 1, '16255': 1, '0125': 1, '0075': 1, 'hue': 1, 'domain367879': 1, 'distilling': 1, 'contextual': 1, 'offering': 1, 'richer': 1, 'ies445456': 1, 'module': 1, 'augmented': 1, 'reviewed': 1, 'condensed': 1, 'retained': 1, 'inferred': 1, 'adaptable': 1, 'generativeimage2t': 1, 'ext': 1, 'git78': 1, 'family': 1, 'entire': 1, 'objec': 1, 'tive': 1, 'ordering80': 1, 'recalloriented': 1, 'understudy': 1, 'gisting': 1, 'evaluation81': 1, 'checkpoint': 1, 'time': 1, 'gener': 1, 'sampling82': 1, 'timestep': 1, 'redistributed': 1, 'receiver': 1, 'operating': 1, 'characteristic': 1, 'curve': 1, 'defined': 1, 'harmonic': 1, 'false': 1, 'threshold': 1, 'generalized': 1, 'mul': 1, 'ticlass': 1, 'pairwise': 1, 'propor': 1, 'correctly': 1, 'preci': 1, 'sion': 1, 'meteor80': 1, 'unigram': 1, 'takes': 1, 'synonyms': 1, 'forms': 1, 'rouge81': 1, 'ngrams': 1, 'rouge1': 1, 'unigrams': 1, '570': 1, '428': 1, 'entirety': 1, '701020': 1, 'multifigure': 1, '410': 1, 'bigger': 1, 'httpsportalgdc': 1, 'cancergov': 1, 'describing': 1, 'notable': 1, '656': 1, '642': 1, 'formalinfixed': 1, 'paraffin': 1, 'embedded': 1, '1048': 1, '837': 1, '211': 1, '881': 1, '754': 1, '127': 1, 'histopa': 1, 'thology': 1, 'incorrect': 1, '1041': 1, '529': 1, '846': 1, '432': 1, '414': 1, 'lowresolution': 1, 'downsamples': 1, 'lected': 1, '922': 1, '519': 1, '294': 1, '109': 1, '444': 1, '215': 1, 'luad83': 1, 'growth': 1, 'lepidic': 1, 'crc100k84': 1, 'mpp': 1, 'adenocarci': 1, 'belonged': 1, 'nine': 1, 'adipose': 1, 'back': 1, 'debris': 1, 'lymphocytes': 1, 'mucus': 1, 'colon': 1, 'mucosa': 1, 'epithelium': 1, 'officially': 1, 'wsss4luad85': 1, '500': 1, 'tumorassociated': 1, '155': 1, 'coreneedle': 1, 'noncancerous': 1, 'combining': 1, 'digestpath86': 1, '660': 1, 'colonoscopy': 1, '324': 1, 'acquired': 1, '93': 1, 'ebrains8788': 1, '2319': 1, '30way': 1, 'least': 1, 'reasonable': 1, '502525': 1, '1151': 1, '595': 1, '4244': 1, 'counts': 1, '474': 1, 'pilo': 1, 'cytic': 1, 'meningothelial': 1, 'pituitary': 1, 'adenoma': 1, '99': 1, '91': 1, 'ganglioglioma': 1, 'hemangioblastoma': 1, 'adamantinomatous': 1, 'craniopharyngioma': 1, 'schwannoma': 1, 'transitional': 1, 'lymphoma': 1, 'gliosar': 1, 'coma': 1, 'ana': 1, 'plastic': 1, 'secretory': 1, 'lipoma': 1, 'hemangiopericytoma': 1, 'nonwinglessrelated': 1, 'wntnonsonic': 1, 'hedgehog': 1, 'shh': 1, 'medulloblastoma': 1, 'langerhans': 1, 'histiocytosis': 1, 'angiomatous': 1, 'hemangioma': 1, 'hestained': 1, 'aggc89': 1, 'panda90': 1, 'dimen': 1, 'sions': 1, '189000': 1, '10000': 1, '29000': 1, 'library7': 1, 'thresholding': 1, 'converting': 1, 'downsample': 1, 'huesaturationvalue': 1, 'hsv': 1, 'blurring': 1, 'closing': 1, 'remove': 1, 'artifacts': 1, 'yield': 1, 'conventions762': 1, 'segmented': 1, 'contiguous': 1, 'caching': 1, 'major': 1, 'anatomical': 1, 'sites': 1, 'impractical': 1, 'exhaustive': 1, 'knearest': 1, 'neighbors': 1, 'knn': 1, 'categorize': 1, '1b': 1, 'contents': 1, 'nonparametric': 1, 'struct': 1, 'ation': 1, 'permutations': 1, 'independent': 1, 'predic': 1, 'tions': 1, 'swapped': 1, 'proportion': 1, 'greater': 1, 'hardware': 1, 'replicated': 1, 'outlined': 1, 'configured': 1, 'multigpu': 1, 'distributeddataparal': 1, 'lel': 1, 'ddp': 1, 'pyt': 1, 'orch': 1, 'httpspytorch': 1, 'org': 1, '24gb': 1, '3090': 1, 'modified': 1, 'maintained': 1, 'timm': 1, '092': 1, 'huggingfaceco': 1, 'httpsgithubcombytedanceibot': 1, 'nlp': 1, '4273': 1, 'accelerate': 1, '0150': 1, 'tokenization': 1, 'accessing': 1, 'integrated': 1, '2140': 1, 'probe': 1, 'lection': 1, 'implementations': 1, 'benchmarked': 1, 'hub': 1, 'httpshuggingfacecomodels': 1, 'httpshugging': 1, 'facecovinidplip': 1, 'httpshuggingfacecomicrosoft': 1, 'biomedclippubmedbert_256vit_base_patch16_224': 1, 'httpshuggingfacecoopenaiclipvitbasepatch16': 1, 'gitbase': 1, 'httpshuggingfacecomicrosoftgitbase': 1, 'gitlarge': 1, 'huggingfacecomicrosoftgitlarge': 1, 'plots': 1, 'miscellaneous': 1, 'linked': 1, 'httpportalgdccancergov': 1, 'httpbmirdsgithub': 1, 'iolungcancer': 1, 'httpdatamende': 1, 'leycomdatasets9xxm58dvs31': 1, 'httpzenodoorgrecord1214456': 1, 'httpwsss4luadgrandchallengeorg': 1, 'educa': 1, 'tional': 1, 'http': 1, 'searchkgebrainseuinstancesdataset8fc108abe2b44068999': 1, '60269dc1f994': 1, 'portals': 1, 'httpaggc22grandchallenge': 1, 'orgdata': 1, 'httppandagrandchallengeorgdata': 1, 'unpro': 1, 'httpncbinlmnihgovpmctoolsopen': 1, 'ftlist': 1, 'apply': 1, 'anonymized': 1, 'project': 1, 'permission': 1, 'thus': 1, 'academic': 1, 'pur': 1, 'httphuggingfacecomahmoodlabconch': 1, 'httpgithubcommahmoodlab': 1, 'documented': 1, 'technical': 1, 'acces': 1, 'sible': 1, 'audience': 1, '71': 1, 'redmon': 1, 'divvala': 1, 'girshick': 1, 'farhadi': 1, '779788': 1, 'luo': 1, 'biogpt': 1, 'mining': 1, 'brief': 1, 'bioinform': 1, 'bbac409': 1, '73': 1, 'dosovitskiy': 1, 'worth': 1, '1616': 1, '9th': 1, 'forumidyicbfdntty': 1, '74': 1, 'bert': 1, 'httpsopenreviewnetforumidydopye6dg': 1, 'silvarodriguez': 1, 'colomer': 1, 'dolz': 1, 'naranjo': 1, 'selflearning': 1, '30943104': 1, '76': 1, 'ecologic': 1, 'species': 1, 'ecology': 1, '297302': 1, '1945': 1, '77': 1, 'kolesnikov': 1, 'zhai': 1, 'beyer': 1, 'revisiting': 1, '19201929': 1, '78': 1, 'res': 1, 'httpsopenreview': 1, 'netforumidb4tmhpn0jc': 1, '79': 1, 'savarese': 1, 'blip2': 1, '40th': 1, '1973019742': 1, 'banerjee': 1, 'lavie': 1, 'mt': 1, 'improved': 1, 'correlation': 1, 'judgments': 1, 'acl': 1, 'workshop': 1, 'intrinsic': 1, 'extrinsic': 1, '6572': 1, '2005': 1, 'cy': 1, 'summaries': 1, 'branches': 1, '7481': 1, '2004': 1, '82': 1, 'lewis': 1, 'dauphin': 1, 'story': 1, '56th': 1, 'gurevych': 1, 'miyao': 1, '889898': 1, 'wei': 1, 'pathologistlevel': 1, 'resected': 1, 'rep': 1, '3358': 1, '84': 1, 'multicenter': 1, 'plos': 1, 'e1002730': 1, 'han': 1, 'semantic': 1, 'httpsdoiorg1048550arxiv220406455': 1, '86': 1, 'da': 1, 'q': 1, 'digestivesystem': 1, '102485': 1, 'httpsdoiorg': 1, '1025493wq48zgx': 1, '89': 1, 'huo': 1, 'comprehensive': 1, 'scanning': 1, 'cloudbased': 1, 'interaction': 1, 'ssrn': 1, 'httpsdoiorg102139': 1, 'ssrn4172090': 1, '154163': 1, 'jinghao': 1, 'insights': 1, 'dynamics': 1, 'his': 1, 'part': 1, 'fund': 1, 'mgh': 1, 'r35gm138216': 1, 'siebel': 1, 'scholars': 1, 'funded': 1, 'nci': 1, 'ruth': 1, 'kirschstein': 1, 'service': 1, 't32ca251062': 1, 'nsf': 1, 'graduate': 1, 'seas': 1, 'scholar': 1, 'r35gm149270': 1, 'niddk': 1, 'p30dk034854': 1, 'janicki': 1, 'kenny': 1, 'administration': 1, 'staff': 1, 'mgb': 1, 'enterprise': 1, 'infrastructure': 1, 'services': 1, 'eris': 1, 'maintaining': 1, 'instrumental': 1, 'mages': 1, 'ramsey': 1, 'administrative': 1, 'responsibility': 1, 'reflect': 1, 'views': 1, 'contributions': 1, 'conceptualized': 1, 'io': 1, 'az': 1, 'avp': 1, 'prepared': 1, 'coauthors': 1, 'inventors': 1, 'provisional': 1, 'us': 1, 'patent': 1, '63610645': 1, 'filed': 1, 'methodological': 1, 'aspects': 1, 'correspondence': 1, 'addressed': 1, 'thanks': 1, 'beck': 1, 'lee': 1, 'cooper': 1, 'geert': 1, 'litjens': 1, 'contribution': 1, 'editor': 1, 'lorenzo': 1, 'righetto': 1, 'collaboration': 1, 'team': 1, 'reprints': 1, 'permissions': 1, 'wwwnaturecomreprints': 1, 'nouns': 1, 'verbs': 1, 'ad': 1, 'since': 1, 'sensitive': 1, 'used52': 1, 'pool': 1, 'candidate': 1, '3438': 1, 'pools': 1, 'resulting': 1, 'dot': 1, 'triangles': 1, 'ks': 1, 'observe': 1, 'near': 1, 'scenarios': 1, 'indicates': 1, 'fails': 1, 'majority': 1, 'unsurprising': 1, 'ensembled': 1, 'bad': 1, 'caricnoma': 1, '124816': 1, 'ncϵ1': 1, 'follow': 1, 'consistently': 1, 'outperform': 1, 'n77': 1, 'n162': 1, 'k50': 1, 'indicated': 1, 'xaxis': 1, 'ideal': 1, 'green': 1, 'gray': 1, 'corrected': 1, 'partially': 1, 'portions': 1, 'noticed': 1, 'regurgitated': 1, 'verbatim': 1, 'works': 1, 'ablations': 1, 'centres': 1, 'yaxis': 1, '122': 1, 'preestablished': 1, 'conventions': 1, 'replaces': 1, 'initializes': 1, 'pathologyrelated': 1, 'cross': 1, 'modal': 1, 'updated': 1, 'mar': 1, 'wishes': 1, 'reproducibility': 1, 'publish': 1, 'structure': 1, 'consistency': 1, 'transparency': 1, 'checklist': 1, 'confirm': 1, 'items': 1, 'groupcondition': 1, 'measurement': 1, 'measurements': 1, 'taken': 1, 'repeatedly': 1, 'describe': 1, 'corrections': 1, 'normality': 1, 'adjustment': 1, 'tendency': 1, 'means': 1, 'variation': 1, 'deviation': 1, 'uncertainty': 1, 'statistic': 1, 'degrees': 1, 'freedom': 1, 'give': 1, 'whenever': 1, 'bayesian': 1, 'priors': 1, 'markov': 1, 'chain': 1, 'monte': 1, 'carlo': 1, 'designs': 1, 'identification': 1, 'pearsons': 1, 'indicating': 1, 'biologists': 1, 'scripts': 1, 'httpgithubcommahmoodlabclam': 1, 'huggingface': 1, '47072': 1, 'pytorch': 1, 'utilizing': 1, 'custom': 1, 'literature': 1, 'editors': 1, 'reviewers': 1, 'strongly': 1, 'encourage': 1, 'deposition': 1, 'repository': 1, 'github': 1, 'submitting': 1, 'accession': 1, 'unique': 1, 'links': 1, 'party': 1, 'adheres': 1, 'httpsportalgdccancergov': 1, 'httpsbmirdsgithubiolungcancer': 1, 'httpsdatamendeleycomdatasets9xxm58dvs31': 1, 'httpszenodoorgrecord1214456': 1, 'httpswsss4luadgrandchallengeorg': 1, 'httpssearchkgebrainseuinstancesdataset8fc108abe2b4406f899960269dc1f994': 1, 'aggc22grandchallengeorgdata': 1, 'httpspandagrandchallengeorgdata': 1, 'copyright': 1, 'unprocessed': 1, 'httpswwwncbinlmnihgovpmc': 1, 'toolsopenftlist': 1, 'biological': 1, 'identitypresentation': 1, 'sexual': 1, 'orientation': 1, 'racism': 1, 'covariate': 1, 'stage': 1, 'pertaining': 1, 'investigators': 1, 'sourced': 1, 'format': 1, 'selfreported': 1, '11855': 1, '9575': 1, '908': 1, '711': 1, 'socially': 1, 'regarding': 1, 'relating': 1, 'ethics': 1, 'oversight': 1, 'irb': 1, 'committee': 1, 'approval': 1, 'fieldspecific': 1, 'fit': 1, 'selection': 1, 'behavioural': 1, 'ecological': 1, 'evolutionary': 1, 'environmental': 1, 'copy': 1, 'document': 1, 'naturecomdocumentsnrreportingsummaryflatpdf': 1, 'disclose': 1, 'disclosure': 1, 'calculations': 1, 'internally': 1, 'subsection': 1, 'successful': 1, 'randomization': 1, 'creating': 1, 'stratified': 1, 'regionlevel': 1, 'item': 1, 'applies': 1, 'eukaryotic': 1, 'palaeontology': 1, 'archaeology': 1, 'organisms': 1, 'concern': 1, 'plants': 1, 'chipseq': 1, 'flow': 1, 'cytometry': 1, 'mribased': 1, 'neuroimaging': 1})}, {'file_name': 'uni', 'word_counts': Counter({'the': 957, 'and': 788, 'of': 624, 'in': 582, 'for': 444, 'we': 317, 'on': 304, 'a': 295, 'with': 250, 'to': 240, 'that': 178, 'uni': 175, 'rois': 174, 'from': 169, 'classification': 160, 'slides': 159, 'as': 152, 'tasks': 139, 'data': 133, 'learning': 133, 'is': 127, 'performance': 125, 'using': 124, 'image': 120, 'et': 116, 'al': 116, 'all': 111, 'at': 110, 'are': 108, 'tissue': 106, 'class': 106, 'which': 104, 'cancer': 103, 'by': 100, 'images': 98, 'tcga': 98, 'training': 97, 'slide': 92, 'subtyping': 86, 'evaluation': 83, 'used': 83, 'models': 79, 'also': 74, 'n': 71, 'pretrained': 69, 'dataset': 69, 'other': 66, 'j': 65, 'model': 64, 'pathology': 64, 'or': 64, 'fewshot': 61, 'cell': 61, 'selfsupervised': 60, 'be': 59, 'this': 57, '2023': 56, 'roi': 54, 'conference': 54, 't': 53, 'encoders': 53, 'fig': 53, 'pixels': 53, 'nature': 52, 'task': 52, 'vision': 52, 'use': 51, 'each': 51, 'can': 50, 'supplementary': 50, 'patches': 50, 'based': 50, '1': 49, 'per': 49, '2': 49, 'were': 49, 'remedis': 48, 'same': 48, 'wsis': 47, 'not': 47, 'across': 47, 'k': 46, 'an': 45, 'features': 44, 'pretraining': 44, 'set': 44, 'histology': 43, 'study': 43, 'histopathology': 42, 'p': 42, 'crc': 42, '224': 42, 'types': 41, 'tumor': 41, 'these': 41, 'classes': 41, 'cpath': 40, 'computer': 39, 'segmentation': 39, 'note': 39, 'our': 39, 'mpp': 39, 'medicine': 38, 'was': 38, 'given': 38, '16': 38, 'he': 38, 'article': 37, 'httpsdoiorg101038s41591024028573': 37, 'ctranspath': 37, 'o': 36, 'carcinoma': 36, 'methods': 36, 'test': 36, 'b': 36, 'datasets': 35, 'such': 35, 'prediction': 35, 'c': 35, 'folds': 35, 'label': 34, 'd': 34, 'm': 34, 'deep': 34, '32': 33, 'information': 33, 'retrieval': 33, '2022': 33, 'simpleshot': 33, 'proceedings': 33, 'into': 32, 'results': 32, '4': 32, 'pancancer': 32, 'extended': 32, 'international': 32, 'e': 31, 'following': 31, 'diagnostic': 30, '5': 30, 'supervised': 30, 'slidelevel': 30, 'breast': 30, 'comparisons': 30, 'examples': 30, 'med': 30, '2021': 30, 'provided': 29, 'many': 29, 'r': 29, 'y': 28, 'l': 28, 'ot108': 28, 'table': 28, 's': 28, '30': 27, 'have': 27, 'prototypes': 27, 'labels': 27, 'evaluate': 27, 'linear': 27, 'recognition': 27, 'clinical': 26, 'dinov2': 26, 'brca': 26, '8': 26, 'public': 26, 'via': 26, 'f': 25, 'rare': 25, 'representations': 25, 'respectively': 25, 'resnet50': 25, 'visual': 25, 'patch': 25, '0': 25, 'rcc': 25, 'may': 25, 'analysis': 25, 'knn': 25, 'similarity': 25, '20': 24, 'detection': 24, 'further': 24, 'similar': 24, 'trained': 24, 'although': 24, 'nat': 24, 'but': 23, 'both': 23, 'grading': 23, 'ot43': 23, 'wsi': 23, 'tables': 23, 'their': 23, 'compare': 23, 'annotated': 23, 'finegrained': 23, '256': 23, 'pattern': 23, 'ffpe': 23, 'foundation': 22, 'oncotree': 22, 'feature': 22, 'abmil': 22, 'outperforms': 22, 'roilevel': 22, 'brain': 22, 'consists': 22, 'g': 22, 'prototype': 22, 'ccrcc': 22, '100': 21, 'mil': 21, 'only': 21, 'mass100k': 21, 'accuracy': 21, 'ebrains': 21, '10': 21, 'pixel': 21, 'misimpleshot': 21, 'challenge': 20, 'transfer': 20, 'large': 20, 'due': 20, 'code': 20, 'two': 20, 'additional': 20, 'implementation': 20, '0001': 20, 'size': 20, 'original': 20, 'polyp': 20, '05': 20, 'versus': 20, 'research': 20, 'ieeecvf': 20, 'available': 19, 'assess': 19, 'observe': 19, 'when': 19, 'prad': 19, 'cptac': 19, 'extracted': 19, '2020': 19, 'whole': 19, 'ieee': 19, 'medical': 19, '2024': 18, 'h': 18, 'diverse': 18, 'more': 18, 'has': 18, 'annotations': 18, 'metastasis': 18, 'tion': 18, 'huncrc': 18, 'average': 18, 'resolutions': 18, '2019': 18, 'version': 18, 'computational': 17, 'weakly': 17, 'encoder': 17, '3': 17, 'details': 17, 'prostate': 17, 'labeled': 17, '025': 17, 'hierarchical': 17, 'probe': 17, 'lusc': 17, 'preprint': 17, 'processing': 17, 'resolution': 16, 'natural': 16, 'evaluated': 16, 'scale': 16, 'representation': 16, 'ing': 16, 'scaling': 16, 'glioma': 16, 'nsclc': 16, 'resized': 16, 'luad': 16, 'volume': 15, 'chen': 15, 'range': 15, 'multiple': 15, 'against': 15, 'one': 15, 'screening': 15, 'network': 15, 'loss': 15, 'query': 15, '075': 15, 'quality': 15, 'official': 15, 'retrieved': 15, 'neural': 15, 'prcc': 15, 'reporting': 15, 'x': 15, 'anal': 15, 'march': 14, 'wholeslide': 14, 'largescale': 14, 'than': 14, 'over': 14, 'capabilities': 14, 'out': 14, 'topk': 14, 'performed': 14, 'number': 14, 'settings': 14, 'strong': 14, 'masked': 14, 'bach': 14, 'panda': 14, 'aggc': 14, 'runs': 14, 'preextracted': 14, 'colorectal': 14, 'probing': 14, '448': 14, 'attention': 14, 'if': 14, 'university': 14, 'trainvalidationtest': 14, 'fold': 14, 'labelstratified': 14, 'tumors': 14, '850862': 13, 'developed': 13, 'downstream': 13, 'instance': 13, 'inhouse': 13, 'bwh': 13, 'different': 13, 'architecture': 13, 'baselines': 13, 'transformer': 13, 'isup': 13, 'sampled': 13, 'reported': 13, 'its': 13, 'support': 13, 'token': 13, 'digital': 13, 'approximately': 13, 'traintest': 13, 'high': 12, 'stateoftheart': 12, 'demonstrate': 12, 'disease': 12, 'type': 12, 'well': 12, 'regarding': 12, 'overall': 12, 'increases': 12, 'development': 12, 'imagenet': 12, 'created': 12, 'collected': 12, 'top5': 12, 'bestperforming': 12, 'dhmc': 12, 'segpath': 12, '15': 12, 'i': 12, 'train': 12, 'equivalent': 12, 'renal': 12, 'unit': 12, 'corresponding': 12, 'layer': 12, 'normal': 12, 'would': 12, 'systems': 12, 'rjc': 12, 'applications': 11, 'addition': 11, 'generalization': 11, '108': 11, 'anatomic': 11, 'pathologist': 11, 'collections': 11, 'lung': 11, 'defined': 11, 'mass1k': 11, 'compared': 11, 'first': 11, 'report': 11, 'score': 11, 'described': 11, 'work': 11, 'machine': 11, '36': 11, '40': 11, '70': 11, '50': 11, 'idh1': 11, 'bracs': 11, 'values': 11, 'external': 11, 'samples': 11, 'experiments': 11, 'validation': 11, '7': 11, 'example': 11, 'adenocarcinoma': 11, 'embedding': 11, 'chrcc': 11, 'heatmaps': 11, 'no': 11, 'usa': 11, 'dysplasia': 11, 'internal': 11, 'sample': 11, 'coarse': 11, 'td': 11, 'myl': 11, 'modeling': 10, 'artificial': 10, 'intelligence': 10, 'single': 10, 'comparison': 10, 'mass22k': 10, 'vit': 10, 'weighted': 10, 'auroc': 10, 'ai': 10, 'works': 10, 'limited': 10, 'hospital': 10, 'finetuning': 10, 'v': 10, '80': 10, '38': 10, '95': 10, 'uniform': 10, 'ct': 10, 'ranspath': 10, 'within': 10, 'efficiency': 10, '11': 10, 'agreement': 10, 'visualization': 10, 'level': 10, 'heatmap': 10, '2242': 10, 'rate': 10, 'transformers': 10, '1000': 10, 'patient': 10, '2018': 10, 'biopsies': 10, 'computing': 10, 'springer': 10, 'adenoma': 10, 'ma': 10, '512': 10, 'million': 9, 'major': 9, 'system': 9, '28': 9, 'categories': 9, '90': 9, 'cancers': 9, 'vitl': 9, 'larger': 9, 'mutation': 9, 'regions': 9, 'gtex': 9, 'assessment': 9, 'next': 9, 'teacher': 9, '72': 9, '64': 9, 'crc100k': 9, 'histomolecular': 9, 'find': 9, 'above': 9, 'points': 9, 'classifica': 9, 'architectures': 9, 'three': 9, 'mhsa': 9, 'visualizations': 9, 'performances': 9, 'pooling': 9, '9': 9, 'parameters': 9, 'it': 9, 'any': 9, 'retrospective': 9, 'contrastive': 9, 'imaging': 9, 'ibot': 9, 'eg': 9, 'center': 9, 'ratio': 9, 'cohort': 9, 'supported': 9, 'accessed': 9, 'been': 8, '34': 8, 'outperforming': 8, 'classifying': 8, 'advances': 8, 'online': 8, 'full': 8, 'least': 8, 'program': 8, 'sizes': 8, 'vitlarge': 8, 'ref': 8, '24': 8, 'follow': 8, 'f1': 8, 'observed': 8, 'perfor': 8, 'mance': 8, 'general': 8, 'atlas': 8, 'primary': 8, 'potential': 8, 'lymph': 8, 'achieves': 8, '60': 8, 'esca': 8, 'distribution': 8, 'u': 8, 'w': 8, 'human': 8, 'specifically': 8, '23': 8, 'how': 8, 'studies': 8, 'evalu': 8, '896': 8, 'benign': 8, 'invasive': 8, '46': 8, 'between': 8, 'opatho': 8, 'evaluating': 8, 'framework': 8, 'networks': 8, 'still': 8, 'prototypical': 8, 'portfolio': 8, 'z': 8, 'survival': 8, 'language': 8, 'harvard': 8, 'consisting': 8, 'magnification': 8, 'clam': 8, 'embeddings': 8, 'whether': 8, 'mask2former': 8, 'splits': 8, 'idh1mutant': 8, 'previous': 7, 'new': 7, 'possible': 7, 'molecular': 7, 'benchmark': 7, 'instead': 7, '43': 7, 'national': 7, 'last': 7, 'algorithm': 7, 'followed': 7, 'under': 7, 'experimental': 7, '42': 7, 'developing': 7, 'approach': 7, 'called': 7, '22': 7, 'camelyon16': 7, 'metrics': 7, '12': 7, '54': 7, '45': 7, '56': 7, 'msi': 7, 'det': 7, 'ext': 7, 'confidence': 7, 'intervals': 7, 'computed': 7, 'metric': 7, 'detailed': 7, 'coarsegrained': 7, 'boxes': 7, 'indicate': 7, 'whiskers': 7, 'extend': 7, 'interquartile': 7, 'consistently': 7, 'comparing': 7, 'additionally': 7, 'gleason': 7, 'regression': 7, 'neighbors': 7, 'simple': 7, 'mean': 7, '128': 7, 'endtoend': 7, 'uses': 7, 'majority': 7, 'highresolution': 7, '6': 7, 'most': 7, 'cells': 7, 'baseline': 7, 'nearest': 7, 'euclidean': 7, 'distance': 7, 'standard': 7, 'cohorts': 7, 'multimodal': 7, 'lu': 7, 'diffuse': 7, 'authors': 7, 'sciences': 7, 'excluded': 7, 'obtained': 7, 'availability': 7, 'meningioma': 7, 'software': 7, 'https': 7, 'codebase': 7, 'zhu': 7, 'histopathological': 6, 'challenges': 6, 'through': 6, 'com': 6, 'source': 6, 'organ': 6, 'predict': 6, 'codes': 6, 'rarecare': 6, 'project': 6, 'create': 6, 'choice': 6, 'mocov3': 6, '37': 6, 'show': 6, 'applied': 6, 'important': 6, 'imagenet1k': 6, 'diseases': 6, 'adapt': 6, 'genome': 6, 'largest': 6, 'without': 6, 'including': 6, 'transplant': 6, 'several': 6, '2016': 6, 'head': 6, '57': 6, '76': 6, '33': 6, '52': 6, 'patients': 6, 'represent': 6, 'balanced': 6, 'except': 6, 'bars': 6, 'gj': 6, 'four': 6, 'quartile': 6, '29': 6, 'having': 6, 'where': 6, 'randomly': 6, 'superior': 6, 'ation': 6, 'demonstrates': 6, '101': 6, 'weight': 6, 'ground': 6, 'better': 6, 'multihead': 6, '200': 6, '47': 6, 'should': 6, 'relatively': 6, 'contains': 6, 'biases': 6, 'averaging': 6, 'semantic': 6, 'squamous': 6, 'indicating': 6, 'maximum': 6, 'respect': 6, 'staining': 6, 'sourced': 6, 'li': 6, 'pathological': 6, 'science': 6, 'pathologists': 6, 'cambridge': 6, 'before': 6, 'then': 6, 'extraction': 6, 'input': 6, 'section': 6, 'considered': 6, '1812': 6, 'resnet50in': 6, 'resulted': 6, 'cases': 6, 'lowgrade': 6, 'description': 6, 'python': 6, 'fm': 6, 'policy': 6, 'generalpurpose': 5, 'zhang': 5, 'objective': 5, 'variability': 5, 'publicly': 5, 'difficulty': 5, 'wide': 5, 'performing': 5, 'must': 5, 'often': 5, 'even': 5, 'response': 5, 'among': 5, 'limitations': 5, '19': 5, 'seen': 5, 'follows': 5, 'total': 5, 'designated': 5, 'nciseer': 5, 'forms': 5, 'basis': 5, 'impact': 5, 'shown': 5, 'increase': 5, 'top1': 5, '14': 5, 'length': 5, 'algorithms': 5, '18': 5, 'frozen': 5, 'analyses': 5, 'tions': 5, 'con': 5, '1b': 5, 'biomarker': 5, 'figs': 5, 'improved': 5, 'metastases': 5, 'nodes': 5, 'global': 5, 'kidney': 5, 'prototyping': 5, '55': 5, '26': 5, '65': 5, 'bwhemb': 5, 'unitopatho': 5, 'til': 5, 'unis': 5, '06': 5, '1620': 5, '045': 5, '83': 5, 'higher': 5, 'leaderboards': 5, 'time': 5, 'contamination': 5, 'outofdomain': 5, 'benchmarks': 5, 'regard': 5, 'experimentation': 5, 'efficient': 5, 'comprehensive': 5, 'top': 5, 'noma': 5, 'figure': 5, 'res': 5, 'stroma': 5, 'groundtruth': 5, 'colored': 5, 'square': 5, 'cls': 5, 'truth': 5, '4482': 5, '8962': 5, '13442': 5, 'lymphocyte': 5, 'reports': 5, 'camelyon17wilds': 5, 'leaderboard': 5, '62': 5, 'finetuned': 5, 'competitive': 5, 'taskspecific': 5, 'distinct': 5, 'mvacc5': 5, 'six': 5, '41': 5, 'commonly': 5, 'resizing': 5, '51': 5, 'introduced': 5, 'eight': 5, 'smooth': 5, 'red': 5, 'convolutional': 5, 'matching': 5, 'oneshot': 5, 'unique': 5, 'deviation': 5, '81': 5, 'prompts': 5, 'outlined': 5, 'incorrect': 5, 'curated': 5, 'testing': 5, 'subtypes': 5, 'shift': 5, 'oncol': 5, '25': 5, 'predicting': 5, 'biomed': 5, 'eng': 5, 'gigapixel': 5, 'your': 5, 'database': 5, '2017': 5, 'wang': 5, '85': 5, 'node': 5, 'huang': 5, 'genomic': 5, 'glioblastoma': 5, 'gliomas': 5, 'text': 5, 'case': 5, 'involved': 5, 'adapted': 5, 'imagenet22k': 5, 'scanner': 5, 'method': 5, 'objectives': 5, 'student': 5, 'community': 5, 'epochs': 5, 'control': 5, 'dimension': 5, 'casestratified': 5, 'combined': 5, 'statistical': 5, 'read': 5, 'astrocytoma': 5, 'anaplastic': 5, 'cha': 5, 'tcgatils': 5, 'detectron2': 5, 'nih': 5, 'yao': 5, 'dfkw': 5, 'correct': 5, 'see': 5, 'about': 5, 'sex': 5, 'characterization': 4, 'present': 4, 'annotation': 4, 'current': 4, 'representative': 4, 'practice': 4, 'solving': 4, 'published': 4, 'end': 4, 'paper': 4, 'nonsmall': 4, 'vary': 4, 'conventional': 4, 'patchlevel': 4, 'attentionbased': 4, 'setup': 4, '111': 4, '35': 4, 'iterations': 4, 'findings': 4, 'backbones': 4, 'increasing': 4, 'hyperparameters': 4, 'techniques': 4, 'prior': 4, 'however': 4, 'learn': 4, '100426': 4, 'massachusetts': 4, 'mgh': 4, 'brigham': 4, 'stage': 4, 'offtheshelf': 4, 'versatility': 4, 'sub': 4, 'include': 4, '2d': 4, 'various': 4, '78': 4, 'margin': 4, 'respective': 4, 'heart': 4, '75': 4, '48': 4, 'fsubtyping': 4, 'csubtyping': 4, 'selfdistillation': 4, '02': 4, '87': 4, '03': 4, '09': 4, 'ot': 4, 'cohens': 4, 'error': 4, '15fold': 4, 'grade': 4, 'representing': 4, 'few': 4, 'constraints': 4, 'during': 4, 'sets': 4, '31': 4, 'small': 4, '4shot': 4, '32shot': 4, 'underrepresented': 4, 'tumorimmune': 4, '32class': 4, 'perform': 4, 'logistic': 4, 'knearest': 4, '156': 4, 'esophageal': 4, 'predictions': 4, 'achieve': 4, '1344': 4, 'predicted': 4, 'recall': 4, 'selfattention': 4, 'color': 4, 'penultimate': 4, 'µm': 4, 'best': 4, 'stain': 4, 'normalization': 4, 'space': 4, 'successful': 4, 'correctly': 4, 'because': 4, 'grand': 4, '048': 4, '17922': 4, 'epithelial': 4, 'muscle': 4, 'blood': 4, 'dense': 4, 'dice': 4, 'swin': 4, 'individual': 4, 'scores': 4, 'extending': 4, 'less': 4, 'nonparametric': 4, 'vectors': 4, 'low': 4, 'predicts': 4, 'visualize': 4, 'normalized': 4, 'clear': 4, 'believe': 4, 'representa': 4, 'visualcentric': 4, 'empirical': 4, 'improve': 4, 'require': 4, 'recipes': 4, 'specific': 4, 'altogether': 4, 'collection': 4, 'knowledge': 4, 'fixed': 4, 'protocol': 4, 'review': 4, 'precision': 4, 'kather': 4, 'microsatellite': 4, 'directly': 4, 'sci': 4, 'reveals': 4, 'learners': 4, '2009': 4, '2015': 4, 'kolesnikov': 4, 'beyer': 4, '2013': 4, 'life': 4, 'trans': 4, 'xu': 4, 'relevant': 4, 'weaklysupervised': 4, 'histo': 4, 'computerassisted': 4, 'intervention': 4, 'rep': 4, 'search': 4, 'you': 4, 'derived': 4, 'patterns': 4, '88': 4, '96': 4, 'property': 4, 'eleventh': 4, '109': 4, 'tokenizer': 4, 'school': 4, 'boston': 4, 'institute': 4, 'preprocessing': 4, 'below': 4, 'nonoverlapping': 4, 'sampling': 4, 'distillation': 4, 'ibot118': 4, 'main': 4, 'while': 4, 'weights': 4, 'regularization': 4, 'fully': 4, 'setting': 4, 'bag': 4, 'steps': 4, 'did': 4, 'ℓ2': 4, 'resize': 4, 'sequence': 4, 'ddimension': 4, '1792': 4, 'hipt': 4, 'purposes': 4, 'adopt': 4, 'made': 4, 'ductal': 4, 'lymphoma': 4, '801010': 4, 'atypical': 4, 'idh1wildtype': 4, '120': 4, 'highgrade': 4, 'centercropped': 4, 'hel': 4, 'ukk': 4, 'wns': 4, 'tubular': 4, 'tubulovillous': 4, 'macenko': 4, 'opensource': 4, 'libraries': 4, 'nvidia': 4, 'links': 4, 'summary': 4, 'policies': 4, 'part': 4, 'ms': 4, 'materials': 4, 'statement': 4, 'tests': 4, 'githubcom': 4, 'gender': 4, 'andrew': 3, 'faisal': 3, 'mahmood': 3, 'morphological': 3, 'significant': 3, 'highperformance': 3, 'proposed': 3, '100000': 3, 'varying': 3, 'unsupervised': 3, 'enabling': 3, 'dataefficient': 3, 'challenging': 3, 'diagnoses': 3, 'array': 3, 'building': 3, 'august': 3, 'they': 3, 'binary': 3, 'broader': 3, 'system77': 3, 'institutes': 3, 'trends': 3, 'scales': 3, 'vitbase': 3, 'vitb': 3, 'complexity': 3, 'area': 3, 'operating': 3, 'characteristic': 3, 'formance': 3, 'twosided': 3, '13': 3, '17': 3, 'diversity': 3, 'improvement': 3, 'exploring': 3, 'worse': 3, 'paip': 3, 'remedis38': 3, 'factors': 3, 'formalinfixed': 3, 'paraffinembedded': 3, 'along': 3, 'meaningful': 3, 'mostly': 3, 'popular': 3, 'sists': 3, 'womens': 3, 'need': 3, 'nuclear': 3, 'metastatic': 3, 'typing': 3, 'promptbased': 3, 'laws': 3, '63': 3, 'uniuniuni': 3, 'classifier': 3, 'seg': 3, 'crops': 3, 'individualagg': 3, 'tract': 3, 'skin': 3, 'bowel': 3, 'liver': 3, 'bladder': 3, '10000': 3, '44': 3, '39': 3, 'generally': 3, 'consist': 3, 'furthermore': 3, '08': 3, '129': 3, '1091': 3, '872': 3, '873': 3, '573': 3, 'inremedis': 3, '01': 3, 'lines': 3, 'quadratic': 3, 'κ': 3, 'centers': 3, 'specified': 3, 'cptacdhmc': 3, 'society': 3, 'cardiac': 3, 'others': 3, 'fea': 3, 'leakage': 3, 'thus': 3, 'existing': 3, 'allograft': 3, 'rejection': 3, 'highlight': 3, 'decreases': 3, 'indomain': 3, 'consortium': 3, 'outperform': 3, 'flexible': 3, 'five': 3, 'median': 3, 'classifiers': 3, 'automated': 3, 'referred': 3, '86': 3, 'shows': 3, '2399': 3, '345021': 3, 'acc1': 3, 'acc5': 3, 'right': 3, '6368': 3, 'represents': 3, 'encoded': 3, 'champkit': 3, '61': 3, 'retrieve': 3, 'visually': 3, 'acck': 3, 'clas': 3, 'sification': 3, 'smaller': 3, 'robustness': 3, 'suggest': 3, 'lymphocytes': 3, '69': 3, 'cnns': 3, 'literature': 3, 'centroid': 3, 'averaged': 3, 'ture': 3, 'unseen': 3, 'assigned': 3, 'run': 3, 'constructed': 3, '2shot': 3, 'decrease': 3, 'stability': 3, 'required': 3, 'dis': 3, '68': 3, 'develop': 3, 'match': 3, '71': 3, 'creating': 3, 'differences': 3, 'pre': 3, 'demonstrated': 3, 'components': 3, 'lower': 3, 'recent': 3, 'providing': 3, 'insights': 3, 'does': 3, 'missing': 3, 'hyperparameter': 3, 'imple': 3, 'will': 3, 'references': 3, 'peer': 3, 'author': 3, 'competing': 3, 'interests': 3, 'tools': 3, 'diagnosis': 3, 'oncology': 3, 'clin': 3, 'genomics': 3, '115': 3, 'prognosis': 3, 'krishnan': 3, 'graph': 3, '21': 3, 'pmlr': 3, 'process': 3, 'journal': 3, 'revisiting': 3, 'zhai': 3, 'benchmarking': 3, 'rajpoot': 3, 'cvf': 3, 'selfsupervision': 3, 'lin': 3, 'spatial': 3, 'biomedical': 3, 'junction': 3, 'adenocarcinomas': 3, '59': 3, 'httpsdoiorg': 3, 'commun': 3, 'somatic': 3, 'lancet': 3, 'endomyocardial': 3, 'landscape': 3, 'lowergrade': 3, '91': 3, 'plain': 3, '104': 3, 'shot': 3, 'health': 3, 'billion': 3, '112': 3, '117': 3, 'bert': 3, 'manuscript': 3, 'applicable': 3, 'entire': 3, 'inference': 3, 'curation': 3, 'found': 3, 'inspired': 3, 'includes': 3, 'dino25': 3, 'understanding': 3, 'crossentropy': 3, 'context': 3, 'tokens': 3, 'centering': 3, 'dataparallel': 3, 'making': 3, 'modified': 3, 'conducted': 3, 'after': 3, 'third': 3, 'sites': 3, 'random': 3, 'embed': 3, 'apply': 3, 'dimensions': 3, 'formed': 3, 'masks': 3, 'value': 3, 'intermediate': 3, 'mitigate': 3, 'classifies': 3, 'normalize': 3, 'vitadapter': 3, 'decay': 3, 'backbone': 3, 'cway': 3, 'kshot': 3, 'episodes': 3, 'papillary': 3, 'chromophobe': 3, 'plots': 3, 'approxi': 3, 'mately': 3, 'blca': 3, 'coad': 3, 'uterine': 3, 'stad': 3, 'hnsc': 3, 'melanoma': 3, 'paad': 3, 'taken': 3, 'radboud': 3, 'trainvalida': 3, 'tiontest': 3, 'provide': 3, 'heldout': 3, 'situ': 3, 'oligodendroglioma': 3, 'status': 3, '1p19q': 3, 'codeleted': 3, '144': 3, '173': 3, 'central': 3, 'cns': 3, 'involving': 3, 'region': 3, 'nct': 3, 'biobank': 3, 'background': 3, 'mucosa': 3, 'camelyon17': 3, 'wilds': 3, '057': 3, 'hyperplastic': 3, 'crcmsi': 3, 'sitespecific': 3, 'cuda': 3, 'library': 3, 'gpus': 3, 'openslidepython': 3, 'physionet': 3, 'mahmoodlabclam': 3, 'pillow': 3, 'basic': 3, 'commons': 3, 'requests': 3, 'ensuring': 3, '121': 3, 'exact': 3, 'yu': 3, 'scikitlearn': 3, 'mapping': 3, 'proteogenomic': 3, 'complex': 3, 'repository': 3, 'lo': 3, 'ds': 3, 'fellowship': 3, 'ahs': 3, 'bc': 3, 'az': 3, 'llw': 3, 'carried': 3, 'material': 3, 'measured': 3, 'leftmost': 3, 'retrieves': 3, 'summaryapril': 3, 'na': 3, 'covariates': 3, 'estimates': 3, 'appropriate': 3, 'collect': 3, 'race': 3, 'ethnicity': 3, 'richard': 2, 'tong': 2, 'ming': 2, 'drew': 2, 'williamson': 2, 'long': 2, 'quantitative': 2, 'requiring': 2, 'entities': 2, 'address': 2, 'efforts': 2, '77': 2, 'resolutionagnostic': 2, 'up': 2, 'terms': 2, 'generalize': 2, 'diagnostically': 2, 'workflows': 2, 'involves': 2, 'problems': 2, 'therapeutic': 2, 'practical': 2, 'accepted': 2, 'list': 2, 'affiliations': 2, 'email': 2, 'faisalmahmoodbwhharvardedu': 2, 'subset': 2, 'realworld': 2, 'comprises': 2, '5564': 2, 'subdivided': 2, '82': 2, 'surveillance': 2, 'epidemiology': 2, '43class': 2, '108class': 2, '2a': 2, 'goal': 2, '21444': 2, '1404': 2, 'paradigm': 2, 'preextracting': 2, 'reflect': 2, 'receiver': 2, 'curve': 2, 'paired': 2, 'permutation': 2, 'monotonic': 2, '125000': 2, 'align': 2, 'improves': 2, '84': 2, 'effective': 2, 'dependent': 2, 'ability': 2, 'mas': 2, 'similarly': 2, 'serves': 2, 'demonstrating': 2, 'remain': 2, 'est': 2, 'build': 2, 'genotypetissue': 2, 'expression': 2, 'provides': 2, 'histopathologic': 2, 'diag': 2, 'nostic': 2, 'ctranspath37': 2, '0976': 2, '0020': 2, '2c': 2, 'idc': 2, 'ilc': 2, 'gbm': 2, 'lgg': 2, 'iew': 2, 'reconstruction': 2, 'alignment': 2, 'agg': 2, 'male': 2, 'genital': 2, 'female': 2, 'esophagogastric': 2, 'pancreas': 2, 'neck': 2, '2000': 2, 'c17wilds': 2, 'mask': 2, 'classify': 2, '04': 2, 'specificity': 2, 'sensitivity': 2, '147': 2, '954': 2, 'millions': 2, '015': 2, 'ypes': 2, '07': 2, 'gastrointestinal': 2, 'ce': 2, 'dashed': 2, 'correspond': 2, 'panda18': 2, 'tures': 2, 'almost': 2, 'greater': 2, '005': 2, 'involve': 2, 'either': 2, '196': 2, '161': 2, 'sophisticated': 2, 'supple': 2, 'mentary': 2, 'parisons': 2, 'cellularmediated': 2, 'wholesight90': 2, 'crane86': 2, 'strength': 2, 'betterpretrained': 2, 'concern': 2, 'cally': 2, 'addi': 2, 'tionally': 2, 'observing': 2, '856': 2, 'independent': 2, 'supports': 2, 'shots': 2, 'depending': 2, 'chosen': 2, 'especially': 2, 'reach': 2, 'foundational': 2, 'compari': 2, 'sons': 2, 'common': 2, 'measure': 2, '3960': 2, '188': 2, '94': 2, 'carci': 2, '7180': 2, '178187': 2, '55360': 2, 'g3': 2, 'g4': 2, 'g5': 2, 'ruth': 2, 'bottom': 2, 'mm': 2, '384': 2, 'highest': 2, 'fnr': 2, 'date': 2, 'differ': 2, 'consider': 2, 'vote': 2, 'tis': 2, 'respec': 2, 'tively': 2, 'atypia': 2, 'affected': 2, '288': 2, '360': 2, 'pix': 2, 'els': 2, 'crypts': 2, 'otherwise': 2, 'agnostic': 2, 'optimal': 2, 'plasma': 2, 'viewed': 2, 'morphologies': 2, 'repeated': 2, '050': 2, 'mi': 2, 'indicated': 2, '8shot': 2, '256shot': 2, 'variance': 2, 'potentially': 2, 'forming': 2, 'some': 2, 'shifted': 2, 'exist': 2, 'textual': 2, 'voting': 2, 'distances': 2, 'blue': 2, 'tested': 2, '4fg': 2, 'effectiveness': 2, 'attributed': 2, 'trainable': 2, '123': 2, 'effects': 2, 'intensity': 2, 'instances': 2, 'enable': 2, 'includ': 2, 'emerged': 2, 'explore': 2, 'good': 2, 'consistent': 2, 'ment': 2, 'suggests': 2, 'own': 2, 'cer': 2, 'being': 2, 'generalist': 2, 'box': 2, 'designed': 2, 'implemented': 2, 'toward': 2, 'vitgiant': 2, 'likely': 2, 'resources': 2, 'represented': 2, 'tuning': 2, 'mented': 2, 'unimodal': 2, 'future': 2, 'block': 2, 'acknowledgements': 2, 'song': 2, 'rev': 2, 'lipkova': 2, 'integration': 2, 'foersch': 2, 'groups': 2, 'instability': 2, 'outcomes': 2, 'histologic': 2, 'integrative': 2, 'cooper': 2, 'opportunities': 2, 'graham': 2, 'interpretable': 2, 'deeplearning': 2, 'style': 2, 'alterations': 2, 'fu': 2, 'bulten': 2, 'therapy': 2, 'scalable': 2, 'robust': 2, 'supervision': 2, 'bootstrap': 2, 'adv': 2, 'inf': 2, 'syst': 2, 'sun': 2, 'gupta': 2, 'misra': 2, 'genet': 2, 'transformerbased': 2, 'azizi': 2, 'benefits': 2, '79': 2, 'q': 2, 'histological': 2, 'kim': 2, 'martel': 2, 'consistency': 2, '49': 2, 'guided': 2, '89': 2, 'van': 2, 'der': 2, 'ciompi': 2, 'intell': 2, 'visuallanguage': 2, 'discriminative': 2, 'zeroshot': 2, 'cvpr': 2, 'biological': 2, 'hörst': 2, 'jco': 2, 'inform': 2, 'multicentric': 2, 'patchbased': 2, '239': 2, '66': 2, '67': 2, 'komura': 2, 'universal': 2, 'tumours': 2, 'multitask': 2, 'mapped': 2, 'phenotypes': 2, '73': 2, 'learningbased': 2, 'plos': 2, '35th': 2, 'campanella': 2, 'welling': 2, 'residual': 2, '155': 2, 'engl': 2, 'tumour': 2, 'open': 2, 'resource': 2, 'association': 2, 'linguistics': 2, 'short': 2, 'papers': 2, '93': 2, 'corpus': 2, 'reproducibility': 2, '99': 2, 'overcome': 2, 'kirillov': 2, 'fang': 2, '105': 2, 'httpsdoiorg1048550arxiv': 2, '106': 2, 'zemel': 2, 'optimization': 2, 'towards': 2, 'httpsdoiorg1048550': 2, '118': 2, 'zhou': 2, 'institutional': 2, 'exclusive': 2, 'publishing': 2, 'solely': 2, 'mit': 2, 'engineering': 2, 'technology': 2, 'approved': 2, 'recruited': 2, 'informed': 2, 'consent': 2, 'waived': 2, 'treated': 2, 'video': 2, 'there': 2, 'transductive': 2, 'his': 2, 'scanned': 2, 'aperio': 2, 'gt450': 2, 'hamamatsu': 2, 's210': 2, 'make': 2, '800': 2, 'resolu': 2, 'toolbox15': 2, 'contours': 2, 'minimum': 2, 'artifacts': 2, 'segmented': 2, 'yielded': 2, '100130900': 2, 'studentteacher': 2, 'cnn': 2, 'augmented': 2, 'views': 2, 'updated': 2, 'denote': 2, 'inputs': 2, 'compute': 2, 'output': 2, 'comparable': 2, 'always': 2, 'overlapping': 2, 'dino': 2, 'vits': 2, 'component': 2, 'modifications': 2, 'untying': 2, 'sinkhornknopp': 2, 'recipe': 2, '142': 2, 'truncated': 2, 'swint14': 2, 'initialized': 2, 'big': 2, 'checkpoints': 2, 'tive': 2, 'nor': 2, 'result': 2, 'patched': 2, 'coordinates': 2, 'threshold': 2, 'downsample': 2, 'removed': 2, 'variant': 2, 'connected': 2, 'aside': 2, 'configuration': 2, 'ally': 2, 'early': 2, 'stopping': 2, 'included': 2, 'batch': 2, 'coefficient': 2, 'share': 2, 'scikitlearn134': 2, 'change': 2, 'multiples': 2, 'forward': 2, 'interpolation': 2, 'positional': 2, 'now': 2, 'patchifying': 2, 'memoryefficient': 2, 'implementations': 2, 'flashattention': 2, 'contentbased': 2, 'lowdimensional': 2, 'keys': 2, 'takes': 2, 'subtracting': 2, 'key': 2, 'interpreted': 2, 'frame': 2, 'related': 2, 'labeling': 2, 'noted': 2, 'normalizing': 2, '1nn': 2, 'describe': 2, '271170': 2, 'around': 2, '8736': 2, 'imbalance': 2, 'requires': 2, 'macro': 2, 'add': 2, 'urothelial': 2, 'colon': 2, 'adenocarci': 2, 'cutaneous': 2, 'lymphocytic': 2, 'sources': 2, 'storage': 2, 'repeatedly': 2, 'select': 2, 'previously': 2, 'clustering': 2, 'col': 2, '400': 2, 'resulting': 2, '160': 2, 'nontumor': 2, 'acceptable': 2, 'segments': 2, 'tcgansclc': 2, 'rocy': 2, '344': 2, '404': 2, 'negative': 2, '502525': 2, 'malignant': 2, 'hyperplasia': 2, 'astrocytomas': 2, 'glioblastomas': 2, '2319': 2, '184': 2, 'core': 2, 'karolinska': 2, 'work90': 2, 'httpswww': 2, 'caveat': 2, 'camelyon': 2, 'acr': 2, '136': 2, 'ade': 2, 'umm': 2, 'epithelium': 2, 'archive': 2, 'nctcrche100k': 2, 'hospitals': 2, 'bilinearly': 2, 'upsampled': 2, 'resection': 2, 'technical': 2, '2048': 2, '1536': 2, '042': 2, '300': 2, 'digitized': 2, 'partial': 2, 'propria': 2, 'mucosae': 2, 'muscularis': 2, 'acc': 2, 'cinoma': 2, 'skcm': 2, 'thca': 2, 'ucec': 2, 'normalization169': 2, '984': 2, 'immunofluorescence': 2, 'dapi': 2, 'split': 2, 'pyt': 2, 'pytorchorg': 2, 'replicated': 2, 'modify': 2, 'maintained': 2, 'timm': 2, 'hugging': 2, 'face': 2, 'githubcomfacebookresearchdinov2': 2, 'gb': 2, 'a100': 2, 'gpu': 2, 'multigpu': 2, 'multinode': 2, 'distributed': 2, 'ddp': 2, '3090': 2, 'openslide': 2, 'httpsgithubcommahmoodlabclam': 2, 'lgssl': 2, 'benchmarked': 2, 'githubcomxiyuewangtranspath': 2, 'httpsgithub': 2, 'scaffold': 2, 'httpsgithubcom': 2, 'facebookresearchmask2former': 2, '174': 2, 'packages': 2, 'opencvpython': 2, 'matplotlib': 2, 'seaborn': 2, 'figures': 2, 'design': 2, 'added': 2, 'portal': 2, 'portals': 2, 'httpszenodoorgrecord1214456': 2, 'httpszenodoorgrecord3832231': 2, 'httpszenodoorgrecord5889558': 2, 'httpszenodoorgrecord7412731': 2, 'listed': 2, 'name': 2, 'permission': 2, 'organizers': 2, 'relies': 2, 'anonymized': 2, 'institution': 2, 'casebycase': 2, 'determine': 2, 'requested': 2, 'comply': 2, 'intellectual': 2, 'privacy': 2, 'obligations': 2, 'academic': 2, '550560': 2, 'mw': 2, 'lee': 2, 'tian': 2, 'douze': 2, 'vol': 2, 'fast': 2, 'visioneccv': 2, '16th': 2, 'european': 2, 'glasgow': 2, 'uk': 2, '2328': 2, 'multiinstance': 2, 'multidimensional': 2, 'acm': 2, 'transactions': 2, '2011': 2, 'bayesian': 2, '2005': 2, '2014': 2, 'variation': 2, 'digit': 2, 'pytorch': 2, 'foundations': 2, 'github': 2, 'thank': 2, 'feedback': 2, 'presidents': 2, 'nigms': 2, 'gg': 2, 'award': 2, 'ss': 2, 'access': 2, 'lpl': 2, 'av': 2, 'agcc': 2, 'comparative': 2, 'increasingly': 2, 'focused': 2, 'visualizing': 2, '1shot': 2, 'misclassified': 2, 'editorial': 2, 'statistics': 2, 'legend': 2, 'effect': 2, 'web': 2, '3813': 2, '930': 2, 'manuscripts': 2, 'please': 2, 'httpszenodoorg': 2, 'participants': 2, 'pertaining': 2, 'investigators': 2, 'groupings': 2, 'social': 2, 'population': 2, 'characteristics': 2, 'recruitment': 2, 'necessary': 2, 'retrospectively': 2, 'sure': 2, 'sections': 2, 'exclusions': 2, 'replication': 2, 'blinding': 2, '850': 1, 'owards': 1, '1234511': 1, 'ding1611': 1, 'lu1234711': 1, '12311': 1, 'guillaume': 1, 'jaume1234': 1, 'song1234': 1, 'bowen': 1, 'chen12': 1, '12348': 1, 'daniel': 1, 'shao12348': 1, 'muhammad': 1, 'shaban1234': 1, 'mane': 1, 'williams12345': 1, 'lukas': 1, 'oldenburg1': 1, 'luca': 1, 'weishaupt12348': 1, 'judy': 1, 'wang1': 1, 'anurag': 1, 'vaidya12348': 1, 'phi': 1, 'le28': 1, 'georg': 1, 'gerber': 1, 'sharifa': 1, 'sahai12349': 1, 'walt': 1, 'williams16': 1, '123410': 1, 'crucial': 1, 'complicating': 1, 'extensively': 1, 'introduce': 1, 'hestained': 1, 'tb': 1, 'staging': 1, 'thousands': 1, 'adept': 1, 'incredibly': 1, 'group': 1, 'simultaneously14': 1, 'contemporary': 1, 'expanded': 1, 'alterations56': 1, 'prognostication79': 1, 'predic': 1, 'tion10': 1, 'applications1114': 1, 'vast': 1, 'scratch': 1, 'gathering': 1, 'received': 1, 'february': 1, 'check': 1, 'updates': 1, 'appears': 1, '851': 1, 'tcgansclc79': 1, 'monly': 1, 'super': 1, 'vised': 1, 'algorithms15374080': 1, 'states81': 1, 'reflective': 1, 'constructing': 1, 'hier': 1, 'archical': 1, 'multiclass': 1, 'necessarily': 1, 'utility': 1, 'richness': 1, 'pretrain': 1, 'subsetted': 1, 'ablating': 1, 'tissuecontaining': 1, 'algo': 1, 'rithm83': 1, '1218': 1, 'trend': 1, '2ce': 1, 'plateauing': 1, '50000': 1, 'images213175': 1, 'variants': 1, 'grows': 1, 'platform85': 1, 'acquiring': 1, 'led': 1, 'reliance': 1, 'proven': 1, 'detection15': 1, 'prediction1617': 1, 'grading18': 1, 'outcome': 1, 'prediction91920': 1, 'data2123': 1, 'fundamental': 1, 'models2427': 1, 'recogni': 1, '2829': 1, '3032': 1, 'sive': 1, 'amounts': 1, 'data3334': 1, '29000': 1, 'types35': 1, '3646': 1, '4753': 1, 'great': 1, 'progress': 1, 'tasks37385466': 1, 'constrained': 1, 'inter': 1, 'tcga691617616774': 1, 'addressing': 1, 'limita': 1, 'critical': 1, 'widespread': 1, 'upon': 1, 'introducing': 1, 'vitl75': 1, 'termed': 1, 'consortium76': 1, 'rich': 1, 'charac': 1, 'terizations': 1, 'biomarkers': 1, '1a': 1, 'yield': 1, '1c': 1, '1d': 1, 'proto': 1, 'highlighting': 1, 'pivotal': 1, 'lies': 1, 'capability': 1, 'deliver': 1, '852': 1, '938': 1, '0022': 1, '2b': 1, 'aggregator': 1, 'gbmlgg': 1, 'local': 1, 'reconstruct': 1, 'endocrine': 1, 'biliary': 1, 'ymphatic': 1, 'soft': 1, 'bone': 1, 'eye': 1, 'organs': 1, '4000': 1, '6000': 1, '8000': 1, 'peritoneum': 1, '36547290': 1, '406080100': 1, 'op43': 1, 'overview': 1, 'achieving': 1, 'algorithm22': 1, 'objective118': 1, 'objective25': 1, 'anatomical': 1, '853': 1, 'able': 1, 'margins': 1, 'investigate': 1, '09710981': 1, '0954': 1, '09460961': 1, '0956': 1, '09490962': 1, '0862': 1, '08500873': 1, '332': 1, 'cb': 1, 'ig': 1, 'oncot': 1, 'ree': 1, '0972': 1, '09680976': 1, '0952': 1, '09460956': 1, '0959': 1, '09550963': 1, '0869': 1, '08600877': 1, 'rine': 1, 'leg': 1, '43way': 1, '108way': 1, 'gen': 1, 'genitalia': 1, 'gi': 1, 'bd': 1, 'macroaveraged': 1, 'op1': 1, '1235': 1, '854': 1, 'camelyon1678': 1, 'urological': 1, 'slides86': 1, 'abmil83': 1, 'contaminated': 1, 'unfairly': 1, 'inflated': 1, '1921': 1, '2235': 1, '264': 1, 'improvements': 1, 'characterized': 1, '2f': 1, '0946': 1, '0014': 1, '2class': 1, '5class': 1, 'tcga8788': 1, 'ebrains89': 1, '12class': 1, '30class': 1, '0076': 1, '3134': 1, '0966': 1, 'motivated': 1, '9195': 1, 'explicitly': 1, 'leaked': 1, 'exhibit': 1, 'optimisti': 1, 'biased': 1, 'studies96': 1, 'gli': 1, 'oma': 1, '973': 1, '947': 1, 'underperforms': 1, 'proteomic': 1, '790': 1, '963': 1, '891': 1, '819': 1, '808': 1, 'underperform': 1, '836': 1, '792': 1, 'emphasize': 1, 'exists': 1, 'themselves': 1, '385997': 1, 'studying': 1, 'scheme': 1, 'generaliza': 1, 'fluctuate': 1, 'repeat': 1, '2gj': 1, 'needs': 1, 'eightfold': 1, 'twice': 1, '2j': 1, 'outper': 1, 'matched': 1, '2i': 1, 'screen': 1, 'classifi': 1, 'cation': 1, 'crc100knonorm98': 1, 'huncrc99': 1, 'opatho100': 1, 'adeno': 1, 'aggc101': 1, 'lympho': 1, 'cyte': 1, 'tcgatils67': 1, 'tumor68': 1, 'discrimi': 1, 'native': 1, 'respectively23': 1, 'score101': 1, 'nearly': 1, '758': 1, '575': 1, '3a': 1, 'gains': 1, 'challeng': 1, '0131': 1, '0027': 1, '253': 1, '3b': 1, '855': 1, '85054': 1, '2968': 1, 'n32361': 1, '56275': 1, '22655': 1, 'perhead': 1, 'acc3': 1, 'mv': 1, 'left': 1, 'overlaid': 1, 'enlarged': 1, 'recallk': 1, 'op': 1, 'normallabeled': 1, 'interpretations': 1, 'prediction1': 1, 'prediction2': 1, 'insets': 1, '657': 1, '0975': 1, '0017': 1, '0974': 1, 'falsenegative': 1, '0246': 1, '0978': 1, '0193': 1, 'accuracies': 1, '952': 1, '965': 1, 'reaches': 1, '974': 1, '983': 1, 'outofthebox': 1, 'classi': 1, 'fiers': 1, 'closer': 1, 'ent': 1, 'ate': 1, 'sue': 1, 'gap': 1, '001': 1, '0188': 1, 'presumably': 1, 'very': 1, 'morphology': 1, 'chal': 1, 'lenging': 1, 'secondbest': 1, 'changes': 1, 'microns': 1, 'alter': 1, 'interpretation': 1, 'cellular': 1, 'degradation': 1, '325': 1, 'do': 1, '2e': 1, 'highlights': 1, 'finergrained': 1, 'obscures': 1, 'localizing': 1, 'detected': 1, 'observations': 1, 'encode': 1, 'semantically': 1, 'valuable': 1, 'known': 1, 'magnifications': 1, 'segpath102': 1, 'segmenting': 1, 'endothe': 1, 'lial': 1, 'leukocytes': 1, 'myeloid': 1, 'mask2former103': 1, 'encoders22104': 1, 'divides': 1, 'separate': 1, 'individually': 1, 'wellknown': 1, 'advantages': 1, 'segmenta': 1, '0827': 1, '0690': 1, '0803': 1, '0003': 1, '0164': 1, '0016': 1, '0008': 1, '0721': 1, '0696': 1, '0695': 1, '0716': 1, 'framework105': 1, 'proposes': 1, 'classification106': 1, 'exemplars': 1, '4a': 1, 'experi': 1, 'mentation': 1, '857': 1, 'chrccccrcc': 1, 'assigns': 1, 'smallest': 1, 'precomputed': 1, 'sharing': 1, 'queried': 1, 'visualizes': 1, 'quartiles': 1, 'fg': 1, 'bounding': 1, '810': 1, '858': 1, 'learner': 1, 'much': 1, 'exceeds': 1, '128shot': 1, '4ce': 1, 'selected': 1, '032159': 1, 'permuting': 1, 'lowest': 1, 'times': 1, 'exceed': 1, 'lowestperforming': 1, 'averag': 1, 'longer': 1, 'accessing': 1, 'curating': 1, 'barriers': 1, 'eases': 1, 'revisit': 1, 'problem': 1, 'prompting55': 1, 'call': 1, '4b': 1, 'patholo': 1, 'gist': 1, 'applying': 1, 'highly': 1, 'righthand': 1, 'side': 1, 'underfit': 1, 'sensitive': 1, 'site': 1, 'underscored': 1, 'disparities': 1, 'alluding': 1, 'skewing': 1, 'dicted': 1, 'showcases': 1, 'discussion': 1, 'containing': 1, 'cancerous': 1, 'pathologies': 1, 'datasets22': 1, 'validated': 1, 'capabili': 1, 'ties': 1, 'versatile': 1, 'devel': 1, 'oping': 1, 'scal': 1, 'affect': 1, 'solutions': 1, 'translatable': 1, 'ences': 1, 'configurations': 1, 'mirrored': 1, 'gaps': 1, 'recently': 1, 'collections107109': 1, 'ssl': 1, 'impactful': 1, 'underperforming': 1, 'counterpart': 1, 'ablations': 1, 'close': 1, 'tandem': 1, 'aforementioned': 1, 'guide': 1, 'practitioners': 1, 'private': 1, 'excels': 1, '859': 1, 'hypothesize': 1, 'paradigms': 1, 'infre': 1, 'quent': 1, 'annotating': 1, 'transformative': 1, 'creative': 1, 'ordinarily': 1, 'orders': 1, 'magnitude': 1, 'presents': 1, 'models110': 1, 'beyond': 1, 'leading': 1, 'carefully': 1, 'target': 1, 'narrow': 1, 'hope': 1, 'directions': 1, 'gener': 1, 'alist': 1, 'flexibility': 1, 'targeting': 1, 'nomenclature': 1, 'vision2275': 1, 'misleading': 1, 'expectations': 1, 'architec': 1, 'lacks': 1, 'visionspecific': 1, 'drastic': 1, 'envision': 1, 'emerge': 1, 'translate': 1, 'demands': 1, 'organizes': 1, 'ating': 1, 'those': 1, 'cytopathology': 1, 'hematopathology': 1, 'breadth': 1, 'certain': 1, '253740112113': 1, 'ranking': 1, 'intentionally': 1, 'overlap': 1, 'significantly': 1, 'acquisition': 1, 'studied': 1, 'reused': 1, 'disparate': 1, 'populations114': 1, 'meaning': 1, 'crossmodal': 1, 'question': 1, 'answering': 1, 'scope': 1, 'concur': 1, 'rent': 1, 'work115116': 1, 'focus': 1, 'models117': 1, 'pathol': 1, 'ogy': 1, 'content': 1, 'sum': 1, 'maries': 1, 'contri': 1, 'butions': 1, 'statements': 1, 'avail': 1, 'bioeng': 1, '930949': 1, 'bera': 1, 'schalper': 1, 'rimm': 1, 'velcheti': 1, 'madabhushi': 1, '703715': 1, '10951110': 1, 'heinz': 1, 'echle': 1, 'bychkov': 1, 'survey': 1, 'stakeholder': 1, '11211127': 1, 'coudray': 1, '15591567': 1, '10541056': 1, 'mobadersany': 1, 'proc': 1, 'natl': 1, 'acad': 1, 'e2970e2979': 1, 'amgad': 1, 'populationlevel': 1, 'enhanced': 1, '8597': 1, 'histologygenomic': 1, '865878': 1, 'vanguri': 1, 'radiology': 1, 'pdl1': 1, 'blockade': 1, '11511164': 1, 'ji': 1, 'genes': 1, 'chromosomes': 1, '540556': 1, 'endoscopic': 1, 'gut': 1, '17091721': 1, 'ozyoruk': 1, 'transforming': 1, 'cryosectioned': 1, '14071419': 1, 'aibased': 1, 'origins': 1, 'unknown': 1, '594': 1, '106110': 1, '555570': 1, 'imagebased': 1, 'clinically': 1, 'actionable': 1, 'genetic': 1, '789799': 1, 'mutations': 1, 'composition': 1, '800810': 1, '154163': 1, 'multistain': 1, '430439': 1, 'coattention': 1, '40154025': 1, 'autoencoders': 1, '1600016009': 1, 'oquab': 1, 'httpsdoiorg1048550arxiv230407193': 1, '860': 1, 'balestriero': 1, 'cookbook': 1, 'httpsdoiorg1048550arxiv230412210': 1, 'xie': 1, 'caron': 1, 'emerging': 1, 'properties': 1, '96509660': 1, 'kornblith': 1, 'norouzi': 1, 'hinton': 1, '15971607': 1, '27': 1, 'grill': 1, 'jb': 1, 'latent': 1, '2127121284': 1, 'deng': 1, '248255': 1, 'russakovsky': 1, '211252': 1, 'shrivastava': 1, 'singh': 1, 'unreasonable': 1, 'era': 1, '843852': 1, 'houlsby': 1, '1210412113': 1, 'goyal': 1, 'mahajan': 1, '63916400': 1, 'bommasani': 1, 'risks': 1, 'httpsdoiorg1048550arxiv210807258': 1, 'yuan': 1, 'florence': 1, 'httpsdoiorg1048550arxiv211111432': 1, 'weinstein': 1, '11131120': 1, 'concepts': 1, 'neurips': 1, '102559': 1, '756779': 1, 'kang': 1, 'park': 1, 'yoo': 1, 'pereira': 1, '33443354': 1, 'eliceiri': 1, 'dualstream': 1, '1431814328': 1, 'lazard': 1, 'lerousseau': 1, 'decencière': 1, 'walter': 1, 'gigassl': 1, '43044313': 1, 'schirris': 1, 'gavves': 1, 'nederlof': 1, 'horlings': 1, 'teuwen': 1, 'deepsmile': 1, 'hrd': 1, '102464': 1, 'vu': 1, 'raza': 1, 'handcrafted': 1, 'h2t': 1, '102743': 1, 'zhao': 1, 'convolution': 1, '48374846': 1, 'retccl': 1, 'clusteringguided': 1, '102645': 1, 'filiot': 1, 'httpsdoiorg101101': 1, '2023072123292757': 1, 'srinidhi': 1, 'fd': 1, 'driven': 1, '102256': 1, 'koohbanani': 1, 'unnikrishnan': 1, 'khurram': 1, 'krishnaswamy': 1, 'selfpath': 1, '28452856': 1, 'ciga': 1, 'self': 1, '100198': 1, 'sgcl': 1, '102845': 1, 'tellez': 1, 'litjens': 1, 'laak': 1, 'compression': 1, 'mach': 1, '567578': 1, 'bianchi': 1, 'yuksekgonul': 1, 'montine': 1, 'zou': 1, 'twitter': 1, '23072316': 1, '53': 1, 'jiang': 1, 'microscopy': 1, '1979819808': 1, 'saldanha': 1, 'npj': 1, 'precis': 1, '1976419775': 1, 'mokhtari': 1, 'histopathologybased': 1, 'inflammatory': 1, '479495': 1, 'jaume': 1, 'interactions': 1, 'pathways': 1, 'httpsdoiorg1048550arxiv230406819': 1, '58': 1, 'histologybased': 1, 'neoadjuvant': 1, 'chemotherapy': 1, 'e2300038': 1, 'wagner': 1, '16501661': 1, 'cellvit': 1, 'precise': 1, '1048550arxiv230615350': 1, 'kaczmarzyk': 1, 'rapid': 1, 'programs': 1, 'biomedicine': 1, '107631': 1, '861': 1, 'locally': 1, '192201': 1, 'nasrallah': 1, 'cryosection': 1, 'who': 1, '526540': 1, 'variational': 1, 'bottleneck': 1, '74547463': 1, 'ikezogwo': 1, 'quilt1m': 1, 'imagetext': 1, 'pairs': 1, 'inferring': 1, 'superresolution': 1, 'integrating': 1, 'transcriptomics': 1, 'biotechnol': 1, 'httpsdoiorg101038s41587023020199': 1, 'saltz': 1, 'organization': 1, 'correlation': 1, 'tumorinfiltrating': 1, '181193': 1, 'encoding': 1, 'texture': 1, '110424': 1, 'kalra': 1, 'yottixel': 1, 'engine': 1, 'archives': 1, '101757': 1, 'schmauch': 1, 'rnaseq': 1, 'express': 1, 'ion': 1, '3877': 1, 'enables': 1, 'simultaneous': 1, '102685': 1, 'diao': 1, 'humaninterpretable': 1, 'densely': 1, '1613': 1, 'wulczyn': 1, 'e0233678': 1, '74': 1, 'riasatian': 1, 'densenet': 1, '102032': 1, 'dosovitskiy': 1, 'worth': 1, '1616': 1, 'words': 1, 'pilot': 1, 'multitissue': 1, 'gene': 1, 'regulation': 1, 'humans': 1, '348': 1, '648660': 1, 'kundra': 1, '221230': 1, 'bejnordi': 1, 'women': 1, 'jama': 1, '318': 1, '21992210': 1, 'campbell': 1, 'carcinomas': 1, '607616': 1, 'shao': 1, 'transmil': 1, 'correlated': 1, 'clinicalgrade': 1, '13011309': 1, 'gatta': 1, 'burden': 1, 'centralised': 1, 'treatment': 1, 'europe': 1, 'rarecarenet': 1, 'populationbased': 1, '10221039': 1, 'ilse': 1, 'tomczak': 1, '21322141': 1, 'ren': 1, '770778': 1, '101854': 1, 'learningenabled': 1, '575582': 1, 'brennan': 1, '462477': 1, '372': 1, '24812498': 1, 'roetzerpejrimovsky': 1, 'pati': 1, 'joint': 1, '1048550arxiv230102933': 1, 'jacovi': 1, 'caciularu': 1, 'goldman': 1, 'goldberg': 1, 'stop': 1, 'uploading': 1, 'strategies': 1, 'mitigating': 1, 'httpsdoiorg1048550arxiv230510160': 1, '92': 1, 'magar': 1, 'schwartz': 1, 'memorization': 1, 'exploitation': 1, '60th': 1, 'annual': 1, 'meeting': 1, '157165': 1, 'brown': 1, '18771901': 1, 'dodge': 1, 'documenting': 1, 'webtext': 1, 'corpora': 1, 'colossal': 1, 'clean': 1, 'crawled': 1, '12861305': 1, 'kapoor': 1, 'narayanan': 1, 'crisis': 1, 'machinelearningbased': 1, '100804': 1, 'xiang': 1, 'lowrank': 1, '97': 1, 'niehues': 1, 'generalizable': 1, '100980': 1, '98': 1, 'multicenter': 1, 'e1002730': 1, 'pataki': 1, 'á': 1, 'enhance': 1, '370': 1, 'barbano': 1, 'polyps': 1, 'icip': 1, '7680': 1, 'huo': 1, 'scanning': 1, 'cloudbased': 1, 'interaction': 1, 'httpsdoiorg102139ssrn4172090': 1, '102': 1, 'restainingbased': 1, 'annotationrelated': 1, '100688': 1, '103': 1, 'cheng': 1, 'schwing': 1, 'girdhar': 1, 'maskedattention': 1, '862': 1, 'eva': 1, 'limits': 1, '1935819369': 1, 'chao': 1, 'wl': 1, 'weinberger': 1, 'maaten': 1, 'nearestneighbor': 1, '191104623': 1, 'snell': 1, 'swersky': 1, '107': 1, 'vorontsov': 1, 'virchow': 1, 'millionslide': 1, '230907778': 1, 'httpsdoiorg1048550arxiv231007033': 1, 'lai': 1, 'domainspecific': 1, 'httpsdoiorg1048550arxiv231013259': 1, '110': 1, 'moor': 1, '616': 1, '259265': 1, 'adapter': 1, 'sclwc': 1, 'crossslide': 1, '1800918021': 1, '113': 1, '19201929': 1, '114': 1, 'algorithmic': 1, 'fairness': 1, 'healthcare': 1, '719742': 1, 'arxiv230712914': 1, '116': 1, 'assistant': 1, '1048550arxiv231207814': 1, 'publishers': 1, 'remains': 1, 'neutral': 1, 'jurisdictional': 1, 'claims': 1, 'maps': 1, 'licensor': 1, 'partner': 1, 'holds': 1, 'rights': 1, 'rightsholders': 1, 'selfarchiving': 1, 'governed': 1, 'law': 1, 'licence': 1, 'america': 1, 'inc': 1, '1department': 1, '2department': 1, '3cancer': 1, 'broad': 1, '4cancer': 1, 'danafarber': 1, '5department': 1, 'informatics': 1, '6harvard': 1, 'john': 1, 'paulson': 1, '7electrical': 1, '8health': 1, 'harvardmit': 1, '9department': 1, 'biology': 1, '10harvard': 1, 'initiative': 1, '11these': 1, 'contributed': 1, 'equally': 1, 'ding': 1, 'mass': 1, 'board': 1, 'ret': 1, 'rospective': 1, 'electronic': 1, 'records': 1, 'deidentified': 1, 'archival': 1, 'underdiscussed': 1, 'integral': 1, 'lifecycle': 1, 'communityaccepted': 1, 'goodness': 1, 'fit113119': 1, 'activity': 1, 'readily': 1, 'consid': 1, 'erably': 1, 'restricts': 1, 'adaptability': 1, 'tcga35': 1, 'opting': 1, 'realize': 1, 'learning3738117': 1, 'applicability': 1, 'restricted': 1, '374041444657117': 1, 'analyses691617616774': 1, 'extensive': 1, 'camelyon78120': 1, 'tcgapretrained': 1, 'limitation': 1, 'partitions': 1, 'observation': 1, '212375': 1, 'analo': 1, 'gous': 1, '16059454': 1, '1064615': 1, 'tology': 1, 'clusteringconstrained': 1, 'multipleinstance': 1, 'performs': 1, 'thresholding': 1, 'saturation': 1, 'channel': 1, 'rgb': 1, 'hsv': 1, 'blurring': 1, 'closing': 1, 'filtering': 1, 'remove': 1, 'coordinate': 1, 'vd142m22': 1, 'jft300m30': 1, 'combines': 1, '24782': 1, 'noncancerous': 1, 'autopsy': 1, 'purposefully': 1, 'preprocess': 1, '75832905': 1, '24297995': 1, 'magnificatin': 1, 'exten': 1, 'sion': 1, 'layout': 1, 'distilla': 1, 'tion117': 1, 'byol27': 1, 'minimizes': 1, 'predictive': 1, 'categorical': 1, 'tributions': 1, 'eacher': 1, 'net': 1, 'minimizing': 1, 'exponential': 1, 'moving': 1, 'strategically': 1, 'masking': 1, 'remaining': 1, 'contextual': 1, 'captures': 1, 'highlevel': 1, 'bert121': 1, 'subsequently': 1, 'propagated': 1, 'receives': 1, 'regarded': 1, 'extractors': 1, 'vitbased': 1, 'performance21122': 1, 'feasible': 1, 'cost': 1, 'collecting': 1, 'smallerscale': 1, 'family': 1, 'introduction': 1, 'intro': 1, 'duction': 1, 'thereby': 1, 'improving': 1, 'modi': 1, 'fications': 1, 'summarized': 1, 'tying': 1, 'softmaxcentering': 1, 'koleo': 1, 'diversity123': 1, 'pretraining124': 1, 'ments': 1, 'flashattention125': 1, 'sharded': 1, 'stochastic': 1, 'depth': 1, 'vd142m': 1, 'leverage': 1, 'fair': 1, 'ibottrained': 1, '12500': 1, 'resnet5084': 1, 'imagenet28': 1, '8543296': 1, 'tasks1520': 1, 'parison': 1, 'transformer126': 1, 'tiny': 1, 'configu': 1, 'ration': 1, 'window': 1, '28289038': 1, 'resnet152': 1, '232230016': 1, 'transfermedium': 1, 'protocol127': 1, 'simclr26': 1, 'distributions': 1, '29753': 1, '2457': 1, 'paip85': 1, '15580262': 1, '32120': 1, '29018': 1, 'simclr': 1, 'upwards': 1, 'clam15': 1, 'dings': 1, 'outputted': 1, '1024': 1, '768': 1, '4096': 1, 'resnet': 1, 'gridlike': 1, 'map': 1, '4096dimensions': 1, 'twodimensional': 1, 'adap': 1, '4096dimensional': 1, 'operations': 1, 'magni': 1, 'fication': 1, 'take': 1, 'divisible': 1, 'magnification5559128': 1, 'malization': 1, 'vitl16': 1, 'ablation': 1, 'inflates': 1, 'outside': 1, 'ventional': 1, 'twostage': 1, 'preextraction': 1, 'permutationinvariant': 1, 'operator': 1, 'aggregates': 1, 'pipeline': 1, 'down': 1, 'oversegmented': 1, 'adjusted': 1, 'segment': 1, 'second': 1, 'nonhe': 1, 'nonffpe': 1, 'mag': 1, 'nification': 1, 'pyramidal': 1, 'format': 1, 'later': 1, 'algorithm83': 1, 'canonical': 1, 'twolayer': 1, 'gated': 1, 'hidden': 1, 'layers': 1, 'dropout': 1, '010': 1, 'adamw': 1, 'optimizer129': 1, 'cosine': 1, 'scheduler': 1, 'whereas': 1, 'sitestratified': 1, 'probing130': 1, 'tasks23': 1, 'recommended': 1, 'fix': 1, 'λ': 1, 'mc': 1, 'lbfgs': 1, 'solver131': 1, 'iterations113': 1, 'technique': 1, 'advocated': 1, 'measuring': 1, 'repre': 1, 'sentation': 1, 'features25132133': 1, 'tance': 1, 'works25': 1, 'devia': 1, 'changing': 1, 'carry': 1, 'evaluations': 1, 'preextract': 1, 'archi': 1, 'tecture': 1, 'transformert': 1, 'passes': 1, 'illustrate': 1, 'converted': 1, 'grid': 1, 'kernel': 1, 'stride': 1, 'incoming': 1, 'chan': 1, 'nels': 1, 'rgbinput': 1, 'outgoing': 1, 'channels': 1, 'flattening': 1, 'transposing': 1, '12544': 1, 'feeding': 1, 'pass': 1, 'computationally': 1, 'expensive': 1, 'tractable': 1, 'memeffattention': 1, 'notebook': 1, 'base117': 1, 'produced': 1, 'dif': 1, 'ferent': 1, 'treat': 1, 'candidates': 1, 'place': 1, 'ity': 1, 'sorted': 1, 'success': 1, 'encoders103': 1, 'nonhierarchical': 1, 'alongside': 1, 'head111': 1, 'ade20k': 1, 'specifi': 1, 'adamw129': 1, 'optimizer': 1, 'step': 1, 'schedule': 1, 'initial': 1, '00001': 1, 'adjust': 1, 'multiplier': 1, 'factor': 1, '095': 1, 'fractions': 1, 'finetune': 1, 'every': 1, 'saved': 1, 'augment': 1, 'jittering': 1, 'lsj': 1, 'augmentation135': 1, '0520': 1, 'crop': 1, 'accommodate': 1, 'models105136': 1, 'pro': 1, 'totype': 1, 'longstanding': 1, 'commu': 1, 'nity106137138': 1, 'posed': 1, 'well43139142': 1, 'contrast': 1, 'traditional': 1, 'metalearning': 1, 'transformations': 1, 'tasks105136143': 1, 'litera': 1, 'drawn': 1, 'ways': 1, 'queries': 1, 'near': 1, 'nearestcentroid': 1, 'protonet106': 1, 'vector': 1, 'surprisingly': 1, 'popularized': 1, 'models136': 1, 'recommendations': 1, 'transformed': 1, 'centered': 1, 'centroids': 1, 'drawing': 1, 'equiva': 1, 'lent': 1, 'draw': 1, 'episode': 1, '4060': 1, 'serving': 1, 'classification55': 1, 'brief': 1, 'ranges': 1, 'tcgaluad': 1, 'averagepooled': 1, 'little': 1, 'considering': 1, 'taking': 1, 'unweighted': 1, 'account': 1, 'harmonic': 1, 'plotting': 1, 'truepositive': 1, 'falsepositive': 1, 'varied': 1, 'interannotator': 1, 'scored': 1, 'retrieving': 1, 'strictly': 1, 'defini': 1, 'semi': 1, 'estimate': 1, 'bootstrapping': 1, 'replicates': 1, 'significance': 1, 'permutations': 1, 'outline': 1, 'span': 1, 'histopathologyspecific': 1, 'elaborated': 1, 'captions': 1, 'devised': 1, '7129': 1, '39441620': 1, 'lobular': 1, 'rectum': 1, 'endometrioid': 1, 'uec': 1, 'stomach': 1, 'dlbclnos': 1, 'mel': 1, 'pancreatic': 1, 'cscc': 1, 'smallcell': 1, 'sclc': 1, 'gastroesophageal': 1, 'gej': 1, 'chronic': 1, 'leukemiasmall': 1, 'cllsll': 1, 'project82': 1, 'op1k': 1, 'op22k': 1, 'held': 1, 'practices': 1, 'extracting': 1, 'clustering144': 1, 'wsisa145': 1, 'deepattnmisl146147': 1, 'others148': 1, 'bagofwords': 1, 'vbow149150': 1, 'formulating': 1, 'collec': 1, 'features151152': 1, 'mil145148': 1, 'retrieval69153': 1, 'extract': 1, 'lection': 1, 'derive': 1, 'fall': 1, 'never': 1, 'visible': 1, 'fitted': 1, 'validate': 1, 'marginal': 1, '0007': 1, 'lapses': 1, '593': 1, '412': 1, 'hypoth': 1, 'esize': 1, 'difficult': 1, 'reducing': 1, 'finding': 1, 'sentinel': 1, 'utrecht': 1, 'meta': 1, 'stasis': 1, 'detection78': 1, 'mislabeled': 1, '399': 1, '9010': 1, 'trainvalidation': 1, '61732': 1, '24327129': 1, 'chronological': 1, 'timeline': 1, 'peerreviewed': 1, 'sophis': 1, 'ticated': 1, 'cases79154155': 1, 'metadata': 1, '1041': 1, '529': 1, '578': 1, '513': 1, '8489798': 1, '563': 1, '485': 1, 'resections': 1, 'dartmouthhitchcock': 1, 'oncocyto': 1, 'slides156': 1, '70426': 1, '39323147': 1, 'moved': 1, 'absent': 1, '1794': 1, 'chrcc156160': 1, 'lowresolution': 1, 'downsamples': 1, '922': 1, '519': 1, '294': 1, 'filtered': 1, 'oncocytomas': 1, 'dhmckidney': 1, '468': 1, '7368997': 1, 'hungarian': 1, 'semmelweis': 1, 'university99': 1, '4way': 1, 'nonneoplastic': 1, 'lesion': 1, 'studys': 1, '1582121': 1, '547': 1, '187': 1, 'irccs': 1, 'fon': 1, 'dazione': 1, 'pascale': 1, 'networking': 1, 'icar': 1, 'council': 1, 'cnr': 1, 'ibm': 1, 'researchzurich161': 1, '3way': 1, 'define': 1, '7way': 1, 'usual': 1, 'flat': 1, '721216': 1, '3956587': 1, '1996': 1, 'glioblas': 1, 'toma': 1, 'atlas8789': 1, '5way': 1, '257': 1, 'oligodendro': 1, '408': 1, 'wildtype': 1, '1094': 1, 'simpler': 1, '2way': 1, '1238': 1, '756': 1, 'tcgagbmlgg': 1, '472231': 1, '525243355': 1, 'vienna89': 1, '3114': 1, '30way': 1, 'lim': 1, 'ited': 1, '474': 1, 'pilocytic': 1, 'menin': 1, 'gothelial': 1, 'pituitary': 1, 'ganglioglioma': 1, 'hemangioblastoma': 1, 'adamantinomatous': 1, 'craniopharyngioma': 1, 'schwannoma': 1, 'transitional': 1, 'nervous': 1, 'gliosar': 1, 'coma': 1, 'fibrous': 1, 'epend': 1, 'ymoma': 1, 'dh1mutant': 1, 'ependymoma': 1, 'secretory': 1, 'lipoma': 1, 'hemangioperi': 1, 'cytoma': 1, 'nonwnt': 1, 'nonshh': 1, 'medulloblastoma': 1, 'langerhans': 1, 'histiocytosis': 1, 'angiomatous': 1, 'hemangioma': 1, '12way': 1, 'adulttype': 1, '837': 1, 'meningiomas': 1, '430': 1, 'mesenchymal': 1, 'nonmeningothelial': 1, '190': 1, 'sellar': 1, 'circumscribed': 1, 'astrocytic': 1, 'ependymal': 1, 'hematolymphoid': 1, 'glioneuronal': 1, 'neuronal': 1, 'cranial': 1, 'paraspinal': 1, 'nerve': 1, 'pediatrictype': 1, 'embryonal': 1, '1151595573': 1, '10616': 1, 'pros': 1, 'tate': 1, 'needle': 1, 'institute18162': 1, 'defines': 1, '6class': 1, 'excluding': 1, 'erroneously': 1, 'kagglecomcompetitionsprostatecancergradeassessment': 1, 'discussion169230': 1, 'had': 1, 'noisy': 1, 'httpswwwkagglecom': 1, 'competitionsprostatecancergradeassessmentdiscussion169230': 1, '9555': 1, '2603': 1, 'g0': 1, 'g1': 1, '1209': 1, 'g2': 1, '1118': 1, '1124': 1, '1102': 1, '7647954954': 1, 'reevaluate': 1, 'strategy': 1, 'separately': 1, '5021': 1, '1688': 1, 'embs': 1, '2444': 1, '2577': 1, 'others86': 1, '3547484900': 1, '1192164332': 1, 'shares': 1, 'crane': 1, 'endpoints': 1, 'emb': 1, '107180': 1, 'nocarcinoma': 1, 'mannheim': 1, 'archive98': 1, 'adipose': 1, '11745': 1, '11413': 1, 'debris': 1, '11851': 1, '12191': 1, 'mucus': 1, '9931': 1, '14128': 1, '9504': 1, 'cancerassociated': 1, '10867': 1, '15550': 1, '1000007180': 1, 'structed': 1, 'crcvalhe7k': 1, 'patchcamelyon': 1, 'pcam163': 1, 'wilds164': 1, '417894': 1, '092100': 1, 'sec': 1, 'challenge120': 1, 'refers': 1, '302436': 1, 'outofdistribution': 1, 'od': 1, 'contain': 1, '34904': 1, 'valod': 1, '80554': 1, 'estod': 1, 'httpswildsstan': 1, 'fordeduleaderboard': 1, '101398': 1, 'task99': 1, 'nine': 1, '4315': 1, '2281': 1, '55787': 1, 'inflammation': 1, '763': 1, 'necrosis': 1, '365': 1, 'suspicious': 1, 'invasion': 1, '570': 1, 'edge': 1, '534': 1, '3470': 1, '31323': 1, '15149': 1, '7675322655': 1, '055': 1, 'iciar': 1, 'bach165': 1, '32080': 1, '072': 1, '52713': 1, '502': 1, 'helsinki': 1, 'samples166': 1, '13057': 1, '8652': 1, '5460': 1, '996': 1, 'empty': 1, '16026': 1, 'textures': 1, '8522': 1, 'stratifying': 1, 'ambiguities': 1, 'category': 1, '21095': 1, '6074': 1, '210956074': 1, '029': 1, 'agency': 1, 'echnology': 1, 'astar': 1, 'singapore101': 1, '203': 1, 'prostatectomies': 1, 'akoya': 1, 'biosciences': 1, 'pixellevel': 1, 'delineating': 1, 'stromal': 1, 'built': 1, '1125640': 1, 'non': 1, '780619345021': 1, 'aggressive': 1, '367229': 1, '078': 1, '320': 1, 'esophagogas': 1, 'tric': 1, 'cologne': 1, 'landesklinikum': 1, 'wiener': 1, 'neustadt': 1, 'berlincharité': 1, '214': 1, 'slides167': 1, 'adventitia': 1, '71131': 1, 'lamina': 1, '2173': 1, '2951': 1, '83358': 1, '56490': 1, 'gastric': 1, '44416': 1, 'muscosa': 1, 'esophagus': 1, '18561': 1, '22117': 1, 'submucosal': 1, 'glands': 1, '1516': 1, '63863': 1, 'ulceration': 1, '753': 1, 'bined': 1, '189142': 1, '5149': 1, '089': 1, '9536': 1, '044': 1, '292': 1, 'colo': 1, 'rectal': 1, 'turin100': 1, '950': 1, '545': 1, '454': 1, '3618': 1, '916': 1, '2186': 1, '62702399': 1, '180': 1, '090': 1, '51918': 1, 'topathology': 1, 'prenormalized': 1, 'malization6': 1, 'according': 1, 'patientlevel': 1, 'instable': 1, '15002': 1, 'stable': 1, '36916': 1, 'evalua': 1, '1955732361': 1, 'tcga68': 1, 'adrenocortical': 1, '4980': 1, '9990': 1, '23530': 1, '23690': 1, 'cervical': 1, 'endocervical': 1, 'cesc': 1, '6270': 1, 'cholangiocarcinoma': 1, 'chol': 1, '900': 1, '8150': 1, '3380': 1, 'multiforme': 1, '23740': 1, '11790': 1, 'kich': 1, '2460': 1, 'kirc': 1, '11650': 1, 'kirp': 1, '6790': 1, 'hepatocellular': 1, 'car': 1, 'lihc': 1, '8370': 1, '16460': 1, '16560': 1, 'lymphoid': 1, 'neoplasm': 1, 'dlbc': 1, '840': 1, 'mesothelioma': 1, 'meso': 1, '2090': 1, 'ovarian': 1, 'serous': 1, 'cystadenocar': 1, 'ov': 1, '2520': 1, '4090': 1, 'pheochromocytoma': 1, 'paraganglioma': 1, 'pcpg': 1, '1350': 1, '9810': 1, '1880': 1, 'sarcoma': 1, 'sarc': 1, '13480': 1, '10060': 1, '9670': 1, 'testicular': 1, 'germ': 1, 'tgct': 1, '6010': 1, 'thymoma': 1, 'thym': 1, '3600': 1, 'thyroid': 1, '11360': 1, 'carcinosarcoma': 1, 'ucs': 1, '2120': 1, 'endometrial': 1, '12480': 1, 'uveal': 1, 'uvm': 1, '1640': 1, '21635055360': 1, 'tcga168': 1, '304097': 1, '6167170': 1, 'tilpositive': 1, 'tils': 1, '54910': 1, 'tilnegative': 1, '249187': 1, '2092213860156275': 1, 'combine': 1, '020': 1, '158687': 1, '022': 1, 'okyo': 1, 'hospital102': 1, 'endothelium': 1, '10647': 1, '26509': 1, 'leukocyte': 1, '24805': 1, '12273': 1, 'mye': 1, 'loid': 1, '14135': 1, '13231': 1, '25909': 1, '31178': 1, 'suecell': 1, 'nontissuecell': 1, 'approximate': 1, 'interpolated': 1, 'bound': 1, 'hardware': 1, 'v3813': 1, 'orch171': 1, 'v200': 1, 'unless': 1, 'speci': 1, 'fied': 1, 'v092': 1, 'httpshuggingfaceco': 1, 'graphics': 1, 'config': 1, 'ured': 1, 'computations': 1, 'v431': 1, 'v120': 1, 'v121': 1, 'httpsgithubcommbananilgssl': 1, 'mentations': 1, 'fer': 1, 'comgoogleresearchmedicalairesearchfoundations': 1, 'fulfillment': 1, 'submitted': 1, 'website': 1, 'physionetorgcontentmedicalairesearchfoundation172173': 1, 'httpsgithubcommahmoodlab': 1, 'v06': 1, 'older': 1, 'compatibility': 1, 'v38': 1, 'orch': 1, 'v190': 1, 'adding': 1, 'czczupvitadapter': 1, 'v930': 1, 'v371': 1, 'v0122': 1, 'miscellaneous': 1, 'linked': 1, 'httpsportal': 1, 'gdccancergov': 1, 'proteomics': 1, 'httpsproteomic': 1, 'datacommonscancergov': 1, 'gtexportalorghome': 1, 'analyzed': 1, '106084m9figsharec5927795v1': 1, '107937': 1, 'tcia9cjf0127': 1, 'httpsiciar2018challengegrandchallenge': 1, 'orgdataset': 1, 'httpszenodoorgrecord7898308': 1, 'httpszenodoorgrecord6604094': 1, 'httpszenodo': 1, 'orgrecord4643645': 1, 'escahttpszenodoorgrecord7548828': 1, 'httpswildsstanfordedudatasets': 1, '1025493wq48zgx': 1, 'httpsbmirdsgithubiokidney': 1, 'httpsbracsicarcnrit': 1, 'httpspanda': 1, 'grandchallengeorg': 1, 'httpszenodoorgrecord6460100': 1, 'hun': 1, 'archive175': 1, 'httpsaggc22grandchallenge': 1, 'org': 1, 'pending': 1, 'publication': 1, 'granted': 1, 'httpsgithubcommahmoodlabuni': 1, 'docu': 1, 'accessible': 1, 'scientific': 1, 'audience': 1, '119': 1, 'oliver': 1, 's4l': 1, 'semisupervised': 1, '14761485': 1, 'bandi': 1, 'devlin': 1, 'chang': 1, 'toutanova': 1, 'bidirectional': 1, 'north': 1, 'american': 1, 'chapter': 1, 'technologies': 1, '122': 1, 'designing': 1, 'sparse': 1, 'sablayrolles': 1, 'schmid': 1, 'jégou': 1, 'spreading': 1, '124': 1, 'touvron': 1, 'vedaldi': 1, 'jegou': 1, 'fixing': 1, 'discrepancy': 1, 'eds': 1, 'wallach': 1, 'curran': 1, 'associates': 1, '125': 1, 'dao': 1, 'ermon': 1, 'rudra': 1, 'ré': 1, 'ioawareness': 1, '126': 1, 'liu': 1, 'windows': 1, '1001210022': 1, '127': 1, 'bit': 1, '491507': 1, 'hu': 1, 'cw': 1, 'interventional': 1, '1983019839': 1, 'loshchilov': 1, 'hutter': 1, 'decoupled': 1, '130': 1, 'bentley': 1, 'trees': 1, 'associative': 1, 'searching': 1, 'communications': 1, '509517': 1, '1975': 1, '131': 1, 'byrd': 1, 'nocedal': 1, '778': 1, 'lbfgsb': 1, 'fortran': 1, 'subroutines': 1, 'boundconstrained': 1, 'mathematical': 1, '1997': 1, '132': 1, 'sarıyıldız': 1, 'kalantidis': 1, 'alahari': 1, 'larlus': 1, 'reason': 1, '133': 1, 'seed': 1, '134': 1, 'pedregosa': 1, '28252830': 1, '135': 1, 'ghiasi': 1, 'copypaste': 1, 'augmentation': 1, '29182928': 1, 'el': 1, 'banani': 1, 'desai': 1, 'johnson': 1, 'languageguided': 1, '1920819220': 1, '137': 1, 'koch': 1, 'salakhutdinov': 1, 'siamese': 1, '32nd': 1, '138': 1, 'vinyals': 1, 'blundell': 1, 'lillicrap': 1, 'kavukcuoglu': 1, 'wierstra': 1, '139': 1, 'jg': 1, '102748': 1, '140': 1, 'slpd': 1, '259269': 1, '141': 1, 'quiros': 1, 'histomorphological': 1, 'unlabeled': 1, 'unannotated': 1, 'arxiv220501931': 1, 'yang': 1, 'yan': 1, 'lowshot': 1, '143': 1, 'tenenbaum': 1, 'isola': 1, 'rethinking': 1, 'xiv': 1, '266282': 1, 'lloyd': 1, 'squares': 1, 'quantization': 1, 'pcm': 1, 'theory': 1, '129137': 1, '1982': 1, '145': 1, 'wsisa': 1, '72347242': 1, '146': 1, '496504': 1, 'jonnagaddala': 1, 'hawkins': 1, '101789': 1, '148': 1, '174182': 1, '149': 1, 'sivic': 1, 'zisserman': 1, 'google': 1, 'object': 1, 'videos': 1, 'ninth': 1, '14701477': 1, '2003': 1, '150': 1, 'feifei': 1, 'perona': 1, 'scene': 1, 'cvpr05': 1, '524531': 1, '151': 1, 'cruzroa': 1, 'caicedo': 1, 'gonzález': 1, 'mining': 1, 'artif': 1, '91106': 1, '152': 1, '591604': 1, '153': 1, '14201434': 1, '154': 1, 'gillette': 1, 'vulnerabilities': 1, '182': 1, '200225': 1, 'satpathy': 1, 'portrait': 1, '43484371': 1, 'biopsy': 1, 'surgical': 1, '7080': 1, '157': 1, '499': 1, '4349': 1, '158': 1, 'renalcell': 1, '374': 1, '135145': 1, '159': 1, 'davis': 1, '319330': 1, 'heterogeneity': 1, 'aggressiveness': 1, '139163': 1, 'brancati': 1, 'baac093': 1, '162': 1, '233241': 1, '163': 1, 'veeling': 1, 'linmans': 1, 'winkens': 1, 'cohen': 1, 'rotation': 1, 'equivariant': 1, 'assisted': 1, 'interventionmiccai': 1, '21st': 1, 'granada': 1, 'spain': 1, 'september': 1, 'ii': 1, '210218': 1, '164': 1, 'koh': 1, 'inthewild': 1, 'shifts': 1, '56375664': 1, '165': 1, 'aresta': 1, '122139': 1, '166': 1, 'brummer': 1, 'pölönen': 1, 'mustjoki': 1, 'brück': 1, 'textural': 1, 'harmonises': 1, 'fingerprints': 1, 'british': 1, '683695': 1, '167': 1, 'tolkach': 1, 'oesophageal': 1, 'e265e275': 1, '168': 1, 'howard': 1, 'signatures': 1, 'bias': 1, '4423': 1, '169': 1, 'symposium': 1, 'nano': 1, '11071110': 1, '170': 1, 'abousamra': 1, 'infiltrating': 1, 'front': 1, '806603': 1, '171': 1, 'paszke': 1, 'imperative': 1, '172': 1, 'goldberger': 1, 'physiobank': 1, 'physiotoolkit': 1, 'physiologic': 1, 'signals': 1, 'circulation': 1, 'e215e220': 1, 'httpsdoiorg1013026grp0z205': 1, 'wu': 1, 'massa': 1, 'wy': 1, 'girshick': 1, 'httpsgithubcomfacebookresearchdetectron2': 1, '175': 1, 'clark': 1, 'tcia': 1, 'maintaining': 1, '10451057': 1, 'darcet': 1, 'dynamics': 1, 'fund': 1, 'r35gm138216': 1, 'scholar': 1, 'r35gm149270': 1, 'niddk': 1, 'p30dk034854': 1, 'nsf': 1, 'graduate': 1, 'seas': 1, 'siebel': 1, 'scholars': 1, 'nci': 1, 'kirschstein': 1, 'service': 1, 't32ca251062': 1, 'german': 1, 'exchange': 1, 'daad': 1, 'janicki': 1, 'kenny': 1, 'administration': 1, 'staff': 1, 'mgb': 1, 'enterprise': 1, 'infrastructure': 1, 'services': 1, 'eris': 1, 'vatanian': 1, 'thiagarajan': 1, 'fevriersullivan': 1, 'kirby': 1, 'navigating': 1, 'contributions': 1, 'conceived': 1, 'jjw': 1, 'organized': 1, 'codebases': 1, 'ww': 1, 'prepared': 1, 'coauthors': 1, 'inventors': 1, 'provisional': 1, 'us': 1, 'patent': 1, 'application': 1, '63611059': 1, 'filed': 1, 'methodological': 1, 'aspects': 1, 'declare': 1, 'correspondence': 1, 'addressed': 1, 'thanks': 1, 'beck': 1, 'francesco': 1, 'contribution': 1, 'handling': 1, 'editor': 1, 'lorenzo': 1, 'righetto': 1, 'collaboration': 1, 'team': 1, 'reprints': 1, 'permissions': 1, 'wwwnaturecomreprints': 1, 'qualitative': 1, 'illustrations': 1, 'panel': 1, 'affects': 1, '20nn': 1, 'structures': 1, 'neither': 1, 'contributory': 1, 'focusing': 1, 'areas': 1, 'like': 1, 'ukkwnstcga': 1, 'assigning': 1, 'top50': 1, 'showing': 1, 'green': 1, 'since': 1, 'misclassify': 1, 'vice': 1, 'versa': 1, 'incorrectly': 1, 'agree': 1, 'nonluad': 1, 'luadlike': 1, 'pathologistannotated': 1, 'retried': 1, 'mismatch': 1, 'misclassifies': 1, 'dec': 1, 'wishes': 1, 'publish': 1, 'form': 1, 'structure': 1, 'transparency': 1, 'checklist': 1, 'confirm': 1, 'items': 1, 'confirmed': 1, 'groupcondition': 1, 'discrete': 1, 'measurement': 1, 'measurements': 1, 'assumptions': 1, 'corrections': 1, 'normality': 1, 'adjustment': 1, 'tendency': 1, 'means': 1, 'associated': 1, 'uncertainty': 1, 'null': 1, 'hypothesis': 1, 'statistic': 1, 'degrees': 1, 'freedom': 1, 'give': 1, 'whenever': 1, 'suitable': 1, 'priors': 1, 'markov': 1, 'chain': 1, 'monte': 1, 'carlo': 1, 'designs': 1, 'identification': 1, 'pearsons': 1, 'calculated': 1, 'biologists': 1, 'articles': 1, 'httpgithubcommahmoodlabclam': 1, '4x8': 1, '80gb': 1, 'configured': 1, '24gb': 1, '092': 1, 'huggingfaceco': 1, '431': 1, 'githubcommbananilgssl': 1, 'githubcomgoogleresearchmedicalairesearch': 1, 'mahmoodlabhipt': 1, 'githubcommahmoodlabclam': 1, 'vitadatper': 1, 'githubcomczczupvitadapter': 1, '371': 1, '0122': 1, 'utilizing': 1, 'custom': 1, 'yet': 1, 'editors': 1, 'reviewers': 1, 'strongly': 1, 'encourage': 1, 'deposition': 1, 'guidelines': 1, 'submitting': 1, 'accession': 1, 'identifiers': 1, 'restrictions': 1, 'party': 1, 'ensure': 1, 'adheres': 1, 'httpsportalgdccancergov': 1, 'httpswwwgtexportalorghome': 1, 'httpsdoiorg106084': 1, 'm9figsharec5927795v1': 1, 'httpsdoiorg107937tcia9cjf0127': 1, 'httpsiciar2018challengegrandchallengeorgdataset': 1, 'httpszenodoorgrecord7898308zgxm3xbxac': 1, 'zenodoorgrecord5889558': 1, 'httpszenodoorgrecord4643645': 1, 'record7548828zenmnnlmjh5': 1, 'httpsdoiorg1025493wq48zgx': 1, 'httpsbmirdsgithubiokidneycancer': 1, 'wwwbracsicarcnrit': 1, 'httpspandagrandchallengeorgdata': 1, 'record6460100': 1, 'shared': 1, 'formal': 1, 'identitypresentation': 1, 'sexual': 1, 'orientation': 1, 'racism': 1, 'covariate': 1, 'though': 1, 'refer': 1, 'readers': 1, 'descriptions': 1, 'aggregate': 1, 'selfreported': 1, '3080': 1, '2474': 1, 'socially': 1, 'ethics': 1, 'oversight': 1, 'irb': 1, 'committee': 1, 'analyzing': 1, 'approval': 1, 'fieldspecific': 1, 'fit': 1, 'selection': 1, 'behavioural': 1, 'ecological': 1, 'evolutionary': 1, 'environmental': 1, 'reference': 1, 'copy': 1, 'document': 1, 'naturecomdocumentsnrreportingsummaryflatpdf': 1, 'disclose': 1, 'disclosure': 1, 'calculations': 1, 'budget': 1, 'file': 1, 'adequate': 1, 'subsection': 1, 'attempts': 1, 'wwwgithubcommahmoodlabuni': 1, 'randomization': 1, 'them': 1, 'straitfied': 1, 'proportions': 1, 'regionlevel': 1, 'here': 1, 'item': 1, 'applies': 1, 'selecting': 1, 'antibodies': 1, 'eukaryotic': 1, 'palaeontology': 1, 'archaeology': 1, 'animals': 1, 'organisms': 1, 'dual': 1, 'plants': 1, 'chipseq': 1, 'flow': 1, 'cytometry': 1, 'mribased': 1, 'neuroimaging': 1})}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find all unique words in all files\n",
        "unique_words = set()\n",
        "for entry  in word_count:\n",
        "  unique_words.update(entry[\"word_counts\"].keys())\n",
        "unique_words = sorted(unique_words)\n",
        "#print(unique_words)\n",
        "\n",
        "matrix = []\n",
        "file_names = []\n",
        "for entry in word_count:\n",
        "  row = [entry[\"word_counts\"].get(word, 0) for word in unique_words]\n",
        "  matrix.append(row)\n",
        "  file_names.append(entry['file_name'])\n",
        "\n",
        "df = pd.DataFrame(matrix, columns=unique_words)\n",
        "df.index = file_names\n",
        "df.index.name = \"Paper Name\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LnmVK6m8Tm5",
        "outputId": "5623ffbf-2131-4677-9e48-54c872b03d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0', '00', '000', '0000', '00000', '00001', '00005', '0001', '0003', '0004', '0005', '0006', '0007', '0008', '0009', '00091', '001', '0010', '0011', '0013', '0014', '0015', '0016', '0017', '0019', '002', '0020', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029', '003', '003045', '0033', '0039', '004', '004042', '0041', '0044', '005', '005041', '005048', '005052', '005082', '0051', '0055', '006', '007', '007058', '00710128', '0075', '0076', '008', '008068', '008075', '0084', '009048', '009063', '0098sefﬁcientnetb7', '01', '010', '010001', '010051', '010089', '0101', '0102', '0107', '011', '0110', '011066', '011070', '012', '012030', '012065', '0122', '0125', '0131', '014', '0140', '014089', '015', '0150', '015049', '016', '016024', '016088', '0164', '0167', '017', '017083', '018031', '018071', '0188', '019079', '0193', '01duetorandominitializationofthetaskspecificmodelanddropouttheperformancemayvaryfordifferent', '01indicating', '02', '020', '0200', '020022', '021078', '021080', '0211', '02138', '022', '022032', '022089', '0221', '0223', '023', '023085', '024', '024039', '0246', '025', '0250', '0256', '026', '027', '027m', '028', '0282', '0286', '029', '0291', '03', '030', '030260199302038', '0309', '030fraction', '031010', '031092', '032159', '0328', '0330', '0356', '0365', '03715', '0380', '0383', '0387', '0390', '039b', '04', '040', '0408', '041034', '042', '042082', '0426', '0428', '0435', '0438', '044', '0442', '0445', '0446', '045', '045045', '0451', '0453', '04567', '046m', '04706', '048', '048025', '048082', '0481', '0482', '0484', '0487', '0488', '049', '049038', '0492', '05', '050', '051', '0510', '051070', '0520', '052038', '0526', '0535', '0536', '0541', '0549', '055', '0553', '05532', '0554s', '0557', '0564', '0566', '057', '058', '058059', '0581', '059', '0591', '0596', '05b', '05pascalvoc2007', '05yl', '06', '0600', '0601', '0605', '0609', '0611', '06139', '0615', '06173', '063058', '0632', '0633', '0634', '064', '0644', '0647', '065', '065050', '0651', '0652', '066', '0660', '066027', '0661', '0663', '0664', '066m', '067', '0671', '0672', '0677', '0679', '068', '068062', '0681', '0685', '0686', '069', '0690', '0692', '0693', '0695', '0696', '07', '070', '070074', '0702', '0703', '0706', '0707', '0708', '0709', '070b', '071', '0710', '071064', '0712', '0713', '0716', '0717', '072', '0720', '0721', '0727', '0729', '073', '0731', '07345', '0738', '074', '0749', '075', '0750', '075080', '0751', '07510872', '0753', '07590877', '076', '0761', '0762', '07630879', '0765', '0767', '0768', '07680', '0769', '07690888', '077', '0770', '07700891', '0772', '07730890', '07760890', '078', '0780', '07802', '078049', '07830897', '0784', '0788', '0789', '079', '07901', '079044', '0792', '0796', '07974', '07bresnext101', '08', '080', '0800', '08000909', '080062', '0801', '0803', '08047', '0806', '08060910', '08062', '0807', '0808', '08094', '081', '0811', '0812', '0815', '0817', '0818', '082', '082080', '0821', '0827', '0828', '0829', '083', '083055', '0831', '0832', '0833', '0835', '0837', '084', '0840', '0841', '0842', '0844', '0847', '0848', '085', '0850', '08500873', '085061', '085116', '0852', '0853', '0854', '08540944', '0857', '0858', '0859', '085m', '086', '08600877', '0861', '0862', '08638', '0864', '0867', '08676', '0869', '087', '0870', '08701000', '0873', '0875', '0879', '088', '088059', '0882', '0885', '0887', '08870968', '08878', '08880970', '08887', '08890937', '089', '0890', '089063', '089064', '089071', '08930975', '08940962', '0896', '08960974', '08970965', '08980966', '08980996', '0899', '08990966', '08m', '08å', '09', '090', '0900', '0901', '09010967', '0902', '09030980', '09031000', '0904', '09040967', '09050969', '09050970', '0907', '09080971', '0909', '091', '0910', '0913', '0914', '09164', '092', '092056', '0921', '092100', '0922', '0924', '09248', '0928', '0929', '093', '0930', '0931', '0932', '09320998', '0933', '0934', '09341000', '0935', '0936', '0937', '09370990', '09371', '0938', '09381000', '0939', '094', '0940', '0941', '0946', '09460956', '09460961', '09461000', '0947', '0948', '09490962', '095', '0950', '0951', '0952', '0953', '09531000', '0954', '09550963', '0956', '0959', '09591000', '096', '09600985', '0962', '0963', '0965', '0966', '09660980', '09670989', '09680976', '0969', '097', '0970', '0971', '09710979', '09710981', '0972', '0973', '09730989', '0974', '0975', '0976', '09760998', '09761000', '0977', '0978', '0979', '09791000', '098', '0980', '09800993', '09801000', '0981', '0983', '09830993', '0984', '0985', '0986', '0987', '0988', '09891000', '099', '0991', '09911000', '0992', '0993', '0995', '09951000', '0996', '0997', '0998', '0999', '0m', '0s', '0x', '1', '10', '100', '1000', '10000', '100000', '1000007180', '10003', '1000d', '1000way', '1000x', '1001', '10010000', '1001210022', '100130900', '100198', '1001layer', '1002', '100200', '100426', '10060', '100688', '100804', '100980', '100and', '100average', '100domain', '100k', '100layer', '100lddtcuni03b1', '100supervised', '101', '101038', '101038s4159201906862', '101109cvpr201690', '101110', '10111020', '101120', '101126scienceabj8754', '101126scienceade2574', '101177', '101398', '10148', '101549', '101563', '101696', '1017', '101757', '101789', '1018', '10184', '101854', '1018653v1p191279', '1019', '101layer', '102', '1020', '102032', '1021', '10221039', '102256', '1024', '102464', '102485', '1024dimensional', '102526', '1025493wq48zgx', '102559', '10261028', '10261034', '10261038', '102645', '102685', '102743', '102748', '102751', '102845', '102m', '103', '1036174', '103layer', '104', '1041', '104105', '104426', '1045', '10451057', '1048', '1048550arxiv230102933', '1048550arxiv230615350', '1048550arxiv231207814', '105', '1050', '1051018651883', '105281zenodo2591652', '105281zenodo7623482', '105281zenodo7623627', '10541056', '1056', '105831', '106', '1060', '106084m9figsharec5927795v1', '106110', '106154', '10615970', '10616', '10626', '1063691916', '1064615', '10647', '1068710698', '107', '1070072', '1071', '107114', '107180', '10721080', '107311', '107631', '107937', '10795', '108', '10811085', '108126', '10867', '1087', '108class', '108ra113', '108way', '109', '10901098', '1091', '10911098', '1092', '1093jamiaocz085', '1094', '1095', '10951110', '10961103', '10971105', '109958', '10a', '10b', '10crop', '10equivalent', '10fold', '10harvard', '10k', '10m', '10mb', '10minute', '10ofstepsandcooldownin90ofstepsspecificallythelearningrateincreaseslinearlyfromzerotothepeak', '10ppv', '10precision', '10random', '10th', '10true', '10way', '10x', '11', '110', '11001112', '1102', '110424', '1105411070', '11071110', '110layer', '110m', '111', '111078', '11110', '111112', '11113', '111144', '1112', '11131120', '1118', '11182', '112', '112025', '1121', '11211127', '112112', '112120', '1123', '11231130', '1124', '1125640', '1126', '11268', '113', '113109', '11311132', '11360', '11371144', '113957jpg', '114', '11411148', '11413', '1148', '11491164', '115', '1150', '1151', '11511164', '115118', '115133', '1151595573', '1153', '11569', '11571', '116', '1162', '11650', '1166', '11691177e4', '117', '1170647', '11745', '1175', '11772053951718819569', '11790', '117m', '118', '1181', '11831192', '11851', '11855', '1187', '1189', '119', '1192164332', '11971215', '11b', '11befﬁcientnetb1', '11oxfordpets', '11random', '11surprisinglyadversarialpretraininggenerallyleadstoaslightdegradation', '11th', '11these', '11u', '11x', '11γ', '12', '120', '120000', '120144', '1202', '1202layer', '1203', '1209', '121', '1210412113', '121209', '1214', '1216', '1216012167', '1218', '12191', '121layer', '122', '122139', '12239341332', '1225', '1226', '12273', '1228252830', '1229', '123', '12311', '12313273', '12331246', '123410', '12341240httpsdoiorg101093', '1234511', '12346', '12348', '12349', '1235', '12351250', '1236withtherevivalofneuralapproachesneuralembeddinghasbecomethenewstaplefor', '1238', '124', '1244', '12480', '124816', '124sst2', '124th', '125', '12500', '125000', '12544', '125765', '125m', '126', '127', '127000', '127136', '127165', '12722172226', '12730', '1278', '12791285httpsdoiorg10', '128', '1280', '12861305', '1288812900', '128931', '128shot', '129', '129136', '129137', '12916', '1295', '12971304', '12a', '12aug2493', '12b', '12class', '12j1', '12l', '12layer', '12lead', '12m', '12msenet', '12n', '12noncherrypicked', '12th', '12to', '12vj', '12w', '12way', '12x', '12xj', '12β', '13', '130', '1300', '130000', '13011309', '13052020', '13057', '131', '1310', '13151322', '1318', '1319', '132', '1323', '13231', '133', '13311334', '133149', '13381345', '134', '1340', '13411352', '1344', '13442', '13480', '134m', '135', '1350', '135145', '135153', '136', '13601371', '1363', '137', '137000', '13781387', '138', '138httpsdoiorg101006csla19941001', '139', '13907', '13911391144', '139163', '1399', '13991406', '13b', '13conditional', '13s', '13th', '13x', '14', '140', '1400', '140000', '140156', '1404', '14052020', '14071419', '14072017', '14081423', '141', '141146', '1412545558', '14128', '14135', '1414', '1415', '14171424', '14177276', '1419', '142', '14201434', '1422', '1425', '143', '1431814328', '14391449', '144', '145', '145149', '14531457', '14531460', '14541551', '145kinetics700', '145m', '146', '146150', '146m', '146mnasneta', '147', '14701477', '1471210512s8s4', '14761485', '147b', '148', '14805ac2', '14829', '149', '1492', '14961503', '1499', '14class', '14dimensional', '14m', '14summaries', '14th', '14x', '15', '150', '1500', '15000', '150000', '15002', '1503', '150528dimensional', '150706228', '150935', '150m', '150millionparameter', '151', '15119291958', '15149', '1516', '1517', '1519', '15191525', '15197', '152', '15213', '1526', '15271554', '1528semeval', '152layer', '153', '153160', '153196', '15321543', '1536', '153m', '154', '154163', '1542m', '155', '15550', '15580262', '15581161', '15591567', '155m', '156', '1560', '1563', '1563815650', '1565', '15658', '1566', '157', '157165', '157166', '157188', '15745', '157600604', '1577', '157ra143', '158', '158042', '158164', '1582121', '158687', '159', '15931594', '1596', '15971607', '15b', '15billionparameter', '15english', '15fold', '15m', '15sentence', '15xdat', '15xgpipe', '16', '160', '1600', '1600016009', '160035httpsdoiorg101038sdata201635', '16026', '16059454', '16071617', '161', '161002357', '161168', '1612229', '1613', '16144ac2', '1616', '1617', '16171623', '162', '1620', '162326', '16255', '162725', '162b', '163', '16311642', '16364', '1639theirsuccessisfueledbyaccesstolargetextcorporaadvanced', '164', '1640', '16460', '164b', '165', '16501661', '1654916559', '16560', '166', '16627', '166510624', '16651672', '167', '16701070', '1672', '167407', '167665', '167b', '168', '16811690', '168174', '168473', '1688', '168907', '168998', '169', '16asarunning', '16i', '16pretrainedusingwikipediaandbookcorpusitthen', '16shot', '16test', '16th', '16x', '16x16', '17', '170', '17011708', '1705', '170544', '1709', '17091721', '171', '171014', '171105225', '171186', '171193', '171327334', '17151725', '1718', '172', '1720', '17241734', '17261272', '173', '17313', '173175', '173182', '1734', '17345168', '17351780', '174', '174182', '17441757', '1748', '175', '175000', '175185', '1755', '176', '1760', '17651775', '1766', '177', '1771', '178', '178187', '1786362', '179', '1792', '17922', '1794', '179714', '17981828', '179887', '17991807', '17dataset', '17m', '17th', '18', '180', '1800918021', '18035', '181', '18109', '181193', '181196', '181198', '1812', '18161823', '1819', '18191837httpsdoiorg101109tkde201339', '182', '1827', '1829', '183', '183197', '1834', '183429', '1836', '184', '184190', '1845', '1846', '185189', '1852', '1853', '1854', '185421', '18561', '18571865', '1858', '1859', '186', '1860', '1861', '1862', '1863', '1864', '1865', '18653v1w191909', '1867', '1868', '187', '1870', '18731888', '1874', '18771901', '188', '1880', '188204', '189', '189000', '189142', '189196', '189484', '18b', '18bsenet', '18layer', '18th', '18x', '19', '190', '190s', '191', '19106', '191104623', '19151929', '1917519186', '192', '19201929', '1920819220', '1921', '192201', '1925', '19281936', '19291958', '193', '19304', '1931', '193199', '1935819369', '1936', '1937533', '1938', '194', '1941', '194207', '1943', '1945', '1947', '1948', '1949', '194m', '1951', '1953', '1955732361', '1957', '1958', '196', '1960s', '1960wecreatethetraindevtestsplitasthey', '1961', '1962', '196201', '1962j', '1964', '1967', '1968', '1969', '197', '1970s', '1972', '1973', '1973019742', '1974', '1975', '1976', '1976419775', '1977', '1978', '1979', '1979819808', '1980', '1980s', '1980s2427', '1982', '1983', '1983019839', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19b', '19imagenet', '19m', '19mgpipe', '19th', '19x', '1a', '1ab', '1b', '1bc', '1bnreluconv33', '1bw', '1bws', '1c', '1chexnet', '1comparing', '1d', '1deepmind', '1department', '1dk', '1e', '1e4', '1e5', '1efﬁcientnetb0', '1examples', '1f', '1f2', '1facebook', '1fair', '1for', '1google', '1httpcodegooglecompcudaconvnet', '1httpimagenetorgchallengeslsvrc2015', '1httpsgithubcomcodelucasnewspaper', '1httpsstanfordmlgroupgithubiocompetitionschexpert', '1httpwikipediaorg', '1layer', '1m', '1model', '1n22', '1nn', '1o', '1openai', '1p19q', '1s', '1shot', '1shows', '1st', '1stanford', '1summary', '1th', '1the', '1u44j', '1w', '1we', '1x', '1x1', '1zeroshot', '1γ', '2', '20', '200', '2000', '20000', '200000', '2000s', '2001', '2002', '200225', '2003', '2004', '2005', '2005semisupervised', '2006', '2007', '20072012', '2008', '20082040', '2008clinical', '2009', '200950000', '200fold', '201', '2010', '2011', '20112018', '2011cba00300', '2011cba00301', '2012', '20123680', '20124128', '2013', '20136667', '20138144', '2013a', '2013b', '2013proceedings', '2014', '201447386', '201475750', '2015', '2016', '20160716', '201611835', '20162019', '2016778', '2016a', '2016b', '2016proceedings', '2017', '201705', '201706b', '20170821', '20171162', '2017188', '20172025', '2017778', '2017801', '2017809', '2017a', '2017b', '2018', '201803b', '201811', '2018807', '2018827', '2018pro', '2018s', '2019', '2019835', '2019proceedings', '2019r1a6a1a10073437', '2019s', '202', '2020', '20200722211482', '20201215422761', '2020a', '2020b', '2020m3a9g7103933', '2020p000233', '2021', '202124128', '20213738', '2022', '2023', '2023072123292757', '2024', '2025', '203', '20394', '204', '2048', '205', '2050', '2053951718819569', '206212', '206213', '207226', '207237', '2074', '208', '2080', '2085', '2090', '2092213860156275', '209407', '209407z17z', '2098', '20caltech101', '20equivalent', '20layer', '20m', '20nn', '21', '210', '210218', '21095', '210956074', '211', '211113891393', '211117931803', '211252', '212', '2120', '21212129', '2122', '212375', '2125', '2127121284', '21272136', '213', '21322141', '214', '2140', '2143', '21442', '21444', '2146', '21462153', '215', '215223', '2153', '2158', '2159', '216', '216251s272s', '21635055360', '21692185', '217', '2173', '2175', '21772185', '218', '2180residue', '2184', '2186', '219', '21963', '2199', '21992210', '21st', '21wascreatedtofacilitateresearchonpharmaceuticalinformation', '21x', '21xgpipe', '21xgpipe990', '22', '220', '22000', '2204359010', '2205', '22062222', '2207', '2208', '221', '22117', '221230', '222', '222232', '2224', '222511538', '22262235', '2227', '22272237httpsdoiorg1018653v1n181202', '223', '223230', '223239', '2233', '2235', '224', '2242', '224224', '2242243to', '224316', '224x224', '225', '225271', '2258', '225food101', '2261', '226234', '22655', '227', '2271', '2276', '22782324', '2281', '228482', '2285', '22b', '22department', '22k', '22nd', '22å', '23', '230', '23002', '2304310', '23072316', '230907778', '231', '2319', '232', '232230016', '232237', '232304310', '2324', '2325', '2326', '2328', '232country211', '233', '2330', '233241', '23372348', '2339023400', '234', '2342', '235', '235238', '23530', '236', '2361', '236256', '23690', '237', '2371623736', '2373', '23740', '2375', '2378023120967171', '2379', '2380', '23882396', '239', '23976', '2399', '23andelmo', '23b', '23m', '24', '240', '24022410', '241', '241261', '241268', '2415', '2419', '242', '2423213294', '2427', '2429', '24297995', '24327129', '2433', '244', '24402448', '2441', '2442', '2443', '2444', '245', '2452', '245250', '2457', '2458', '246', '2460', '2463697722', '24714', '2474', '24782', '24805', '24812498', '248255', '249', '249187', '249269', '24932537', '2496', '24b', '24befﬁcientnetb4', '24gb', '24layer', '24lead', '24m', '24th', '24xgpipe', '25', '250', '2500', '25000', '2500residue', '2502', '2503', '250layer', '251', '2516', '252', '2520', '252264', '25250', '2527825294', '25296', '253', '2530', '2531', '253256', '253440186624648966489643264', '25347429', '2537', '253740112113', '253905', '254', '2542', '254262', '255', '2552', '255285', '2554', '25542558', '2555', '2556', '2557', '256', '256256', '256480', '25664', '25672577httpsdoiorg1018653v1d191259', '256d', '256m', '256shot', '256sized', '257', '2577', '25792605', '258', '25842591', '25909', '259265', '259269', '25982600', '25c', '25containsasetofresearchquestionseachwithareferencetextfrom', '25m', '25th', '26', '260', '2603', '26032', '2606', '262', '26212630', '2629', '2630', '26302640', '263273', '263274', '26358', '2636', '2637', '26378051202110art2', '2638a', '264', '2644', '26442655httpsdoiorg1018653v1n191271', '265', '26509', '265283', '265292', '2658', '26612671', '266282', '2663', '267', '268', '268279', '268m', '26912700', '2692', '26m', '26th', '26x', '27', '270', '2700', '270000', '271170', '272', '2722', '27222728', '2729', '272m', '273', '274', '2740', '275', '275278', '2755', '27709', '277282', '278287', '2788', '279', '2794', '27m', '27th', '28', '280', '2807', '28082019', '281', '281289', '2814175', '28182826', '282', '2820', '2820172025', '282289', '28252830', '2828', '28289038', '2829', '284', '28452856', '2854', '28744', '2878', '288', '2883', '289', '289296', '289300', '289316', '28952905httpsdoiorg', '289stanfordcars', '28fer2013', '28m', '28th', '28the', '29', '29000', '29012910', '29018', '29039', '29049', '291017', '29182928', '292', '29212929', '292301', '29242932', '2930', '29332941', '29333', '2934', '29377', '294', '2941', '294298', '2943494357', '2951', '2955', '296', '2968', '297', '297302', '29753', '2978', '298', '29802988', '298383', '299x299', '2a', '2ad', '2and', '2architecture', '2b', '2c', '2cancer', '2ce', '2chexnet', '2class', '2clip', '2cscw122', '2d', '2department', '2df', '2domainspecific', '2e', '2f', '2forexampleitdoesnotincludeanyquestionansweringtask', '2gj', '2httpsgithubcomcld2ownerscld2', '2i', '2in', '2irhythm', '2irwith', '2j', '2k', '2layer', '2m', '2model', '2n', '2nd', '2new', '2o', '2performance', '2prior', '2r12', '2school', '2shot', '2stanford', '2the', '2this', '2v', '2vs', '2vs1', '2way', '2which', '2xi', '2y', '2z', '2π', '2πto', '2φ', '3', '30', '300', '300000', '300k', '300million', '3017589', '302436', '3032', '303338', '304', '304097', '305', '306', '3061', '306309', '307', '3072', '3079', '30793087', '308', '3080', '30805', '3087', '3090', '309317', '30943104', '30andslanted', '30cifar100', '30class', '30fortheoptimizerusingastandardslantedtriangularlearningrateschedulewithwarmupin', '30m', '30o', '30s', '30second', '30stl10', '30th', '30way', '30x', '31', '310', '3100', '31043112', '311', '31113119', '3114', '31178', '312', '313', '31323', '3132353246', '3133', '3134', '3137', '3142', '3144', '315', '315323', '31533160', '3155', '316', '3162224022410', '318', '318362', '319', '319330', '31b', '31s', '31st', '32', '320', '32000', '32080', '321', '32120', '321261', '321324', '321347', '322', '322k', '323', '3232', '3239', '324', '325', '32573267', '326', '32663280', '3270', '3275', '3277', '328', '328339', '328339httpsdoiorg1018653v1p181031', '32883291', '32943302', '32b', '32befﬁcientnetb3', '32c3', '32class', '32k', '32nd', '32shot', '33', '331x331', '332', '3333', '333338', '3342', '33443354', '3347', '335', '33521', '3358', '336', '33668', '3369', '337', '33703374', '3371', '337344', '3380', '339167', '33928', '3393133938', '33964', '33b', '33m', '33rd', '34', '340354', '341', '342', '3431', '3438', '344', '345', '345021', '345159030', '3457', '345m', '346', '34623471', '347', '3470', '347356', '347357', '348', '34904', '349358', '34983505', '34layer', '34m', '34specificallythisap', '35', '350', '350000', '350360', '3513', '352', '3534', '3547484900', '355362httpswwwaclweborganthologyd111033', '3558', '355993', '3568', '357', '357362', '3576', '3579', '35813589', '358362', '359', '35b', '35m', '35th', '35wascreatedforeval', '36', '360', '3600', '36000', '361', '36109', '36153620httpsdoiorg1018653v1d191371', '3618', '362', '3638', '3641', '3646', '365', '36547290', '3658', '365m', '3662', '367', '367229', '368', '36916', '36m', '37', '370', '3700', '37000', '370377', '371', '372', '373', '374', '374041444657117', '375', '3750', '3755', '377', '3780', '3781', '3782', '3784', '3785', '379', '3799538017', '37b', '38', '381', '38109', '38113941', '3813', '381383', '3834', '384', '384residue', '385', '385390', '385997', '386m', '387', '387394', '387398', '38763887', '3877', '389', '389396', '38th', '39', '390', '39003908', '3914', '3917', '392', '39323147', '394', '39423951', '39441620', '395', '3950', '3956587', '3960', '396404', '397', '39733981', '39854009', '399', '3992', '399406', '399419', '39b', '39cifar10', '39inthisarticleweusethewordpiecealgorithm', '3_2_3', '3a', '3ac', '3ais', '3alec', '3artificial', '3b', '3billionparameter', '3c', '3cancer', '3class', '3cscw', '3d', '3department', '3df', '3dhistech', '3dimensional', '3division', '3e', '3f', '3feb11371155', '3flops', '3g4', '3gb', '3h4r', '3hr4_a', '3httpgeneticassociationdbnihgov', '3j7', '3layer', '3lyw', '3n', '3numpylike', '3o', '3performance', '3phase', '3r', '3rd', '3scaling', '3showsexamplesofhowdomainspecificpretrainingwithindomain', '3stanford', '3way', '3we', '3x', '3x3', '3zeroshot', '4', '40', '400', '4000', '40000', '400000', '400m', '401050', '40154025', '403', '4031', '4034', '403410', '4038', '404', '404404', '404410', '4046', '405', '405527', '4056', '4058', '406', '4060', '406080100', '407', '408', '409', '4090', '4096', '409640961000', '4096dimensional', '4096dimensions', '40m', '40th', '40was', '40zeroshot', '41', '410', '410414', '410418', '411', '4116', '412', '4122', '4129', '413417', '413451', '414', '4142', '4146', '4146andevolves', '415', '415422', '417', '41714186', '4175', '417894', '41802', '4182', '41824192', '41834192', '418435', '419', '41b', '41befﬁcientnetb5', '41e27', '41m', '41x', '41xdat', '42', '420', '4216', '422761', '4244', '42474255', '425', '425430', '4261', '426437', '4273', '428', '42b', '42bameobanetc', '42bnasneta', '42containsmultiplequestionansweringtasksannotatedbybiomedicalexperts', '43', '430', '4300', '430439', '43044313', '431', '4312', '4315', '432', '432440', '433', '4341', '43484371', '4349', '436', '436444', '437', '437441', '437470', '438', '43844403', '439', '43class', '43m', '43rd', '43way', '43x', '44', '440', '441', '442', '4423', '4424', '442444', '443', '443524', '444', '44416', '44575', '446461', '4474', '448', '4482', '448455', '448456', '449', '45', '450', '4500', '451', '451532', '452', '454', '4551', '455469', '4561discarded90trainingfilesthattheauthors', '457', '457372', '457373', '4576', '45854592', '458596365', '4599', '45b', '45k5k', '45x', '46', '460', '462', '4622', '462477', '462479', '463034', '464', '4646', '4648', '465', '4654', '466', '467', '467480', '46750', '468', '4693', '46x', '47', '470', '4700', '4701', '4702', '4703', '4704', '4705', '4706', '4707', '47072', '4708', '471', '472231', '473', '4733', '47334742', '473481', '474', '475', '4753', '479398', '479495', '47totransformer', '47x', '48', '4800', '480524', '480x480', '481', '482', '483', '48374846', '484', '4844', '484489', '48454854', '4849andbert', '485', '486', '48722792289', '488', '489', '48905', '48k', '48m', '49', '49044916', '4908', '491', '491507', '492', '492518', '4939', '494', '495', '4957', '496504', '4980', '499', '4996', '49x', '4a', '4ac', '4b', '4c', '4cancer', '4ce', '4deeper', '4department', '4f', '4fg', '4google', '4jl', '4k', '4l3a', '4m', '4massachusetts', '4o', '4scaling', '4shot', '4summarization', '4th', '4the', '4these', '4this', '4to', '4w', '4way', '4x', '4x8', '4zeroshot', '5', '50', '500', '5000', '50000', '500000', '500999', '500x', '501', '50101152layer', '502', '5020', '5021', '502525', '50257', '504507', '5052', '507', '509517', '50c', '50k', '50layer', '50x', '51', '510', '510k', '511538', '511e1000579', '512', '512dimensional', '512wide', '513', '5134', '513526', '5140', '51426', '5149', '515', '5155', '516', '5162', '517210', '518', '519', '51918', '52', '5203', '520524', '521', '52157166', '522', '5220', '523', '524', '524531', '525', '525243355', '5255', '525536', '5258', '526540', '5266', '52713', '5281zenodo3509134', '5284', '529', '529533', '52x', '53', '530', '531131', '5317', '532', '533', '5330', '533536', '534', '53438', '5347', '535', '53549', '536551', '537540', '5385', '53consistsofsentencesfrompubmedabstractswithman', '53m', '54', '540', '540556', '541', '5414', '542', '5427639115118', '5445', '545', '546', '5460', '5464', '546552', '547', '5476', '5484', '54910', '54yearold', '55', '550000', '550560', '552', '5528', '553', '553568', '55360', '554', '5548', '555', '555570', '556', '5564', '55665', '556m', '556mefﬁcientnetb7', '557', '5572', '55785582', '55787', '557m', '558', '5584', '559', '5592', '55x', '56', '560', '560x560', '561', '56275', '563', '563579', '56375664', '564', '5642', '56490', '565', '5656', '5657greatly', '567', '567578', '569', '56layer', '56m', '56th', '56xgpipe', '57', '570', '570576', '571', '5716', '572', '573', '5738', '575', '575582', '576581', '577', '5770', '578', '57a', '57b', '57th', '57x', '58', '580', '580587', '581', '58125', '581595', '583', '583589', '584', '585', '585357362', '586', '5865', '587', '588', '589', '58m', '59', '590', '590596', '591', '591604', '592', '5923', '5926473', '593', '594', '595', '596', '5964', '597', '598743752', '59875995', '599604', '59986008', '59th', '5a', '5b', '5bc', '5cdf', '5class', '5d', '5de', '5department', '5ef', '5electrical', '5flops', '5gh', '5httpspubmedncbinlmnihgov', '5i', '5layer', '5o', '5preliminary', '5right', '5t', '5the', '5these', '5veterans', '5way', '5we', '5with', '5yet_b', '5yqp_b', '5zeroshot', '6', '60', '6000', '60000', '6002', '600x600', '601', '6010', '601030', '6012', '601609', '602', '6024', '602608', '60269dc1f994', '603606', '603612', '605', '60549576', '6061', '6074', '607616', '609', '609616', '60b', '60m', '60mefﬁcientnetb1', '60th', '60x', '61', '6102', '61083719', '611', '61101', '61205395171881956', '6128', '61341', '61361136003', '6149', '615', '616', '6167170', '617', '6170', '6172', '61732', '6174', '618', '6181', '619', '61x', '62', '621', '6218', '623629', '62490191', '62500', '6270', '62702399', '629', '62946305', '62andsubsequenteffortshavefocusedoncrawling', '63', '630645', '631', '63107362', '63118917', '63118918', '63118919', '63118921', '632', '6324', '6325', '634', '6351', '63610645', '63611059', '6368', '637', '63863', '63916400', '64', '640', '64000', '641', '642', '643', '645963', '646661', '648', '648660', '64992', '64d', '64k', '64m', '64x', '65', '650000', '650m', '650million', '65240', '653', '654669e3', '656', '6569', '657', '6572', '6574e3', '6580', '6585', '6597', '65983866', '66', '660', '661', '661016', '66456649', '665', '666', '6667', '667', '6671httpsdoiorg1018653v1d182012', '668', '66m', '66mnot', '67', '670', '67075140', '671', '674', '6743', '6772', '6777', '678', '6784', '6790', '679682', '67hatefulmemes', '67m', '68', '681697', '682', '683695', '685', '6850', '6856', '6869', '6871', '688', '6892', '689691', '68m', '69', '693', '6932', '693707', '694699', '697', '699', '6bc', '6bym', '6bym_a', '6class', '6de', '6department', '6fes81', '6harvard', '6httpsgithubcomnvidiadeeplearningexamples', '6identical', '6linear', '6model', '6n2', '6o', '6percentage', '6sk0', '6sk080', '6t1z83', '6these', '6vr479', '6vr4a', '6w303', '6w6w82', '6y4f', '6y4f77', '6yj1', '6yj178', '7', '70', '700', '701', '701020', '7024', '702710', '7036', '703715', '704', '70426', '706', '706710', '7070', '7080', '708718', '70m', '70x', '71', '710', '711', '71131', '712725', '7129', '713595', '714', '7146', '715', '7154', '715723', '717', '7170', '7180', '7186', '719', '719742', '72', '720', '7204', '7206', '721216', '722729', '723', '7234', '72347242', '724', '725', '7254', '725738', '727', '7270', '728', '7292', '7298', '73', '7302', '7306', '7312', '7318', '732', '73257', '733', '7330', '7336173158', '7338', '7344', '7352', '7364', '7368997', '7369', '7372', '7374', '7378httpswwwaclweborganthologyw041213', '738', '7380', '74', '740', '740755', '742', '7422', '744', '7444', '74547463', '746', '7470', '748', '7481', '75', '750', '7500', '7504', '751', '7520', '7524', '753', '754', '7540', '7544', '7548', '756', '756779', '75696', '7572', '758', '75832905', '7586', '759', '75average', '76', '760', '76109', '7611', '7614', '762', '7627', '762m', '763', '764', '7641', '7646', '7647954954', '7669', '7670', '7675322655', '767580', '768', '7680', '7694', '7696', '76x', '77', '770', '7704', '7705', '770778', '771', '771157173', '772', '772016', '7722', '7724', '7729', '773', '774', '77495500', '775', '7751', '77512', '7758', '776', '7769', '777', '7771', '7772', '7778', '778', '7786', '779', '7791', '779788', '779815', '77ucf101', '77x', '78', '780619345021', '7807', '781', '7814', '7820', '784', '7851', '7855', '7863', '7868', '787', '7877', '788', '7883', '7886', '789', '7890', '789799', '7898', '78m', '78mresnext101', '78sun397', '79', '790', '7900', '79000', '7901', '7902', '7905', '791', '7910', '7915', '7916', '791793', '7918', '792', '792802', '793', '7934', '7942', '7943', '7952', '7961', '7962', '7966', '797', '798', '7980', '7992', '7996', '7a', '7b', '7class', '7electrical', '7for', '7harvard', '7inch', '7jtl84', '7lqm', '7m6b', '7mez', '7o', '7oc9', '7qqa', '7qym', '7random', '7t', '7th', '7way', '7x7', '7zeroshot', '8', '80', '800', '8000', '80000', '8003', '8004', '800810', '8010', '801010', '8012', '802', '8020', '8023', '80248035', '8034', '8038', '8041', '8044', '8048', '80554', '8063', '8065', '8066', '806603', '807', '8074', '8077', '808', '8084', '8088', '809', '8090', '8091', '80912', '80gb', '81', '810', '8106', '8107', '811', '8116', '811u', '8121', '8122', '8123', '8125', '813', '8130', '8134', '8140', '8142', '8144', '8150', '8154', '8158', '816821', '8170', '8171', '8174', '8176', '818', '8187', '818833', '819', '8190', '8192', '8196', '81imagenet', '81x', '82', '820', '8206', '8208', '8213', '8216', '8218', '822', '8220', '8222', '8224', '823', '8232', '8234', '8236', '8238', '824', '8242', '8247', '825', '82528262', '826', '8260', '8262', '8268', '827', '8271', '828', '8280', '828835', '829', '8297', '829835', '829837', '82imagenet', '83', '8302', '8304', '8314', '83178326', '8319', '833', '83311', '83358', '8336', '834', '8342', '8343', '83445howeveraprevailingassumptionisthatoutdomaintext', '83445themainquestion', '835', '835353359', '8354', '8356', '835843', '836', '8362', '8365', '8369', '837', '8370', '837840', '837845', '8382', '839', '8393', '8396', '839843', '83imagenet', '84', '840', '8403', '8407', '8411', '8414', '8423', '843', '8437', '843852', '8439', '844', '8452', '8454', '846', '8470', '8472', '8489798', '8492', '84b', '84imagenet', '84m', '84mefﬁcientnetb3', '84x', '85', '850', '8500', '85000', '8502', '85054', '850862', '850864', '851', '852', '8522', '8528', '853', '85321', '8538', '854', '8543', '8543296', '854601', '8548', '855', '8559', '856', '8562', '8563', '8564', '8567', '857', '8576', '858', '8587', '859', '8597', '859883', '85average', '85m', '86', '860', '861', '861122782324', '86148618', '86163', '861874', '862', '8625', '863', '8632', '863874', '864', '865', '8652', '865878', '865884', '866', '8662', '867', '868', '869', '86m', '86x', '87', '870', '871', '8715', '871876', '872', '8720', '8726', '873', '8736', '873876', '87388', '873880', '874', '87488763', '875', '8756', '876', '8760', '8761', '8765', '8768', '8777', '8782', '879894', '8799', '87m', '87x', '88', '880', '8804', '880888', '881', '8810', '8825', '8834', '8839', '884', '88448856', '8853', '886', '889', '889898', '89', '89000', '8905', '891', '8913', '8922', '8925', '8935', '894', '8943', '89494', '8952', '896', '8962', '8978', '898', '899', '8990', '8993', '8994', '8999', '89m', '89mefﬁcientnetb4', '89x', '8_8_', '8b', '8department', '8gram', '8grams', '8health', '8httpswwwncbinlmnihgovpmc', '8i', '8inch', '8isanotableexceptionasitgenerates', '8joi', '8m', '8millionparameter', '8o', '8random', '8scaling', '8shot', '8was', '8å', '9', '90', '900', '9010', '90101', '901908', '901909', '902', '9020', '9039', '90499058', '90585', '905912', '90631', '907', '908', '9080', '909', '90average', '90ctober1986letterstonature', '91', '911', '91106', '91125', '9114', '91144', '9119', '9123', '91232', '912921', '913', '9136', '914', '914920', '915', '916', '917', '9179', '9195', '9198', '92', '92093', '9212', '922', '923', '9230', '9231', '9235', '923925', '9245', '924623', '9249', '925', '9251', '925af', '92669', '926932', '927579', '9285', '929', '9296', '92m', '93', '930', '9300', '9305', '930949', '9311', '9312', '9317', '932', '9320', '932938', '933', '9330', '9331', '9333', '9334', '9336', '9337', '934', '9341', '9345', '935943', '936', '938', '939', '94', '94043', '941', '9411', '94110', '9419', '942', '94305', '944', '945', '94539463', '947', '94720', '948', '949', '95', '950', '9504', '950965', '951', '951958', '952', '9536', '954', '955', '9555', '956', '9575', '9578', '958', '9587', '958962', '958963', '959', '96', '960', '961', '96104', '962', '963', '965', '96509660', '967', '9670', '969', '96x', '97', '970', '971', '9729', '973', '9738', '974', '975', '975th', '977', '9776', '98', '980', '98052', '981', '9810', '98113', '9817351780', '98195', '983', '983999e24', '984', '985', '98637', '98799889', '988', '989', '99', '993', '9931', '9958', '9959', '996', '998', '9990', '99accuracy', '99b', '9as', '9correspondstobiobertwhichconductedpretraining', '9department', '9health', '9random', '9th', '9thefirstcolumnintable', '_', '_0', '_9_0_ct_o_b_e_r_1_98_6', '__', '________', '_________', '__________________', '__a', '__e', '_natu_r_e_v_o_l_', '_s', 'a', 'a1', 'a100', 'a11', 'a12', 'a2', 'a21', 'a22', 'a26', 'a2644', 'a31', 'a33', 'a41', 'a42', 'a650', 'aaai', 'aaai19', 'aam', 'aaron', 'aarti', 'aavd', 'aawt', 'ab', 'abadi', 'abandoned', 'abbasi', 'abbeel', 'abbrevi', 'abbreviated', 'abbreviation', 'abc', 'abductive', 'abdul', 'aberrant', 'abil', 'abilities', 'abilitiesassociative', 'ability', 'ablated', 'ablating', 'ablation', 'ablations', 'able', 'ably', 'abmil', 'abmil59', 'abmil83', 'abnormal', 'abnormalities', 'abnormality', 'abousamra', 'about', 'above', 'abramovich', 'abriata', 'abs14061078', 'abs14090473', 'abs14094842', 'abs14103916', 'abs14105401', 'abs14122007', 'abs14123555', 'abs151200567', 'abs160604199', 'abs161002391', 'abs170303906', 'abs190800723', 'absence', 'absent', 'absolute', 'abstrac', 'abstract', 'abstractand', 'abstracting', 'abstraction', 'abstractive', 'abstracts', 'abun', 'abundant', 'ac', 'acad', 'academic', 'academies', 'academy', 'acc', 'acc1', 'acc3', 'acc5', 'accel', 'acceler', 'accelera', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accent', 'accept', 'acceptable', 'acceptably', 'accepted', 'accepts', 'acces', 'access', 'accessed', 'accessible', 'accessing', 'accession', 'accessions', 'accident', 'acck', 'accm', 'accmflops', 'accommodate', 'accompanied', 'accompanying', 'accomplish', 'accomplished', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accounted', 'accounts', 'accu', 'accumulate', 'accumulated', 'accumulates', 'accumulation', 'accumulator', 'accuracies', 'accuracy', 'accuracyparameters', 'accurate', 'accurately', 'ace', 'acero', 'acetyltransferase', 'acharya', 'achieve', 'achieved', 'achieves', 'achieving', 'achinelearning', 'acid', 'acids', 'acidsxi', 'acinar', 'acinarlike', 'acknowledge', 'acknowledgements', 'acknowledgementsthe', 'acknowledges', 'acknowledgments', 'acl', 'aclijcnlp', 'aclweborganthologyj901003', 'aclweborganthologyvolumesw1950', 'acm', 'acommonsetofhyperparametersforeachdatasetthatworkwellforbothoutdomainandindomainlanguage', 'acoustic', 'acoustics', 'acquire', 'acquired', 'acquiring', 'acquisition', 'acr', 'across', 'acs', 'act', 'acta', 'acter', 'acting', 'action', 'actionable', 'actions', 'activ', 'activate', 'activated', 'activating', 'activation', 'activations', 'active', 'activesite', 'activities', 'activity', 'activitywhen', 'acts', 'actual', 'actually', 'acuities', 'acuity', 'acute', 'acyl', 'acylcoa', 'ad', 'adam', 'adam33', 'adamantinomatous', 'adams', 'adams12', 'adams1222', 'adamts', 'adamw', 'adamw129', 'adanet', 'adap', 'adapt', 'adaptability', 'adaptable', 'adaptation', 'adapted', 'adapter', 'adapters', 'adapting', 'adaption', 'adaptive', 'add', 'added', 'addi', 'adding', 'addition', 'additional', 'additionally', 'additive', 'address', 'addressable', 'addressed', 'addresses', 'addressing', 'adds', 'addsk', 'ade', 'ade20k', 'adeno', 'adenocarci', 'adenocarcinoma', 'adenocarcinomas', 'adenoma', 'adept', 'adequate', 'adequately', 'adg', 'adhere', 'adheres', 'adhesin', 'adhikari', 'adipose', 'aditya', 'adjacent', 'adjectives', 'adjunct', 'adjust', 'adjustable', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'adjusts', 'adlam', 'adler1', 'administered', 'administration', 'administrative', 'adnn', 'adobe', 'adopt', 'adopted', 'adoptfaster', 'adopting', 'adoption', 'adopts', 'adrenocortical', 'aduke', 'adult', 'adults', 'adulttype', 'adv', 'advance', 'advanced', 'advancement', 'advancements', 'advances', 'advancing', 'advantage', 'advantageinusinganindomainvocabularyisthattheinputwillbeshorterindownstreamtasksasshowninta', 'advantageous', 'advantages', 'advent', 'adventitia', 'adventure', 'adversarial', 'adversities', 'advertise', 'advice', 'advised', 'advisor', 'advocated', 'ae', 'ae1', 'ae1ae3', 'ae3', 'aeavaxjjayj', 'aeaw', 'aeax', 'aeaxaxiaw', 'aeaxi', 'aeaxj', 'aeay', 'aeayidydxi', 'aeayyd', 'aerial', 'aeronautical', 'af', 'afalse', 'afatrial', 'afeaturizer', 'affairs', 'affect', 'affected', 'affecting', 'affects', 'affiliations', 'affinities', 'affinity', 'afford', 'aforementioned', 'afraid', 'africa14department', 'africa15institute', 'african', 'after', 'afﬁnity', 'again', 'against', 'agarwal', 'agata', 'agcc', 'age', 'aged', 'agencies', 'agency', 'agenet', 'agents', 'ages', 'agesex', 'agg', 'aggarwal', 'aggc', 'aggc101', 'aggc22grandchallengeorgdata', 'aggc89', 'aggre', 'aggregate', 'aggregated', 'aggregates', 'aggregating', 'aggregation', 'aggregation37', 'aggregator', 'aggressive', 'aggressively', 'aggressiveness', 'agirre', 'agnosed', 'agnoses', 'agnostic', 'ago', 'ago335', 'agonist', 'agrawal', 'agree', 'agreed', 'agreement', 'agreements', 'aguera', 'ahdritzet', 'ahead', 'ahmed', 'ahmet', 'ahoo', 'ahs', 'ai', 'ai1', 'ai1þ', 'aibased', 'aicomplete', 'aidan', 'aidancstorontoedu', 'aidengu', 'aids', 'aik', 'aikhk', 'aikþ', 'aikþþ', 'aim', 'aimed', 'aimistanford', 'aims', 'air', 'aircraft', 'aircraftinceptionv4', 'airspace', 'aistats', 'aj', 'ajaz', 'ajb', 'ak', 'akhondi', 'akikazu', 'akin', 'akin1', 'akinori', 'akoya', 'aksoy', 'al', 'al9', 'alahari', 'alain', 'alandalus', 'alatan', 'alayrac', 'albeit', 'alberdina', 'albert', 'alberti', 'album', 'alcorn', 'alec', 'alecopenaicom', 'alemi', 'alent', 'alex', 'alexander', 'alexandra', 'alexnet', 'alexnetlevel', 'alexwangamanpreetsinghjulianmichaelfelixhillomerlevyandsamuelrbowman2019glueamultitaskbenchmarkand', 'alfasly', 'alfonso', 'alfred', 'algae', 'algebraic', 'algo', 'algorithm', 'algorithm103', 'algorithm22', 'algorithm3638', 'algorithm59', 'algorithm66', 'algorithm83', 'algorithmic', 'algorithms', 'algorithms15374080', 'algorithmsieee', 'alhambra', 'ali', 'alien', 'align', 'align31', 'aligned', 'aligning', 'alignment', 'alignments', 'aligns', 'alin', 'alist', 'alistair', 'all', 'allan', 'allatom', 'allb', 'allcnn', 'allel', 'aller', 'alleviate', 'alleviating', 'alley', 'alllℓ', 'allocates', 'allocating', 'allocation', 'allograft', 'allow', 'allowed', 'allowing', 'allows', 'allthetypesaremergedintooneinthecasewhentherearemultipleentitytypesthe', 'alluding', 'ally', 'almeida', 'almost', 'alok', 'alone', 'along', 'alongside', 'alpha', 'alphabet', 'alphafold', 'alphafold2', 'alphafoldand', 'alphafoldmultimer', 'alphanumeric', 'alpola', 'alquraishi', 'already', 'alrfou', 'als', 'alsentzer', 'also', 'altch', 'alter', 'alterations', 'alterations1718', 'alterations56', 'altering', 'alterna', 'alternate', 'alternates', 'alternative', 'alternatively', 'alternatives', 'although', 'alto', 'altogether', 'altschuh', 'altschul', 'alty', 'alveolar', 'alverio', 'always', 'alysis', 'alzahra', 'am', 'ama', 'amamoto', 'aman', 'amanda', 'amanpreet', 'amari', 'amazon', 'amber', 'amber32', 'amber99sb', 'ambi', 'ambiguities', 'ambiguity', 'ambiguous', 'ambitious', 'amblyopia', 'ambrose', 'ambula', 'ambulatory', 'amc', 'amenable', 'ameobaneta', 'america', 'american', 'amgad', 'amia', 'amin', 'amina', 'aminations', 'amino', 'aminohydrolase', 'amio', 'amira', 'amittai', 'amodei', 'amoebaneta', 'amoebanetc', 'among', 'amongst', 'amount', 'amounts', 'amphitheatre', 'ample', 'amplify', 'ampliﬁes', 'ams', 'amsterdam', 'an', 'ana', 'anaemia', 'anaesthesia', 'anaesthetized', 'anal', 'analo', 'analog', 'analogies', 'analogous', 'analogously', 'analy', 'analyse', 'analysed', 'analysers', 'analyses', 'analyses691617616774', 'analysing', 'analysis', 'analysis1319', 'analysisshows', 'analyst', 'analyt', 'analytically', 'analytics', 'analyze', 'analyzed', 'analyzing', 'anamorphic', 'ananiadou', 'anant', 'ananthanarayanan', 'anaplastic', 'anastasia', 'anastasios', 'anatomic', 'anatomical', 'anatomy', 'ance', 'ancient', 'and', 'and100', 'and225', 'andacetyltransferase', 'andary', 'andbookcorpus', 'andelmoontenbenchmarkingdatasetsin', 'anderson', 'andes', 'anditscontextualrepresentationatthetoplayerisusedastheinputinthefinalclassificationanotherstandard', 'anditscorrespondingtraindevtestsplitcreatedbyleeetal', 'andl', 'andlanguage', 'andmoviestowardsstorylikevisualexplanationsbywatchingmoviesandreadingbooksin', 'andor', 'andre', 'andreetto', 'andreotti', 'andrew', 'andria', 'android', 'andréaisenstadt', 'andy', 'anet64', 'anfinsen', 'ang', 'angela', 'angiomatous', 'angle', 'angles', 'anguelov', 'anhoucke', 'anil', 'animal', 'animals', 'anishchenko', 'anishchenko12', 'ankur', 'ann', 'anna', 'annees', 'anno', 'annota', 'annotate', 'annotated', 'annotating', 'annotation', 'annotationrelated', 'annotations', 'annotator', 'annotators', 'announced', 'annoying', 'annu', 'annual', 'annually', 'anonymized', 'anonymous', 'another', 'anothers', 'answer', 'answered', 'answerer', 'answering', 'answering15', 'answeringfor', 'answers', 'ant', 'antagonist', 'antani', 'anthony', 'antialiased', 'antibodies', 'antiga', 'antman', 'antonio', 'antropova', 'anubhai', 'anurag', 'any', 'anything', 'anyway', 'anywhere', 'ao', 'aowal', 'ap', 'apart', 'aperio', 'api', 'apidianaki', 'apoptosis', 'apparent', 'apparently', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appears', 'appended', 'appendedto', 'appendix', 'appl', 'applica', 'applicability', 'applicable', 'applicablefigure1', 'application', 'applications', 'applications1114', 'applicationsingeneraldomainnlpthecreationofcomprehensivebenchmarkssuchasglue', 'applied', 'applies', 'apply', 'applying', 'appoints', 'appreciably', 'appreciated', 'approach', 'approachconcatenatesthebertencodingofthegivenentitymentionseachobtainedbyapplyingmaxpooling', 'approached', 'approaches', 'approaches1', 'approaches39', 'approaching', 'appropri', 'appropriate', 'appropriately', 'approval', 'approved', 'approx', 'approxi', 'approximate', 'approximatehxw', 'approximately', 'approximately3', 'approximating', 'approximation', 'approximations', 'approximator', 'apress', 'april', 'apubmedabstractaswellasanannotatedlabelofwhetherthetextcontainstheanswertotheresearchquestion', 'ar', 'ar10', 'ar100', 'arabia', 'aragon', 'arakiembo', 'arandjelovic', 'arani', 'arated', 'arbeláez', 'arbitrarily', 'arbitrary', 'arbitraryshaped', 'arcas', 'archaeology', 'archi', 'archibald', 'archical', 'archies', 'architec', 'architectural', 'architecture', 'architecture41', 'architectures', 'archival', 'archive', 'archive175', 'archive3', 'archive98', 'archived', 'archives', 'arco', 'arcompeting', 'arcs', 'arctic', 'ardalani', 'ardefarley', 'are', 'are163264', 'area', 'areas', 'areidentity', 'arent', 'aresta', 'arg', 'arg26lys', 'arg32gln', 'argentina', 'argmax', 'argue', 'argued', 'argues', 'arighi', 'arise', 'arises', 'arisesis', 'arising', 'arithmetic', 'arivesmetacom', 'arman', 'armstrong', 'army', 'arnold', 'arose', 'around', 'arous', 'arrange', 'arranged', 'arrangements', 'arranging', 'array', 'arrays', 'arrhyth', 'arrhythmia', 'arrhythmias', 'arrival', 'arrive', 'arriving', 'arrows', 'art', 'artefact', 'artefacts', 'artetxe', 'arthe', 'arthur', 'arti', 'article', 'articles', 'articlesnature', 'artif', 'artifact', 'artifacts', 'artificial', 'artificialintelligence', 'artillery', 'artistic', 'artiﬁcer', 'artiﬁcial', 'artiﬁcially', 'artiﬁcielle', 'arts', 'arunacha', 'arxiv', 'arxiv11020183', 'arxiv12022745', 'arxiv12070580', 'arxiv12120402', 'arxiv13065151', 'arxiv13080850', 'arxiv13123005', 'arxiv13126120', 'arxiv14085093', 'arxiv14095185', 'arxiv14125903', 'arxiv14126806', 'arxiv14126980', 'arxiv150203167', 'arxiv150302531', 'arxiv150500387', 'arxiv150602078', 'arxiv150605869', 'arxiv150804025', 'arxiv150807909', 'arxiv151102301', 'arxiv151106421', 'arxiv151106440', 'arxiv151200103', 'arxiv160106733', 'arxiv160202410', 'arxiv160203483', 'arxiv160206023', 'arxiv160207261', 'arxiv160207360', 'arxiv160308029', 'arxiv160403640', 'arxiv160507146', 'arxiv160507648', 'arxiv160507716', 'arxiv160606031', 'arxiv160608415', 'arxiv160701097', 'arxiv160706450', 'arxiv160805859', 'arxiv160806993', 'arxiv160907843', 'arxiv160908144', 'arxiv161000956', 'arxiv161002357', 'arxiv161010099v2', 'arxiv161102683', 'arxiv161204426', 'arxiv170106538', 'arxiv170303130', 'arxiv170303400', 'arxiv170310722', 'arxiv170401444', 'arxiv170404368', 'arxiv170404861', 'arxiv170502315', 'arxiv170502364', 'arxiv170503122v2', 'arxiv170504304', 'arxiv170509850', 'arxiv170605137', 'arxiv170606551', 'arxiv170701836', 'arxiv170707328', 'arxiv171004087', 'arxiv171004934', 'arxiv171005941', 'arxiv171010501', 'arxiv171011041', 'arxiv171100043', 'arxiv171105225', 'arxiv171105225v3', 'arxiv171106373', 'arxiv171200409', 'arxiv171204440', 'arxiv180106146', 'arxiv180109927', 'arxiv180110198', 'arxiv180205365', 'arxiv180307703', 'arxiv180400079', 'arxiv180407461', 'arxiv180407839', 'arxiv180500932', 'arxiv180504833', 'arxiv180600451', 'arxiv180602847', 'arxiv180608730', 'arxiv180703819', 'arxiv180804444', 'arxiv180807042', 'arxiv180807233', 'arxiv180810792', 'arxiv181002891', 'arxiv181004805', 'arxiv181101241', 'arxiv181101778', 'arxiv181107056', 'arxiv181111553', 'arxiv181112231', 'arxiv181210860', 'arxiv190102860', 'arxiv190108149', 'arxiv190108634', 'arxiv190110444', 'arxiv190111373', 'arxiv190200423', 'arxiv190201313', 'arxiv190210811', 'arxiv190411486', 'arxiv190511946', 'arxiv190605849', 'arxiv190706987', 'arxiv190711692', 'arxiv190807490', 'arxiv190911740', 'arxiv191004867', 'arxiv191010683', 'arxiv191211370', 'arxiv200107966', 'arxiv200108361', 'arxiv200108856', 'arxiv200304297', 'arxiv200311539', 'arxiv200406100', 'arxiv200406165', 'arxiv200408994', 'arxiv200414444', 'arxiv200500955', 'arxiv200504790', 'arxiv200514165', 'arxiv200606195', 'arxiv200606666', 'arxiv200607733', 'arxiv200610029', 'arxiv200610503', 'arxiv200615222', 'arxiv200616228', 'arxiv200616934', 'arxiv200700644', 'arxiv2008', 'arxiv200801018', 'arxiv200801232', 'arxiv201000747', 'arxiv201011929', 'arxiv201103395', 'arxiv201204638', 'arxiv201212871', 'arxiv201215723', 'arxiv210303206', 'arxiv210901652', 'arxiv220111903', 'arxiv220402311', 'arxiv220501931', 'arxiv230712914', 'arxivorgabs14063676v3', 'arxivorgabs14120233', 'arxivorgabs14127755', 'arxivorgabs150203044', 'ary', 'arzucan', 'as', 'asap', 'asbioulw', 'asc', 'ascending', 'ascent', 'ascertain', 'ashington', 'ashish', 'ashraf', 'asics', 'aside', 'ask', 'asked', 'askell', 'asker', 'asks', 'asp', 'asp213', 'aspect', 'aspects', 'aspired', 'assem', 'assemble', 'assembled', 'assemblies', 'assembling', 'assembly', 'assess', 'assessed', 'assessing', 'assessment', 'assessments', 'assets', 'assign', 'assigned', 'assigning', 'assignment', 'assigns', 'assiri', 'assist', 'assistance', 'assistant', 'assisted', 'assistive', 'asso', 'assoc', 'associ', 'associate', 'associated', 'associates', 'associating', 'association', 'association26', 'associations', 'associative', 'assume', 'assumed', 'assuming', 'assumption', 'assumptions', 'assymétrique', 'astar', 'asterisk', 'asthma', 'astonished', 'astrocytic', 'astrocytoma', 'astrocytomas', 'astu', 'asyetunrecognized', 'asymptot', 'asymptoti', 'asymptotically', 'asynchronous', 'asynchronously', 'asynchrony', 'at', 'at41', 'atanen', 'ate', 'ated', 'atelec', 'atelecta', 'atelectasis', 'atelectasispositive', 'ately', 'atemperature', 'athens', 'athttpakamsblurb', 'ating', 'ation', 'ative', 'ative3294053bionlp', 'atlanta', 'atlas', 'atlas8789', 'atom', 'atomic', 'atomiclevel', 'atomicresolution', 'atoms', 'atomsprioritize', 'atrial', 'atrioventricular', 'attached', 'attacking', 'attained', 'attempt', 'attempted', 'attempting', 'attempts', 'atten', 'attend', 'attended', 'attending', 'attends', 'attention', 'attention31', 'attentional', 'attentionbased', 'attentionpooled', 'attentionqkv', 'attentionqwq', 'attentionweighted', 'attentive', 'attitudes', 'attn', 'attracted', 'attractions', 'attractive', 'attractors', 'attributable', 'attribute', 'attributed', 'attributes', 'attribution', 'atu', 'atuj', 'atypia', 'atypical', 'au', 'auc', 'auc0811', 'auc0828', 'auc0854', 'auc0858', 'aucbalanced', 'aucs', 'aude', 'audience', 'audio', 'auditing', 'aug', 'augmen', 'augment', 'augmentation', 'augmentation135', 'augmentations', 'augmented', 'augmenting', 'augments', 'august', 'augustin', 'auken', 'auli', 'aunt', 'aunts', 'auprc', 'auroc', 'aussie', 'austria16medical', 'austria17biotechmedgraz', 'austria18department', 'austrian', 'author', 'authorized', 'authors', 'authorsaddressygurtinnhchengmlucasnusuyamaxliutnaumannjgaoandhpoonmicrosoftresearchonemicrosoft', 'auto', 'autoaugment', 'autocontext', 'autoencoder', 'autoencoders', 'automate', 'automated', 'automati', 'automatic', 'automatically', 'automation', 'automatique', 'automaton', 'automl', 'autonomous', 'autopsy', 'autoregressive', 'autre', 'autres', 'auxil', 'auxiliary', 'av', 'av1', 'avail', 'availability', 'availabilitythe', 'available', 'availableclinicalbertembeddingsin', 'avait', 'avant', 'avaswanigooglecom', 'avb', 'avenue', 'aver', 'averag', 'average', 'averaged', 'averagepooled', 'averages', 'averaging', 'averagingovermultiplerunswithdifferentrandomizationinpracticeweobservethatthedevelopmentperfor', 'avg', 'avi', 'avions', 'avoid', 'avoided', 'avoiding', 'avons', 'avp', 'avr', 'avsec', 'aw', 'awake', 'award', 'awards', 'aware', 'away', 'aweaeaw', 'awhile', 'awni', 'awnicsstanfordedu', 'aws', 'axelrod', 'axial', 'axialattention', 'axialdeeplab', 'axillary', 'axis', 'axis0', 'axis1', 'axons', 'ay', 'ay1', 'ayahoo', 'aydogdu', 'ayk', 'az', 'azahara', 'azalia', 'azar', 'azene', 'azizi', 'azure', 'až', 'b', 'b0', 'b1', 'b1w2', 'b1þþ', 'b2', 'b3', 'b3h', 'b4', 'b5', 'b6', 'b7', 'ba', 'baac093', 'baby', 'babyn', 'bach', 'bach165', 'back', 'back1', 'backbone', 'backbones', 'backcoupling', 'background', 'backgrounds', 'backprop', 'backpropaga', 'backpropagate', 'backpropagated', 'backpropagating', 'backpropagation', 'backward', 'backwards', 'bacterial', 'bad', 'bada', 'badly', 'baek', 'baek12', 'baeket', 'bag', 'bagheri', 'baglevel', 'bagofwords', 'bagul', 'bagul1', 'bahdanau', 'bai', 'baileykelloggieeeacm', 'bajgar', 'bake', 'baker', 'baker1223', 'bakerlaborg', 'bakerrosettaserver', 'bakerscience', 'bakeshop', 'balance', 'balanced', 'balances', 'balancing', 'baldini', 'baldwin', 'balestriero', 'ball2', 'ballard', 'ballard14', 'ballas', 'ballpark', 'banaji', 'bananas', 'banani', 'band', 'bandi', 'banerjee', 'bank', 'bankier', 'banks', 'bansal', 'banupriya', 'banﬁeld', 'bar', 'barbano', 'barbato', 'barbieri12', 'barbu', 'barely', 'barham', 'bark', 'baroni', 'barrault', 'barrel', 'barret', 'barrett', 'barrier', 'barriers', 'barry', 'bars', 'bart', 'barwinska', 'barz', 'barzilay', 'basal', 'base', 'base117', 'based', 'basedmodelsingpt', 'baseline', 'baselines', 'bases', 'basic', 'basis', 'baskaran', 'basu', 'batch', 'batched', 'batches', 'batching', 'batchsize', 'bates', 'bates14', 'batra', 'batten', 'batter', 'battery', 'battle', 'battles', 'bay', 'bayesian', 'bbac409', 'bbarrel', 'bc', 'bc2gm', 'bc5', 'bc5chem', 'bc5chemical', 'bc5chemicalbc5disease', 'bc5disease', 'bd', 'bdnn', 'be', 'beach', 'beam', 'beamline', 'bear', 'bears', 'beasts', 'beat', 'beaten', 'beating', 'beats', 'beattie', 'beautiful', 'beautifully', 'became', 'because', 'bechmann', 'beck', 'become', 'becomes', 'bed', 'bedding', 'been', 'before', 'began', 'begin', 'beginners', 'beginning', 'begins', 'begun', 'beh', 'behavior', 'behavioral', 'behaviors', 'behaviour', 'behavioural', 'behaviourally', 'behind', 'behzad', 'beijing', 'being', 'beit', 'bejnordi', 'bekkerman', 'bel', 'beled', 'beler', 'belhumeur', 'belief', 'beliefs', 'believe', 'believed', 'believes', 'bell', 'bellopardo', 'belong', 'belonged', 'belongie', 'belonging', 'belongs', 'below', 'bels', 'beltagy', 'ben', 'bench', 'benchmark', 'benchmarked', 'benchmarking', 'benchmarks', 'bender', 'beneath', 'beneficial', 'benefit', 'benefited', 'benefits', 'benefits36', 'beneﬁcial', 'beneﬁt', 'beneﬁted', 'beneﬁts', 'bengio', 'bengio3', 'benign', 'benjamin', 'benjamini', 'benjaminnyejunyijessyliromapatelyinfeiyangiainjmarshallaninenkovaandbyroncwallace2018acorpuswithmulti', 'bentaieb', 'bentley', 'bepler', 'ber', 'bera', 'berbaum', 'berg', 'berger', 'bergercell', 'berghammer', 'berghammer1', 'berglund', 'berkeley', 'berlincharité', 'bernard', 'bernardi', 'bernardino', 'bernstein', 'berrada', 'bers', 'bert', 'bert121', 'bert16', 'bert2', 'bertbase', 'berti', 'bertlarge', 'bertlike', 'bertmodelsinmostbiomedicalnlptasksoftenbyasignificantmarginthegainsaremostsubstantialagainst', 'bertmodelstrainedusingoutdomaintextnotablyalthoughthepretrainingcorpusisthelargestforroberta', 'bertmodelwastrainedonwikipedia', 'bertstyle37', 'bertusingonlyabstractstheimprovementissomewhatmixedacrossthetaskswithsomegainingandothers', 'besides', 'bespoke', 'best', 'bestperforming', 'bestsingletemplate', 'bet', 'bethard', 'bethge', 'better', 'betterpretrained', 'between', 'betweenclass', 'beutel', 'beware', 'beyer', 'beygelzimer', 'beyond', 'beyondin', 'bfactor', 'bfd', 'bfduniclust30', 'bg', 'bh', 'bharambe', 'bharti', 'bhattacharya', 'bhavik', 'bhuwan', 'bi', 'bianchi', 'biannual', 'bias', 'biascontrolled', 'biased', 'biases', 'biasing', 'biasini', 'bibasilar', 'bid', 'bidirectional', 'biegert', 'bien', 'biennially', 'big', 'bigeminy', 'bigger', 'biggest', 'bigrams', 'bil', 'bilateral', 'bile', 'bilenets', 'bilenko', 'biliary', 'bilinearly', 'bilingual', 'bilities', 'bill', 'billion', 'billionaire', 'billionaires', 'billionparameters', 'billions', 'billionscale', 'bills', 'billy', 'bilstm', 'bin', 'binarization', 'binarized', 'binary', 'binder', 'binding', 'bined', 'bing', 'binned', 'binocular', 'binucleiation', 'bio', 'bioasq', 'bioasqchallengein', 'biobank', 'biobert', 'bioc', 'biochem', 'biochemistry', 'bioclinical', 'biocre', 'biocreative', 'bioenergy', 'bioeng', 'bioengineering', 'bioentity', 'biofamsa', 'biofilm', 'biogpt', 'bioimaging', 'bioinform', 'bioinform5', 'bioinformatics', 'bioinformatics12', 'bioinformatics16', 'bioinformatics18', 'bioinformaticsbtz682', 'biol', 'biol13', 'biol131', 'biol16', 'biol191', 'biol215', 'biol335', 'biol51', 'biolog', 'biological', 'biologically', 'biologist', 'biologists', 'biology', 'biology9', 'biomarker', 'biomarkers', 'biomed', 'biomedclip', 'biomedclipbased', 'biomedclippubmedbert_256vit_base_patch16_224', 'biomedical', 'biomedicallanguagemodelsasshownintable', 'biomedicalnlpapplicationstofacilitatethisstudywecreateblurbacomprehensivebenchmarkforbiomed', 'biomedicine', 'biometrics', 'bionlp', 'bionlpacl', 'biophys', 'biophysical', 'biophysics', 'biopsies', 'biopsy', 'biorxiv', 'biosci', 'biosciences', 'biosses', 'biotechnol', 'biotechnol35', 'biotechnology', 'biotite', 'bioul', 'birch', 'bird', 'birds', 'birdsnap', 'birdsnap32', 'birim', 'birkedalhansenproc', 'birthplace', 'bischke', 'bischoffj', 'bishop', 'bisong', 'bissacco', 'biswas', 'bit', 'bitl', 'bitm', 'bits', 'bixby', 'bizarre', 'black', 'blackness', 'blackwell', 'bladder', 'blaise', 'blanca', 'blanchet', 'blank', 'blanks', 'blaschko', 'blastoma', 'blastp', 'blau', 'blazakis', 'blca', 'ble', 'ble8whichmakeslearningeasierfigure', 'blended', 'bleu', 'blew', 'blies', 'blind', 'blinding', 'blindly', 'blinky', 'blip', 'blip2', 'bllip', 'blobs', 'block', 'blockade', 'blockin', 'blocks', 'bloemfontein', 'blog1', 'blondel', 'blood', 'bloom', 'bloomer', 'blue', 'bluebert', 'bluegray', 'blundell', 'blur', 'blurb', 'blurba', 'blurbbiomedicallanguageunderstandingreasoningbench', 'blurbiscomprisedofacomprehensivesetofbiomedicalnlptasksfrompubliclyavailabledatasetsincluding', 'blurring', 'bmc', 'bmvc', 'bn', 'bninception', 'bnreluconv', 'bnreluconv1', 'boag', 'board', 'boardcertified', 'boarding', 'boat', 'boats', 'bodenstein1', 'bodies', 'body', 'bokeh', 'bold', 'bolded', 'boldface', 'boleda', 'bolei', 'boltzmann', 'bolukbasi', 'bombardment', 'bombing', 'bommasani', 'bond', 'bonds', 'bone', 'bones', 'bonferroni', 'book', 'bookcorpus', 'books', 'booktest', 'boost', 'boosted', 'boosting', 'bootstrap', 'bootstrapping', 'bor', 'border', 'borderline', 'bordes', 'boris', 'born', 'borrow', 'borth', 'bos', 'boser', 'boson', 'bossard', 'boston', 'both', 'bottleneck', 'bottlenecked', 'bottlenecks', 'bottom', 'bottoms', 'bottomup', 'bottou', 'bougares', 'bought', 'bougiatiotis', 'boulos', 'boulton', 'bound', 'boundaries', 'boundary', 'boundconstrained', 'bounded', 'bounding', 'bounds', 'bourn', 'bourn2', 'bousquet', 'bow', 'bowel', 'bowen', 'bowker', 'bowl', 'bowman', 'box', 'boxed', 'boxes', 'boxplot', 'bpb', 'bpc', 'bpe', 'bpe51', 'bpm', 'bracketed', 'brackets', 'bracs', 'brad', 'bradbury', 'bradley', 'bragg', 'brain', 'brains', 'brakel', 'brancati', 'branch', 'branches', 'brandon', 'brandy', 'brane', 'brants', 'bravo', 'brca', 'brca1dysregulated', 'breadth', 'break', 'breakdown', 'breaking', 'breaks', 'breakthrough', 'breakthroughs', 'breast', 'breastcancer', 'breastcancermetastasis', 'breed', 'bregler', 'breiman', 'brendel', 'brennan', 'bres', 'bretonnel', 'brett', 'brevet', 'brevity', 'brian', 'bridge', 'bridges', 'bridging', 'bridgland', 'bridgland14', 'brief', 'briefly', 'brier', 'brigade', 'brigadier', 'briggman', 'briggs', 'brigham', 'bright', 'brightfield', 'brightness', 'bring', 'brings', 'brini', 'british', 'brittle', 'britz', 'broad', 'broader', 'broadway', 'broken', 'bromley', 'bronchus', 'bronstein', 'brother', 'brothers', 'brought', 'brown', 'brownclusters', 'browne', 'brownet', 'brox', 'broydenfletchergoldfarbshanno', 'brp', 'brubaker', 'bruce', 'brucher', 'brummer', 'brundage', 'brunk', 'brute', 'brück', 'bs', 'bt', 'buchan', 'buchatskaya', 'bud', 'budget', 'bug', 'buhlheller1516', 'build', 'building', 'builds', 'buildup', 'built', 'bulent', 'bulk', 'bull', 'bulletin', 'bullock', 'bulten', 'bunch', 'bundled', 'buolamwini', 'burden', 'burget', 'burke18', 'burleyet', 'burned', 'burovski', 'burr', 'burst', 'business', 'businessman', 'buster', 'but', 'but84x', 'bution', 'butions', 'butler', 'butter', 'buttercream', 'buttermilk', 'buy', 'buying', 'bwh', 'bwhemb', 'by', 'bychkov', 'byconductingdomainspecificpretrainingfromscratchpubmedbertconsistentlyoutperformsalltheother', 'bydifferent', 'bylayer', 'byol', 'byol27', 'bypass', 'bypasses', 'bypassing', 'byrd', 'byte', 'bytelevel', 'bytenet', 'bytepair', 'bytes', 'c', 'c015', 'c1', 'c10', 'c100', 'c155', 'c164', 'c17wilds', 'c1lslide', 'c2', 'c26c27', 'c2lpatch', 'c8c9', 'ca', 'cache', 'caching', 'caciularu', 'caded', 'cadieu', 'caffe', 'caglar', 'cai', 'caicedo', 'caiming', 'cake', 'cakes', 'cal', 'calbert', 'calculat', 'calculate', 'calculated', 'calculating', 'calculation', 'calculations', 'caleb', 'calender', 'cali', 'calibrated', 'calibration', 'califor', 'california', 'calized', 'call', 'called', 'calledefﬁ', 'calling', 'calls', 'cally', 'calorimetry', 'caltech101', 'caltech101256', 'caltech256', 'calvespereira', 'calvin', 'cam', 'cambium', 'cambridge', 'came', 'camelyon', 'camelyon16', 'camelyon1678', 'camelyon17', 'camelyon17wilds', 'camelyon78120', 'cameo', 'camera', 'cameras', 'campaign', 'campanella', 'campbell', 'camped', 'campo', 'cams', 'can', 'cana', 'canada', 'canada19life', 'canada20howard', 'canadian', 'canalwaysbenefitfrommoretextincludingoutdomaintextinfactnoneofthepriorbiomedicalrelatedbert', 'cancer', 'cancerassociated', 'cancerbioinformatics', 'cancercell', 'cancergov', 'cancerous', 'cancers', 'candidate', 'candidates', 'candido1', 'candy', 'canes', 'cannot', 'canonical', 'cant', 'cao', 'capa', 'capabili', 'capabilities', 'capability', 'capable', 'capac', 'capacity', 'capitalism', 'captain', 'caption', 'captioners', 'captioning', 'captions', 'capture', 'captured', 'captures', 'capturing', 'car', 'carbon', 'carbonaceous', 'carbonell', 'carbonnitrogen', 'carci', 'carcinoma', 'carcinomas', 'carcinosarcoma', 'card', 'cardi', 'cardiac', 'cardiogenic', 'cardiogram', 'cardiol', 'cardiolo', 'cardiologist', 'cardiologistlevel', 'cardiologists', 'cardiology', 'cardiologyamerican', 'cardiom', 'cardiomediastinal', 'cardiomegaly', 'cardiomyocyte', 'cardiovasc', 'cardiovascular', 'care', 'careful', 'carefully', 'carey', 'caricnoma', 'carl', 'carlin', 'carlini', 'carlo', 'carmsd', 'carnegie', 'carnegiemellon', 'carolyn', 'caron', 'carpuat', 'carr', 'carrara', 'carreira', 'carried', 'carrier', 'carry', 'carrying', 'cars', 'carsinceptionv4', 'carson', 'carter', 'caruana', 'carve', 'carves', 'cary', 'cas', 'casadio', 'cascade', 'cascadecorrelation', 'case', 'casebycase', 'casecontrol', 'cased', 'casedo', 'cases', 'cases79154155', 'casestratified', 'casing', 'casionallyfor', 'casp', 'casp13', 'casp14', 'casp1415', 'casper', 'caspround', 'castile', 'castro', 'casual', 'cat', 'cataloged', 'catalogue', 'catalogue7', 'catalysis', 'catalytic', 'catanzaro', 'catarina', 'catastrophic', 'catch', 'catching', 'categorical', 'categories', 'categorizatio', 'categorization', 'categorize', 'categorized', 'categorizer', 'categorizes', 'category', 'categoryan', 'categoryby', 'catherine', 'cation', 'cations', 'cative', 'cats', 'caught', 'causal', 'cause', 'caused', 'causes', 'causing', 'caution', 'cave', 'caveat', 'cavemen', 'cavern', 'caverne', 'cavities', 'cavity', 'cb', 'cbow', 'cbt', 'cbtcn', 'cbtne', 'cc', 'ccby', 'ccnet', 'ccrcc', 'ccrccprccluadlusclymph', 'ccs', 'cd', 'cdc', 'cdnopenaicomresearchcoverslanguageunsupervised', 'ce', 'cecilia', 'cedure', 'ceeded', 'ceeding', 'ceedings', 'ceedingsofthe12thinternationalworkshoponsemanticevaluationsemevalnaaclhlt2018neworleanslouisianausajune56', 'ceiling', 'ceived', 'cel', 'celeb', 'celeba', 'celebrity', 'celebrityrecognition', 'celerator', 'celi', 'cell', 'cellcell', 'cellmatrix', 'cellphone', 'cells', 'cells46', 'cellular', 'cellularmediated', 'cellvit', 'cemetery', 'cent', 'center', 'centercropped', 'centered', 'centering', 'centers', 'cently', 'central', 'central256256', 'centralis', 'centralised', 'centre', 'centres', 'centreville', 'centroid', 'centroids', 'centuries', 'century', 'cept', 'ceptive', 'ceptron', 'cer', 'ceramide', 'ceramides', 'cereb', 'cerebral', 'cernocky', 'cers1', 'certain', 'certainty', 'certified', 'cervical', 'cesareni', 'cesc', 'cess', 'cessed', 'cessing', 'cfi', 'cha', 'chain', 'chains', 'chainsa', 'chainwas', 'chair', 'chal', 'challeng', 'challenge', 'challenge120', 'challengeacm', 'challengearxiv14090575', 'challengeijcv', 'challenges', 'challenging', 'champkit', 'chan', 'chanan', 'chance', 'chandra', 'chang', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chao', 'chaotic', 'chapman', 'chapter', 'chapteroftheassociation', 'char', 'charac', 'character', 'characterised', 'characteristic', 'characteristics', 'characterization', 'characterize', 'characterized', 'characterizes', 'characterizing', 'characterlevel', 'characters', 'charge', 'chargepayment', 'charges', 'charles', 'charlotte', 'charniak', 'chase', 'chasing', 'chassang', 'chat', 'chatraryamontri', 'chatﬁeld', 'chaudhur', 'chaudhury', 'chaumond', 'chauvetpontdarc', 'che', 'cheaper', 'check', 'checklist', 'checkpoint', 'checkpoints', 'chef', 'chehao', 'chelba', 'chelsea', 'chem', 'chem281', 'chemical', 'chemicalprotein', 'chemistry', 'chemistrya', 'chemotherapy', 'chemprot', 'chen', 'chen12', 'chen1211', 'cheng', 'cheques', 'cherian', 'chernobyl', 'cherry', 'chess', 'chest', 'chestx', 'chestxray', 'chestxray14', 'chestxray8', 'cheung', 'chex', 'chexnet', 'chexnets', 'chexpert', 'chicago', 'chicken', 'chidyausiku', 'chiefly', 'chihhsuan', 'chil', 'chilamkurthy', 'child', 'child1', 'children', 'childrens', 'chimerax', 'china', 'chine', 'chinereadingcomprehension', 'chines', 'chinese', 'chino', 'chinos', 'chintala', 'chippy', 'chips', 'chipseq', 'chirality', 'chisholm', 'chitchat', 'chitecture', 'chitectures', 'chiu', 'chloramphenicol', 'cho', 'chocolate', 'choe', 'choi', 'choice', 'choices', 'chol', 'cholangiocarcinoma', 'chollet', 'choose', 'choosing', 'chopped', 'chopra', 'choromanska', 'chose', 'chosen', 'chosen177276', 'choses', 'chowdheryet', 'chowdhury', 'chrcc', 'chrcc156160', 'chrccccrcc', 'chris', 'christian', 'christianinﬂamed', 'christine', 'christmas', 'christoph', 'christopher', 'chromophobe', 'chromosomes', 'chronic', 'chronological', 'chroot', 'chrysosporiumfig', 'chrzanowski', 'chu', 'chuang', 'chung', 'chunks', 'church', 'chute1', 'ci', 'cia', 'cialist', 'cialized', 'cially', 'cian', 'ciated', 'cicero', 'ciency', 'cient', 'ciently', 'cientnets', 'cies', 'cif', 'cifar', 'cifar10', 'cifar100', 'cifar10100', 'ciga', 'cihr', 'cij', 'cimrman', 'cinema', 'cinoma', 'ciodaro', 'ciompi', 'cir', 'circles', 'circuiting', 'circuitry', 'circuits', 'circuits11', 'circularshaped', 'circulation', 'circum', 'circumscribed', 'circumstance', 'circumstances', 'cire', 'cirelli', 'ciresan', 'cis', 'citation', 'citeseer', 'cities', 'city', 'ciureailcus1', 'civil', 'civilian', 'civilization', 'ciﬁc', 'cky', 'cl64', 'claim', 'claims', 'clam', 'clam15', 'clammahmoodlaborg', 'clamped', 'clancy1', 'clarendon', 'clarify', 'clark', 'clarkepearson', 'clas', 'clashes', 'class', 'classagnostic', 'classbased', 'classdefining', 'classes', 'classi', 'classic', 'classical', 'classifi', 'classifica', 'classification', 'classification106', 'classification25', 'classification30', 'classification4245', 'classification55', 'classificationfor', 'classified', 'classifier', 'classifiers', 'classifies', 'classify', 'classifying', 'classifywhetherthe', 'classimbalanced', 'classiﬁ', 'classiﬁca', 'classiﬁcation', 'classiﬁer', 'classiﬁers', 'classroom', 'classspecific', 'classweighted', 'claudia', 'clean', 'cleaned', 'cleaning', 'clear', 'clearly', 'clematide', 'clemens', 'clement', 'cleophile', 'clesfrompmc', 'cleverest', 'clevr', 'clevrcounts', 'clevrcounts182', 'cliff', 'clifford', 'climb', 'clin', 'clini', 'clinical', 'clinicalbert', 'clinicalgrade', 'clinically', 'clinicians', 'clip', 'clip30', 'clip44', 'clipadapter', 'clipbased', 'clipped', 'clipresnet', 'clips', 'clipvit', 'cllsll', 'clonidine', 'close', 'closed', 'closely', 'closer', 'closes', 'closest', 'closing', 'closure', 'clothes', 'cloud', 'cloudbased', 'cloze', 'clozestyle', 'cls', 'clsd', 'clsencoding', 'clsisusedtocompute', 'clsq', 'clss1', 'clstokenisprependedtothebeginningtorepresentthepairthebertencodingof', 'club', 'clude', 'cluded', 'clus', 'cluster', 'clustered', 'clusterh1', 'clustering', 'clustering144', 'clusteringconstrained', 'clusteringconstrainedattention', 'clusteringguided', 'clusters', 'cm', 'cnn', 'cnnbased', 'cnns', 'cnr', 'cns', 'co', 'coad', 'coadaptation', 'coadaptations', 'coalition', 'coarse', 'coarsegrained', 'coarsegraining', 'coarsely', 'coarser', 'coarsetofine', 'coast', 'coated', 'coates', 'coattention', 'coauthors', 'coca', 'coca32', 'coco', 'cocoa', 'cocos', 'code', 'codebase', 'codebases', 'coded', 'codeleted', 'coder', 'coders', 'codes', 'codie', 'coding', 'coef', 'coefficient', 'coefﬁ', 'coefﬁcient', 'coefﬁcients', 'coenzyme', 'coevolution', 'coevolutionary', 'cognition', 'cognitiva', 'cognitive', 'cogswell', 'cohan', 'cohen', 'cohens', 'cohort', 'cohorts', 'col', 'colab', 'colab74', 'colaboratory', 'cold', 'coli', 'colin', 'coll', 'collaboration', 'collaborative', 'collaborators', 'collapsed', 'colleagues', 'collec', 'collect', 'collected', 'collecting', 'collection', 'collections', 'collections107109', 'collective', 'collectively', 'college', 'collier', 'collins', 'collinss', 'collisions', 'collobert', 'colo', 'cologne', 'colomer', 'colon', 'colonel', 'colonoscopy', 'color', 'coloragnostic', 'colorblue', 'colorectal', 'colored', 'coloring', 'colors', 'colorspeciﬁc', 'colossal', 'colour', 'colourmap', 'colours', 'columbia', 'columbus', 'column', 'columnar', 'columns', 'com', 'coma', 'comawniecg', 'combat', 'combi', 'combina', 'combination', 'combinations', 'combinato', 'combinatorial', 'combine', 'combined', 'combines', 'combining', 'comchiggsboson', 'comdeepmindalphafold', 'come', 'comeau', 'comes', 'comexdbmnist', 'comfacebookresearchesm', 'comfortable', 'comgoogleresearchmedicalairesearchfoundations', 'comhas2k1plotnine', 'coming', 'comings', 'commanded', 'commands', 'comment', 'commented', 'comments', 'commercial', 'commit', 'committee', 'committeelabeled', 'committees', 'common', 'commonbiomedicaltermsevenforscibertwhichgeneratesitsvocabularypartiallyfrombiomedicaltextthe', 'commonly', 'commons', 'commonsense', 'commu', 'commun', 'communica', 'communicate', 'communicated', 'communication', 'communications', 'communities', 'community', 'community77', 'communityaccepted', 'communityacquired', 'comp', 'compa', 'compact', 'compactly', 'compactness', 'compactnessas', 'companies', 'companion', 'company', 'comparable', 'comparably', 'comparative', 'comparator', 'compare', 'compared', 'comparedtotheotherbiomedicalrelatedpretrainingeffortsscibert', 'compares', 'compari', 'comparing', 'comparingclinicalbertswithpubmedbertonbiomedicalnlptasksshowthatevenrelatedtextsuchasclinical', 'comparison', 'comparisons', 'compatibility', 'compatible', 'compel', 'compelled', 'compelling', 'compensate', 'compet', 'competent', 'competing', 'competition', 'competitions', 'competitions1', 'competitionsprostatecancergradeassessmentdiscussion169230', 'competitive', 'competitively', 'compile', 'compiler', 'compiler75', 'complement', 'complementary', 'complements', 'complete', 'completed', 'completely', 'completing', 'completion', 'completions', 'complex', 'complexes', 'complexity', 'complicate', 'complicated', 'complicating', 'complications', 'comply', 'component', 'componential', 'components', 'composed', 'composing', 'composite', 'composition', 'compositional', 'compositionality', 'compositions', 'compound', 'comprehension', 'comprehensive', 'compress', 'compressed', 'compresses', 'compressing', 'compression', 'compressiont', 'comprise', 'comprised', 'comprises', 'compu', 'comput', 'computa', 'computation', 'computation9', 'computational', 'computationally', 'computations', 'compute', 'computed', 'computer', 'computeraided', 'computerassisted', 'computerinterpreted', 'computerized', 'computers', 'computervision', 'computes', 'computing', 'comsoedinglabmmseqs2', 'comtensorflowtensorflow', 'comwatchvkoqfxbpploet1390s', 'con', 'concatenate', 'concatenated', 'concatenates', 'concatenating', 'concatenation', 'concathead1', 'concei', 'conceivable', 'conceived', 'concentration', 'concept', 'concepts', 'conceptual', 'conceptualized', 'conceptuallyevidencebasedmedicalinformationextractionisakintoslotfillingasittriestoidentify', 'concern', 'concerned', 'concerning', 'concerns', 'conch', 'conclude', 'conclusion', 'conclusion8485', 'conclusions', 'concordance', 'concrete', 'concur', 'concurrent', 'concurrently', 'condensed', 'condition', 'conditional', 'conditionals', 'conditioned', 'conditioning', 'conditions', 'conducive', 'conduct', 'conducted', 'conducting', 'conduction', 'conducts', 'conf', 'conf_idence', 'confer', 'conference', 'conferenceof', 'confers', 'confi', 'confidence', 'confidences', 'confident', 'confidently', 'config', 'configu', 'configuration', 'configurations', 'configured', 'confirm', 'confirmation', 'confirmed', 'conflicts', 'conformational', 'conforming', 'confounding', 'confuse', 'confused', 'confusing', 'confusion', 'cong', 'cong56', 'congestive', 'congo', 'conjecture', 'conjunction', 'connaissance', 'conneau', 'connec', 'connect', 'connectall', 'connected', 'connecteddense', 'connectedif', 'connecticut', 'connecting', 'connection', 'connections', 'connectionsone', 'connectionthere', 'connectiv', 'connectivity', 'connectomic', 'connectomics55', 'connects', 'connor', 'conquest', 'conse', 'consecutive', 'consensus', 'consent', 'consequence', 'consequences', 'consequently', 'conservation', 'conservative', 'conserved', 'consid', 'consider', 'considerable', 'considerably', 'considerations', 'considered', 'considering', 'considers', 'consis', 'consist', 'consisted', 'consistency', 'consistent', 'consistently', 'consisting', 'consists', 'consoli', 'consolidation', 'consolidationnegative', 'consortium', 'consortium76', 'consortiumbioinformatics319', 'constant', 'constants', 'constituents', 'constitute', 'constitutes', 'constituting', 'constrain', 'constrained', 'constraining', 'constraint', 'constraints', 'construc', 'construct', 'constructed', 'constructing', 'construction', 'consult', 'consultant', 'consumer', 'consumergrade', 'consuming', 'contact', 'contactmap', 'contactmaps', 'contactp', 'contacts', 'contain', 'contained', 'containing', 'contains', 'containsm', 'contaminated', 'contamination', 'contemporary', 'content', 'contentaddressable', 'contentbased', 'contents', 'contest', 'contests', 'context', 'contextaware', 'contextdependent', 'contextless', 'contexts', 'contextual', 'contextualize', 'contextualized', 'contextualizes', 'contextxnm', 'contiguous', 'continent', 'continents', 'continual', 'continually', 'continualpretraining', 'continuations', 'continue', 'continued', 'continues', 'continuous', 'continuously', 'contours', 'contract', 'contractions', 'contralateral', 'contrary', 'contrast', 'contrastive', 'contrastively', 'contrastwholeword', 'contri', 'contrib', 'contribute', 'contributed', 'contributes', 'contribution', 'contributions', 'contributionsmb', 'contributor', 'contributors', 'contributory', 'control', 'controlled', 'controlling', 'controls', 'conv', 'conv1', 'conv1x1', 'conv2', 'conv3', 'conv31', 'conv3x3', 'conv4', 'conv5', 'conv51', 'convenient', 'convention', 'conventional', 'conventionally', 'conventions', 'conventions762', 'conver', 'converge', 'convergence', 'converges', 'converging', 'converging5', 'conversation', 'conversational', 'conversely', 'convert', 'converted', 'converting', 'converts', 'convey', 'conveyed', 'conveys', 'convirt', 'convnet', 'convnetbased', 'convnets', 'convo', 'convolu', 'convolution', 'convolutional', 'convolutions', 'convs2s', 'conﬁdence', 'conﬁdent', 'conﬁguration', 'conﬁgurations', 'conﬁgured', 'conﬁrmation', 'conﬁrmed', 'conﬂicts', 'cooccurrences', 'cookbook', 'cookie', 'cookies', 'cool', 'cooldown', 'cooled', 'cooper', 'coor', 'coordi', 'coordinate', 'coordinated', 'coordinates', 'cope', 'copied', 'copies', 'copy', 'copypaste', 'copyright', 'copyrightsforcomponentsofthisworkownedbyothersthantheauthorsmustbehonoredabstractingwithcreditispermittedtocopy', 'coqa', 'coqas', 'cor', 'coram', 'cordoba', 'core', 'coreb', 'coreneedle', 'coreneedlebiopsied', 'corenlp', 'cores', 'corn', 'cornell', 'corner', 'coronary', 'corpora', 'corporal', 'corporawhichdoesnot', 'corps', 'corpus', 'corpusitdoesthisbyfirstshatteringallwordsinthecorpusandinitializingthevocabularywithcharactersand', 'corr', 'corrado', 'corre', 'correct', 'corrected', 'correction', 'corrections', 'correctly', 'correctness', 'correla', 'correlated', 'correlates', 'correlation', 'correlations', 'correspond', 'corresponded', 'correspondence', 'corresponding', 'correspondingly', 'corresponds', 'corrugated', 'corruption', 'cortes', 'cortex', 'cortex45', 'cortical', 'cosine', 'cosinesimilarity', 'cosmochim', 'cospos100002idmodel', 'cost', 'costa4', 'costly', 'costs', 'cote', 'cotton', 'coudray', 'cough', 'could', 'council', 'count', 'counter', 'counteract', 'counterintuitive', 'counterpart', 'counterparts', 'counting', 'countless', 'countries', 'country', 'counts', 'county', 'coup', 'coupled', 'coupling', 'couplings2224', 'couprie', 'cour', 'courage', 'cournapeau', 'course', 'courtiol', 'courville', 'couture', 'cov', 'covariance', 'covariate', 'covariates', 'covariation', 'cover', 'coverage', 'covered', 'covering', 'covers', 'covery', 'covington', 'cowie', 'cowie14', 'cox', 'cozzetto', 'cpath', 'cpi', 'cpis', 'cplanet', 'cpr', 'cptac', 'cptacdhmc', 'cpu', 'cpus', 'craft', 'crammer', 'crane', 'crane86', 'cranial', 'craniopharyngioma', 'crash', 'crasslike', 'crawford', 'crawl', 'crawled', 'crc', 'crc100k', 'crc100k84', 'crc100knonorm98', 'crcc', 'crcmsi', 'crcvalhe7k', 'cream', 'crease', 'creases', 'creasing', 'create', 'created', 'creates', 'createspace', 'creating', 'creation', 'creative', 'creativepioneering', 'creators', 'creatures', 'credibility', 'credit', 'crevice', 'crew', 'crf', 'crf33', 'crg', 'cribriform', 'crichton', 'cried', 'crime', 'criminal', 'cringe', 'crisis', 'crisscross', 'crit', 'criteria', 'criterion', 'criti', 'critical', 'crop', 'cropped', 'cropping', 'crops', 'cross', 'cross_entropy_losslogits', 'crossattention', 'crossattn', 'crosschain', 'crossentropy', 'crosses', 'crossgpu', 'crossing', 'crosslayer', 'crosslinguistic', 'crossmodal', 'crossmodality', 'crossslide', 'crossvalidated', 'crossvalidation', 'crowded', 'crowdlabeled', 'crowdsourced', 'crowdsourcing', 'cru', 'crucial', 'crucially', 'crude', 'crumbs', 'crunchy', 'crush', 'crushed', 'cruzroa', 'cryo', 'cryoelectron', 'cryoem', 'cryogenic', 'cryosection', 'cryosectioned', 'crypts', 'crystal', 'crystallization', 'crystallographic', 'crystallography', 'crystals', 'cs', 'cscc', 'cscl', 'cscv', 'cslg', 'cst', 'csubtyping', 'csurka', 'ct', 'cterminal', 'ctranspath', 'ctranspath37', 'ctranspath62', 'ctranspath62a', 'cu', 'cuadros', 'cubanski', 'cubuk', 'cuda', 'cues', 'cui', 'cuit', 'cuits', 'cukierski', 'culating', 'culation', 'culminated', 'cult', 'cumulative', 'cun', 'cun3', 'cuni03b1', 'cuntoor', 'cup', 'cups', 'cur', 'curacies', 'curacy', 'curate', 'curated', 'curatedﬁltered', 'curating', 'curation', 'curb', 'curr', 'curran', 'currency', 'current', 'currently', 'curse', 'curtis', 'curve', 'curves', 'curving', 'cus', 'cussion', 'custom', 'customary', 'custommade', 'cut', 'cutaneous', 'cutcut', 'cute', 'cutoff', 'cutout', 'cuttingedge', 'cvf', 'cvpr', 'cvpr05', 'cvpr09', 'cvpr18', 'cw', 'cway', 'cy', 'cyan', 'cyberinfrastructure', 'cybern', 'cycle', 'cycles', 'cyganski', 'cygwin', 'cystadenocar', 'cysteine', 'cystic', 'cysts', 'cyte', 'cytic', 'cytokeratin', 'cytoma', 'cytometry', 'cytopathology', 'cytoplasm', 'cytoplasmic', 'czarnecki', 'czczupvitadapter', 'cα', 'd', 'd05', 'd1', 'd10', 'd13a', 'd170d176', 'd2', 'd20', 'd30d40', 'd335d343', 'd4', 'd439d444', 'd464d474', 'd480d489', 'd520d528', 'd570d578', 'd60', 'd723d733', 'd80', 'd_e', 'd_i', 'd_t', 'da', 'da3', 'daad', 'dab', 'dabakeruwedu', 'dag', 'dagunts', 'dahl', 'dai', 'daily', 'daisy', 'dal', 'dalchebuc', 'dalhousie', 'dall', 'dallas', 'dally', 'dalwadi19', 'damage', 'damaged', 'damazio', 'damental', 'damour', 'dampening', 'damping', 'danafarber', 'dance', 'daniel', 'daniela', 'danihelka', 'danqi', 'dant', 'danube', 'dao', 'dapi', 'dapprentissage', 'daram', 'darbre', 'darcet', 'dard', 'dards', 'dario', 'dark', 'darrell', 'dartmouth', 'dartmouthhitchcock', 'darwin', 'das', 'dashed', 'dat', 'data', 'data2123', 'data3', 'data3334', 'data910', 'database', 'databases', 'databasescientific', 'datacommonscancergov', 'dataconnected', 'datadependent', 'dataefficient', 'dataefﬁcient', 'datagenerating', 'dataparallel', 'datascopecptac', 'dataset', 'datasetarxiv', 'datasets', 'datasets22', 'datasetsize', 'datasetsizedependent', 'datasetsourexperimentsshowthatdomainspecificpretrainingservesasasolidfoundationforawiderangeofbiomedical', 'datasetsthereforewegroupthedatasetsbytheirtasktypescomputetheaveragescoreforeachtasktypeand', 'dataspecific', 'date', 'dates', 'dating', 'dation', 'daughter', 'dauparas', 'dauparas12', 'dauparaset', 'dauphin', 'dauter', 'dautres', 'dautume', 'dave', 'david', 'davidson', 'davies', 'davis', 'davos', 'day', 'dayan', 'days', 'daysthe', 'dazione', 'db', 'dbi', 'dc', 'dc15', 'dd', 'ddd', 'ddd14www12rrr13', 'ddi', 'ddimension', 'ddp', 'de', 'deac0205ch11231', 'deal', 'dealt', 'deaminase', 'dean', 'deane', 'death', 'deaton', 'debate', 'debiasing', 'debris', 'debruycker', 'debudajgrabysz', 'dec', 'decade', 'decades', 'decameric', 'decanlp', 'decathlon', 'decay', 'decayed', 'decays', 'december', 'decencière', 'decide', 'decided', 'deciding', 'decision', 'decisionmaking', 'declaration', 'declare', 'declerck', 'decode', 'decoder', 'decoders', 'decoding', 'decompbioldatahhsuitedatabaseshhsuite_dbs', 'decomposable', 'decomposition', 'deconstructing', 'decorated', 'decoupled', 'decrease', 'decreased', 'decreases', 'decreasing', 'decrementing', 'dedicated', 'deducing', 'deduplication', 'deemed', 'deep', 'deepaccnet', 'deepatt', 'deepattnmisl146147', 'deeper', 'deepest', 'deepface', 'deeplearning', 'deeplearningbased', 'deeply', 'deeplyfused', 'deeplysupervised', 'deepmind', 'deepminds', 'deepsmile', 'default', 'defeat', 'deficiencies', 'deficiency', 'define', 'defined', 'defines', 'defini', 'defining', 'definition', 'definitively', 'deformations', 'deforming', 'defrayed', 'degenerative', 'degiovanni12', 'degra', 'degradation', 'degradations', 'degrade', 'degrades', 'degree', 'degrees', 'dehghani', 'deidentified', 'deidentify', 'deja', 'del', 'delakis', 'delalleau', 'delangue', 'delay', 'delays', 'dele', 'deleted', 'deletion', 'deliberately', 'delicate', 'delicious', 'delimiter', 'delimiters', 'delineate', 'delineated', 'delineating', 'delineation', 'deliver', 'delivered', 'delivery', 'della', 'delo', 'delong', 'delongs', 'delving', 'dem', 'demands', 'dementia', 'demis', 'demner', 'demnerfushman', 'demo', 'demon', 'demonstra', 'demonstrate', 'demonstrated', 'demonstrates', 'demonstrating', 'demonstration', 'demonstrations', 'den', 'dence', 'deng', 'dengel', 'denim', 'deniro', 'denis', 'denk', 'denker', 'denny', 'denoising', 'denom', 'denomination', 'denote', 'denoted', 'denotes', 'denoting', 'denoyer', 'dense', 'densed', 'densely', 'denselysampled', 'densenet', 'densenet121', 'densenet121k', 'densenet161', 'densenet161k', 'densenet161k48', 'densenet169', 'densenet169k', 'densenet201', 'densenet201k', 'densenet264', 'densenetb', 'densenetbc', 'densenetbc100', 'densenetc', 'densenets', 'densenetsbc', 'density', 'dent', 'dently', 'denton', 'denzler', 'deoras', 'deorowicz', 'depart', 'department', 'departmental', 'departure', 'depend', 'dependen', 'dependence', 'dependencies', 'dependency', 'dependent', 'depending', 'depends', 'depicted', 'deploy', 'deployed', 'deployment', 'depos', 'deposited', 'deposition', 'depriva', 'deprivation', 'deprived', 'depth', 'depths', 'depthwidthresolution', 'depthwise', 'der', 'derek', 'deri', 'dering', 'derivative', 'derivatives', 'derive', 'derived', 'derives', 'dermatologistlevel', 'des', 'desai', 'descendants', 'descending', 'descent', 'describe', 'described', 'describes', 'describing', 'description', 'descriptionof', 'descriptions', 'descriptive', 'descriptors', 'desertion', 'desiderata', 'design', 'design4', 'design43', 'design59', 'designated', 'designed', 'designing', 'designs', 'desirable', 'desired', 'desjardins', 'desmaison', 'desmoplastic', 'desouza', 'desperately', 'despite', 'destigter', 'destroyed', 'destructive', 'desy', 'det', 'detached', 'detail', 'detailed', 'details', 'detect', 'detectable', 'detected', 'detecting', 'detection', 'detection15', 'detection20', 'detection5', 'detection78', 'detector', 'detector2', 'detectors', 'detectorsarxiv12070580', 'detectron2', 'detects', 'deter', 'determinants', 'determination', 'determinations', 'determine', 'determined', 'determined5', 'determinedstructures', 'determines', 'determining', 'deterministic', 'detokenizer', 'detokenizers', 'dev', 'deva', 'devastiating', 'devel', 'develop', 'developed', 'developers', 'developing', 'development', 'developmentally', 'developments', 'develops', 'devia', 'deviation', 'deviations', 'device', 'devices', 'devices3536', 'devil', 'devin', 'devise', 'devised', 'devito', 'devlin', 'devon', 'deﬁne', 'deﬁned', 'deﬁnitely', 'df', 'dff', 'dfkw', 'dfns', 'dgx2', 'dh', 'dh1mutant', 'dhariwal', 'dhcontactdeepmindcom', 'dhingra', 'dhmc', 'dhmckidney', 'di', 'diab', 'diabetes', 'diabetic', 'diag', 'diagenesis', 'diagnos', 'diagnose', 'diagnoses', 'diagnosing', 'diagnosis', 'diagnosis3334', 'diagnostic', 'diagnostically', 'diagnostics', 'diagram', 'diagrams', 'dialect', 'dialectic', 'dialog', 'dialogbased', 'dialogues', 'diamond', 'diamos', 'dian', 'diao', 'diate', 'diaz', 'dicarlo', 'dicates', 'dicating', 'dicative', 'dice', 'dickstein', 'dict', 'dicted', 'diction', 'dictionary', 'dictions', 'did', 'didnot', 'didnt', 'die', 'diederik', 'diederikpkingmaandjimmyba2015adamamethodforstochasticoptimizationin', 'diego', 'dieleman', 'dient', 'dients', 'dif', 'differ', 'differen', 'difference', 'differences', 'different', 'differentiable', 'differential', 'differentiated', 'differentiates', 'differentiating', 'differentiation', 'differs', 'difficult', 'difficulties', 'difficulty', 'diffuse', 'difﬁciles', 'difﬁcul', 'difﬁcult', 'difﬁcultieee', 'difﬁculties', 'difﬁculty', 'digestivesystem', 'digestpath', 'digestpath86', 'digit', 'digital', 'digitization', 'digitize', 'digitized', 'digitizing', 'digitrecognition', 'digits', 'dijk13', 'dilated', 'dilemma', 'dill', 'dimaio12', 'dimen', 'dimension', 'dimensional', 'dimensionality', 'dimensions', 'dimensionstable', 'dimer', 'dimeric', 'dimin', 'diminished', 'diminishes', 'diminishing', 'dina', 'dinan', 'dinant', 'dinates', 'ding', 'ding1', 'ding1611', 'ding17', 'dingerpymolopensource', 'dings', 'dinklage', 'dino', 'dino25', 'dinov2', 'diograph', 'diographic', 'diographs', 'diologist', 'diologists', 'diomegaly', 'dipanjan', 'diperna', 'diploma', 'direc', 'direct', 'directed', 'direction', 'directions', 'directly', 'directly4750', 'directory', 'dis', 'dis44', 'disagreements', 'disambiguate', 'disambiguation', 'disc', 'discard', 'discarded', 'discarding', 'discharge', 'discharged', 'disclose', 'disclosed', 'disclosure', 'disconnected', 'discontin', 'discontinu', 'discontinuous', 'discordance', 'discordances', 'discordant', 'discounted', 'discourse', 'discourselevel', 'discov', 'discover', 'discovered', 'discoveries', 'discovering', 'discovers', 'discovery', 'discrepancies', 'discrepancy', 'discrete', 'discrimi', 'discrimination', 'discriminative', 'discs', 'discuss', 'discussed', 'discussion', 'discussion169230', 'discussions', 'disease', 'disease1213', 'diseases', 'disentangle', 'dish', 'disintegrin', 'disjointed', 'disk', 'disks', 'disorders', 'disparate', 'disparities', 'dispensing', 'displaced', 'displacing', 'display', 'displayed', 'displayedmiddle', 'displays', 'dispute', 'dissecting', 'dissertation', 'distance', 'distance340', 'distances', 'distant', 'distilla', 'distillation', 'distilling', 'distills', 'distin', 'distinct', 'distinction', 'distinctions', 'distinctive', 'distinguish', 'distinguishable', 'distinguished', 'distinguishing', 'distogram', 'distort', 'distorting', 'distortion', 'distortions', 'distracting', 'distraction', 'distri', 'distrib', 'distributed', 'distributeddataparal', 'distribution', 'distributional', 'distributions', 'dit', 'ditch', 'dition', 'ditional', 'ditioned', 'ditions', 'dium', 'dive', 'divergence', 'diverging', 'diverse', 'diversity', 'diversity123', 'diversiﬁed', 'divide', 'divided', 'divides', 'dividing', 'dividual', 'diving', 'divisible', 'division', 'divvala', 'diﬀer', 'diﬀerence', 'diﬀerent', 'diﬃ', 'djibouti', 'djo', 'djolonga', 'dk', 'dlbc', 'dlbclnos', 'dle', 'dler', 'dmitry', 'dmodel', 'dmodeldimensional', 'dmodelh', 'dmr8107494', 'dna', 'dnn', 'dnnbased', 'dnns', 'dnorm', 'do', 'doc', 'doc50', 'docfund', 'docking', 'dockq', 'docu', 'document', 'documentd', 'documented', 'documenting', 'documentlevelnary', 'documents', 'dodge', 'doersch', 'does', 'doesnt', 'dog', 'dogan', 'dogs', 'doi', 'doi101038nature14539', 'doing', 'doiorg101038s4155102000682w', 'doiorg101038s4159101802683', 'doll', 'dollar', 'dolz', 'dom', 'domain', 'domain367879', 'domain4454', 'domainbioinformatics', 'domainpacking', 'domains', 'domainspecific', 'domainthangeneraldomaintextaddingthemdoesnotconferanyadvantageasisevidentbytheresultsofclin', 'domainvocabularytable', 'domes', 'dominance', 'dominant', 'dominantly', 'dominate', 'dominated', 'domly', 'donahue', 'donald', 'done', 'dong', 'donghui', 'donner', 'donors', 'dont', 'door', 'doors', 'doplasmic', 'dorsett', 'dos', 'doshivelez', 'dosovitskiy', 'dot', 'dotdash', 'dotproduct', 'dots', 'dotted', 'double', 'doubleback', 'doubled', 'doubles', 'doubling', 'doubts', 'douglas', 'doukhan', 'douze', 'down', 'download', 'downloaded', 'downloading', 'downplay', 'downregulator', 'downsample', 'downsampled', 'downsamples', 'downsampling', 'downscale', 'downscaling', 'downstream', 'downward', 'doya', 'dozat', 'doğan', 'dp', 'dp5od026389', 'dr', 'drafts', 'dragnet', 'drake', 'dramatic', 'dramatically', 'dras', 'drasti', 'drastic', 'drastically', 'draw', 'drawback', 'drawing', 'drawings', 'drawn', 'dream', 'dred', 'dreds', 'dren', 'dress', 'dressed', 'drew', 'drifting', 'drifts', 'drites', 'drive', 'driven', 'drives', 'driving', 'drizzled', 'drobysheva', 'drop', 'droplets', 'dropout', 'dropout44', 'dropoutdroppath', 'droppath', 'dropped', 'dropping', 'drops', 'drug', 'drugdisease', 'drugdrug', 'ds', 'dsn', 'dt', 'dtd166', 'du', 'dual', 'dualstream', 'duan', 'duan1', 'dubbed', 'dubourg', 'duce', 'duced', 'duces', 'ducharme', 'duchesnay', 'ductal', 'ducted', 'duction', 'duda', 'due', 'dues', 'duh', 'duke', 'dummification', 'dummified', 'dummify', 'dummifying', 'dummy', 'dumontier', 'dune', 'dungworth', 'dunne', 'duplicate', 'duplicated', 'duplicates', 'dur', 'duration', 'dure', 'dures', 'during', 'durme', 'dustin', 'duty', 'dutyendoftextmoscow', 'dv', 'dvances', 'dvd', 'dvdimensional', 'dw', 'dwr', 'dx', 'dy', 'dyer', 'dyi', 'dynamic', 'dynamical', 'dynamically', 'dynamics', 'dynamischen', 'dynasty', 'dysplasia', 'dziedzic', 'dzmitry', 'e', 'e0118432', 'e0161879', 'e0220182', 'e0233678', 'e1', 'e1002195', 'e1002730', 'e1003963', 'e1005324', 'e1005659', 'e1007597', 'e1008707', 'e1008865', 'e104129', 'e1293e1301', 'e2016239118', 'e2021785118', 'e215e220', 'e2300038', 'e253e261', 'e265e275', 'e28766', 'e2970e2979', 'e52690', 'e56', 'e6', 'ea', 'eaaz3041', 'each', 'eacher', 'eaejawt', 'eager', 'ear', 'ear8352055', 'earectopic', 'earivr', 'earlier', 'earliest', 'early', 'earlystopping', 'earn', 'ears', 'earth', 'earths', 'eas', 'ease', 'eases', 'easier', 'easiest', 'easily', 'east', 'eastman', 'easy', 'easytouse', 'ebiomedicine', 'ebm', 'ebrahimpour', 'ebrains', 'ebrains8788', 'ebrains89', 'ebrecht13', 'ec', 'ec0o9', 'eccles', 'eccv', 'ecg', 'ecgrelated', 'ecgs', 'ech', 'echappes', 'echle', 'echnologies', 'echnology', 'echos', 'ecker', 'eclare', 'eclipse', 'ecol', 'ecologic', 'ecological', 'ecology', 'ecommerce', 'economical', 'ectopic', 'ectopy', 'ector', 'ecv', 'ed', 'edaldi', 'eddy', 'edelmang', 'edema', 'edenbrandt', 'edfr', 'edge', 'edges', 'edit', 'editable', 'edited', 'edition', 'editor', 'editorial', 'editors', 'editorsreviewers', 'eds', 'edu', 'educa', 'educational', 'edupubrosettafoldall_human_gpcr_unknown_modelstargz', 'edward', 'ee', 'eeg', 'ef', 'eff', 'effec', 'effect', 'effective', 'effectively', 'effectiveness', 'effector', 'effects', 'effi', 'efficiency', 'efficiency4x', 'efficient', 'efficiently', 'efficiently44', 'efficientnet', 'efficientnetnoisystudent', 'effort', 'effort14', 'effortless', 'efforts', 'effusion', 'effusions', 'efros', 'efﬁ', 'efﬁciency', 'efﬁcient', 'efﬁciently', 'efﬁcientnet', 'efﬁcientnetb0', 'efﬁcientnetb1', 'efﬁcientnetb2', 'efﬁcientnetb3', 'efﬁcientnetb4', 'efﬁcientnetb5', 'efﬁcientnetb6', 'efﬁcientnetb7', 'efﬁcientnetb7843', 'efﬁcientnetb7917', 'efﬁcientnetb7930', 'efﬁcientnetl2', 'efﬁcientnets', 'efﬁcientnetstyle', 'eg', 'eg30000instandardbertmodelsor50000inroberta', 'egblood', 'egdiabetic', 'egg', 'eggs', 'egories', 'egorization', 'egou', 'egplacebo', 'egregious', 'egrltvyctvq', 'ei', 'eid', 'eigen', 'eigenvalue', 'eigenvalues', 'eigenvector', 'eight', 'eightfold', 'eighth', 'einberger', 'einstein', 'eisenstein', 'either', 'ej', 'ekaterina', 'ekg', 'ekim', 'el', 'elaborate', 'elaborated', 'elaine', 'ele', 'electrocardiogram', 'electrocardiograms', 'electrocardiography', 'electrocardiol', 'electrochemical', 'electrodes', 'electron', 'electronic', 'electronics', 'electrophysiol', 'electrophysiologists', 'eled', 'elegant', 'elegantly', 'element', 'elementary', 'elements', 'elementwise', 'elephants', 'eleventh', 'elfwing', 'elgamacy', 'elgammal', 'elhihi', 'elhoseiny', 'eliceiri', 'elimi', 'eliminates', 'eling', 'elizalde', 'elkan', 'ellen', 'elling', 'elmos', 'elnaggar', 'els', 'else', 'elsen', 'elsevier', 'elsewherea', 'eluded', 'em', 'email', 'emails', 'emami', 'emanating', 'ematical', 'ematically', 'emb', 'embed', 'embedded', 'embedding', 'embeddings', 'embeddingsjournal', 'embo', 'embrace', 'embracing', 'embryonal', 'embryos', 'embs', 'emd21645', 'emer', 'emerge', 'emerged', 'emergence', 'emergent', 'emerges', 'emerging', 'emilio', 'emily', 'emnlp', 'emnlp14', 'emnlp18', 'emnlpijcnlp19', 'emo', 'emperor', 'emphasis', 'emphasize', 'emphasized', 'emphasizes', 'emphysema', 'empire', 'empirical', 'empirically', 'empiricallyarxiv', 'emplate', 'employ', 'employed', 'employees', 'employs', 'empower', 'empowered', 'empty', 'emrs', 'en', 'enable', 'enabled', 'enables', 'enabling', 'enan', 'enantiomer', 'enantiomers', 'encapsulate', 'encase', 'ence', 'ences', 'encode', 'encoded', 'encoder', 'encoderdecoder', 'encoders', 'encoders103', 'encoders22104', 'encodes', 'encoding', 'encodings', 'encountered', 'encounters', 'encourage', 'encouraged', 'encouragement', 'encourages', 'encouraging', 'end', 'ende', 'ended', 'ending', 'endocervical', 'endocrine', 'endolysin', 'endometrial', 'endometrioid', 'endomyocardial', 'endoscopic', 'endothe', 'endothelium', 'endpoints', 'ends', 'endto', 'endtoend', 'enemies', 'energy', 'energybased', 'enforced', 'enforcement', 'enforcing', 'enfr', 'eng', 'engel', 'engelshoven', 'engi', 'engine', 'engineer', 'engineering', 'engineering26', 'engineers', 'engines', 'engl', 'england', 'english', 'englishfrench', 'englishgerman', 'englishtofrench', 'englishtogerman', 'enhance', 'enhanced', 'enhancing', 'enjoy', 'enlarge', 'enlarged', 'enlargement', 'enlisted', 'enlistment', 'enormous', 'enough', 'enriched', 'ensemble', 'ensembled', 'ensembles', 'ensembling', 'ensor', 'ensorflow', 'ensorflow70', 'ensure', 'ensures', 'ensuring', 'ent', 'entailment', 'entered', 'entering', 'enterprise', 'entire', 'entirely', 'entirety', 'entities', 'entitiesiereplacean', 'entity', 'entitylevel', 'entityrelation', 'entitywitha', 'entrants', 'entries', 'entropic', 'entropy', 'entry', 'ents', 'enumerate', 'environmental', 'environments', 'envision', 'envoy', 'enwik8', 'enzyme', 'enzymes', 'eo', 'eos', 'eosin', 'eosinophilic', 'ep', 'epend', 'ependymal', 'ependymoma', 'epidemiological', 'epidemiology', 'epilepsy', 'episode', 'episodes', 'epithelial', 'epithelialrestricted', 'epithelioid', 'epithelium', 'epoch', 'epochs', 'eprints', 'epstein', 'eq', 'eqn1', 'eqn2', 'equal', 'equally', 'equalweighted', 'equation', 'equations', 'equipment', 'equipped', 'equiv', 'equiva', 'equivalence', 'equivalent', 'equivalents', 'equivariant', 'er', 'era', 'erably', 'erage', 'eral', 'erally', 'erardmarchant', 'eras', 'erate', 'erating', 'eration', 'erative', 'erence', 'erez', 'ergodic', 'erhan', 'eric', 'eris', 'ermon', 'ern', 'ernievil', 'ernst', 'err', 'erratic', 'erroneously', 'error', 'error3', 'errorconfidence', 'errorcorrecting', 'errorderivative', 'errorleft', 'errors', 'errorsurface', 'ers', 'ery', 'es', 'esann', 'esca', 'escahttpszenodoorgrecord7548828', 'escherichia', 'eschewing', 'esize', 'eskimo', 'esm', 'esm1b', 'esm2', 'esm2s', 'esmfold', 'esmfolda', 'esmfoldpredicted', 'esmfolds', 'esmfoldv0', 'esophageal', 'esophagogas', 'esophagogastric', 'esophagus', 'especially', 'espeholt', 'esrf', 'essen', 'essence', 'essential', 'essentially', 'est', 'establish', 'established', 'establishes', 'establishing', 'establishment', 'estep', 'esteva', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'estod', 'esx', 'et', 'etc', 'ete', 'eters', 'ethical', 'ethics', 'ethnicity', 'eu', 'euclidean', 'eukaryotic', 'euro', 'europe', 'european', 'eurosat', 'eurosat371', 'eurospeech', 'eva', 'eval', 'evals', 'evalu', 'evalua', 'evaluat', 'evaluate', 'evaluated', 'evaluates', 'evaluating', 'evaluation', 'evaluation81', 'evaluations', 'evalue', 'evance', 'evans', 'evans14', 'evanset', 'evasion', 'even', 'event', 'eventually', 'ever', 'everest', 'everingham', 'every', 'everyday', 'everyone', 'everything', 'everywhere', 'evi', 'evidence', 'evidencebased', 'evidenced', 'evidencepositive', 'evident', 'evo', 'evoformer', 'evolu', 'evolution', 'evolutionarily', 'evolutionary', 'evolutionaryhistorybased', 'evolutionaryscale', 'evolutions', 'evolve', 'evolved', 'evolving', 'ewerth', 'ex', 'ex1', 'ex22a2', 'exact', 'exactly', 'exam', 'exami', 'examina', 'examination', 'examinations', 'examine', 'examined', 'examines', 'examining', 'example', 'exampleagenementionoftenreferstoboththednaandgeneproductssuchasthernaandproteinfollowing', 'examplebased', 'examplenaloxone', 'examples', 'examples21', 'examplewe', 'exceed', 'exceeded', 'exceeding', 'exceeds', 'excel', 'excellent', 'excels', 'except', 'exception', 'exceptional', 'exceptionally', 'exceptions', 'exchange', 'excitatory', 'excited', 'exciting', 'exclude', 'excluded', 'excludes', 'excluding', 'exclusion', 'exclusions', 'exclusive', 'exclusively', 'exclusivity', 'executed', 'execution', 'exemplar', 'exemplars', 'exemplifies', 'exempliﬁed', 'exempted', 'exhaustive', 'exhaustively', 'exhibit', 'exhibited', 'exhibiting', 'exhibits', 'exisiting', 'exist', 'existed', 'existence', 'existing', 'exists', 'exit', 'exp', 'expand', 'expanded', 'expansion', 'expect', 'expectation', 'expectations', 'expected', 'expedited', 'expensive', 'exper', 'experi', 'experience', 'experimen', 'experiment', 'experimental', 'experimentalists', 'experimentallevel', 'experimentally', 'experimentation', 'experimented', 'experiments', 'expert', 'expertise', 'expertlevel', 'experts', 'expired', 'explain', 'explainable', 'explained', 'explains', 'explanation', 'explanations', 'explicit', 'explicitly', 'expliquait', 'explique', 'explode', 'exploit', 'exploitation', 'exploited', 'exploiting', 'exploits', 'explor', 'exploration', 'explorations', 'exploratory', 'explore', 'explored', 'exploring', 'explosion', 'explosions', 'expo', 'exponential', 'exponentially', 'exponentiallymany', 'exposed', 'exposes', 'exposing', 'exposure', 'expres', 'express', 'expressed', 'expression', 'expressions', 'expressive', 'expressiveness', 'expxi', 'expxj', 'expz', 'expzexpz', 'expτ', 'expτut', 'ext', 'exten', 'extend', 'extended', 'extending', 'extends', 'extension', 'extensive', 'extensively', 'extent', 'external', 'externally', 'extra', 'extrac', 'extract', 'extracted', 'extracting', 'extraction', 'extraction33', 'extractionexisting', 'extractions', 'extractive', 'extractor', 'extractors', 'extracts', 'extrapolate', 'extras', 'extreme', 'extremely', 'extrinsic', 'exxon', 'eye', 'eyelid', 'eyes', 'ezana', 'ezk', 'eﬀect', 'eﬀu', 'eﬀusion', 'f', 'f1', 'f1000res', 'f1x1', 'f4', 'f6', 'f8', 'f_loor', 'fa', 'fabrics', 'fac', 'face', 'facebook', 'facebookresearchdetectron2', 'facebookresearchmask2former', 'facecovinidplip', 'faced', 'faces', 'facial', 'facilitate', 'facilitating', 'facilities', 'facing', 'fact', 'factoidstyle', 'factor', 'factorcentering', 'factorization', 'factorize', 'factorizing', 'factors', 'fahlman', 'fail', 'failed', 'fails', 'failure', 'failures', 'fair', 'faire', 'fairface', 'fairly', 'fairness', 'fairseq', 'faisal', 'faisalmahmoodbwhharvardedu', 'fait', 'faithfullyfollowed', 'fake', 'fall', 'falls', 'false', 'falsenegative', 'falsepositive', 'falsethechemprotannotationisnotexhaustiveforallchemicalproteinpairsfollowingprevious', 'famasked', 'famil', 'familiar', 'familiarity', 'families', 'family', 'famous', 'famsa', 'famsa65', 'fan', 'fang', 'fantastic', 'fape', 'far', 'farabet', 'farhadi', 'fariselli', 'farther', 'fashion', 'fast', 'fasta', 'faster', 'fastest', 'fastforward', 'father', 'faulkner', 'fault', 'fauw', 'favor', 'favoring', 'favorite', 'favour', 'fawcett', 'fazelzarandi1', 'fc', 'fc77', 'fcaption', 'fcontrast', 'fd', 'fda', 'fdacleared', 'fea', 'feasibility', 'feasible', 'feasibly', 'feature', 'featurebased', 'featured', 'featureextraction', 'featuremap', 'featuremaps', 'features', 'features151152', 'features25132133', 'featuresarxiv', 'featureseg', 'featuring', 'featurization', 'featurizer', 'february', 'febs', 'fed', 'federal', 'fee', 'feed', 'feedback', 'feedfor', 'feedforward', 'feeding', 'feel', 'feet', 'fei', 'feichtenhofer', 'feifei', 'feigin', 'fel', 'feldman', 'felix', 'fell', 'felleman', 'fellow', 'fellows', 'fellowship', 'female', 'feng', 'fer', 'ferdinand', 'ference', 'ferences', 'ferent', 'fergus', 'fernandez', 'fernando', 'ferred', 'fethi', 'fever', 'fevriersullivan', 'few', 'fewer', 'fewshot', 'ff', 'ffnx', 'ffpe', 'fg', 'fgvc', 'fgvcaircraft', 'fgvcaircraft113', 'fi', 'fibrillation', 'fibrillation222336', 'fibrillation3132', 'fibrosis', 'fibrotic', 'fibrous', 'fication', 'fications', 'ficial', 'ficiencyweusepaddingortruncationtosettheinputlengthto128tokensforgadand256tokensforchemprot', 'ficients', 'ficulty', 'fidelis', 'fidelity', 'fidence', 'fidler', 'fied', 'field', 'field32', 'fieldprogrammable', 'fields', 'fieldspecific', 'fiers', 'fifteen', 'fifth', 'fig', 'fig1', 'figs', 'figure', 'figure1', 'figures', 'figurnov', 'figurnov14', 'fil', 'file', 'filed', 'files', 'filiot', 'fill', 'filling', 'film', 'films', 'filter', 'filtered', 'filtering', 'filters', 'final', 'finally', 'financial', 'find', 'finding', 'findings', 'finds', 'fine', 'finegrained', 'finergrained', 'finetune', 'finetuned', 'finetuning', 'fingerprints', 'finite', 'finn', 'firing', 'firooz', 'first', 'fisch', 'fischer', 'fisher', 'fit', 'fit113119', 'fitnet', 'fitnets', 'fits', 'fitted', 'fitting', 'fitzgerald', 'five', 'fivelayer', 'fix', 'fixed', 'fixedsize', 'fixi', 'fixing', 'fixres', 'fj', 'fk', 'fkf1', 'flagship', 'flamingo', 'flashattention', 'flashattention125', 'flat', 'flattening', 'flaugnatti', 'flava', 'flawedfacedatacom', 'fleischner', 'flexibility', 'flexible', 'flexibly', 'fli', 'flips', 'floating', 'flops', 'flopsaccuracy', 'flopsn', 'flopsresnet152', 'florence', 'florencio', 'flow', 'flower', 'flowers', 'flowers102', 'flowers102125', 'flowing', 'flows', 'fluctuate', 'fluid', 'flutter', 'flynn', 'fm', 'fnr', 'foamy', 'focal', 'focus', 'focused', 'focuses', 'focusing', 'foersch', 'fogelmansoulié', 'fois', 'fol', 'fold', 'folded', 'folder', 'folding', 'folds', 'foldseek', 'foldseekdetermined', 'follow', 'followed', 'following', 'follows', 'followthecommonpracticeandevaluateontheabstractlevel', 'fon', 'fonc', 'fonctionnelle', 'fonctionnement', 'font', 'fonts', 'food', 'food101', 'fool', 'fooled', 'for', 'forbidden', 'forbiomedicaldomainspecificpretrainingwegeneratethevocabularyandconductpretrainingusingthelatest', 'force', 'forced', 'forcedtodivertparameterizationcapacityandtrainingbandwidthtomodelbiomedicaltermsusingfragmented', 'forcement', 'forces', 'ford', 'fordeduleaderboard', 'fore', 'foreground', 'foreign', 'forest', 'forests', 'forgetting', 'forgot', 'form', 'formal', 'formalinfixed', 'formalized', 'formally', 'formance', 'format', 'formation', 'formed', 'former', 'forming', 'forms', 'formula', 'formulate', 'formulated', 'formulating', 'formulation', 'formulations', 'fornamed', 'fornia', 'forsaken', 'forth', 'fortran', 'fortunato', 'forumidee277p3ayc', 'forumidyicbfdntty', 'forward', 'forwardpass', 'fossil', 'fossils', 'found', 'found86', 'foundation', 'foundational', 'foundations', 'founder', 'fountain', 'four', 'fourchain', 'fourclass', 'fourhorned', 'fourier', 'fourlayer', 'fourrecent', 'fourteenth', 'fourth', 'fov', 'fovea', 'fovs', 'fox', 'fp', 'fp16', 'fpgabased', 'fractal', 'fractalnet', 'fractalnets', 'fraction', 'fractions', 'fracture', 'fracture005', 'fractures', 'frage', 'fragkiadaki', 'fragmentation', 'fragments', 'frame', 'framealigned', 'framed', 'frames', 'framework', 'framework105', 'france', 'francesca', 'francesco', 'francisco', 'francois', 'frangi', 'frank', 'franken', 'franquet', 'frasconi', 'free', 'freedom', 'freefloating', 'freely', 'freeman', 'freetext', 'freetype', 'fren', 'french', 'frenchenglish', 'frequencies', 'frequency', 'frequencyagnostic', 'frequencyweighted', 'frequent', 'frequently', 'fresh', 'freud', 'freund', 'frey', 'frictional', 'fridge', 'friedland', 'friend', 'friends', 'frisbee', 'fro', 'frodo', 'froide', 'from', 'from1', 'frombioprotocol', 'frombos', 'frome', 'frompermissionsacmorg', 'front', 'frontal', 'frontalview', 'frontier', 'frontière', 'frosting', 'frozen', 'fruit', 'fruitful', 'fruits', 'fs', 'fsubtyping', 'ft', 'ftfy', 'ftlist', 'fu', 'fuchs', 'fukushima', 'ful', 'fulfilled', 'fulfillment', 'fulkerson', 'full', 'fulltextarticlesbluebert', 'fully', 'fullyconneced', 'fullyconnected', 'fullysegmented', 'fun', 'func', 'function', 'function37', 'functiona', 'functional', 'functionality', 'functionf', 'functioning', 'functions', 'functions2', 'functiontu', 'fund', 'fundamental', 'fundamentally', 'funded', 'funding', 'funds', 'fundus', 'fungus', 'funny', 'fur', 'further', 'furthermore', 'fused', 'fushman', 'fusing', 'fusion', 'fusions', 'futermancell', 'futermaniubmb', 'future', 'futures', 'fuzzy', 'fvs', 'fw', 'fwf', 'fx', 'fxwi', 'fying', 'fz', 'g', 'g0', 'g009', 'g032', 'g1', 'g129', 'g154r', 'g2', 'g216', 'g253', 'g3', 'g324', 'g362', 'g368', 'g4', 'g403', 'g420', 'g427', 'g473', 'g480', 'g488', 'g498', 'g5', 'gabriel', 'gad', 'gain', 'gaining', 'gains', 'gainsovergeneraldomainlanguagemodels', 'gallagher', 'gamal', 'gambardella', 'game', 'games', 'games100', 'gamper', 'gan', 'ganglia', 'ganglioglioma', 'ganguli', 'ganisms', 'ganjoo', 'gao', 'gap', 'gaps', 'garantie', 'garcia', 'garcia9101120', 'gard', 'gardner', 'garet', 'garg', 'garnett', 'garra', 'garshasb', 'garvie', 'gas', 'gastric', 'gastroesophageal', 'gastrointestinal', 'gate', 'gated', 'gates', 'gathering', 'gating', 'gation', 'gations', 'gatta', 'gatys', 'gaussian', 'gave', 'gavves', 'gb', 'gbd', 'gbg', 'gbm', 'gbmlgg', 'gcrf', 'gdccancergov', 'gdt', 'gdt33', 'ge', 'gebru', 'geert', 'gehring', 'gehrmann', 'geiger', 'geirhos', 'gej', 'gelly', 'gelus', 'gen', 'gence', 'gender', 'gene', 'genedisease', 'geneous', 'gener', 'general', 'generaldomain', 'generalisation', 'generalist', 'generalists', 'generality', 'generaliza', 'generalizability', 'generalizable', 'generalization', 'generalize', 'generalized', 'generalizes', 'generalizing', 'generally', 'generalpurpose', 'generate', 'generated', 'generatedby', 'generates', 'generating', 'generation', 'generation4041', 'generations', 'generative', 'generativeimage2t', 'generator', 'generic', 'generictagsuchasdrugor', 'generous', 'genes', 'genet', 'genet98', 'genetic', 'genetically', 'genia', 'genital', 'genitalia', 'genome', 'genomic', 'genomics', 'genotypetissue', 'gent', 'genuine', 'geo', 'geochem', 'geochim', 'geoff', 'geoffrey', 'geographic', 'geolo', 'geolocalization', 'geomean', 'geometric', 'geometry', 'geometryaware', 'georg', 'george', 'georgia', 'georgios', 'gerber', 'germ', 'german', 'germany', 'gersh', 'gershman', 'gerstung', 'gestalt', 'gestalts', 'gests', 'gestures', 'get', 'gets', 'getting', 'gettysburg', 'gflopsimage', 'gg', 'gh', 'gh349cornelledu', 'ghaffari', 'ghassemi', 'ghemawat', 'ghiasi', 'gholami', 'ghost', 'ghosts', 'ght', 'gi', 'gianni', 'giant', 'gibraltar', 'gifts', 'gigabytes', 'gigapixel', 'gigapixels', 'gigapixelsized', 'gigascience', 'gigassl', 'giles', 'gillespie', 'gillette', 'gillian', 'gillick', 'gimelshein', 'gimpel', 'gin', 'gina', 'ginsburg', 'gion', 'gir', 'giraud', 'giraﬀe', 'girdhar', 'girish', 'girl', 'girshick', 'gish', 'gist', 'gisting', 'gists', 'git', 'git78', 'gitbase', 'github', 'githubcom', 'githubcomawniecg', 'githubcomczczupvitadapter', 'githubcomdeepmind', 'githubcomfacebookresearchdinov2', 'githubcomgoogleresearchmedicalairesearch', 'githubcommahmoodlabclam', 'githubcommbananilgssl', 'githubcompandasdevpandas', 'githubcomxiyuewangtranspath', 'githubioprojectschexnet', 'gitlarge', 'give', 'given', 'gives', 'giving', 'giy065', 'gizem', 'gj', 'gkioxari', 'gland', 'glands', 'glasgow', 'glass', 'glassman91011', 'glassmanet', 'glaze', 'gle', 'gleason', 'gli', 'glimpse', 'glioblas', 'glioblastoma', 'glioblastomas', 'glioma', 'gliomas', 'glioneuronal', 'gliosar', 'glish', 'glitch', 'gln', 'global', 'globallocal', 'globally', 'globular', 'gloria', 'glorious', 'glorot', 'glossary', 'glotz', 'glove', 'glucosamine6phosphate', 'glucose', 'glue', 'gly115', 'gly116', 'gly154arg', 'glyat', 'glycine', 'gm127390', 'gnmt', 'gnu', 'go', 'goal', 'goals', 'goeldin', 'goes', 'goh', 'gohagan', 'gohlke', 'going', 'gold', 'goldberg', 'goldberger', 'goldein', 'golden', 'goldie', 'goldilocks', 'goldman', 'goldschmidt', 'goldstandard', 'golgi', 'golovizin', 'gomez', 'gommers', 'gon', 'gonaturecom7cjbaa', 'gone', 'gong', 'gonzalvo', 'gonzález', 'good', 'goodfellow', 'goodness', 'goodrich', 'google', 'googlenet', 'googlers', 'googles', 'gool', 'goroshin', 'goswami', 'got', 'gothelial', 'gotten', 'gous', 'gouvernement', 'gouws', 'gov', 'govern', 'governed', 'government', 'gowlett', 'goyal', 'gpcr', 'gpcrs', 'gpipe', 'gplv3', 'gpt', 'gpt2', 'gpt2s', 'gpt3', 'gptstyle', 'gpu', 'gpus', 'gq', 'gra', 'grabska', 'grad', 'gradcam', 'gradcams', 'grade', 'gradi', 'gradient', 'gradientbased', 'gradients', 'gradientweighted', 'grading', 'grading18', 'grading2122', 'grading75', 'gradually', 'graduate', 'graham', 'gramfort', 'grams', 'granada', 'grand', 'grandchallengeorg', 'grandchallengeorgdata', 'grandchild', 'grangier', 'grant', 'granted', 'grantham', 'granting', 'grants', 'granulated', 'graph', 'graphbased', 'graphical', 'graphics', 'graphs', 'grateful', 'grating', 'gratuite', 'gratuity', 'grave', 'graves', 'gray', 'graybioinformatics', 'graz', 'great', 'greater', 'greatest', 'greatly', 'greece', 'greedily', 'greedy', 'green', 'green14', 'greenberg', 'greenspan', 'greenwald', 'greff', 'gregor', 'grenoble', 'gression', 'gretchen', 'grewal', 'grey', 'grid', 'gridlike', 'grifﬁn', 'grill', 'grille', 'grisel', 'grishin6721', 'grl', 'gross', 'grosse', 'grossman', 'groth', 'ground', 'grounded', 'groundtruth', 'group', 'group1986', 'groupcondition', 'grouped', 'groupings', 'groups', 'groups6', 'grow', 'growing', 'grows', 'growth', 'gt450', 'gtex', 'gtexportalorghome', 'gtsrb', 'gtsrb184', 'gtx', 'gu', 'guadarrama', 'guage', 'guan', 'guarantee', 'guaranteed', 'gudyś', 'guer', 'guerre', 'guestrin', 'guglin', 'guide', 'guided', 'guidelines', 'guillaume', 'guillaumin', 'guishes', 'gulcehre', 'gulrajani', 'gulshan', 'gun', 'gunboat', 'guo', 'gupta', 'gurcan', 'gurevych', 'gursel', 'guseynov', 'gut', 'gutenberg', 'guys', 'gyrus', 'gyu', 'göbel', 'gülçehre', 'h', 'h1', 'h183q', 'h2', 'h2t', 'h3', 'h3c', 'h4', 'ha', 'haas', 'haberland', 'had', 'haddow', 'hadnt', 'hadsell', 'hadﬁeld', 'haem', 'haematology', 'haematoxylin', 'haering', 'haffner', 'haghgoo1', 'haghpanahi', 'haghpanahi26', 'hahnbeom', 'hai', 'haiku', 'hakime', 'halabi3', 'haldane', 'half', 'halfspaces', 'halfwave', 'halifax', 'halil', 'hall', 'hallacy', 'hallmark', 'hallmarks', 'halve', 'halved', 'halves', 'halving', 'ham', 'hamamatsu', 'hamarneh', 'hamburg', 'hamming', 'hamner', 'hamper', 'han', 'hanahan', 'hanazawa', 'hand', 'handan', 'handbook', 'handcrafted', 'handengineered', 'handful', 'handle', 'handled', 'handles', 'handling', 'hands5051', 'handwriting', 'handwritten', 'hanks', 'hanley', 'hannun', 'hansell', 'hao', 'haphazard', 'happen', 'happening', 'happens', 'harald', 'harb', 'harbor', 'hard', 'hardcoding', 'hardware', 'hardwareaware', 'hardwaresuchasgpusandaculminationofadvancesinoptimizationmethodssuchasadam', 'hardwood', 'hare', 'hariharan', 'harmeling', 'harmful', 'harmon', 'harmonic', 'harmonises', 'harmony', 'harris', 'hart', 'harvard', 'harvardmit', 'harvest', 'has', 'has2k1plotnine', 'hasaunt', 'hasfewer', 'hasll1', 'hassabis', 'hassabis14', 'hat', 'hata', 'hate', 'hateful', 'hauser', 'have', 'haveb', 'haveno', 'haverinen', 'haves', 'haveyou', 'having', 'havior', 'hawkins', 'hayes', 'hayman', 'hays', 'haystack3134', 'haz', 'hdf5', 'he', 'head', 'head111', 'headed', 'headhwo', 'headi', 'heads', 'headscarf', 'headtohead', 'health', 'healthcare', 'healthy', 'heard', 'heart', 'heartbeat', 'heartbeats', 'heat', 'heatmap', 'heatmaps', 'heavily', 'heavy', 'heb', 'hebb', 'hebbian', 'hechtman', 'hed', 'hedgehog', 'hedges', 'hegde', 'height', 'heigold', 'heindl', 'heinz', 'heinzinger', 'hel', 'helber', 'held', 'heldout', 'helen', 'helices', 'heller', 'hellinger', 'helmstaedter', 'help', 'helped', 'helpful', 'helping', 'helps', 'helsinki', 'hemangioblastoma', 'hemangioma', 'hemangioperi', 'hemangiopericytoma', 'hematolymphoid', 'hematopathology', 'hemisphere', 'hemorrhage', 'henaff', 'hence', 'henderson', 'hendrycks', 'henighan', 'hennigan', 'henrik', 'henriksen', 'hepatocellular', 'her', 'herbert', 'herd', 'here', 'hereby', 'herent', 'heresizethe', 'herexftfallback', 'heritage', 'hermann', 'hermannneyuteessenandreinhardkneser1994onstructuringprobabilisticdependencesinstochasticlanguagemodelling', 'hernia', 'hernie', 'herrerozazo', 'hershel', 'hes', 'hesitate', 'hestained', 'hestness', 'hetero', 'heterocomplexes', 'heterocontacts', 'heterodimeric', 'heterogene', 'heterogeneity', 'heterogeneous', 'heteromer', 'heterotypic', 'heuristic', 'heuristics', 'hhblits', 'hhblits61', 'hhsearch', 'hhsearch66', 'hhsuite', 'hhsuite3', 'hi', 'hiand', 'hibiting', 'hibits', 'hidden', 'hiding', 'hie13', 'hier', 'hierarchical', 'hierarchies', 'hierarchy', 'hieu', 'higgs', 'high', 'highaccuracy', 'highattention', 'highcapacity', 'highconfidence', 'highdimensional', 'higher', 'higheraccuracy', 'higherlevel', 'higherresolution', 'highertraining', 'highest', 'highgrade', 'highlevel', 'highlight', 'highlighted', 'highlighting', 'highlights', 'highly', 'highlyoptimized', 'highoverlap', 'highperformance', 'highquality', 'highresolution', 'highsensitivity', 'highsimilarity', 'highthroughput', 'highvalue', 'highway', 'hill', 'hillier', 'hilly', 'him', 'himself', 'hin', 'hindering', 'hinders', 'hint', 'hinting', 'hinton', 'hinton45', 'hintoncsutorontoca', 'hintont', 'hints', 'hipt', 'hiranuma', 'his', 'his182', 'his182a', 'his183gln', 'histiocytes', 'histiocytosis', 'histo', 'histocat', 'histogram', 'histologic', 'histological', 'histology', 'histologybased', 'histologygenomic', 'histomolecular', 'histomorphological', 'histopa', 'histopathologic', 'histopathological', 'histopathology', 'histopathologybased', 'histopathologyrelevant', 'histopathologyspecific', 'historical', 'history', 'hit', 'hitchcock', 'hits', 'hiwi', 'hiwicidenotes', 'hjs', 'hk', 'hl135274', 'hmm', 'hmmer', 'hmmer368', 'hmmhmm', 'hmms', 'hmmsearch', 'hnsc', 'ho', 'hoang', 'hoc', 'hoch', 'hochberg', 'hochreiter', 'hoffman', 'hoi', 'hoifung', 'hoifungmicrosoftcom', 'hold', 'holder', 'holding', 'holds', 'holes', 'holger', 'holiday', 'holla', 'hollon', 'holst', 'holter', 'holub', 'home', 'homeland', 'homemaker', 'homme', 'homo', 'homolog', 'homologous', 'homologs', 'homologue', 'homologues', 'homology', 'homomers', 'homotrimer', 'homotypic', 'hong', 'hongfei', 'hongkun', 'hongsuck', 'honkala', 'honolulu', 'hoogs', 'hopcroft', 'hope', 'hopf', 'hopfield', 'hopfweld', 'hoping', 'hopstaken', 'horizontal', 'horizontally', 'horlings', 'horn', 'hornak', 'horns', 'horvitz', 'hospital', 'hospital102', 'hospitalized', 'hospitals', 'hospitalscale', 'host', 'hosts', 'hou', 'houlsby', 'hour', 'hours', 'house', 'houses', 'houthi', 'hovernet', 'hovy', 'how', 'howard', 'however', 'howto100m', 'hoyer', 'hoyle', 'hp', 'hrd', 'hs', 'hseu', 'hslidei', 'hsv', 'ht', 'ht1', 'htimes', 'html', 'htmlbased', 'http', 'httpaggc22grandchallenge', 'httparxivorg', 'httparxivorgabs12022160', 'httparxivorgabs12120142', 'httparxivorgabs13126229', 'httparxivorgabs14090473', 'httparxivorgabs14091556', 'httparxivorgabs14114280', 'httparxivorgabs14126980', 'httparxivorgabs150205698', 'httpauthorslibrarycaltechedu7694', 'httpbmirdsgithub', 'httpclam', 'httpclammah', 'httpcodegooglecompcudaconvnet', 'httpcolahgithubio', 'httpcreativecommonsorglicensesby40', 'httpdatamende', 'httpeddylaborgsoftwarehmmer', 'httpfilesipduw', 'httpfilesipduwedupubrosettafoldgpcr_benchmark_', 'httpfilesipduwedupubrosettafoldhuman_prottargz', 'httpfilesipduwedupubrosettafoldmr_modelstargz', 'httpgithubcommahmoodlab', 'httpgithubcommahmoodlabclam', 'httphuggingfacecomahmoodlabconch', 'httpjmlrorgpapersv2120074html', 'httpmscocoorgdatasetdetectionschallenge2015', 'httpncbinlmnihgovpmctoolsopen', 'httppandagrandchallengeorgdata', 'httppapersnipsccpaper1102hierarchicalrecurrentneuralnetworksfor', 'httpportalgdccancergov', 'https', 'httpsaclanthologyorg2020emnlpmain112', 'httpsaggc22grandchallenge', 'httpsakamsblurb', 'httpsarxivorg', 'httpsarxivorgabs160304467', 'httpsbfdmmseqs', 'httpsbfdmmseqscom', 'httpsbmirdsgithubiokidney', 'httpsbmirdsgithubiokidneycancer', 'httpsbmirdsgithubiolungcancer', 'httpsbracsicarcnrit', 'httpscamelyon17', 'httpscamelyon17grandchallengeorgdata', 'httpscameo3dorg', 'httpscancerimagingarchivenet', 'httpscdnrcsborgresources', 'httpscdnrcsborgresourcessequenceclustersbc40out', 'httpscloudgooglecomvisiondocs', 'httpscorpusbyueduiweb', 'httpsdatamendeleycomdatasets9xxm58dvs31', 'httpsdeepmindcomblogopensourcingsonnet', 'httpsdoi', 'httpsdoiorg', 'httpsdoiorg10', 'httpsdoiorg101038', 'httpsdoiorg101038s4155102000682w', 'httpsdoiorg101038s41586021038192', 'httpsdoiorg101038s41586021038281', 'httpsdoiorg101038s41587023020199', 'httpsdoiorg101038s4159101802683', 'httpsdoiorg101038s41591024028564', 'httpsdoiorg101038s41591024028573', 'httpsdoiorg101101', 'httpsdoiorg10110120200722211482', 'httpsdoiorg10110120210510', 'httpsdoiorg10110120211004463034', 'httpsdoiorg10110120220207479398', 'httpsdoiorg10110120221120517210', 'httpsdoiorg101109tmi20203021387', 'httpsdoiorg1011453458754', 'httpsdoiorg1013026grp0z205', 'httpsdoiorg1018653', 'httpsdoiorg102139', 'httpsdoiorg102139ssrn4172090', 'httpsdoiorg102210', 'httpsdoiorg1022489cinc2017065469', 'httpsdoiorg1022489cinc2017066138', 'httpsdoiorg1022489cinc2017070060', 'httpsdoiorg1022489cinc2017166054', 'httpsdoiorg1022489cinc2017360239', 'httpsdoiorg1025493wq48zgx', 'httpsdoiorg1048550', 'httpsdoiorg1048550arxiv', 'httpsdoiorg1048550arxiv210807258', 'httpsdoiorg1048550arxiv211111432', 'httpsdoiorg1048550arxiv220406455', 'httpsdoiorg1048550arxiv230300915', 'httpsdoiorg1048550arxiv230406819', 'httpsdoiorg1048550arxiv230407193', 'httpsdoiorg1048550arxiv230412210', 'httpsdoiorg1048550arxiv230510160', 'httpsdoiorg1048550arxiv230911510', 'httpsdoiorg1048550arxiv231007033', 'httpsdoiorg1048550arxiv231013259', 'httpsdoiorg105281zenodo7566741', 'httpsdoiorg106084', 'httpsdoiorg107937tcia9cjf0127', 'httpsesmatlascom', 'httpsftpebiacuk', 'httpsftpebiacukpubdatabasesmetagenomicspeptide_database2018_12', 'httpsftpebiacukpubdatabasesuniprot', 'httpsftpebiacukpubdatabasesuniprotprevious_releases', 'httpsftpwwpdb', 'httpsftpwwpdborgpubpdbderived_data', 'httpsgithub', 'httpsgithubcom', 'httpsgithubcombokehbokeh', 'httpsgithubcombytedanceibot', 'httpsgithubcomdeep', 'httpsgithubcomdeepmind', 'httpsgithubcomdeepminddmhaiku', 'httpsgithubcomfacebookfbresnettorch', 'httpsgithubcomfacebookresearchdetectron2', 'httpsgithubcomgooglejax', 'httpsgithubcomliuzhuang13densenet', 'httpsgithubcommahmoodlab', 'httpsgithubcommahmoodlabclam', 'httpsgithubcommahmoodlabuni', 'httpsgithubcommatplotlibmatplotlib', 'httpsgithubcommbananilgssl', 'httpsgithubcommwaskomseaborn', 'httpsgithubcomnumpynumpy', 'httpsgithubcomopenaiclip', 'httpsgithubcomopenaigpt2', 'httpsgithubcomopenmm', 'httpsgithubcompandasdevpandas', 'httpsgithubcomrefresh', 'httpsgithubcomschro', 'httpsgithubcomschrodingerpymolopensource', 'httpsgithubcomsoedinglabhhsuite', 'httpsgithubcomsoedinglabmmseqs2', 'httpsgithubcomstatsmodelsstatsmodels', 'httpsgithubcomtensorflowtensorflow', 'httpshugging', 'httpshuggingfaceco', 'httpshuggingfacecomicrosoft', 'httpshuggingfacecomicrosoftgitbase', 'httpshuggingfacecomodels', 'httpshuggingfacecoopenaiclipvitbasepatch16', 'httpsiciar2018challengegrandchallenge', 'httpsiciar2018challengegrandchallengeorgdataset', 'httpsirhythmgithubiocardiol_test_set', 'httpsmediumcomblaisea', 'httpsncbinlmnihgovpmctoolsopenftlist', 'httpsopenreview', 'httpsopenreviewnet', 'httpsopenreviewnetforumid9vrb9d0wi4', 'httpsopenreviewnetforumidydopye6dg', 'httpspanda', 'httpspandagrandchallengeorgdata', 'httpsphysionetorgchal', 'httpsportal', 'httpsportalgdc', 'httpsportalgdccancer', 'httpsportalgdccancergov', 'httpspredictioncenterorgcasp14', 'httpsproteomic', 'httpspytorch', 'httpsresearchgooglecom', 'httpsresearchgooglecomcolaboratory', 'httpsrobetta', 'httpssearchkgebrainseuinstancesdataset8fc108abe2b4406f899960269dc1f994', 'httpsstanfordmlgroup', 'httpswildsstan', 'httpswildsstanfordedudatasets', 'httpswsss4luadgrandchallengeorg', 'httpswww', 'httpswwwcdcgovfeatures', 'httpswwwgtexportalorghome', 'httpswwwkaggle', 'httpswwwkagglecom', 'httpswwwncbinlmnihgovpmc', 'httpswwwpnasorg', 'httpswwwpredictioncenterorgcasp14doccasp14_abstractspdf', 'httpswwwpythonorg', 'httpswwwscienceorg', 'httpswwwtensorfloworgxla', 'httpswwwusergwdg', 'httpswwwusergwdgdecompbiol', 'httpswwwusergwdgdecompbioldatahhsuitedatabaseshhsuite_dbs', 'httpswwwusergwdgdecompbioluniclust2018_08', 'httpswwwwwpdborgftppdbftpsites', 'httpswwwyoutube', 'httpswwwyoutubecom', 'httpszenodo', 'httpszenodoorg', 'httpszenodoorgrecord1214456', 'httpszenodoorgrecord3832231', 'httpszenodoorgrecord4643645', 'httpszenodoorgrecord5068265', 'httpszenodoorgrecord5889558', 'httpszenodoorgrecord6460100', 'httpszenodoorgrecord6604094', 'httpszenodoorgrecord7412731', 'httpszenodoorgrecord7898308', 'httpszenodoorgrecord7898308zgxm3xbxac', 'httpszhanglabdcmbmed', 'httpszhanglabdcmbmedumichedutmalign', 'httpwsss4luadgrandchallengeorg', 'httpwww', 'httpwwwimagenetorgchallengeslsvrc2012', 'httpwwwnaturecomreprints', 'httpyann', 'httpzenodoorgrecord1214456', 'hu', 'hua', 'huang', 'hub', 'hubbard', 'hubel', 'hue', 'huesaturationvalue', 'huffman', 'huge', 'hugging', 'huggingface', 'huggingfaceco', 'huggingfacecomicrosoftgitlarge', 'hughes', 'hula', 'hum', 'human', 'humanai', 'humancomputer', 'humaninterpretable', 'humanintheloop', 'humanlevel', 'humanlike', 'humanonly', 'humanreadable', 'humans', 'humanwritten', 'humboldtstrasse', 'hun', 'huncrc', 'huncrc99', 'hundred', 'hundreds', 'hungarian', 'huo', 'hurts', 'husband', 'husky', 'hussien', 'hutter', 'hwa', 'hwaproc', 'hx', 'hxw', 'hyberbolic', 'hybrid', 'hybridize', 'hydrogen', 'hydrolase', 'hydrolyzes', 'hydrophobic', 'hyper', 'hyperparameter', 'hyperparameters', 'hyperplane19', 'hyperplasia', 'hyperplastic', 'hypertension', 'hypoth', 'hypotheses', 'hypothesis', 'hypothesize', 'hypothesized', 'hypothesizes', 'hz', 'högberg', 'hörst', 'hℓ', 'hℓx0', 'hℓxℓ', 'i', 'i04', 'i1', 'i121i129', 'i1505', 'i1s', 'i2b2', 'i2t', 'i2tsource', 'i2ttcga', 'i49i58', 'i52i59', 'i_e', 'i_f', 'ia', 'iandola', 'iarity', 'iary', 'iat', 'ib', 'ibb', 'iberian', 'ibm', 'ibot', 'ibot118', 'ibot74', 'ibottrained', 'ical', 'icalbert', 'ically', 'icalnlpwecreateanewbenchmark', 'ican', 'icar', 'icccn', 'iccv', 'iccv05', 'iccv15', 'iciar', 'icip', 'iclr', 'iclr15', 'iclr19', 'icml', 'icmlm', 'icpr', 'icvgip', 'id', 'id303', 'idation', 'idc', 'idea', 'ideal', 'ideally', 'ideas', 'iden', 'identi', 'identical', 'identically', 'identifed', 'identification', 'identified', 'identifiers', 'identify', 'identifying', 'identity', 'identitypresentation', 'identiﬁcation', 'identiﬁes', 'idh1', 'idh1mutant', 'idh1wildtype', 'idioventricular', 'idues', 'ie', 'ieaw', 'ieee', 'ieeecvf', 'ier', 'ies', 'ies314454', 'ies445456', 'iew', 'if', 'ig', 'igarashi', 'igel', 'ignorance', 'ignore', 'ignored', 'ignores', 'ignoring', 'igor', 'ih', 'ihc', 'ii', 'ii97', 'iid', 'iihay', 'iii', 'iii1526012', 'iii1618134', 'iiiv', 'iis', 'iis1149882', 'ij', 'ijcv', 'ijeax', 'ijxnm', 'ik', 'ikezogwo', 'il', 'il12', 'il12p35', 'il12r', 'il12rb2', 'il12rb2a', 'il12ril12', 'il23', 'il23p19', 'il23r', 'ilar', 'ilarities', 'ilarity', 'ilc', 'ill', 'illegal', 'illia', 'illiapolosukhingmailcom', 'illu', 'illumination', 'illustrate', 'illustrated', 'illustrates', 'illustrating', 'illustration', 'illustrations', 'illustrative', 'ilona', 'ilse', 'ilsvrc', 'ilsvrc14', 'ilsvrc15', 'ilsvrc2010', 'ilsvrc2012', 'ily', 'ilya', 'ilyacsutorontoca', 'im', 'im2gps', 'ima', 'imag', 'image', 'image_encoder', 'image_encoderi', 'imagebased', 'imagebert', 'imagecaption', 'imaged', 'imagelevel', 'imagenet', 'imagenet1k', 'imagenet21k', 'imagenet22k', 'imagenet28', 'imagenet47', 'imagenet61', 'imageneta', 'imagenetbased', 'imagenetleft', 'imagenetorgchallengeslsvrc2012', 'imagenetr', 'imagenettest', 'imagenettrained', 'imagenetv2', 'imageomic', 'imageonly', 'imageprocessing', 'images', 'images213175', 'images3650515658', 'images54', 'imagetext', 'imagetile', 'imagetotext', 'imagetoword', 'imagey', 'imagine', 'imaging', 'imam', 'imately', 'imbalance', 'imbalance49', 'imbalanced', 'imbecile', 'iments', 'imitate', 'immediate', 'immediately', 'immense', 'immune', 'immunofluorescence', 'immunohistochemical', 'immunohistochemistry', 'immunology', 'impact', 'impacted', 'impactful', 'impacts', 'impeached', 'impede', 'imperative', 'imperfect', 'imperial', 'imple', 'implemen', 'implement', 'implementa', 'implementation', 'implementations', 'implemented', 'implementing', 'implements', 'implementsflexible', 'implica', 'implicated', 'implications', 'implicit', 'implicitly', 'implied', 'implies', 'impor', 'importance', 'important', 'importantly', 'impose', 'imposition', 'impossible', 'impractical', 'impression', 'impressive', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'impulses', 'imputation', 'imran', 'in', 'in2007', 'in224256384480640', 'inaccurate', 'inactive', 'inadequate', 'inadvances', 'inaistats', 'inal', 'inary', 'inate', 'inated', 'inates', 'inations', 'inc', 'incep', 'inception', 'inceptionresnet', 'inceptionresnetv2', 'inceptionv1', 'inceptionv2', 'inceptionv3', 'inceptionv4', 'incidence', 'incidents', 'includ', 'include', 'included', 'includes', 'including', 'includingnone', 'includingyesnofactoidlistandsummaryquestionspertainingtoourobjectiveofcomparingneurallanguage', 'inclusion', 'incoming', 'incomplete', 'incompletely', 'inconsistent', 'incorporate', 'incorporated', 'incorporates', 'incorporating', 'incorrect', 'incorrectly', 'increas', 'increase', 'increased', 'increases', 'increases2', 'increasing', 'increasingly', 'incredible', 'incredibly', 'increment', 'incremental', 'incremented', 'incvpr', 'indeed', 'indeniable', 'indepen', 'independence', 'independent', 'independently', 'indepth', 'index', 'indexes', 'india', 'indiana', 'indicate', 'indicated', 'indicates', 'indicating', 'indication', 'indications', 'indicative', 'indicator', 'indigeneity', 'indigenous', 'indirect', 'indirections', 'indirectly', 'indistribution', 'individual', 'individualagg', 'individually', 'indomain', 'indomaintextinpretrainingshouldhelpwithdomainspecificapplicationsindeedpriorworkhasshownthat', 'induce', 'induced', 'inductive', 'industrial', 'industry', 'ineccv', 'inedites', 'ineffective', 'inefficiency', 'inefﬁciencies', 'inefﬁcient', 'inejad', 'inequality', 'inessential', 'inevitably', 'inexpensive', 'inf', 'infancy', 'infantry', 'infeasible', 'infection', 'infections', 'infectious', 'infer', 'inference', 'inference90', 'inferencing', 'inferior', 'inferotemporal', 'inferred', 'inferring', 'infiltrating', 'infinitely', 'inflammation', 'inflammatory', 'inflated', 'inflates', 'influence', 'influencing', 'infor', 'inforcement', 'inform', 'informa', 'informatics', 'informatics46', 'informatics47', 'information', 'informative', 'informed', 'informs', 'infrastructure', 'infre', 'infrequent', 'ing', 'ing2', 'ingful', 'ingly', 'ingraham', 'ingredient', 'ingredients', 'ings', 'ington', 'inher', 'inherent', 'inherently', 'inherit', 'inheritance', 'inheriting', 'inherits', 'inhibition', 'inhibitors', 'inhibitory', 'inhibits', 'inhouse', 'ini', 'iniccv', 'iniclr', 'inicml', 'inieee', 'ininves', 'initial', 'initialization', 'initialize', 'initialized', 'initializes', 'initializing', 'initially', 'initiallyassume', 'initiate', 'initiated', 'initiative', 'initiatives', 'inject', 'innaturallanguageprocessingnlp', 'inner', 'innerlayer', 'innips', 'innovation', 'innovations', 'inourstandardpubmedbertpretrainingweusedpubmedabstractsonlywealsotriedaddingfulltextarti', 'inpatient', 'inproceedings', 'input', 'inputoutput', 'inputs', 'inputted', 'inputting', 'inremedis', 'insensitive', 'insert', 'inserted', 'inserts', 'inset', 'insets', 'inside', 'insideanentity', 'insight', 'insightful', 'insights', 'insignificant', 'inspec', 'inspecializeddomainslikebiomedicinepastworkhasshownthatusingindomaintextcanprovideadditional', 'inspect', 'inspecting', 'inspection', 'inspiration', 'inspired', 'inspiring', 'instability', 'instable', 'instagram', 'instagrampretrained', 'installed', 'installing', 'installupgrade', 'instance', 'instancelevel', 'instances', 'instanta', 'instantaneous', 'instead', 'insti', 'instil', 'institute', 'institute18162', 'institutes', 'institution', 'institutional', 'institutions', 'institutionspecific', 'instn', 'instructional', 'instructions', 'instrumental', 'insufficient', 'insufﬁcient', 'insulin', 'insurance', 'int', 'intact', 'inte', 'integral', 'integrate', 'integrated', 'integrates', 'integrating', 'integration', 'integrative', 'intel', 'intell', 'intell14', 'intellectual', 'intelligence', 'intelligent', 'intended', 'intense', 'intensities', 'intensity', 'intensive', 'intentionally', 'inter', 'interac', 'interact', 'interacting', 'interaction', 'interactions', 'interactionsjournal', 'interactionsonpubmedabstractsnotethatsomepriorwork', 'interactionsupregulator', 'interactive', 'interannotator', 'interatomic', 'intercellular', 'interchangeably', 'interconnec', 'interconnections', 'interdependent', 'interest', 'interested', 'interesting', 'interestingly', 'interestinglylarge', 'interests', 'intereststhe', 'interface', 'interfacedriven', 'intergouvernementale', 'interleukin12', 'interme', 'intermedi', 'intermediate', 'interna', 'internal', 'internalize', 'internalizes', 'internally', 'international', 'internationale', 'internet', 'interobserver', 'interpolated', 'interpolates', 'interpolation', 'interpre', 'interpret', 'interpreta', 'interpretability', 'interpretable', 'interpretation', 'interpretationdirection', 'interpretations', 'interpreted', 'interpreting', 'interquartile', 'interrater', 'interresidue', 'intersec', 'intersection', 'interstitial', 'intertwined', 'interval', 'intervaland', 'intervals', 'intervening', 'intervention', 'interventional', 'interventionmiccai', 'interventions', 'intfold6', 'intheclass', 'inthefirst10ofstepsthendecayslinearlytozerointheremaining90ofstepstrainingisdone', 'intheinputembeddinglayerthatmaximizetheadversariallossthusforcingthemodeltonotonlyoptimizethe', 'intheloop', 'inthewild', 'inthisarticlewealsoexploreadversarialpretraininganditsimpactondownstreamapplicationsmotivatedby', 'inthisarticlewechallengeaprevailingassumptioninpretrainingneurallanguagemodelsandshowthatdomain', 'inthisarticlewewillusebiomedicineasarunningexampleinourstudyofdomainspecificpretraininginother', 'inthissectionweconductathoroughevaluationtoassesstheimpactofdomainspecificpretraininginbiomed', 'inthissectionweprovideabriefoverviewofneurallanguagemodelpretrainingusingbert', 'into', 'intofx', 'intomoresophisticatedpretrainedlanguagemodelsfromlstminulmfit', 'intraattention', 'intracellular', 'intrachain', 'intractability', 'intractable', 'intraluminal', 'intraoperative', 'intravenous', 'intricate', 'intrigued', 'intrinsic', 'intro', 'introduc', 'introduce', 'introduced', 'introduces', 'introducing', 'introduction', 'intui', 'intuition', 'intuitions', 'intuitive', 'intuitively', 'invaluable', 'invariance', 'invariant', 'invariants', 'invasion', 'invasions', 'invasive', 'inventors', 'inverse', 'inversely', 'invertebrates', 'inverted', 'invertible', 'investi', 'investigate', 'investigated', 'investigating', 'investigation', 'investigations', 'investigative', 'investigator', 'investigators', 'invoke', 'involve', 'involved', 'involves', 'involving', 'inﬁltration', 'inﬂuence', 'inﬂuences', 'inﬂuential', 'io', 'ioawareness', 'ioffe', 'ioj', 'iolungcancer', 'ion', 'ioroutside', 'iosifescu', 'ioﬀe', 'ip', 'ipa', 'iphone', 'ir', 'irb', 'irccs', 'ireland', 'irene', 'irhythm', 'irregularity', 'irrelevant', 'irrespective', 'irretrievable', 'irugulpati', 'irvin', 'irvin1', 'irving', 'is', 'is071', 'isa', 'isabel', 'isard', 'isbidirectional', 'iscas', 'ishaan', 'ishes', 'ising', 'islam', 'islamaj', 'island', 'isola', 'isolated', 'isolation', 'isomorphic', 'isomorphism', 'isotonic', 'isotope', 'isotopic', 'isstillhelpfulandpreviousworktypicallyadoptsamixeddomainapproachsuchasbystartingdomainspecific', 'issue', 'issues', 'isthestandardtaggingschemethatclassifieseachtokenasthebeginningofanentity', 'isunlikely', 'isup', 'it', 'italian', 'italians', 'italicized', 'italics', 'italy', 'itasser', 'itations', 'ited', 'item', 'items', 'iter', 'itera', 'iteration', 'iterations', 'iterations113', 'iterative', 'iteratively', 'ith', 'ities', 'itive', 'its', 'itself', 'itsrepresentation', 'ity', 'iv', 'ivan', 'ivdatabaseonline', 'ively', 'ivr', 'ivridioventricular', 'ivy', 'iweb', 'iwith', 'ix', 'ixy', 'iyyer', 'iz', 'ization', 'izations', 'ize', 'ized', 'izing', 'j', 'j1', 'j1kfjx1', 'j2y', 'j2ynfyg', 'j305', 'ja', 'jaakkola', 'jabri', 'jack', 'jackel', 'jacket', 'jackhm', 'jackhmmer', 'jackhmmer60', 'jacob', 'jacobian', 'jacobs', 'jacovi', 'jaderberg', 'jaegle', 'jai', 'jain', 'jain14', 'jaitly', 'jake', 'jakob', 'jama', 'james', 'jamie', 'janet', 'janicki', 'january', 'jarrett', 'jaskolski', 'jason', 'jaume', 'jaume1234', 'jauvin', 'javadi', 'javed', 'jawahar', 'jax', 'jayne', 'jb', 'jc', 'jco', 'jd', 'je', 'jean', 'jeans', 'jeb', 'ject', 'jeff', 'jefferson', 'jeffrey', 'jegelka', 'jegou', 'jelinek', 'jennifer', 'jeremy', 'jeremyhowardandsebastianruder2018universallanguagemodelfinetuningfortextclassificationin', 'jersey', 'jerusalem', 'jesse', 'jesús', 'jeﬀrey', 'jf', 'jfgao', 'jft300m', 'jft300m30', 'jg', 'jhp', 'ji', 'jia', 'jian', 'jianfeng', 'jiang', 'jianpeng', 'jiansunmicrosoftcom', 'jiao', 'jie', 'jimmy', 'jin', 'jindi', 'jindong', 'jineket', 'jingcheng', 'jingfei', 'jinghao', 'jingjing', 'jingqi', 'jinhyukleewonjinyoonsungdongkimdonghyeonkimsunkyukimchanhosoandjaewookang2019biobertapretrained', 'jirvin16', 'jirvin16csstanfordedu', 'jittering', 'jj', 'jjw', 'jk', 'jm', 'jma', 'jmax0in2', 'jnlpba', 'jo', 'job', 'johan', 'john', 'johnson', 'johnstone', 'joined', 'joint', 'jointly', 'joints', 'joliette', 'jolla', 'jonas', 'jonathan', 'jonathon', 'jones', 'jones3', 'jong', 'jongwookopenaicom', 'jonnagaddala', 'joo', 'jordan', 'jorge', 'jos', 'jose', 'joshi', 'joshua', 'joulin', 'journal', 'journey', 'joy', 'jozefowicz', 'jpurkar', 'jr', 'js', 'json', 'judgments', 'judicious', 'judiciously', 'judson', 'judy', 'jue', 'juergens', 'jul', 'julian', 'july', 'jump', 'jumper', 'jumper14', 'jumperdeepmindcom', 'jumping', 'jumps', 'jun', 'junction', 'junctional', 'junctions', 'june', 'junghoon', 'jungle', 'junichi', 'junyoung', 'jurgens', 'jurisdictional', 'jusquaux', 'just', 'justas', 'justified', 'justin', 'jw', 'j¼1', 'jégou', 'jürgen', 'k', 'k0', 'k23', 'k3x3', 'k40', 'k50', 'k5x5', 'k80', 'kabeli1', 'kaczmarzyk', 'kadlec', 'kaggle', 'kagglecomcompetitionsprostatecancergradeassessment', 'kahe', 'kahou', 'kai', 'kaiming', 'kaiser', 'kalai', 'kalantidis', 'kalchbrenner', 'kalenichenko', 'kalfaoglu', 'kalkan', 'kalra', 'kamyshenkov', 'kanavati', 'kand', 'kandel', 'kang', 'kannala', 'kaplan', 'kapoor', 'karam', 'karayev', 'karen', 'karkkainen', 'karlen', 'karma', 'karolinska', 'karpagavalli', 'karpathy', 'karthik', 'kasumi', 'kather', 'katherine', 'kathryn', 'katie', 'katz', 'kavukcuoglu', 'kavukcuoglu1', 'kaylie', 'kcg', 'keep', 'keeping', 'kelcey', 'kelly', 'kemp', 'kempenet', 'kennel', 'kenny', 'kenton', 'kept', 'keratiniza', 'keratinization', 'kerkwijk', 'kern', 'kernel', 'kernelbased', 'kernels', 'kerry', 'keskar', 'kesselman', 'keutzer', 'kevin', 'key', 'keyes', 'keynote', 'keys', 'keyvalue', 'keyword', 'keywords', 'khaki', 'khalid', 'khalilbiochem', 'khan', 'khimulya', 'kholy', 'khorasani', 'khosla', 'khurram', 'khushi', 'ki', 'kian', 'kianinejad', 'kich', 'kidney', 'kidneybiopsy', 'kiela', 'kilian', 'killed', 'killeen', 'killshots', 'kim', 'kimberly', 'kimble', 'kimi', 'kimianet', 'kimianet64', 'kinch7', 'kind', 'kind26', 'kinds', 'kinetic', 'kinetics700', 'king', 'kingdom', 'kingma', 'kings', 'kingsbury', 'kipling', 'kirby', 'kirc', 'kirillov', 'kirk', 'kirkpatrick', 'kiros', 'kirp', 'kirschstein', 'kis', 'kitten', 'kittens', 'kitti', 'kj', 'kjjk', 'kjki', 'kk', 'klaus', 'klein', 'kleinberg', 'kleindienst', 'kligfield', 'klug', 'km', 'kn', 'knearest', 'knight', 'knn', 'knobs', 'know', 'knowing', 'knowl', 'knowledge', 'knowledgebase', 'knowledgebased', 'knowledgepowered', 'known', 'knα', 'ko', 'ko1', 'koch', 'kocisky', 'koehn', 'koh', 'kohl', 'kohl14', 'kohli', 'kohli1', 'kohonen', 'kolchanov', 'koleo', 'kolesnikov', 'kolesov', 'komodakis', 'komura', 'kondylisgenome', 'kong', 'konstantinos', 'konstas', 'konukoglu', 'koohbanani', 'kooistraet', 'koonin', 'kopf', 'koray', 'korea', 'korean', 'koren', 'korhonen', 'kornblith', 'koryat', 'kostrikov', 'kp', 'kqw4cornelledu', 'kr', 'krallinger', 'kramer', 'kramers', 'kraus', 'krause', 'krauth', 'kravitz', 'kremeret', 'kreps', 'kriausakul', 'krikun', 'kris', 'krishna', 'krishnan', 'krishnaswamy', 'kristan', 'kristina', 'krithara', 'krizcsutorontoca', 'krizhevsky', 'kronor', 'krueger', 'kruszewski', 'kryshtafovych', 'krzysztof', 'ks', 'kshot', 'kt', 'kth', 'ku', 'kuchaiev', 'kucukural', 'kuhlman', 'kuksa', 'kumar', 'kundra', 'kuprel', 'kusner', 'kuznetsov', 'kw', 'kwiatkowski', 'kwon', 'kyle', 'kyunghyun', 'k¼1', 'l', 'l1', 'l190', 'l1τðs', 'l2', 'l250', 'l2_normalizenpdoti_f', 'l2_normalizenpdott_f', 'l50p', 'la', 'laak', 'labaka', 'label', 'labeled', 'labelefficient', 'labeler', 'labelers', 'labeling', 'labell', 'labelled', 'labelme', 'labelpreserving', 'labels', 'labelstratified', 'labelu', 'labor', 'laboratories', 'laboratory', 'laborer', 'laborious', 'lack', 'lacking', 'lacks', 'ladder', 'lafferty', 'lagos', 'laguna', 'lahaie', 'lai', 'laid', 'laion5b', 'lake', 'lakhani', 'lakoff', 'lalaniet', 'lalbum', 'laleh', 'lam', 'lambada', 'lamblin', 'lamina', 'lampert', 'lample', 'lan', 'lancet', 'land', 'landed', 'landesklinikum', 'landmarks', 'landscape', 'lang', 'langerhans', 'langford', 'langlotz', 'langlotz3', 'language', 'language8', 'language_understanding_paperpdf', 'languagecomputational', 'languageguided', 'languageimage', 'languages', 'lap', 'lapata', 'lapedes', 'lapedriza', 'lapping', 'lapses', 'laptev', 'lar', 'larg', 'large', 'largely', 'larger', 'largerscale', 'largescale', 'largest', 'largesttodate', 'largethis', 'laritza', 'larized', 'larlus', 'larly', 'larochelle', 'larrysmithlorrainektanaberiejohnsonneeandochengjukuoifangchungchunnanhsuyushilinetal2008overview', 'larson', 'larson3', 'larsson', 'lary', 'lary38', 'lasparaginase', 'lasse', 'last', 'lasted', 'lastly', 'late', 'latency', 'latent', 'later', 'lateral', 'latest', 'lation', 'lationship', 'latter', 'launch', 'laurens', 'lautre', 'lavanya', 'lavie', 'law', 'lawrence', 'laws', 'laxalde', 'lay', 'layer', 'layer_i', 'layered', 'layerie', 'layerl', 'layerlike', 'layernormx', 'layerour', 'layerparts', 'layers', 'layers8', 'layersalthough', 'layerseg', 'layerwise', 'layout', 'lazar', 'lazard', 'lazaridou', 'lbfgs', 'lbfgsb', 'lbnl', 'lc', 'ld', 'lddt', 'lddtcuni03b1', 'lddtcα', 'lddtcα34', 'le', 'le1', 'le2', 'le28', 'lead', 'leaderboard', 'leaderboards', 'leadership', 'leading', 'leadless', 'leads', 'leakage', 'leaked', 'leaks', 'leaky', 'leaman', 'leap', 'learn', 'learned', 'learner', 'learners', 'learnersopenai', 'learning', 'learning2', 'learning28', 'learning3738117', 'learning9198', 'learningbased', 'learningcapability', 'learningenabled', 'learningmachine', 'learningpmlr', 'learns', 'lease', 'least', 'leastattended', 'leastsquares', 'leave', 'leaves', 'leaving', 'lebiere', 'lecocq', 'lecran', 'lect', 'lected', 'lection', 'lections', 'lective', 'lecun', 'lecun12', 'led', 'lede3', 'ledition', 'lee', 'lee12', 'left', 'leftmost', 'leftthe', 'lefttoright', 'leftward', 'leg', 'lege', 'legend', 'legends', 'legs', 'lehman', 'lei', 'leipidic', 'lel', 'lelism', 'lem', 'lempitsky', 'lems', 'lenantiomers', 'lending', 'lends', 'lenet5', 'lenge', 'lenge2017', 'lenges', 'lenging', 'length', 'lengths', 'lenses', 'lent', 'lenz', 'leo', 'leopard', 'lepidic', 'lerer', 'lerousseau', 'les', 'lesion', 'lesk', 'less', 'lessons', 'let', 'lett', 'letters', 'lettersnature', 'lettersto', 'letterstonature', 'letting', 'leu50pro', 'leukemia', 'leukemiasmall', 'leukocyte', 'leukocytes', 'leung', 'level', 'levels', 'leverage', 'leveraged', 'leverages', 'leveraging', 'levesque', 'levine', 'levy', 'lewis', 'lexical', 'lexicography', 'lexicon', 'leycomdatasets9xxm58dvs31', 'lf', 'lga', 'lgg', 'lgnv1v2v4it', 'lgssl', 'li', 'liaison', 'lial', 'liang', 'liang17', 'liao', 'liaw', 'liberman', 'libraries', 'library', 'library7', 'licence', 'license', 'licensed', 'licensee', 'licenses', 'licensor', 'licihiwi', 'licly', 'lidocaine', 'lie', 'lienard', 'lies', 'lieutenant', 'life', 'lifecycle', 'ligand', 'ligands', 'ligence', 'light', 'lighter', 'lighting', 'lightningfast', 'lightweight', 'lignancy', 'ligne', 'lihc', 'lijia', 'like', 'likebertnecessitates', 'likelihood', 'likely', 'lile', 'lillicrap', 'lily', 'lim', 'limestone', 'limi', 'limit', 'limita', 'limitation', 'limitations', 'limited', 'limitedmemory', 'limiting', 'limits', 'lin', 'lin12', 'linda', 'line', 'linear', 'linearly', 'lined', 'linen', 'lines', 'linet', 'linewidth', 'ling', 'linguistic', 'linguistics', 'linguistics18', 'linguisticshttpswww', 'linguisticshttpswwwaclweb', 'linguisticshttpswwwaclweborganthologyvolumess161', 'linguisticshttpswwwaclweborganthologyvolumess172', 'linguisticshttpswwwaclweborganthologyvolumess181', 'link', 'linked', 'linking', 'links', 'linmans', 'lintelligence', 'linux', 'linzen', 'lion', 'lions', 'lior', 'lipid', 'lipkova', 'lipman', 'lipocalinlike', 'lipoma', 'lippe', 'lippincottra', 'lipton', 'lisa', 'lished', 'list', 'listed', 'listened', 'listing', 'lists', 'lit', 'litera', 'literally', 'literature', 'lithium', 'litjens', 'litovchenko', 'little', 'littleexplored', 'liu', 'liuzhuang13mailstsinghuaeducn', 'live', 'lived', 'liver', 'living', 'livio', 'liwei', 'lj', 'll1', 'llayer', 'llion', 'lliongooglecom', 'lloyd', 'llw', 'lm', 'lmlm', 'lmrp', 'lms', 'ln', 'lnk', 'lnstn', 'lo', 'loading', 'lobe', 'lobes', 'lobular', 'loca', 'local', 'localdistance', 'localisation', 'locality', 'localiza', 'localization', 'localize', 'localizes', 'localizing', 'locally', 'located', 'location', 'locations', 'lockhart', 'loessner', 'log', 'log10neff', 'logarithmically', 'logic', 'logical', 'logicinspired', 'logistic', 'logit', 'logits', 'logittransformed', 'loglikelihood', 'loglinear', 'logo', 'logparameterized', 'logprobability', 'logpwitwi0t1xiθϕψ', 'logpwtw0t1ξ', 'logpy', 'logpyc', 'logpyo', 'logscale', 'logsumexp', 'loguniformly', 'loid', 'london', 'londres', 'long', 'long78', 'longer', 'longest', 'longitudinal', 'longrange', 'longstanding', 'longterm', 'longtermdependencies', 'longuethiggins', 'look', 'looked', 'looking', 'lookup', 'loop', 'loper', 'loperation', 'loproteinase', 'lord', 'lorenzo', 'lors', 'loshchilov', 'losingwehypothesizethatthereasonforthisbehavioristwofoldfirstpmcinclusionisinfluencedbyfunding', 'loss', 'loss37', 'loss44', 'loss45', 'loss_i', 'loss_t', 'loss_t2', 'losses', 'lossy', 'lost', 'lot', 'lots', 'loves', 'low', 'lowattention', 'lowdimensional', 'lowed', 'lower', 'lowercase', 'lowercased', 'lowered', 'lowergrade', 'lowerlevel', 'lowermemory', 'lowersensitivity', 'lowest', 'lowestperforming', 'lowgrade', 'lowlevel', 'lowmidhigh', 'lowrank', 'lowresolution', 'lowship', 'lowshot', 'lowsimilarity', 'loy', 'lozanopérez', 'lp', 'lpatch', 'lpl', 'lr', 'lrate', 'lrbp', 'lrec', 'ls', 'lsj', 'lslide', 'lstm', 'lstm22', 'lsvrc2010', 'lt', 'ltotal', 'lu', 'lu1', 'lu1234711', 'luad', 'luad83', 'luadlike', 'luadlusc', 'luadrelevant', 'luadtcga', 'luan', 'luan1', 'luca', 'lucas', 'lucero', 'lucia', 'lucic', 'luck', 'lui', 'lukas', 'lukasz', 'lukaszkaisergooglecom', 'luke', 'lular', 'lumbar', 'lumen', 'lundqvist', 'lung', 'lungbiopsy', 'lungren', 'lungren3', 'lungresection', 'luo', 'luong', 'lusc', 'lution', 'lutional', 'lutionarily', 'lutionary', 'lutions', 'lvcsr', 'lvdmaatenfbcom', 'lx', 'lxmert', 'lxy', 'lyman', 'lymph', 'lymphatic', 'lymphnodemetastasis', 'lympho', 'lymphocyte', 'lymphocytes', 'lymphocytic', 'lymphoid', 'lymphoma', 'lyon', 'lys190', 'lys194', 'lyskov', 'lyu', 'lðs', 'm', 'm23', 'm40', 'm5s', 'm9figsharec5927795v1', 'ma', 'ma14', 'maaten', 'macenko', 'mach', 'macherey', 'machine', 'machinelearning', 'machinelearningbased', 'machinery', 'machines', 'macko', 'maclean', 'macmahon', 'macmillan', 'macro', 'macroaverage', 'macroaveraged', 'macromolecular', 'macrophages', 'madabhushi', 'madams', 'made', 'madhavan', 'madhi', 'madinat', 'madison', 'mag', 'magar', 'magazine', 'magenta', 'mages', 'magnetic', 'magni', 'magnifica', 'magnificatin', 'magnification', 'magnification5559128', 'magnifications', 'magnitude', 'magnitudes', 'mahajan', 'mahaney', 'mahdi', 'mahmood', 'mahmoodlabclam', 'mahmoodlabhipt', 'mahmoodlaborg', 'mai', 'mail', 'main', 'mainly', 'mainstream', 'maintain', 'maintained', 'maintainers', 'maintaining', 'maire', 'maji', 'major', 'majority', 'mak', 'make', 'makes', 'makeﬁle', 'making', 'makinga', 'male', 'maleki', 'malignant', 'malik', 'malization', 'malization6', 'maml', 'mammoths', 'man', 'manage', 'managed', 'management', 'manager', 'managing', 'manan', 'mance', 'mandar', 'mandarin', 'mandela', 'mane', 'manifold', 'manipulated', 'manipulation', 'manmade', 'mann', 'manner', 'mannheim', 'manning', 'mannwhitney', 'manoj', 'mans', 'manson', 'mansoor', 'manual', 'manually', 'manuscript', 'manuscriptcompeting', 'manuscripts', 'many', 'manyfold', 'manynlpapplicationscanbeformulatedas', 'manzagol', 'mao', 'map', 'map5', 'mapped', 'mapping', 'mappings', 'maps', 'maps58', 'mar', 'marc', 'march', 'marco', 'margaret', 'margin', 'marginal', 'marginally', 'margins', 'maria', 'mariani', 'marianna', 'maries', 'marilla', 'marine', 'marization', 'mark', 'marked', 'markedly', 'marker', 'markers', 'market', 'marking', 'markings', 'marklund1', 'markov', 'markowitz', 'marks', 'markwefocusonpubmedbasedbiomedicalapplicationsandleavetheexplorationoftheclinicaldomainand', 'marlow', 'marneffe', 'maron', 'marr', 'marron', 'martel', 'martens', 'martin', 'martínez', 'martın', 'mary', 'maryam', 'maría', 'marıa', 'mas', 'masci', 'mask', 'mask10areleftunchangedand10arerandomlyreplacedbyatokenfromthe', 'mask2former', 'mask2former103', 'masked', 'maskedattention', 'masking', 'maskm', 'masks', 'masoumeh', 'mass', 'mass100k', 'mass1k', 'mass22k', 'massa', 'massachusetts', 'masses', 'massive', 'massively', 'massons', 'mastering', 'masters', 'match', 'matched', 'matcher', 'matches', 'matching', 'mate', 'mated', 'mately', 'matena', 'mateo', 'materi', 'material', 'materialization', 'materialize', 'materially', 'materials', 'math', 'mathemati', 'mathematical', 'mathers', 'mathieu', 'matics', 'mation', 'matplotlib', 'matrices', 'matriceswq', 'matrix', 'matrixws', 'matteo', 'matter', 'matters', 'matthew', 'matthewpetersmarkneumannmohitiyyermattgardnerchristopherclarkkentonleeandlukezettlemoyer2018deepcontex', 'mattingly', 'max', 'max0x', 'max0xw1', 'max0z', 'maxent', 'maxi', 'maxim', 'maximal', 'maximize', 'maximized', 'maximizes', 'maximizing', 'maximum', 'maxout', 'maxoutdropout', 'maxpooled', 'maxpooling', 'maxpoolingbased', 'maxz', 'may', 'mayank', 'mayjuly', 'mayo', 'mayorov', 'maze', 'maziarz', 'mb', 'mbconv', 'mbconv1', 'mbconv6', 'mc', 'mccain', 'mccallum', 'mccandlish', 'mccann', 'mcclelland', 'mcclosky', 'mccloskys', 'mccormick', 'mccoy', 'mcculloch', 'mccullough', 'mcdermott', 'mcdonald', 'mcgarvey', 'mcguffin', 'mcgufﬁe', 'mckinney', 'mcloud', 'mcmullan', 'mcneil', 'mcpartlon', 'mcquilton', 'md', 'mdar', 'me', 'mea', 'mean', 'mean0', 'mean00', 'meaning', 'meaningful', 'meaningless', 'meanings', 'means', 'meanspatial', 'meanstd', 'meant', 'meas', 'measur', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'measuring', 'mecha', 'mechan', 'mechanical', 'mechanisation', 'mechanism', 'mechanisms', 'mechanistic', 'med', 'medbertbertlargeappearstoyieldimprovedperformanceinsomepreliminaryexperimentsweleavean', 'medclip', 'media', 'medial', 'median', 'mediastinum', 'mediate', 'medical', 'medicine', 'medieval', 'medina', 'medium', 'mednli', 'medulloblastoma', 'meet', 'meeting', 'meetings', 'mehta', 'mehta1', 'meier', 'meila', 'meilerplos', 'mel', 'melanoma', 'melinda', 'mellen', 'melted', 'mem', 'members', 'membrane', 'memeffattention', 'memes', 'memisevic', 'memories', 'memorization', 'memorize', 'memorizing', 'memory', 'memory89', 'memorya', 'memorydriven', 'memoryefficient', 'memoryfailsoft', 'memoryn', 'memorynetworks', 'memoryneural', 'memoryparallel', 'men', 'mena', 'mengling', 'menick', 'menin', 'meningioma', 'meningiomas', 'meningothelial', 'mensink', 'menswear', 'menswearprep', 'ment', 'ment712', 'mental', 'mentary', 'mentation', 'mentations', 'mented', 'mentez', 'mention', 'mentioned', 'mentions', 'ments', 'mentum', 'mentwise', 'mer', 'mercer', 'mercial', 'merely', 'merge', 'merges', 'merging', 'meridian', 'meringue', 'merity', 'merrienboer', 'mesenchymal', 'meso', 'mesothelioma', 'message', 'messiness', 'mestre', 'met', 'meta', 'metab', 'metabolic', 'metabolism', 'metabolomics', 'metaclust', 'metadata', 'metage', 'metagenomic', 'metagenomics', 'metal', 'metalbinding', 'metalearning', 'metallopro', 'metalloprotease', 'metalloproteases', 'metamap', 'metaphors', 'metas', 'metastable', 'metastases', 'metastasis', 'metastatic', 'metastaticlymphnode', 'metatranscriptomes', 'meteor', 'meteor80', 'meteorites', 'meth', 'method', 'methodological', 'methodologies', 'methodology', 'methods', 'methods16', 'methods20', 'methodsbackpropagation', 'metres', 'metric', 'metric34', 'metrics', 'mets', 'mexican', 'meyer', 'meyer14', 'mezquita', 'mf', 'mfz', 'mgb', 'mgh', 'mgnify', 'mgnify6', 'mgnify90', 'mgyp000936678158', 'mgyp004000959047', 'mh', 'mhe', 'mhsa', 'mi', 'mias21', 'michael', 'michaelis', 'michaellucas', 'michal', 'michalina', 'michel', 'micro', 'microaverage', 'microbial', 'microbiology', 'microbiome', 'microenviron', 'microenvironmental', 'micrometastases', 'micrometastasis', 'micrometreperpixel', 'microns', 'micropapillary', 'microrules', 'microsatellite', 'microscope', 'microscope39', 'microscopes', 'microscopy', 'microsoft', 'microsoft49', 'microstructure', 'mid', 'middle', 'middleright', 'midpoint', 'miech', 'might', 'mike', 'mikolov', 'mil', 'mil145148', 'milan', 'milbased', 'mild', 'mile', 'miles', 'milevet', 'miliar', 'military', 'miller', 'million', 'millionparameter', 'millions', 'millionslide', 'millman', 'millán8', 'milmmil', 'mimic', 'mimicbased', 'mimiccxr', 'mimiciii', 'mimics', 'min', 'mincholé', 'mind', 'minderer', 'mindful', 'mindsonnet', 'mined', 'ming', 'mingwei', 'mingxing', 'minhthang', 'mini', 'minibatch', 'minibatches', 'minima', 'minimal', 'minimize', 'minimized', 'minimizes', 'minimizing', 'minimum', 'mining', 'miningbioinformatics', 'ministre', 'minkyung', 'minn1in2', 'minor', 'minority', 'minors', 'minsky', 'minstep_num05step_numwarmup_steps15', 'mint', 'mintu', 'minty', 'minuscule', 'minute', 'minutes', 'minwei', 'mirabello', 'miraxscan', 'mirdita', 'mirditaet', 'mirella', 'mirhoseini', 'mirror', 'mirrored', 'mirroring', 'mirroringshifting', 'mirza', 'misalignment', 'miscellaneous', 'misclassification', 'misclassifications', 'misclassified', 'misclassifies', 'misclassify', 'misdi', 'misdiagnosis157', 'misgendering', 'mishkin', 'mishra', 'misimpleshot', 'mislabeled', 'mislabelled', 'mislead', 'misleading', 'mismatch', 'misplace', 'misra', 'missed', 'missile', 'missing', 'mistakes', 'mistic', 'mit', 'mitani', 'mitbih', 'mitchell', 'mitchellet', 'mite', 'mitigate', 'mitigating', 'mitosis', 'mitterer', 'miura', 'mix', 'mixed', 'mixeddomain', 'mixes', 'mixing', 'mixture', 'mixtureofexperts', 'mixtures', 'miyake', 'miyao', 'mization', 'mizero', 'mizero56', 'mkr', 'ml', 'mlm', 'mlml', 'mlnet', 'mlp', 'mlps', 'mm', 'mm2', 'mmil', 'mmilmil', 'mmseqs', 'mmseqs2', 'mmseqs264', 'mmseqs2linclust63', 'mnas', 'mnasnet', 'mnih', 'mnist', 'mnist100', 'mo', 'mobadersany', 'mobile', 'mobilenets', 'mobilenetv1', 'mobilenetv2', 'mobilesize', 'mobileye', 'mobitz', 'moco', 'mocov3', 'mod', 'modakis', 'modal', 'modalities', 'modality', 'mode', 'model', 'model30', 'modelable', 'modelagnostic', 'modeled', 'modeling', 'modelingw', 'modelling', 'modelling40', 'models', 'models1', 'models105136', 'models110', 'models117', 'models136', 'models2427', 'models71', 'modelsdescribed', 'modelsexistingpretrainingworktypicallyfocusesonthenewswireandwebdomainsforexampletheoriginal', 'modelsfigure', 'modelstaskagnostic', 'moder', 'moderate', 'moderatesized', 'modern', 'modest', 'modi', 'modifi', 'modification', 'modifications', 'modified', 'modifies', 'modify', 'modifying', 'modiﬁcation', 'modiﬁcations', 'modiﬁed', 'modiﬁes', 'modular', 'module', 'moduleonly', 'modules', 'modulethatperformstaskspecifictransformationssuchasappendingspecialinstancemarker', 'modulo', 'modﬁed', 'moe', 'moen', 'mohamed', 'mohammad', 'mohammadhadi', 'mohammed', 'mohan', 'mohri', 'moi', 'moimˆeme', 'moist', 'moisture', 'mokhtari', 'mol', 'moldovan', 'molecular', 'molecules8', 'molloy', 'mollura', 'moment', 'momentum', 'momma', 'mon', 'mona', 'monary', 'mondiale', 'monest', 'money', 'mong3', 'moni', 'monia', 'monika', 'monitor', 'monitored', 'monitoredthe', 'monitoring', 'monkeys', 'monly', 'monocular', 'monocularly', 'monogr', 'monolingual', 'monomeric', 'monotoni', 'monotonic', 'monrocq', 'monschein', 'mont', 'montana', 'monte', 'month', 'months', 'montine', 'montréal', 'montufar', 'monuteaux', 'moodlaborg', 'moody', 'moon', 'moor', 'moore', 'moorish', 'moors', 'moras', 'morbidity', 'morcos', 'more', 'moreaccurate', 'moreover', 'moreprocessed', 'morgenstern', 'mori', 'morning', 'morphological', 'morphologies', 'morphology', 'mortality', 'mortars', 'morton', 'mosaic', 'moscow', 'moskewicz', 'mosque', 'most', 'mostly', 'mother', 'mothorax', 'mothroax', 'motif', 'motifs', 'motion', 'motivate', 'motivated', 'motivates', 'motivating', 'motivation', 'motivations', 'moult', 'mount', 'mountain', 'mountains', 'mountcastle', 'mouse', 'move', 'moved', 'movement', 'moves', 'movies', 'moviestheater', 'moving', 'mp', 'mpp', 'mqan', 'mr', 'mrfs', 'mribased', 'mrph', 'ms', 'msa', 'msas', 'mscoco', 'msi', 'mt', 'mtw', 'mu', 'much', 'muchimproved', 'mucin', 'mucosa', 'mucosae', 'mucus', 'muhammad', 'mukherjee', 'mukherjeeet', 'muktabh', 'mul', 'mulbregt', 'mulholland', 'muller', 'mullerbudack', 'mulliset', 'multi', 'multicenter', 'multicentric', 'multiclass', 'multicolumn', 'multicrop', 'multicystic', 'multidimensional', 'multidrug', 'multifigure', 'multifocal', 'multiforme', 'multifunctional', 'multigpu', 'multigrid', 'multihead', 'multiheaded', 'multiheadqkv', 'multiheadselfattentionmechanismwhichhasdemonstratedsuperiorityinleveraginggpubasedparallelcom', 'multiinstance', 'multilabel', 'multilayer', 'multilevel', 'multimedia', 'multimeric', 'multimodal', 'multinode', 'multinomial', 'multiobjective', 'multiomic', 'multiorgan', 'multipanel', 'multiple', 'multipleinstance', 'multiples', 'multiplex', 'multipli', 'multiplication', 'multiplicative', 'multiplicatively', 'multiplied', 'multiplier', 'multiply', 'multiplyadds', 'multiplying', 'multiprotein', 'multiscale', 'multisequence', 'multistage', 'multistain', 'multisymbol', 'multitask', 'multitissue', 'multiview', 'mum', 'munications', 'munich', 'munity', 'murphy', 'murray', 'muscle', 'muscosa', 'muscularis', 'muslim', 'muslims', 'must', 'mustjoki', 'mutate', 'mutation', 'mutations', 'mutual', 'mutually', 'mv', 'mvacc5', 'mvp', 'mw', 'mx20303', 'my', 'mye', 'myeloid', 'myers', 'myl', 'myle', 'myles', 'myoclonus', 'myriad', 'myself', 'mystery', 'mz', 'münich', 'n', 'n1', 'n110', 'n13', 'n131', 'n133', 'n135', 'n162', 'n2', 'n26', 'n32361', 'n43', 'n46', 'n53', 'n55', 'n558', 'n63', 'n66', 'n67', 'n68', 'n77', 'n92', 'n_atu_r_e_v_o_l_3_23_9_0c_t_o_b_er_19_86', 'na', 'na2', 'naaclhlt19', 'nacyltransferase', 'nadjacent', 'nagaiprotein', 'nagpal', 'nagy', 'nahar', 'naiclip', 'nair', 'naive', 'naively', 'najman', 'nakov', 'nal', 'nallapati', 'nally', 'naloxone', 'nals', 'naman', 'name', 'named', 'namely', 'names', 'nangia', 'nano', 'naoto', 'naotous', 'napeau', 'naples', 'narang', 'naranjo', 'narasimhan', 'narayanan', 'narayanaswamy', 'narrated', 'narrow', 'nary', 'nascent', 'naseem', 'nasnet', 'nasneta', 'nasrallah', 'nat', 'natarajan', 'natbiomedeng', 'natbiomedeng556', 'natbiomedeng558', 'natbiomedeng560', 'natbiomedeng562', 'natbiomedeng564', 'natbiomedeng566', 'natbiomedeng568', 'natbiomedeng570', 'nate', 'nates', 'nation', 'national', 'nations', 'native', 'natl', 'natn', 'natoms', 'natural', 'naturally', 'nature', 'nature__naturey0l', 'naturecomauthorspoliciesreportingsummaryflatpdf', 'naturecomdocumentsnrreportingsummaryflatpdf', 'naturemedicine', 'naturemedicine66', 'naturemedicine68', 'naumann', 'nauta', 'naval', 'navigating', 'navigation', 'nb', 'nba', 'nbest', 'nc', 'ncan', 'ncaption', 'ncbi', 'ncbidisease', 'nccaccctuccccre_vccol', 'nci', 'nciseer', 'nclass', 'ncontrast', 'nct', 'nctcrche100k', 'ncαc', 'ncϵ1', 'nd2', 'ndcg', 'ndwr', 'ne', 'neal', 'near', 'nearby', 'nearduplicate', 'nearduplicates', 'nearest', 'nearestcentroid', 'nearestneighbor', 'nearing', 'nearly', 'nearrandom', 'nearzero', 'neces', 'necessaire', 'necessarily', 'necessary', 'necessity', 'neck', 'neckline', 'necrosis', 'necrotic', 'nectivity', 'necula', 'nederlof', 'need', 'needed', 'needing', 'needle', 'needles', 'needs', 'neelakantan', 'neering', 'neff', 'negation', 'negations', 'negative', 'negatively', 'negatives', 'negbio', 'neglect', 'negligible', 'neighbor', 'neighborhood', 'neighborhoods', 'neighboring', 'neighbors', 'neighbouring', 'neil', 'neither', 'nels', 'nelson', 'nemati', 'nembutal', 'nential', 'nentidis', 'neoadjuvant', 'neocognitron', 'neocognitron46', 'neoplasm', 'neoplastic', 'neous', 'nepal', 'nephew', 'nephropathy', 'ner', 'nerr', 'ners', 'nerve', 'nervous', 'nesterov', 'net', 'net2', 'netadapt', 'netforumidb4tmhpn0jc', 'netherlands', 'netorgchallenges', 'nets', 'network', 'network75', 'networkarxiv13124400', 'networkbased', 'networking', 'networknamed', 'networks', 'networks2', 'networksarxiv', 'networksarxiv13024389', 'networksjournal', 'netzen', 'netzer', 'netﬂix', 'neu', 'neuberger', 'neuman', 'neumann', 'neural', 'neuralnetwork', 'neuralnetworkinspired', 'neuralnetworksforsemanticclassificationandinformationretrievalin', 'neurips', 'neuro', 'neurobiological', 'neurobiology', 'neurocomputing', 'neurodynamics', 'neuroimaging', 'neurol', 'neurological', 'neuron', 'neuronal', 'neuronalen', 'neuronelike', 'neurones', 'neurons', 'neurophysiological', 'neurosci', 'neuroscience', 'neurosciences', 'neustadt', 'neutral', 'never', 'neverthe', 'nevertheless', 'new', 'newcompound', 'newer', 'newhouse', 'newly', 'news', 'newsletter', 'newspaper1', 'newstest2013', 'newstest2014', 'newswire', 'next', 'nextbest', 'nextbestperforming', 'nextsentencepredictionnsp', 'nexttoken', 'nextword', 'nfg', 'nframes', 'ng', 'ng1', 'ngan', 'ngiam', 'ngram', 'ngrams', 'nguyen', 'nhgri', 'ni', 'nia', 'niazi', 'nice', 'nicely', 'niche', 'nicholas', 'nick', 'nickisch', 'niddk', 'nidetzky', 'niece', 'niehues', 'niethammer', 'nification', 'nigel', 'night', 'nigms', 'nih', 'nike', 'niki', 'nikipgooglecom', 'nikita', 'nikolov', 'nikolov14', 'nilsback', 'nin', 'nine', 'ninetyfive', 'ning', 'ninth', 'nips', 'nique', 'niques', 'niques18', 'nis', 'nisms', 'nition', 'nitish', 'nitude', 'nity', 'nity106137138', 'nivre', 'nized', 'niﬁcant', 'niﬁcantly', 'nlp', 'nlpbabionlp04', 'nltk', 'nmr', 'nn', 'nn2', 'no', 'no20150015', 'noam', 'noamgooglecom', 'nobel', 'noble', 'nocarcinoma', 'nocedal', 'nod', 'node', 'nodes', 'nodule', 'nodules', 'nogueira', 'noise', 'noisetolerant', 'noisier', 'noisy', 'noisyand4143', 'noisyor', 'noland', 'noma', 'nomenclature', 'nomes', 'nominal', 'nominally', 'nominallyremembered', 'non', 'nonattention', 'nonattentionbased', 'nonborderline', 'nonbottleneck', 'noncancerous', 'noncherrypicked', 'noncoding', 'noncommercial', 'nonconnected', 'nonconvex', 'nondeprived', 'none', 'nonenglish', 'nonenlarged', 'nonetheless', 'nonexpert', 'nonffpe', 'nongap', 'nonhe', 'nonhematoxylin', 'nonhierarchical', 'nonhistopathology', 'nonhuman', 'nonhumans', 'nonidentical', 'nonindigenous', 'noninformative', 'nonlin', 'nonlinear', 'nonlinearities', 'nonlinearity', 'nonlinearityfx', 'nonlinearly', 'nonluad', 'nonmeningothelial', 'nonmuslims', 'nonneoplastic', 'nonoverlapping', 'nonparametric', 'nonparticipation', 'nonprovisional', 'nonracemic', 'nonresidual', 'nonresponders', 'nonsaturating', 'nonshh', 'nonsmall', 'nonsmallcell', 'nonsmallcelllungcancer', 'nonsparse', 'nonsubtyping', 'nonsymmetric', 'nonsymmetrical', 'nonsymmetry', 'nontissuecell', 'nontrainable', 'nontrivial', 'nontumor', 'nontumour', 'nonuniform', 'nonvision', 'nonwinglessrelated', 'nonwnt', 'nonzero', 'noor', 'nor', 'norb', 'norm', 'normal', 'normalities', 'normality', 'normaliza', 'normalization', 'normalization169', 'normalization3', 'normalization42', 'normalizationjournal', 'normalize', 'normalized', 'normalizing', 'normallabeled', 'normally', 'norms', 'norouzi', 'north', 'northholland', 'northwest', 'northwestern', 'norway', 'nosek', 'nosis', 'nostic', 'not', 'notable', 'notably', 'notations', 'note', 'notebook', 'noted', 'notes', 'noteworthy', 'nothing', 'notice', 'noticeable', 'noticeably', 'noticed', 'noticing', 'noting', 'notion', 'notions', 'notorious', 'nouncentric', 'nouns', 'nous', 'nouveau', 'nouvelles', 'nov', 'nova', 'novel', 'november', 'novo', 'novoa', 'now', 'now89', 'nowdeleted', 'nowlan', 'np', 'npair', 'nparangen', 'npdoti_e', 'npexpt', 'npj', 'npn', 'nr', 'nres', 'ns', 'nsclc', 'nseq', 'nserc', 'nsf', 'nslcc', 'nsp', 'nsp2', 'nt', 'nterminal', 'nthe', 'ntn', 'nu', 'nuclear', 'nuclease', 'nucleases', 'nuclei', 'nucleic', 'nucleoli', 'nucleus', 'null', 'num', 'numan', 'number', 'numbered', 'numbers', 'numerical', 'numerous', 'numpy', 'numpy72', 'nvdia', 'nvg', 'nvidia', 'ny', 'o', 'o1', 'oa', 'oakdenrayner', 'ob', 'obdulia', 'objec', 'object', 'objective', 'objective118', 'objective25', 'objectives', 'objectnet', 'objects', 'obligations', 'obscures', 'obser', 'observa', 'observation', 'observations', 'observe', 'observed', 'observing', 'obsevations', 'obstacle', 'obstacles', 'obtain', 'obtained', 'obtaining', 'obtains', 'obvious', 'obviously', 'oc', 'occasion', 'occluded', 'occlusion', 'occular', 'occupying', 'occur', 'occurred', 'occurrence', 'occurrences', 'occurring', 'occurs', 'ocean', 'oceans', 'ochoa', 'ocr', 'oct', 'oct0ber1986', 'october', 'ocular', 'od', 'odd', 'odena', 'odintsov1', 'odobez', 'odology', 'ods', 'oe', 'oesophageal', 'of', 'of001', 'of052', 'of099', 'of325000', 'of4000', 'of500000', 'of617', 'of6n', 'ofageneraldomainpretrainedmodelasexemplifiedbybiobert', 'ofdistribution', 'ofensembles', 'off', 'offcenter', 'offer', 'offering', 'offers', 'office', 'official', 'officially', 'offroad', 'offset', 'offtheshelf', 'offx', 'ofhℓ', 'ofsinglemodel', 'oftasksadditionallywe', 'often', 'ofthe18thbionlpworkshopandsharedtask', 'ofthe2ndclinical', 'ofthe54thannualmeetingoftheassociationforcomputationallinguisticsvolume1longpapers', 'oftheart', 'ofﬁce', 'ofﬁcial', 'og', 'ogists', 'ognition', 'ogy', 'oh', 'ohio', 'ohlsson', 'ohta', 'oi', 'oil', 'oj', 'oja', 'ojna', 'ok', 'oka', 'okin', 'oklobdzija', 'oknd', 'oknd2', 'okyo', 'ol', 'olaf', 'olah', 'old', 'oldenburg1', 'older', 'oldest', 'oleksii', 'oligarch', 'oligodendro', 'oligodendroglioma', 'oliphant', 'olism', 'oliva', 'oliver', 'olivers', 'olmea', 'ologist', 'ologists', 'ologkn', 'ology', 'olunteers', 'olution', 'olv', 'olympia', 'olympic', 'olympics', 'oma', 'omer', 'omics', 'omit', 'omitted', 'omnisupervised', 'omsimple', 'on', 'on2', 'on25000', 'on35', 'once', 'oncifar10', 'oncocyto', 'oncocytomas', 'oncol', 'oncology', 'oncot', 'oncotree', 'ond', 'ond2', 'one', 'one_state_unknown_modelstargz', 'oneal', 'onedimensional', 'onegpu', 'onehalf', 'oneill', 'oneneuron', 'oneofn', 'ones', 'oneshot', 'onesided', 'onethird', 'oneversusrest', 'ong', 'ongoing', 'onimagenet', 'onk', 'online', 'only', 'onoff', 'onr', 'onset', 'onsite', 'onstrated', 'ont', 'ontario', 'ontheclassificationtaskssuchasyesnoquestionansweringinblurbandleavetheinclusionofmorecomplex', 'onto', 'ontology', 'oo', 'ooc9', 'oord', 'op', 'op1', 'op1k', 'op22k', 'op43', 'opacities', 'opacity', 'opatho', 'opatho100', 'ope', 'open', 'openai', 'openaiclip', 'opencv', 'opencvpython', 'opened', 'openfold', 'openi', 'opening', 'openmm', 'openmmopenmm', 'openreviewnet', 'opens', 'openset', 'openslide', 'openslidepython', 'opensource', 'oper', 'operate', 'operates', 'operating', 'operation', 'operations', 'operator', 'operators', 'opin', 'oping', 'opinion', 'opperman14', 'oppo', 'oppor', 'opportu', 'opportunities', 'opportunity', 'opposed', 'opposite', 'oppression', 'ops', 'opt', 'opti', 'optic', 'optical', 'optimal', 'optimisti', 'optimization', 'optimize', 'optimized', 'optimizer', 'optimizer129', 'optimizer45', 'optimizes', 'optimizing', 'optimum', 'opting', 'option', 'optional', 'options', 'optometry', 'oquab', 'or', 'orange', 'orax', 'orcasub', 'orch', 'orch171', 'orconditional', 'order', 'ordering', 'ordering80', 'orderly', 'orderofmagnitude', 'orders', 'ordinarily', 'ordinary', 'ordinates', 'ored', 'oregon', 'oreilly', 'orf8', 'org', 'org101002prot26171', 'org1022489cinc2017178245', 'organ', 'organisms', 'organiza', 'organization', 'organizations', 'organize', 'organized', 'organizers', 'organizes', 'organs', 'organthologyvolumess132', 'orgdata', 'orgdataset', 'orgpubpdbderived_data', 'orgrecord4643645', 'ori', 'orientation', 'orientational', 'orientations', 'orientationselective', 'oriented', 'ories', 'orig', 'origi', 'origin', 'original', 'originalbertalgorithmalsouses', 'originally', 'originate', 'originated', 'origins', 'oriol', 'ork', 'orkshop', 'ornd', 'oropharyngeal', 'orr', 'orthet', 'orthogonal', 'ortholog', 'orthologs', 'ory', 'orytime', 'osadchy', 'oscar', 'osdi16', 'oshua', 'osindero', 'oss', 'ostensen', 'ot', 'ot1', 'ot108', 'ot43', 'otemporal', 'oth', 'othenertasksinblurbareonlyconcernedaboutoneentitytypeinjnlpba', 'other', 'others', 'others148', 'others3338', 'others4953', 'others86', 'otherwise', 'ott', 'ounces', 'our', 'ours', 'ourselves', 'ous', 'out', 'outbound', 'outcome', 'outcomes', 'outdomain', 'outdoor', 'outer', 'outgoing', 'outliers', 'outline', 'outlined', 'outof', 'outofdistribution', 'outofdomain', 'outofthebox', 'outoftheclass', 'outofvocabulary', 'outpatient', 'outper', 'outperform', 'outperformed', 'outperforming', 'outperforms', 'output', 'outputs', 'outputted', 'outputting', 'outrageously', 'outside', 'outstanding', 'outﬁt', 'outﬁts', 'ov', 'ovarian', 'ovchinnikov', 'ovchinnikov34', 'oven', 'over', 'overall', 'overcome', 'overcomes', 'overcoming', 'overfeat', 'overfit', 'overfitting', 'overhead', 'overlaid', 'overlap', 'overlapped', 'overlapping', 'overlaps', 'overlay', 'overlaying', 'overload', 'overloaded', 'overloads', 'overly', 'overperforms', 'overreader', 'overreporting', 'oversampled', 'oversampling', 'oversegmented', 'overshadowed', 'oversight', 'overview', 'overviewbmc', 'overviewofthebiocreativevichemicalproteininteractiontrackin', 'overﬁt', 'overﬁtting', 'overﬁttingjmlr', 'overﬁttingone', 'overﬂow', 'ovids', 'ovoa', 'owards', 'owing', 'own', 'owner', 'ownerauthors', 'ownergroup', 'owners', 'owning', 'oxford', 'oxford29', 'oxfordiiit', 'oxidoreductase', 'ozkan', 'ozyilmaz', 'ozyoruk', 'oﬁr', 'oﬃce', 'p', 'p0', 'p01gm063210', 'p1', 'p100', 'p101', 'p11', 'p1p2p3α1λ1α2λ2α3λ3t', 'p213', 'p26', 'p29432', 'p30dk034854', 'pa', 'paad', 'pac', 'pace', 'pacholska1', 'pacity', 'package', 'packages', 'packed', 'packers', 'packing', 'pacman', 'pacmans', 'pacts', 'padded', 'padding', 'paediatric', 'page', 'pages', 'pain', 'painstaking', 'painstakinglyreproduced', 'paint', 'paip', 'paip85', 'pair', 'paired', 'pairing', 'pairings', 'pairs', 'pairs1', 'pairwise', 'pal', 'palace', 'palaeontology', 'pale', 'pales', 'paliouras', 'palm', 'palmedo', 'palo', 'paloma', 'palomaki', 'palpatine', 'paluri', 'pamela', 'pan', 'panahi', 'panathinaiko', 'pancancer', 'pancreas', 'pancreatic', 'pand', 'panda', 'panda18', 'panda90', 'pandas', 'pandasdevpandas', 'pandemonium', 'pandisease', 'panel', 'panels', 'pang', 'panions', 'panoptic', 'pans', 'pantissue', 'pants', 'pany', 'paolo', 'papal', 'paper', 'paper39', 'paperno', 'papers', 'papersassociation', 'papert', 'papillary', 'papillon', 'pappagari', 'par', 'paradigm', 'paradigms', 'paraffin', 'paraffinembedded', 'paraganglioma', 'paragraph', 'paragraphs', 'paral', 'parallel', 'parallelism', 'parallelismarxiv', 'parallelizable', 'parallelization', 'parallelized', 'parallelpathways', 'paralysed', 'param', 'parame', 'parameter', 'parameterefficient', 'parameterefﬁcient', 'parameterfree', 'parameterized', 'parameters', 'parametersaccuracy', 'paramratio', 'params', 'paramsresnet152', 'paras', 'paraspinal', 'parce', 'parchment', 'pare', 'pared', 'parentheses', 'pares', 'parfum', 'parikh', 'parison', 'parisons', 'parisotto', 'park', 'park12', 'parker', 'parkhi', 'parkway', 'parmar', 'paroxysmal', 'parse', 'parsed', 'parser', 'parsing', 'part', 'parthenogenesis', 'parti', 'partial', 'partially', 'participant', 'participants', 'participate', 'participating', 'particle', 'particles', 'particular', 'particularly', 'partitioned', 'partitioning', 'partitions', 'partly', 'partment', 'partner', 'parts', 'party', 'parwani', 'parwani8', 'pas', 'pasadena', 'pascal', 'pascale', 'pascalvoc2007', 'pascanu', 'pass', 'passage', 'passed', 'passes', 'passing', 'passive', 'passively', 'passos', 'passwd', 'past', 'paszke', 'pat', 'pataki', 'patch', 'patchbased', 'patchcamelyon', 'patchcamelyon195', 'patched', 'patches', 'patches4', 'patchifying', 'patching', 'patchlevel', 'patel', 'patel3', 'patent', 'patented', 'path', 'pathnarratives', 'pathog', 'pathogenic', 'pathol', 'patholo', 'pathological', 'pathologies', 'pathologist', 'pathologistannotated', 'pathologistcurated', 'pathologistlevel', 'pathologists', 'pathology', 'pathology14', 'pathologynew', 'pathologyrelated', 'pathologyrelevant', 'pathologyspecific', 'pathomic', 'paths', 'pathsbioinformatics', 'pathway', 'pathways', 'pati', 'patience', 'patient', 'patientconfidentiality', 'patientlevel', 'patients', 'patientspecific', 'patrick', 'pattern', 'patterned', 'patternrecognition', 'patterns', 'patwary', 'paul', 'paulson', 'paulus', 'pavillon', 'pavkovkeller1517', 'pavlick', 'paw', 'pay', 'paying', 'paz', 'pc', 'pc1', 'pc2', 'pc20', 'pc21', 'pca', 'pcam163', 'pcm', 'pcpg', 'pctep2020084238', 'pda', 'pdb', 'pdb5', 'pdb6yj1pdb', 'pdb70', 'pdb7066', 'pdes', 'pdi', 'pdl1', 'pdrop', 'peace', 'peak', 'peaks', 'pearce', 'pearson', 'pearsonr', 'pearsons', 'pecans', 'pecially', 'pedersen', 'pedestrian', 'pedestrians', 'pediatric', 'pediatrictype', 'pedregosa', 'pedro', 'peer', 'peerreviewed', 'pell', 'pelling', 'pellock', 'pen', 'penalize', 'penalized', 'penalizes', 'penalty', 'pendency', 'pending', 'pendix', 'penelope', 'penetration', 'peng', 'pengcell', 'pengcheng', 'peninsula', 'penn', 'pennington', 'pentothal', 'penultimate', 'peo', 'people', 'pepos', 'pepos2i', 'pepos2i1', 'peposk', 'pepper', 'peppermint', 'peppermintjojo', 'peptidase', 'peptide', 'per', 'peraro', 'perceiver', 'perceiving', 'percentage', 'percentile', 'percentiles', 'percep', 'perception', 'perceptron', 'perceptronconvergence', 'perceptrons', 'perclass', 'percy', 'pereira', 'pereira12', 'perekrestenko', 'perelygin', 'perera', 'perez', 'perfect', 'perfectly', 'perfor', 'perform', 'performance', 'performance21122', 'performances', 'performed', 'performers', 'performing', 'performs', 'perfume', 'perhaps', 'perhead', 'perimentalists', 'perimentally', 'period', 'periods', 'peripheral', 'peritoneum', 'perkel', 'perktold', 'perlayer', 'permanent', 'permettrait', 'permission', 'permissions', 'permissive', 'permit', 'permits', 'permitted', 'permutation', 'permutationinvariant', 'permutations', 'permuting', 'perona', 'perou', 'perpixel', 'perplex', 'perplexities', 'perplexity', 'perresidue', 'perronnin', 'perrot', 'persequence', 'persist', 'person', 'personal', 'personnes', 'perspective', 'perspectives', 'perspex', 'pert', 'pertaining', 'pertask', 'pertise', 'perturbations', 'pervised', 'perword', 'perwordpiece', 'pet', 'peter', 'peters', 'petersen1', 'peterson', 'petition', 'petitive', 'petraiii', 'petroleum', 'pets', 'petsresnet152', 'pettersenet', 'pezeshki', 'pezzelle', 'pfam', 'pfeiffer', 'pfeiffers', 'phage', 'pham', 'phanerochaete', 'pharmacol', 'pharmacological', 'pharmacovigilance', 'phase', 'phases', 'phd', 'phenomena', 'phenomenon', 'phenotypes', 'phenotyping', 'pheochromocytoma', 'phi', 'philadelphia', 'philanthropy', 'philbin', 'phillips', 'philosophy', 'pho', 'phoebe', 'phone', 'phoneme', 'phonemes', 'phones', 'phonetic', 'phospholipase', 'photo', 'photograph', 'photographs', 'photomicrographs', 'photos', 'phrase', 'phrases', 'phylogenetic', 'phys', 'physical', 'physician', 'physicians', 'physics', 'physics16', 'physiobank', 'physiognomys', 'physiognomysnewclothesf2d4b59fdd6a', 'physiol', 'physiologi', 'physiologic', 'physiological', 'physiologically', 'physiology', 'physionet', 'physionet26', 'physionetcomputing', 'physionetorgcontentmedicalairesearchfoundation172173', 'physiotoolkit', 'pi', 'pi3k', 'pi3kg', 'pib', 'pibþb', 'pic', 'pick', 'picked', 'pickwellmacpherson', 'pico', 'picothe', 'picture', 'pictured', 'picus', 'piece', 'pieces', 'piecesseparatedbyahyphenthesewordpiecesoftenhavenobiomedicalrelevanceandmayhinderlearningindownstream', 'pierro', 'pietra', 'pii', 'pik', 'pillinger', 'pillow', 'pilo', 'pilocytic', 'pilot', 'pilote', 'pimentel', 'pinch', 'ping', 'pings', 'pink', 'pinky', 'pinsker', 'pinta', 'pinto', 'pio', 'pioneering', 'piovesanforster', 'pipeline', 'pipelines', 'pipelines51', 'piratory', 'pires', 'pitch', 'pitts', 'pittsburgh', 'pituitary', 'pivotal', 'pix', 'pixel', 'pixelislands', 'pixellevel', 'pixelpatchlevel', 'pixelperfect', 'pixels', 'pk', 'pl', 'place', 'placebo', 'placed', 'placement', 'places', 'plain', 'plain110', 'plain18', 'plain20', 'plain32', 'plain34', 'plain44', 'plain56', 'plainresidual', 'plains', 'plan', 'plane', 'planet', 'planetphoto', 'plans', 'plant', 'plantcellwalllike', 'plants', 'plasma', 'plastic', 'plasticity', 'plate', 'plateauing', 'plateaus', 'platform', 'platform85', 'platformaware', 'platinumiridium', 'platt', 'plausibility', 'plausible', 'play', 'played', 'player', 'players', 'playing', 'plays', 'plco', 'plddt', 'ple', 'please', 'pled', 'pleiss', 'plementation', 'pleomorphism', 'ples', 'plete', 'plethora', 'pleu', 'pleural', 'pleuropulmonary', 'plexes', 'plexiform', 'plexity', 'plicative', 'plicit', 'pling', 'plip', 'plip54', 'plipbased', 'ploring', 'plos', 'plot', 'plotnine', 'plots', 'plotted', 'plotting', 'pluckthun', 'plus', 'pmc', 'pmcclip', 'pmcpath', 'pmlr', 'pn', 'pnasnet', 'pneu', 'pneumo', 'pneumonia', 'pneumoniaindexhtml', 'pneumoth', 'pneumotho', 'pneumothorax', 'po', 'poblenz', 'poggio', 'point', 'pointed', 'pointer', 'pointergenerator', 'points', 'pointwise', 'poised', 'poisson', 'poitiers', 'poland', 'polat', 'policies', 'policy', 'polin', 'political', 'pollard', 'polo', 'polosukhin', 'polygonal', 'polymerase', 'polynet', 'polyp', 'polyps', 'polyspecificity', 'pomeranian', 'ponential', 'ponentiated', 'ponents', 'pontdarc', 'pontil', 'pontingtrends', 'pool', 'poole', 'pooled', 'pooler', 'pooling', 'pooling37', 'pools', 'poon', 'poon2021domainspecificlanguagemodelpretrainingforbiomedicalnaturallanguageprocessing', 'poor', 'poorer', 'poorly', 'poplin', 'popovici', 'popular', 'popularity', 'popularized', 'population', 'populationbased', 'populationlevel', 'populations', 'populations114', 'populationspecific', 'port', 'portability', 'portable', 'portal', 'portals', 'portance', 'portant', 'ported', 'portfolio', 'portion', 'portions', 'portrait', 'ports', 'portugaly', 'pose', 'posed', 'poses', 'posi', 'posis', 'posit', 'position', 'positional', 'positioned', 'positions', 'positionsi', 'positionwise', 'positive', 'positively', 'positivenegative', 'positives', 'positiveversusnegative', 'possi', 'possibilite', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'postdoctoral', 'postgresql', 'posthoc', 'postlateral', 'postnegation', 'postprediction', 'postprocessing', 'posts', 'posunk', 'pot', 'potapenko', 'potapenko14', 'potchefstroom', 'potchen', 'poten', 'potential', 'potentially', 'potentiallyuseful', 'potentials', 'potentialsare', 'potteret', 'potts', 'poultney', 'pound', 'pounding', 'poungponsri', 'pour', 'pourbabaee', 'poutputinput', 'povey', 'powder', 'powdered', 'power', 'powered', 'powerful', 'powers', 'pp', 'ppl', 'ppn', 'ppv', 'pr', 'pr3', 'prac', 'practical', 'practically', 'practice', 'practices', 'practiceto', 'practicing', 'practitioners', 'prad', 'pragmatic', 'pranav', 'pranavsrcsstanfordedu', 'prcc', 'pre', 'preactivation', 'preactivationalso', 'precautions', 'preceding', 'preci', 'precipitated', 'precis', 'precise', 'precisely', 'precision', 'precisionrecall', 'precluded', 'precludes', 'precomputed', 'precondition', 'preconditioning', 'predefined', 'predetermine', 'predetermined', 'predeﬁned', 'predic', 'predict', 'predictable', 'predictably', 'predicted', 'predicting', 'prediction', 'prediction1', 'prediction10114445', 'prediction1112', 'prediction12', 'prediction144243', 'prediction1617', 'prediction2', 'prediction2526', 'prediction810', 'prediction91920', 'predictions', 'predictive', 'predictor', 'predicts', 'predominant', 'predominantly', 'preestablished', 'preextract', 'preextracted', 'preextracting', 'preextraction', 'preferred', 'preheat', 'prehension', 'prehensive', 'prehistoric', 'prelim', 'preliminary', 'prelunet', 'premier', 'prenegation', 'prenormalized', 'preparation', 'prepare', 'prepared', 'prepares', 'preparing', 'prepended', 'preposi', 'preprint', 'preprocess', 'preprocessed', 'preprocessedsignals', 'preprocessing', 'prerequisite', 'pres', 'prescribed', 'prescription', 'presence', 'present', 'presentations', 'presented', 'presenting', 'presents', 'preser', 'preserve', 'preserved', 'president', 'presidential', 'presidents', 'preslav', 'presoftmax', 'prespecified', 'press', 'pressive', 'pressurised', 'presumably', 'presumed', 'pretability', 'pretation', 'pretrain', 'pretrained', 'pretraining', 'pretraining124', 'pretraining7', 'pretrainingfor', 'pretrainingisconductedusingpubmedabstractsand', 'pretraininglargeneurallanguagemodelsonunlabeledtexthasproven', 'pretrainingwithpubmedtextleadstobetterperformanceinbiomedicalnlptasks', 'prettenhofer', 'pretty', 'prevailing', 'prevalence', 'prevalencedependent', 'prevalences', 'prevalent', 'prevent', 'prevented', 'preventing', 'previ', 'previous', 'previous_releasesrelease2020_01uniref', 'previously', 'previously4', 'pri', 'price', 'primakoff', 'primarily', 'primary', 'primate', 'prime', 'primitive', 'principal', 'principle', 'principled', 'principles', 'print', 'prints', 'prior', 'prioritization', 'prioritize', 'prioritizing', 'priority', 'priors', 'pris', 'prising', 'prisingly', 'pritzel', 'pritzel14', 'privacy', 'private', 'prize', 'pro', 'proach', 'proaches', 'proachwouldinitializewiththestandardbertmodel', 'prob', 'probabil', 'probabilistic', 'probabilities', 'probability', 'probable', 'probably', 'probe', 'probes', 'probing', 'probing130', 'problem', 'problem8has', 'problematic', 'problems', 'problemswill', 'proc', 'proce', 'procedure', 'procedures', 'proceed', 'proceeded', 'proceeding', 'proceedings', 'proceedingsofthe2015conferenceofthenorthamericanchapter', 'proceedingsofthe2015ieeeinternational', 'proceedingsofthe2018conferenceofthenorthamericanchapteroftheassociationforcomputational', 'proceedingsofthe2019conferenceonempiricalmethodsinnaturallanguageprocessingandthe9thinternational', 'proceedingsofthe3rdinternationalconference', 'proceedingsofthe56thannual', 'proceedingsofthe6thbiocreativechallengeevaluationworkshop', 'proceedingsofthejointeuropeanconferenceonmachinelearningandknowledgediscoveryindatabases', 'process', 'processed', 'processes', 'processing', 'processing52', 'processingcategorizationcontentaddressable', 'processor', 'processors', 'procurement', 'procédure', 'prodomain', 'prodomainhas', 'produce', 'producea', 'produced', 'produces', 'produces365', 'producing', 'product', 'production', 'productiongrade', 'products', 'professionals', 'profile', 'profiles', 'profiling', 'profit', 'prognosis', 'prognostic', 'prognostication79', 'program', 'program37', 'programmable', 'programme', 'programmer', 'programming', 'programs', 'progres', 'progress', 'progress1014', 'progression', 'progressive', 'progressively', 'prohibitively', 'proj', 'projec', 'project', 'project63', 'project82', 'projectauthor', 'projected', 'projectiles', 'projection', 'projections', 'projects', 'prolifer', 'proliferation', 'prolonged', 'prominent', 'promise', 'promised', 'promising', 'promoted', 'promoter', 'promotes', 'promotion', 'prompt', 'promptbased', 'prompted', 'prompting', 'prompting55', 'promptly', 'prompts', 'prone', 'pronounced', 'proof', 'propagate', 'propagated', 'propagates', 'propagation', 'proper', 'properly', 'properties', 'property', 'propor', 'proportion', 'proportional', 'proportionally', 'proportions', 'proposal', 'proposals', 'propose', 'proposed', 'proposes', 'proposition', 'propria', 'propriate', 'proprietary', 'prorok', 'pros', 'prosky', 'prospect', 'prospects', 'prosperous', 'prostate', 'prostatectomies', 'prostatic', 'protein', 'proteinase3', 'proteincoupled', 'proteinlevel', 'proteinprotein', 'proteins', 'proteogenomic', 'proteome', 'proteome39', 'proteomescale', 'proteomic', 'proteomics', 'proto', 'protocol', 'protocol127', 'protocol36', 'protocols', 'protonet106', 'prototype', 'prototypes', 'prototypical', 'prototyping', 'protsch', 'prove', 'proved', 'provement', 'proven', 'proves', 'provide', 'provided', 'provides', 'providing', 'provision', 'provisional', 'provisos', 'proximity', 'proxy', 'proxylessnas', 'pruksachatkun', 'pruning', 'pseudo', 'pseudocluster', 'pseudocode', 'pseudoorthogonality', 'psicov', 'psnk', 'psns1', 'psych', 'psychoanalysis', 'psychology', 'pt', 'ptb', 'ptm', 'pu', 'pub', 'pubdatabasesmetagenomicspeptide_database2018_12', 'publ', 'public', 'publication', 'publications', 'publicavailable', 'publicly', 'publiclyavailable', 'publish', 'published', 'publisher', 'publishers', 'publishing', 'pubmed', 'pubmed5', 'pubmedbased', 'pubmedbert', 'pubmedcentralpmc', 'pubmedpmc', 'pubmedqa', 'pubmedsentenceseachofwhichisannotatedbyfiveexpertlevelannotatorswithanestimatedsimilarityscore', 'puigcerver', 'puis', 'pul', 'pulkit', 'pulled', 'pulling', 'pulmonary', 'pulses', 'punctuation', 'pup', 'pupils', 'pur', 'pure', 'purely', 'purging', 'purity', 'purpose', 'purposefully', 'purposes', 'pursue', 'pursued', 'pursuit', 'push', 'pushed', 'pushes', 'pushing', 'pushmeet', 'pustuiren', 'put', 'putation', 'putational', 'putationandmodelinglongrangedependenciesintextscomparedtorecurrentneuralnetworkssuchas', 'pute', 'puter', 'putes', 'putoutput', 'puts', 'putting', 'pvalue', 'px', 'py', 'pyc', 'pymol', 'pyo', 'pyramid', 'pyramidal', 'pyrosetta', 'pyt', 'python', 'python73', 'pytorch', 'pytorchorg', 'pyysalo', 'pérez', 'pölönen', 'q', 'qa', 'qadir', 'qassim', 'qassims', 'qbio', 'qc', 'qi', 'qian', 'qiao', 'qiki', 'qin', 'qingyu', 'qkv', 'qrs', 'quadratic', 'quadratically', 'qualcomm', 'qualitative', 'qualitatively', 'qualities', 'quality', 'quan', 'quant', 'quantification', 'quantify', 'quantifying', 'quantile', 'quantita', 'quantitative', 'quantitatively', 'quantities', 'quantity', 'quantization', 'quantizing', 'quarterback', 'quartile', 'quartiles', 'quat', 'quaternary', 'quattoni', 'que', 'quebec', 'quel', 'quelque', 'quelques', 'quence', 'quences', 'quent', 'quential', 'quently', 'quentlywestartwiththesamepretrainedbertmodelandconductadditionalinvestigationontheimpactforthe', 'quered', 'queried', 'queries', 'query', 'querying', 'ques', 'question', 'questionable', 'questionanswer', 'questionanswering', 'questionansweringin', 'questionq', 'questions', 'questiontext', 'queuing', 'qui', 'quickly', 'quil', 'quilt1m', 'quintero', 'quire', 'quired', 'quires', 'quiros', 'quite', 'quoc', 'quot', 'quotecomment', 'quotes', 'quune', 'r', 'r1', 'r10', 'r1024', 'r13', 'r15', 'r17', 'r19', 'r2', 'r20', 'r22', 'r25', 'r256', 'r26k', 'r3', 'r32q', 'r35gm138216', 'r35gm138216a', 'r35gm149270', 'r512', 'ra', 'rabal', 'rabinovich', 'rabinowitz', 'rable', 'rabouille', 'race', 'racemization', 'racers', 'racism', 'racy', 'rad1', 'rad2', 'rad3', 'radaid', 'radar', 'radboud', 'radford', 'radi', 'radio', 'radiograph', 'radiographic', 'radiographs', 'radiography', 'radiol', 'radiolo', 'radiologist', 'radiologistannotated', 'radiologistlabeled', 'radiologistlevel', 'radiologists', 'radiology', 'radmaj', 'radnet', 'radosavovic', 'rads', 'rafal', 'raffel', 'raghu', 'rahhal', 'rahtu', 'raiko', 'raina', 'rainbow', 'raining', 'raise', 'raised', 'raises', 'raising', 'raisinoid', 'raison', 'rajamanickam', 'raji', 'rajpoot', 'rajpurkar', 'rajpurkar1', 'ral', 'ramabhadran', 'ramachandran', 'ramakrishnan', 'ramalho', 'raman', 'ramanan', 'ramanathan', 'ramapuram', 'ramesh', 'rameter', 'rameters', 'rametertuningforeachmodeloneachdatasethoweverthiswouldincuraprohibitiveamountofcomputation', 'ramsey', 'ran', 'random', 'random224', 'random3', 'random80arereplacedby', 'randomization', 'randomly', 'randomseedsespeciallyforsmalldatasetslikebiossesbioasqandpubmedqawereporttheaveragescores', 'randy', 'ranganath', 'range', 'ranges', 'ranging', 'ranked', 'ranking', 'ranks', 'ranspath', 'ranzato', 'rao', 'rao1', 'raoet', 'raoof', 'rapid', 'rapidly', 'raquel', 'rare', 'rareact', 'rarecare', 'rarecarenet', 'rarecareneta', 'rarely', 'rarity', 'rasmus', 'rate', 'rate1e53e55e5batchsize1632andepochnumber260ideallywewouldconductseparatehyperpa', 'rated', 'rately', 'rateof6', 'rates', 'rather', 'rathinaswamy18', 'rathmell', 'ratio', 'ration', 'rational', 'ratiotoefﬁcientnet', 'ratiotoefﬁcientnetflops', 'raven', 'raw', 'rawmsa', 'rax', 'ray', 'ray14', 'raza', 'rb', 'rc', 'rcc', 'rcctcga', 'rcnn', 'rcnn32', 'rd', 'rdmodeldk', 'rdmodeldv', 'rds', 're', 'rea', 'reach', 'reached', 'reaches', 'reaching', 'reaction', 'read', 'read8', 'readable', 'reader', 'readers', 'readily', 'reading', 'reading42', 'readjustments', 'readjusts', 'readonly', 'reads', 'ready', 'real', 'realistic', 'reality', 'realize', 'realized', 'really', 'realtime', 'realvalued', 'realworld', 'reanalyzed', 'rearranges', 'reason', 'reasonable', 'reasonably', 'reasoned', 'reasoning', 'reasons', 'rebels', 'rec', 'reca', 'recalculating', 'recalibrated', 'recall', 'recall49', 'recallable', 'recallimagetotext', 'recallk', 'recalloriented', 'recapitulate', 'recasens', 'recast', 'recategorize', 'receive', 'received', 'receiver', 'receiveroperatingcharacteristic', 'receives', 'recent', 'recently', 'recentlydeveloped', 'recentlyintroduced', 'receptive', 'receptor', 'receptors', 'recht', 'recipe', 'recipes', 'recog', 'recogni', 'recognit', 'recognition', 'recognition14', 'recognition2223', 'recognition52', 'recognition53', 'recognition57', 'recognition59', 'recognition7', 'recognition8', 'recognition9', 'recognitionbmc', 'recognitiongenome', 'recognitionneural', 'recognize', 'recognized', 'recognizing', 'recommend', 'recommendation', 'recommendations', 'recommended', 'reconnaˆıt', 'reconquista', 'reconstruct', 'reconstructing', 'reconstruction', 'record', 'record6460100', 'record7548828zenmnnlmjh5', 'recordbreaking', 'recorded', 'recording', 'recordings', 'records', 'recover', 'recovered', 'recovers', 'recovery', 'recruited', 'recruitment', 'rect2', 'rectal', 'rectangle', 'rectangles', 'rectangular', 'rectified', 'rectifier', 'rectifiers', 'rectiﬁed', 'rectiﬁer', 'rectiﬁers', 'rectly', 'rectum', 'recur', 'recurrence', 'recurrent', 'recurrentneuralnetworkbased', 'recursive', 'recursively', 'recycling', 'red', 'reddit', 'reddy', 'redesigned', 'redgreenblue', 'redistribute', 'redistributed', 'redistribution', 'redmon', 'redmond', 'redrawn', 'reduc', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'redun', 'redundancy', 'redundancyreduced', 'redundant', 'ree', 'reed', 'reenlisted', 'reevaluate', 'reexamine', 'ref', 'refer', 'referees', 'reference', 'references', 'referencing', 'referred', 'referring', 'refers', 'refine', 'refined', 'refinement', 'reflect', 'reflected', 'reflective', 'reformulate', 'reformulates', 'reformulation', 'refreshbiofamsa', 'refs', 'regard', 'regarded', 'regarding', 'regardless', 'regards', 'regenerate', 'regex', 'regime', 'regimemonocular', 'regimen', 'regiment', 'region', 'regionlevel', 'regionofinterest', 'regions', 'regres', 'regression', 'regu', 'regular', 'regularities', 'regulariza', 'regularization', 'regularized', 'regularizer', 'regularizing', 'regularly', 'regulate', 'regulated', 'regulates', 'regulation', 'regurgitated', 'rehabilitated', 'rehmsmeier', 'reid', 'reign', 'reiman1', 'rein', 'reinforce', 'reinforced', 'reinforcement', 'reissr', 'rejection', 'rel', 'rela', 'relabel', 'relate', 'related', 'relating', 'relation', 'relationextraction', 'relationextractionwasgenerallyframedasaclassificationproblemwithmanuallycraftedfeaturetemplatesto', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relativity', 'relaxa', 'relaxation', 'relay', 'release', 'release2020_01uniref', 'released', 'releases', 'relevance', 'relevant', 'reliability', 'reliable', 'reliably', 'reliance', 'relied', 'relies', 'relieve', 'relu', 'reluaddition', 'relufxx', 'relus', 'rely', 'relying', 'remain', 'remainder', 'remainder2930', 'remained', 'remaining', 'remains', 'remarkably', 'remedis', 'remedis38', 'remember', 'remembered', 'reminiscent', 'remixes', 'remmert', 'remote', 'remove', 'removed', 'removes', 'removing', 'remy', 'ren', 'renal', 'renalcell', 'renalcellcarcinoma', 'rences', 'rendered', 'rendering', 'rent', 'rently', 'renumbering', 'reorientation', 'rep', 'reparameterization', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repetition', 'replac', 'replace', 'replaced', 'replacement', 'replacement57', 'replaces', 'replacing', 'replica', 'replicate', 'replicated', 'replicates', 'replication', 'report', 'reported', 'reporting', 'reports', 'repositories', 'repository', 'repr', 'repre', 'represen', 'represent', 'representa', 'representation', 'representational', 'representationfor', 'representationisusuallyproducedbypoolingthoseofindividualtokensmaxoraverageforcomputationalef', 'representationlearning', 'representations', 'representations21', 'representative', 'representatives', 'represented', 'representing', 'represents', 'reprints', 'reproduce', 'reproduced', 'reproducibility', 'reproducing', 'reproduction', 'republish', 'request', 'requested', 'requests', 'require', 'required', 'requirement', 'requires', 'requiring', 'reran', 'reranking', 'rerelease', 'rereleases', 'res', 'res101', 'res46', 'res46w', 'res47', 'res48', 'res49', 'res50', 'rescaled', 'research', 'research21', 'researchbmc', 'researchers', 'researchzurich161', 'resec', 'resected', 'resection', 'resections', 'resemblance', 'resemble', 'resembled', 'resembles', 'resentation', 'resentations', 'resentatives', 'resenting', 'resents', 'reserve', 'reserved', 'reserving', 'resi', 'reside', 'resident', 'residents', 'residual', 'residual110', 'residual1202', 'residuals', 'residue', 'residueresidue', 'residues', 'residues63', 'resisc45', 'resisc45119', 'resistance', 'resisted', 'resize', 'resized', 'resizing', 'resnet', 'resnet1000', 'resnet1001', 'resnet101', 'resnet110', 'resnet11056layer', 'resnet152', 'resnet152x2', 'resnet18', 'resnet20', 'resnet200', 'resnet32', 'resnet34', 'resnet44', 'resnet50', 'resnet50101152', 'resnet5084', 'resnet50in', 'resnet50s', 'resnet56', 'resnetd', 'resnets', 'resnext', 'resnext101', 'resolu', 'resolution', 'resolutionagnostic', 'resolutionchannelslayers', 'resolutions', 'resolve', 'resolved', 'resort', 'resource', 'resourceconstrained', 'resourcehungry', 'resourceintensive', 'resources', 'resourcesfunding', 'respec', 'respect', 'respectable', 'respective', 'respectively', 'respectively23', 'respects', 'respiration', 'respiratory', 'respon', 'respond', 'responding', 'responds', 'response', 'responsenormalization', 'responsenormalized', 'responses', 'responsibilities', 'responsibility', 'responsible', 'rest', 'restainingbased', 'restera', 'resting', 'restore', 'restoring', 'restraints', 'restrict', 'restricted', 'restricting', 'restriction', 'restrictions', 'restrictive', 'restricts', 'resubmitted', 'result', 'resultant', 'resulted', 'resulting', 'resultit', 'results', 'resultsinbothdatasetsasitappearstopreventoverfittingwhilepreservingusefulentityinformationweleave', 'resumed', 'resurgence', 'ret', 'retained', 'retaining', 'retccl', 'retention', 'rethink', 'rethinking', 'reticular', 'reticulum', 'retina', 'retinal', 'retinopathy', 'retrained', 'retreated', 'retried', 'retrieval', 'retrieval4548', 'retrieval69153', 'retrieve', 'retrieved', 'retrieves', 'retrieving', 'retrospective', 'retrospectively', 'return', 'returned', 'returns', 'reuse', 'reuseby', 'reused', 'reusing', 'reussi', 'reuters', 'rev', 'reveal', 'revealed', 'reveals', 'reverberating', 'reverse', 'reversed', 'review', 'reviewed', 'reviewers', 'reviewing', 'reviewinsight', 'reviews', 'reviews144043', 'revised', 'revisit', 'revisiting', 'revived', 'reviving', 'revolution', 'revolutionized', 'revolutionizing', 'reward', 'rewards', 'reweighting', 'rewon', 'reynolds', 'rezarta', 'rezende', 'reﬂec', 'reﬂect', 'reﬂections', 'reﬂects', 'rgb', 'rgbinput', 'rhdvdmodel', 'rhinehart', 'rhinoceros', 'rhythm', 'rhythms', 'rial', 'rially', 'riasatian', 'rib', 'ribeiro', 'ric', 'rich', 'richard', 'richemond', 'richer', 'richness', 'ricky', 'rico', 'ricosennrichbarryhaddowandalexandrabirch2016neuralmachinetranslationofrarewordswithsubwordunitsin', 'riding', 'rie', 'ried', 'riehl', 'riesselman', 'rieta', 'riety', 'rigeminy', 'righetto', 'right', 'righthand', 'rightmost', 'rights', 'rightsholders', 'rightsided', 'rightvs', 'rigid', 'rigidbody', 'rigorous', 'rimm', 'rin', 'rine', 'rings', 'ringshia', 'ripley', 'riquelme', 'rise', 'rishub', 'risk', 'risks', 'rithm', 'rithm83', 'rithmic', 'rithms', 'rival', 'rivaled', 'rivaling', 'river', 'rives', 'rives12', 'rized', 'rizvi', 'rj', 'rjc', 'rjr', 'rk', 'rl', 'rl2558', 'rm', 'rmdl', 'rms', 'rmsd', 'rmsd95', 'rmsprop', 'rn', 'rn50x16', 'rn50x4', 'rn50x64', 'rna', 'rnaseq', 'rnn', 'rnns', 'ro', 'ro1ai51321', 'road', 'rob', 'robert', 'roberta', 'roberto', 'roberts', 'roberttinn', 'robetta', 'robin', 'robinson', 'robiol', 'robot', 'robots', 'robust', 'robustly', 'robustness', 'robyn', 'roc', 'rock', 'rocs', 'rocy', 'rodgers', 'rodrigues12', 'rodriguez', 'rodríguez', 'roelofs', 'roentgenogram', 'roetzerpejrimovsky', 'roger', 'rogers', 'rohrbach', 'roi', 'roilevel', 'rois', 'role', 'roller', 'romain', 'roman', 'rome', 'romeraparedes', 'romeraparedes14', 'romero', 'ron', 'ronald', 'roney', 'rongkai', 'ronneberger', 'ronneberger14', 'rons', 'room', 'root', 'rootmeansquare', 'roots', 'ror', 'rors', 'rosanna', 'rose', 'rosenblatt', 'rosenman', 'rosenow', 'rosettacommonsrosettafold', 'rosettafold', 'rosettafolda', 'rosettafoldpredicted', 'roshan', 'roshtkhari', 'rospective', 'ross', 'rossum', 'roststructure30', 'rotamer', 'rotamers', 'rotation', 'rotations', 'roth', 'rouge', 'rouge1', 'rouge81', 'rougescore', 'rough', 'roughly', 'round', 'rounding', 'route', 'routes', 'routine', 'roux', 'row', 'rows', 'rowwise', 'roy', 'royal', 'rr', 'rrc', 'rrr', 'rrustemi', 'rt', 'rthe', 'rtx2080', 'rubin', 'rubisch', 'ruder', 'rudimentary', 'rudra', 'rudyard', 'ruhul', 'ruins', 'rule', 'rulebased', 'rules', 'rumclhart', 'rumelhart', 'run', 'running', 'runs', 'runtime', 'ruoming', 'rush', 'ruslan', 'russ', 'russakovsky', 'russell', 'russia', 'russian', 'rusu', 'ruth', 'ruyssen', 'rv', 'ryan', 'ryder', 'ré', 'réseau', 'rıo', 's', 's1', 's10', 's11', 's17', 's2', 's210', 's3', 's4', 's4158602026492', 's4159101802683', 's4a', 's4b', 's4c', 's4d', 's4httpsdoiorg101186', 's4l', 's5', 's5a', 's5b', 's5c', 's6', 's7', 's7d', 's8', 's9', 'sa', 'saak', 'sabah', 'saber', 'sabiha', 'sablayrolles', 'sacheti', 'sacketts', 'sackinger', 'sacrificing', 'saddle', 'safe', 'safwan', 'sagmeister15', 'sahai', 'sahai12349', 'said', 'saif', 'saigaj', 'sail', 'sainath', 'sainbayar', 'saito', 'sajedin', 'sake', 'saladin', 'salakhutdi', 'salakhutdinov', 'saldanha', 'saleh', 'salient', 'saliently', 'saligrama', 'salimans', 'salmen', 'salt', 'saltz', 'salvatore', 'sam', 'same', 'sameer', 'samir', 'sammut', 'samoyed', 'samoyeds', 'sample', 'sampled', 'sampler', 'samples', 'samples166', 'sampling', 'sampling82', 'sampo', 'samsung', 'samuel', 'samvat', 'samy', 'san', 'sanchez', 'sandberg3', 'sander', 'sandhini', 'sandler', 'sandwiched', 'sanghavi', 'sanh', 'sanity', 'sanja', 'sanmitra', 'sans', 'sansombiochim', 'santa', 'santamaría', 'santos', 'sap', 'sarah', 'sarc', 'sarcoma', 'sarcomas', 'sarily', 'sariyildiz', 'sarscov2', 'sarıyıldız', 'sastry', 'satellite', 'satheesh', 'satisfac', 'satisfaction', 'satisfactory', 'satisfied', 'satpathy', 'saturate', 'saturated', 'saturates', 'saturating', 'saturation', 'sauce', 'saudi', 'savarese', 'saved', 'saving', 'saw', 'saxe', 'say', 'says', 'sb', 'sc', 'scaffold', 'scal', 'scalability', 'scalable', 'scalar', 'scale', 'scalearxiv', 'scaled', 'scales', 'scaling', 'scan', 'scanned', 'scanner', 'scanners', 'scannerspecific', 'scanning', 'scans', 'scarce', 'scarcity', 'scatterplots', 'scenario', 'scenarios', 'scene', 'scenes', 'scent', 'schaeffer6', 'schalper', 'schapiro', 'schaumberg', 'schedule', 'scheduler', 'schedules', 'scheibel', 'schema', 'schemata', 'schematic', 'schematically', 'scheme', 'scheme76', 'schemebio', 'schemes', 'schemethatonlydifferentiatesbetweeninandoutofanentityclassificationisdoneusing', 'scheres', 'scheuerman', 'schirris', 'schlant', 'schlipsing', 'schläpfer', 'schmauch', 'schmid', 'schmidhuber', 'schmidt', 'schneider', 'scholar', 'scholars', 'school', 'schools', 'schoonvelde', 'schraudolph', 'schroeder', 'schuhmann', 'schuster', 'schwannoma', 'schwarcz', 'schwartz', 'schwarzenegger', 'schwede', 'schwemmer', 'schwenk', 'schwing', 'schölkopf', 'sci', 'sci27', 'sciaky', 'scibert', 'science', 'sciencemagorgaboutsciencelicensesjournalarticlereuse', 'scienceorgdoi101126scienceade2574', 'sciences', 'sciencesciencemagorgcontent3736557871suppldc1', 'scientific', 'scientist', 'scientists', 'scientiﬁc', 'scikitlearn', 'scikitlearn134', 'scipy', 'scipyorg', 'sclc', 'sclwc', 'scope', 'score', 'score101', 'score76', 'scored', 'scoreforasentencepairbydefaultaspecial', 'scores', 'scores48', 'scoring', 'scotia', 'scrape', 'scraped', 'scrapes', 'scratch', 'scratchcan', 'scratchcanprovideasolidfoundationforbiomedicalnlpleadingtonewstateoftheartperformanceacrossa', 'scratchthe', 'screen', 'screening', 'scribed', 'script', 'scripts', 'scroll', 'sd', 'se', 'se3', 'se3equivariant', 'sea', 'seaborn', 'seamlessly', 'search', 'search1316', 'searched', 'searches', 'searching', 'searchkgebrainseuinstancesdataset8fc108abe2b44068999', 'seas', 'seascapes', 'seattle', 'sebastian', 'sec', 'sec31', 'second', 'secondary', 'secondbest', 'secondbestperforming', 'seconddegree', 'seconde', 'secondorder', 'seconds', 'secondwhich', 'secretary', 'secreted', 'secretions', 'secretory', 'section', 'section24', 'section24t', 'sections', 'secularized', 'security', 'sedra', 'see', 'seed', 'seeded', 'seeds', 'seejoor', 'seek', 'seekins3', 'seeks', 'seem', 'seemingly', 'seems', 'seen', 'seer', 'sees', 'seg', 'segmen', 'segment', 'segmenta', 'segmentation', 'segmented', 'segmenting', 'segments', 'segpath', 'segpath102', 'segurabedmar', 'seixas', 'sejnowski', 'seldom', 'select', 'selected', 'selecting', 'selection', 'selectionreduction', 'selective', 'selectively', 'selectivity', 'selectivityinvariance', 'selects', 'self', 'selfarchiving', 'selfattention', 'selfattentive', 'selfconnection', 'selfdistillation', 'selfdistillation35', 'selfdriving', 'selfestimates', 'selflearning', 'selforganizing', 'selfpath', 'selfreported', 'selfridge', 'selfsupervised', 'selfsupervision', 'selftrained', 'selftraining', 'sellar', 'sels', 'selvaraju', 'semantic', 'semantically', 'semantics', 'semevalacl', 'semevalnaaclhlt', 'semi', 'semiautomatically', 'semis', 'semisupervised', 'semmelweis', 'sen', 'sends', 'senet', 'senior', 'senior1', 'senioret', 'sennrich', 'sense', 'sensi', 'sensible', 'sensibly', 'sensing', 'sensitive', 'sensitivities', 'sensitivity', 'sensitivity04', 'sent', 'sentation', 'sentations', 'sentative', 'sentence', 'sentenced', 'sentencelevel', 'sentences', 'sentential', 'sentiment', 'sentinel', 'seo', 'seoul', 'sep', 'sepa', 'separa', 'separable', 'separate', 'separated', 'separately', 'separating', 'separation', 'sepassi', 'sepp', 'seps2', 'sept', 'septa', 'septal', 'september', 'septembre', 'septokenisinsertedtoseparatethetwosentencesandaspecial', 'seq', 'seq2seq', 'seqres', 'sequence', 'sequence75', 'sequencea', 'sequencebased', 'sequenceclustersbc40out', 'sequencelevel', 'sequences', 'sequences5456', 'sequences67', 'sequencestructure', 'sequencethe', 'sequencetosequence', 'sequencex', 'sequencing', 'sequential', 'sequentially', 'ser', 'ser212', 'ser33', 'sercu', 'sercu1', 'sergey', 'series', 'serine', 'serious', 'serman', 'sermanet', 'serous', 'servaes', 'servation', 'servations', 'serve', 'served', 'server', 'servers', 'serves', 'service', 'services', 'serving', 'servings', 'sess', 'session', 'sessions', 'set', 'setb', 'setlevel', 'sets', 'setting', 'setting36', 'settings', 'settingsexactly', 'settins', 'settled', 'settling', 'setup', 'setups', 'seuil', 'seung', 'sev', 'seven', 'seventh', 'several', 'severe', 'severely', 'severity', 'sevilla', 'sex', 'sexprimant', 'sexual', 'seyoun', 'sf', 'sgcl', 'sgd', 'sh', 'shaban1234', 'shabir', 'shaded', 'shades', 'shah', 'shakhnarovich', 'shallow', 'shallower', 'shalom', 'shamma', 'shankai', 'shankar', 'shannon', 'shannonbell', 'shannonsm', 'shao', 'shao12348', 'shaoqing', 'shape', 'shapehiwici1', 'shapes', 'sharan', 'sharded', 'share', 'shared', 'shares', 'sharifa', 'sharing', 'sharir', 'sharp', 'shashank', 'shashikumar', 'shashua', 'shatter', 'shattered', 'shattering', 'shaw', 'shazeer', 'she', 'sheet', 'sheets', 'shelf', 'shelhamer', 'shell', 'shelter', 'shen', 'shepard', 'sheppard', 'sheridan', 'sherrington', 'shh', 'shick', 'shift', 'shifted', 'shiftinvariant', 'shifts', 'shikano', 'shimomura', 'shin', 'shindyalov', 'shirt', 'shlens', 'shmatko', 'shmueli1', 'shocking', 'shoe', 'shoemaker', 'shoes', 'shone', 'shooshan', 'shoot', 'shopping', 'shore', 'short', 'shortage', 'shortcircuiting', 'shortcomings', 'shortcut', 'shortcuts', 'shortens', 'shorter', 'shortest', 'shortterm', 'shorttime', 'shot', 'shots', 'should', 'shouted', 'show', 'showcas', 'showcases', 'showed', 'showing', 'shown', 'shows', 'shpanskaya', 'shpanskaya3', 'shrink', 'shrivastava', 'shrunk', 'shuckett', 'shufﬂed', 'shufﬂenet', 'shufﬂenets', 'shufﬂing', 'shuster', 'shutova', 'shuttleworth', 'shyam', 'si', 'siam', 'siamese', 'sibased', 'siberian', 'sible', 'sic', 'sicap', 'sicap75', 'side', 'sidebyside', 'sidechain', 'sided', 'sideeffect', 'sider', 'siderably', 'siderthesimpler', 'siebel', 'sification', 'sificationmethod', 'sifre', 'sift', 'sig', 'siggraph', 'sight', 'sights', 'sigkdd', 'sigm', 'sigmoid', 'sigmoids', 'sigmoidweighted', 'sigmund', 'sign', 'signal', 'signaling', 'signals', 'signaltonoise', 'signature', 'signatures', 'signed', 'significance', 'significant', 'significantly', 'signifies', 'signify', 'signiﬁ', 'signiﬁcance', 'signiﬁcant', 'signiﬁcantly', 'signs', 'sij', 'silhouette', 'silins', 'silk', 'silvarodriguez', 'silveira', 'silver', 'silver1', 'silverwhite', 'silviana', 'sim', 'simard', 'simclr', 'simclr26', 'simclrv2', 'simi', 'similar', 'similarities', 'similarity', 'similaritybased', 'similaritythe', 'similarly', 'similarlysized', 'simmering', 'simmerling', 'simon', 'simonyan', 'simple', 'simplelanguage', 'simpleproperties', 'simpler', 'simpleshot', 'simplest', 'simplicity', 'simplification', 'simplifies', 'simplify', 'simplifying', 'simpliﬁed', 'simpliﬁes', 'simply', 'simulate', 'simulated', 'simulation', 'simulations', 'simulator', 'simultaneous', 'simultaneously', 'simultaneously14', 'simulus', 'sin', 'since', 'sine', 'singapore101', 'singer', 'singh', 'single', 'singlecrop', 'singledimension', 'singledisease', 'singlehead', 'singlelead', 'singlemodel', 'singlepanel', 'singlepass', 'singleprecision', 'singlesequence', 'singleview', 'singleword', 'sinkhornknopp', 'sinpos100002idmodel', 'sinus', 'sinusoid', 'sinusoidal', 'sinusoids', 'sion', 'sionality', 'sions', 'sippl', 'sis', 'sister', 'sists', 'sit', 'site', 'sites', 'sitespecific', 'sitestratified', 'sition', 'sitting', 'situ', 'situation', 'situations', 'sive', 'sivic', 'six', 'sixteen', 'sixth', 'sixtyone', 'size', 'size11113', 'sizea', 'sizeable', 'sized', 'sizes', 'sizes32168', 'siﬁcation', 'siﬁers', 'siﬁes', 'sj', 'sk', 'skcm', 'skeletal', 'sketch', 'sketched', 'skewing', 'skill', 'skilled', 'skills', 'skin', 'skip', 'skipconnection', 'skipconnnections', 'skipping', 'skipthought', 'skolnick', 'skolnickproteins', 'skrede', 'sl', 'slanted', 'slayers', 'slide', 'slidelevel', 'slides', 'slides156', 'slides167', 'slides86', 'slight', 'slightly', 'slim', 'slots', 'slower', 'slowly', 'slp', 'slpd', 'sm', 'smaira', 'small', 'smallcell', 'smaller', 'smallerscale', 'smallerseed', 'smallest', 'smallmolecule', 'smart', 'smartphone', 'smartphones', 'smekalova', 'smetanin1', 'smily', 'smith', 'smola', 'smooth', 'smoothed', 'smoother', 'smoothing', 'smoothly', 'smorgasbord', 'sn', 'sn1', 'snap', 'snaps', 'sneaker', 'sneakermaker', 'sneakers', 'snell', 'snip', 'snk1', 'snorkel', 'snorkelling', 'snow', 'sns1', 'so', 'soares', 'sobieraj', 'soc', 'socher', 'social', 'socially', 'sociation', 'society', 'societys', 'socius', 'soft', 'softened', 'softfailure', 'softmax', 'softmaxcentering', 'softmaxqkt', 'softtissue', 'software', 'soheil', 'sohl', 'sohn', 'soigner', 'soil', 'solaiman', 'solely', 'solid', 'solidation', 'solidpattern', 'solidstatedrive', 'solomon', 'solution', 'solutionby', 'solutions', 'solv', 'solve', 'solved', 'solver', 'solver131', 'solvers', 'solves', 'solving', 'somatic', 'some', 'something', 'sometimes', 'somewhat', 'son', 'sonably', 'song', 'song1234', 'sonnet', 'sonnet71', 'sons', 'sonya', 'soomro', 'soon', 'sophia', 'sophis', 'sophisticated', 'soroosh', 'sort', 'sortascending', 'sorted', 'sorting', 'sorts', 'sos', 'sota', 'sought', 'sound', 'sounds', 'soundwaves', 'source', 'sourced', 'sourcedomainishighlyrelevanttothetargetonefordomainswithabundantunlabeledtextsuchasbiomedicine', 'sources', 'sourcespecific', 'sourcing', 'south', 'southern', 'southwestern', 'soyer', 'soğancıoğlu', 'sp', 'space', 'spacea', 'spaced', 'spaceefﬁcient', 'spaces', 'spain', 'spain1', 'span', 'spanish', 'spanning', 'spans', 'spare', 'sparked', 'sparse', 'sparsecoding', 'sparsely', 'sparselygated', 'spartan', 'spassievaet', 'spatial', 'spatiallocality', 'spatially', 'spe', 'speaker', 'speaking', 'spec', 'speci', 'special', 'specialcased', 'specialists', 'specialization', 'specialize', 'specialized', 'specially', 'species', 'specif', 'specifi', 'specific', 'specifically', 'specificity', 'specificity1', 'specified', 'specify', 'specifying', 'specimen', 'specimens', 'speciﬁ', 'speciﬁc', 'speciﬁcally', 'speciﬁcation', 'speciﬁed', 'speciﬁes', 'spectively', 'spectrograms', 'spectrum', 'speculate', 'speculation', 'speech', 'speeches', 'speechrecognition', 'speed', 'speedup', 'speedy', 'speer', 'spend', 'spent', 'sphere', 'spheres', 'sphingolipid', 'spike', 'spikes', 'spin', 'spine', 'spins', 'spirational', 'spirit', 'spisak', 'spite', 'spixels', 'splashing', 'splendid', 'splicing', 'split', 'splits', 'splitter', 'splitting', 'splus', 'spoke', 'spond', 'sponded', 'spondence', 'sponding', 'sponses', 'sponsor', 'spontaneous', 'spontaneously', 'spot', 'spotting', 'spread', 'spreading', 'spring', 'springenberg', 'springer', 'springernew', 'spurious', 'sq', 'sq42thefocushase', 'squad', 'squamous', 'square', 'squared', 'squares', 'squeezeandexcitation', 'squeezenet', 'squeezenets', 'squeezenext', 'src', 'srikrishna', 'srinidhi', 'srivastava', 'ss', 'ssh', 'sshadd', 'ssl', 'sslide', 'sslidei', 'ssrn', 'ssrn4172090', 'st', 'st1', 'sta', 'stability', 'stable', 'stack', 'stacked', 'stacking', 'stacks', 'stad', 'stadium', 'staff', 'stage', 'stages', 'staging', 'stain', 'staining', 'stains', 'stakeholder', 'stallkamp', 'stan', 'stances', 'stand', 'standalone', 'standard', 'standardization', 'standardize', 'standardized', 'standards', 'standing', 'stands', 'stanford', 'stanislav', 'stant', 'stantial', 'stanway', 'staphylococcal', 'star', 'starikovsky', 'stark', 'start', 'startandendtagstoentitiesinquestionforrelationencodingweconsiderthreeschemesinthe', 'started', 'startend', 'starting', 'starts', 'startups', 'stasis', 'stat', 'state', 'stated', 'statement', 'statements', 'stateof', 'stateofthe', 'stateoftheart', 'states', 'states81', 'static', 'station', 'stationarity', 'stationary', 'statistic', 'statistical', 'statistically', 'statistics', 'statsmodels', 'statsmodelsstatsmodels', 'status', 'statutory', 'std', 'stead', 'steadily', 'steady', 'steeper', 'steepest', 'steinegger', 'steinegger23', 'steiner', 'steinhardt', 'steinkraus', 'stems', 'stenius', 'step', 'step87', 'stephan', 'stephanie', 'stepped', 'steps', 'stereochemical', 'stereotypical', 'steric', 'sterol', 'steven', 'stevenbethardmarinecarpuatmariannaapidianakisaifmmohammaddanielmceranddavidjurgenseds2017', 'steyerberg', 'stfc', 'stick', 'stickier', 'sticking', 'sticks', 'stiff', 'stig', 'stiles', 'still', 'stimulated', 'stimulation', 'stimuli', 'stimulus', 'stitch', 'stitched', 'stitute', 'stl10', 'stn', 'stochas', 'stochastic', 'stochiometry', 'stockholm', 'stoichiometry', 'stomach', 'stone', 'stop', 'stopped', 'stopping', 'stops', 'stopword', 'storage', 'store', 'stored', 'stores', 'storm', 'stormolect', 'story', 'storytelling', 'stoyanov', 'str0027541', 'straightforward', 'straints', 'straitfied', 'strange', 'strate', 'strated', 'strategically', 'strategies', 'strategy', 'strategy2847', 'stratification', 'stratified', 'stratifying', 'stratigraphic', 'stream', 'streaming', 'streamlined', 'street', 'streetwear', 'strength', 'strengthen', 'strengths', 'stretch', 'strictly', 'stride', 'strides', 'strike', 'string', 'strings', 'striving', 'stroma', 'stromal', 'strong', 'stronger', 'strongly', 'strongperforming', 'stroud', 'strub', 'struc', 'struct', 'structed', 'structing', 'structural', 'structurally', 'structure', 'structureactivity', 'structured', 'structures', 'structures113', 'structures1819', 'structureslicense', 'structuresone', 'struggle', 'ström', 'stu', 'stuck', 'stuckwith', 'stud', 'student', 'studentteacher', 'studied', 'studies', 'studies4454', 'studies445456', 'studies445458', 'studies96', 'studio', 'study', 'study35', 'study36', 'studying', 'studys', 'stumpe', 'stunning', 'style', 'su', 'sual', 'sub', 'subbiah', 'subblock', 'subcaptions', 'subcellular', 'subdivided', 'subfigure', 'subgraph', 'subgroup', 'subhashini', 'subie', 'subimages', 'subject', 'subjected', 'subjective', 'sublayer', 'sublayers', 'sublayerx', 'submarine', 'submerged', 'submissions', 'submit', 'submitted', 'submitting', 'submucosal', 'suboptimal', 'subpart', 'subprob', 'subproblem', 'subramanian', 'subramanya', 'subregions', 'subroutines', 'subsample', 'subsampled', 'subsamples', 'subsampling', 'subse', 'subsection', 'subsections', 'subsequent', 'subsequently', 'subset', 'subsets', 'subsetted', 'subslide', 'subspace', 'subspaces', 'subspecialized', 'subspecialty', 'substan', 'substances', 'substantial', 'substantially', 'substantiallyfrombiomedicalliteraturetotheextentthatweobservebertmodelspretrainedonclinicalnotes', 'substantiallyoutperformscontinualpretrainingofgenericlanguagemodelsthusdemonstratingthattheprevailing', 'substantiated', 'substantively', 'substitute', 'substituting', 'substitution', 'substitutions', 'substrate', 'subsystem', 'subtasks', 'subtle', 'subtract', 'subtracted', 'subtracting', 'subtyp', 'subtype', 'subtypes', 'subtyping', 'subtyping67', 'subunit', 'subunits', 'subword', 'subwords', 'succeeded', 'success', 'successes', 'successesincounteringadversarialattacksincomputervisionadversarialpretrainingintroducesperturbations', 'successful', 'successfully', 'successively', 'successor', 'successors', 'such', 'sudo', 'sue', 'suecell', 'suffer', 'suffers', 'suffi', 'suffice', 'suffices', 'sufficient', 'sufficiently', 'sufﬁcient', 'sufﬁciently', 'sug', 'sugar', 'suggest', 'suggested', 'suggesting', 'suggestions', 'suggestive', 'suggests', 'sugiyama', 'suhail', 'suis', 'suit', 'suitable', 'suitably', 'suite', 'suited', 'sukhbaatar', 'sule', 'sullivan', 'sulted', 'sults', 'sum', 'summa', 'summaries', 'summarization', 'summarize', 'summarized', 'summarizes', 'summarizing', 'summary', 'summaryapril', 'summarymarch', 'summation', 'summed', 'summer', 'summers', 'summing', 'summits', 'sums', 'sun', 'sundaram', 'sung', 'sup', 'super', 'supercomputing', 'superfamily', 'superficial', 'superglue', 'superimposed', 'superintendent', 'superior', 'supermarket', 'superposition', 'superpositionfree', 'superresolution', 'supervi', 'supervise', 'supervised', 'supervisedlearning', 'supervising', 'supervision', 'supervision5', 'supervisor', 'supervisory', 'superyacht', 'superﬁcially', 'suppl', 'supple', 'supplement', 'supplemental', 'supplementary', 'supplemented', 'support', 'supported', 'supporting', 'supports', 'suppose', 'supposition', 'suppress', 'suppressing', 'supraventricular', 'sur', 'sure', 'sured', 'sures', 'surface', 'surge', 'surgery', 'surgical', 'surpass', 'surpassed', 'surpasses', 'surpassing', 'surpris', 'surprised', 'surprising', 'surprisingly', 'surrogate', 'surround', 'surrounded', 'surrounding', 'surveillance', 'survey', 'survival', 'survived', 'susan', 'suspect', 'suspicion', 'suspicious', 'susskind', 'sustained', 'sutskever', 'suture', 'sutured', 'suturing', 'suzek', 'svetnik', 'svhn', 'svhnthe', 'svm', 'svt', 'swap', 'swapped', 'swapping', 'sweden', 'swedish', 'sweep', 'sweeps', 'swept', 'swering', 'swers', 'swersky', 'swetter', 'swin', 'swint14', 'swish', 'swiss', 'swissprottrembl', 'switch', 'sy', 'syllables', 'symbol', 'symbolic', 'symbolizing', 'symbols', 'symlink', 'symmetric', 'symmetrical', 'symmetry', 'symp', 'symposium', 'sympsumm', 'symptom', 'synagogue', 'synagogues', 'synapse', 'synapses', 'synaptic', 'synchronous', 'synchrony', 'synchrotron', 'synchrotronradiation', 'syndrome4', 'synergistic', 'synonyms', 'synsets', 'syntactic', 'syntax', 'synth', 'synthase', 'synthesizes', 'synthetic', 'syria', 'syrian', 'sys', 'syst', 'system', 'system10', 'system65', 'system77', 'systemati', 'systematic', 'systematically', 'systems', 'systems46', 'systems99', 'systemtheoreticapproach', 'sze', 'szegedy', 'szeliski', 'szepesvari', 'szlam', 'szolovits', 'szurmant', 'sánchez', 'söding', 'södingnat', 'sˆur', 't', 't1', 't1024', 't1044', 't1049', 't1056', 't1057', 't1064', 't1074', 't1076', 't1091', 't2i', 't32ca251062', 't32hg002295', 't4in', 't6ss', 't9', 't_e', 't_et', 't_f', 'ta', 'table', 'table12', 'table13', 'table2', 'table3', 'table3for', 'table4', 'table5', 'table7', 'tables', 'tablished', 'tabulated', 'tachycardia', 'tact', 'tacts', 'tacular', 'tag', 'tagging', 'tags', 'tagswouldbefurther', 'tahseen', 'tai', 'taigman', 'tailed', 'tailored', 'tailoring', 'tain', 'tained', 'taines', 'tains', 'tainty', 'takagi', 'takahashi', 'take', 'takeda', 'taken', 'takes', 'taking', 'takukudoandjohnrichardson2018sentencepieceasimpleandlanguageindependentsubwordtokenizeranddetokenizerforneu', 'talking', 'tallec', 'tally', 'tamas', 'tamò', 'tan', 'tan1', 'tance', 'tandem', 'tang', 'tangent', 'tangible', 'tangles', 'tango2', 'tanh', 'tanhx', 'tanhz', 'tanmingxinggooglecom', 'tant', 'tao', 'taori', 'tap', 'tapaswi', 'tapelike', 'tar', 'target', 'targetapplicationdomainhaslittletextofitsownandcantherebybenefitfrompretrainingusingrelateddomains', 'targeted', 'targeting', 'targets', 'tariqul', 'tart', 'tasis', 'task', 'task11', 'task39', 'task99', 'taskagnostic', 'tasked', 'taskindependent', 'tasklearning', 'tasks', 'tasks105136143', 'tasks1520', 'tasks19', 'tasks23', 'tasks36', 'tasks37385466', 'taskshavebeenproposedforemergingapplicationscenariossuchasevidencebasedmedicalinformationextrac', 'taskspecific', 'taskspecificproblemformulationandmodelingchoices', 'taskspeciﬁc', 'tasted', 'tate', 'tateisi', 'tation', 'tational', 'tations', 'tative', 'taught', 'taurus', 'taxes', 'taylor', 'tb', 'tcga', 'tcga168', 'tcga35', 'tcga68', 'tcga691617616774', 'tcga8788', 'tcgacptac', 'tcgagbmlgg', 'tcgakich', 'tcgakirc', 'tcgakirp', 'tcgaluad', 'tcgalusc', 'tcgansclc', 'tcgansclc79', 'tcgapretrained', 'tcgatils', 'tcgatils67', 'tcia', 'tcia9cjf0127', 'td', 'tea', 'teach', 'teacher', 'teachers', 'teaches', 'teal', 'team', 'teams', 'tears', 'tease', 'teaspoon', 'tech', 'technical', 'technician', 'technicians', 'technique', 'techniques', 'techniques6768', 'technol', 'technologies', 'technology', 'tection', 'tecture', 'tectures', 'teddy', 'tedious', 'tee', 'tegrated', 'teh', 'teijeiro', 'tein', 'teins', 'tejani', 'telemetry', 'telepathology', 'tell', 'tellez', 'tells', 'telomeric', 'tem', 'tem29', 'tematically', 'temperature', 'template', 'templatebased', 'templates', 'temporal', 'temporarily', 'tems', 'ten', 'tence', 'tency', 'tend', 'tended', 'tendency', 'tends', 'tenenbaum', 'tens', 'tensive', 'tensor', 'tensor2tensor', 'tensorflow', 'tensorflowtensor2tensor', 'tensorﬂow', 'tenth', 'tential', 'tentionscomputedusingthegiventokensrepresentationinthepriorlayerasthequerythefinallayeroutputs', 'tently', 'teome', 'teplyashin', 'ter', 'tered', 'terizations', 'term', 'termed', 'terminate', 'termination', 'terminator', 'terminus', 'terms', 'tern', 'ternal', 'ternational', 'terns', 'terpret', 'terpretation', 'terrestrial', 'territory', 'ters', 'tertiary', 'tesco', 'test', 'testable', 'tested', 'tester', 'testicular', 'testing', 'tests', 'testset', 'testtime', 'testuggine', 'tetramer', 'teuwen', 'texaco', 'texas', 'text', 'text72', 'text8', 'text83', 'text_encoder', 'text_encodert', 'textaware', 'textbook', 'textbooks', 'textcaption', 'textfile', 'textimage', 'textonly', 'texts', 'textspanincludingtheentitiesthemselvesthisintroducesapotentialriskthattheneuralnetworkmaysimply', 'textt', 'texttldr', 'texttoimage', 'texttotext', 'textual', 'textural', 'texture', 'textures', 'textvideo', 'textvqa', 'tf', 'tflops', 'tg', 'tgct', 'th', 'thaler', 'than', 'thank', 'thanks', 'that', 'thatai', 'thatcontinualpretrainingmaynotbeabletocompletelyundosuboptimalinitializationfromthegeneraldomain', 'thatdomainspecific', 'thats', 'thattheuseoftransformerbasedmodels', 'thca', 'the', 'the2019', 'the2242243', 'the617', 'thea', 'theart', 'theaverageamongpioelementsinthedatasetoccasionallytwopicoelementsmightoverlapwitheachother', 'theb', 'thebioasqcorpus', 'thebiocreativeiigenementioncorpus', 'thebiocreativevchemicaldiseaserelationcorpus', 'thecifar10', 'thedrugdruginteractioncorpus', 'thefeaturizer', 'theflow', 'thegeneraldomain', 'thegeneraldomainvocabularythedomainspecificvocabularyenablespubmedberttolearnbetterattentionpatternsand', 'theinsideandoutsideofentitiesbutforbertmodelsonceagaintheutilityofmorecomplextaggingschemes', 'their', 'theirs', 'thelwell', 'them', 'them8182', 'theme', 'themodeling', 'themselves', 'then', 'thenorthamerican', 'theo', 'theocratic', 'theoreti', 'theoretical', 'theoreticallybest', 'theories', 'theoriginal', 'theory', 'thepredict', 'thepubmedqadataset', 'ther', 'therapeutic', 'therapeuticresponse', 'therapy', 'there', 'thereafter', 'thereby', 'therefore', 'thereof17', 'thermodynamic', 'these', 'thesis', 'theunderlyingpretrainedlanguagemodeltoourstandardpubmedbertwwmpubmedvocabularypretrained', 'theuzeros', 'thevocabularyandpretrainsfromscratchusingbiomedicineandcomputerscienceasrepresentativesforscien', 'thex', 'they', 'theyll', 'theyre', 'theℓth', 'thiagarajan', 'thibodeaulaufer', 'thickening', 'thierry', 'thin', 'thing', 'thingelse', 'things', 'think', 'thinks', 'thinwalled', 'third', 'thirddegree', 'thirds', 'thirion', 'thirteen', 'thirteenth', 'thirty', 'thirtythird', 'this', 'thiscasetheonegeneratedfromwikipediaandbookcorpusalthoughconvenientthisisamajordisadvantage', 'thology', 'thoma', 'thomas', 'thomascwiegersandzhiyonglu2016biocreativevcdrtaskcorpusaresourceforchemicaldiseaserelationextraction', 'thomee', 'thompson', 'thoracic', 'thorax', 'thorough', 'thoroughly', 'thoroughness', 'thorpescience', 'those', 'though', 'thought', 'thoughtful', 'thousand', 'thousands', 'threatening', 'three', 'threeclass', 'threedimensional', 'threetrack', 'threeway', 'thresh', 'threshold', 'thresholded', 'thresholding', 'thresholds', 'threw', 'thrombospondin', 'throne', 'thrones', 'through', 'throughfeature', 'throughout', 'throughput', 'throwing', 'thrun', 'thus', 'thym', 'thymoma', 'thyroid', 'ti', 'tial', 'tially', 'tials', 'tialspropagating', 'tian', 'tiary', 'tiation', 'tiber', 'tibet', 'tic', 'ticated', 'ticing', 'tickle', 'ticlass', 'tie', 'tied', 'tienet', 'ties', 'tiffany', 'tificliteraturehoweverfromtheperspectiveofbiomedicalapplicationsscibertstilladoptsthemixeddomain', 'tified', 'tifying', 'tigative', 'tiger', 'tighter', 'tii', 'tij', 'tij0', 'tijand', 'tijjs', 'tijvj', 'tijvjvj', 'tijwas', 'til', 'tile', 'tiled', 'tilelevel', 'tiles', 'tiling', 'tilingual', 'till', 'till28', 'tilnegative', 'tilpositive', 'tils', 'tim', 'timal', 'timate', 'timates', 'timation', 'time', 'time7686', 'time79', 'timearxiv', 'timeconsuming', 'timecostly', 'timedelay', 'timeline', 'times', 'timese', 'timestep', 'timing', 'timings', 'timistic', 'timm', 'timodal', 'timothy', 'tinct', 'ting', 'tings', 'tinn', 'tinue', 'tinued', 'tiny', 'tiomers', 'tion', 'tion10', 'tion117', 'tion1314', 'tion3335', 'tion44howeveralthoughcomprehensivebenchmarksandleaderboardsareavailableforthegeneraldomains', 'tional', 'tionally', 'tionarily', 'tionaryscale', 'tioning', 'tionnement', 'tions', 'tions2024', 'tiontest', 'tiple', 'tis', 'tischer', 'tise', 'tison', 'tissue', 'tissuecontaining', 'tissues', 'tissuesite', 'tistical', 'tistically', 'titask', 'titatively', 'title', 'titled', 'titles', 'tity', 'tiu', 'tivation', 'tive', 'tively', 'tivity', 'tivs', 'tizhoosh', 'tj', 'tji', 'tjis', 'tjj', 'tjjis', 'tk', 'tl', 'tl2', 'tlc', 'tldr', 'tle', 'tm', 'tmalign', 'tmh', 'tmh2', 'tmscore', 'tmscore27', 'tmscores', 'tn', 'to', 'to004', 'to36', 'to60', 'toaugment', 'tobacco', 'todorov', 'toend', 'tof', 'togerman', 'together', 'tographs', 'tohigher', 'toimage', 'token', 'tokenization', 'tokenized', 'tokenizer', 'tokens', 'tokensfor', 'tokensseptoaddresstheproblemofoutofvocabularywordsneurallanguagemodelsgenerateavocabulary', 'told', 'toledo', 'tolerant', 'tolerate', 'tolkach', 'tology', 'tom', 'toma', 'tomasmikolovkaichengregcorradoandjeffreydean2013efficientestimationofwordrepresentationsinvectorspacearxiv1301', 'tomaso', 'tomczak', 'tomixeddomain', 'tomize', 'tomography', 'tomoko', 'tompson', 'ton', 'tong', 'toni', 'toniou', 'tony', 'too', 'took', 'tool', 'toolbox', 'toolbox15', 'toolkit', 'tools', 'toolsopenftlist', 'tooltoidentifygeneanddiseasementionsandcreatethepositiveexamplesfromtheannotatedsentencesinthe', 'top', 'top1', 'top1top5', 'top5', 'top50', 'topathology', 'topattended', 'topf', 'topic', 'topics', 'topk', 'topleft', 'toplevel', 'topol', 'topological', 'topology', 'topped', 'topper', 'topping', 'topright', 'tor', 'toraks', 'torch', 'tored', 'toronto', 'torralba', 'tors', 'torsion', 'torsten', 'tortions', 'tory', 'toshev', 'toshihisa', 'total', 'totally', 'tothecorrespondingtokenrepresentationsfinallyfollowingpriorworkwealsoconsidersimplyconcatenating', 'totype', 'touch', 'toujours', 'tours', 'tous', 'tout', 'toutanova', 'touvron', 'toward', 'towards', 'toy', 'toyish', 'tp', 'tpami', 'tpk', 'tpkauthor', 'tpu', 'tr', 'tr47', 'tra', 'trace', 'track', 'tracking', 'tracks', 'tract', 'tractable', 'tracted', 'tracting', 'traction', 'trade', 'tradeoff', 'tradeoffs', 'tradi', 'trading', 'tradition', 'traditional', 'traditionally', 'traffic', 'trafﬁc', 'train', 'trainable', 'traindevtest', 'trained', 'trainees', 'training', 'trainingare', 'traininghttpswwwcsubccaamuham01ling530papersradford2018improvingpdf', 'trainingset', 'trainingvalidation', 'trains', 'traintest', 'trainval', 'trainvalida', 'trainvalidatetest', 'trainvalidation', 'trainvalidationtest', 'trajectories', 'trajectory', 'tran', 'trans', 'transactions', 'transcribe', 'transcription', 'transcriptomics', 'transduc', 'transduction', 'transductive', 'transfer', 'transfer6970', 'transferable', 'transferlearning', 'transferlearningfromunlabeledtextthisstartswithsimplestandalonewordembeddings', 'transfermedium', 'transferred', 'transferring', 'transfers', 'transfo', 'transfor', 'transform', 'transforma', 'transformation', 'transformations', 'transformative', 'transformed', 'transformer', 'transformer126', 'transformerbased', 'transformerjournal', 'transformers', 'transformersarxiv', 'transformerstyle', 'transformert', 'transformerxl', 'transforming', 'transforminput', 'transforms', 'transforms40', 'transhci', 'transi', 'transition', 'transitional', 'transitioning', 'transitions', 'transl', 'transla', 'translatable', 'translate', 'translated', 'translates', 'translating', 'translation', 'translation1617', 'translation177276', 'translation1875', 'translational', 'translations', 'translationsrepresenting', 'transmembrane', 'transmil', 'transmission', 'transparency', 'transplant', 'transport', 'transporter', 'transposing', 'trapped', 'trav', 'travailler', 'travel', 'traveling', 'travelling', 'travels', 'traversal', 'traverse', 'treat', 'treatable', 'treated', 'treating', 'treatment', 'treatment22', 'tree', 'treebank', 'trees', 'tremely', 'tremendous', 'trend', 'trends', 'trevor', 'triaging', 'trial', 'trials', 'triangle', 'triangles', 'triangular', 'tributes', 'tribution', 'tributions', 'tric', 'trichelair', 'trichrome', 'trick', 'tricks', 'tried', 'tries', 'trieval', 'trigeminy', 'trinh', 'triple', 'triples', 'triplet', 'trischler', 'tristan', 'trivia', 'trivial', 'trivialized', 'trivially', 'trix', 'troduce', 'troduced', 'troester', 'trol', 'trolled', 'tron', 'trouble', 'trp', 'trp156', 'trp298', 'trrosetta', 'true', 'truepositive', 'truly', 'trump', 'trumps', 'truncated', 'truncation', 'trunk', 'trust', 'truth', 'try', 'trying', 'ts', 'tsatsaronis', 'tschannen', 'tsinghua', 'tsne', 'tsoi', 'tsp', 'tsujii', 'tsuneki', 'tsuruoka', 'tt', 'ttest', 'tu', 'tual', 'tualizedwordrepresentationsin', 'tube', 'tuberculosis', 'tuberk', 'tubular', 'tubulopapillary', 'tubulovillous', 'tude', 'tudes', 'tuesday', 'tumor', 'tumor68', 'tumorassociated', 'tumorcontaining', 'tumorimmune', 'tumorinfiltrating', 'tumors', 'tumour', 'tumournormal', 'tumours', 'tun', 'tune', 'tuned', 'tuning', 'tunities', 'tunyasuvunakool', 'tunyasuvunakool14', 'tunyasuvunakoolet', 'tuples', 'turaga', 'turakhia', 'ture', 'tures', 'turin100', 'turing', 'turk', 'turker', 'turkers', 'turn', 'turned', 'turning', 'turns', 'turtleneck', 'tutional', 'tutorial', 'tv', 'tvj', 'tvjoi', 'tw', 'tweed', 'tweedie', 'tween', 'twelve', 'twentythree', 'twentytwo', 'twice', 'twitter', 'two', 'twoclass', 'twodimensional', 'twofold', 'twogpu', 'twolayer', 'twoparadigmsforneurallanguagemodelpretrainingtoptheprevailingmixeddomainparadigmassumesthatout', 'twosided', 'twostage', 'twotrack', 'twoway', 'twowell', 'tx', 'ty', 'ty1has', 'ty1is', 'tyers', 'tying', 'type', 'types', 'types35', 'typi', 'typical', 'typically', 'typing', 'typology', 'tyr189', 'tzu', 'täckström', 'tʹ', 'u', 'u0', 'u1', 'ua', 'uahj', 'uahkð', 'ually', 'uating', 'uation', 'ubiquitous', 'uboat', 'ubuntu', 'ucec', 'ucf101', 'uchibe', 'ucs', 'ud', 'udit', 'uec', 'ufar', 'ui', 'uignore', 'uj', 'uk', 'uk9program', 'ukbased', 'ukk', 'ukkwnstcga', 'ular', 'ulations', 'ulceration', 'ule', 'ules', 'ulla', 'ullman', 'ulti', 'ultimate', 'ultimately', 'ultradeep', 'um', 'umap', 'ument', 'uments', 'umichedutmalign', 'umm', 'umulticlass', 'un', 'unable', 'unambiguously', 'unannotated', 'unavailable', 'unaware', 'uncased', 'uncer', 'uncertain', 'uncertainly', 'uncertainties', 'uncertainty', 'unchanged', 'unchangedand', 'uncharacterized', 'uncle', 'unclear', 'unclipped', 'unconstrained', 'uncovers', 'uncurated', 'undeniably', 'under', 'underdiscussed', 'underexplored', 'underfit', 'undergo', 'undergoes', 'undergoing', 'underline', 'underlying', 'underperform', 'underperforming', 'underperforms', 'underpinning', 'underpins', 'underrepresented', 'underscored', 'underspecified', 'underspeciﬁcation', 'understand', 'understanding', 'understood', 'understudy', 'undertaking', 'underwater', 'underwent', 'underﬁt', 'underﬁts', 'underﬁtting', 'undesirable', 'undetected', 'undoubtedly', 'undue', 'une', 'unesco', 'unexpectedly', 'unexplored', 'unfairly', 'unfamiliar', 'unfil', 'unfilled', 'unfiltered', 'unfold', 'unfolded', 'unfolding', 'ung', 'uni', 'uni0394uni0394', 'uni0394uni0394z', 'uni2190', 'uniclust', 'uniclust2018_08', 'uniclust30', 'uniclust3036', 'unicode', 'unicodervl', 'unicorn', 'unicorns', 'unidirectional', 'unified', 'uniform', 'uniformly', 'unifying', 'unigram', 'unigrams', 'unimodal', 'unimportant', 'uninstalling', 'unintelligible', 'unintentional', 'union', 'uniperceiver', 'uniprot', 'unique', 'uniref', 'uniref50', 'uniref90', 'uniref9067', 'unis', 'unit', 'united', 'uniter', 'unitj', 'unitopatho', 'units', 'uniuniuni', 'univ', 'univen1ity', 'universal', 'university', 'university99', 'université', 'uniﬁed', 'unk', 'unknown', 'unknownambiguous', 'unlabeled', 'unlabelled', 'unless', 'unlike', 'unlikely', 'unlimited', 'unlisted', 'unlock', 'unlocks', 'unnecessarily', 'unnecessary', 'unnikrishnan', 'unnormalized', 'unpaired', 'unpro', 'unprocessed', 'unpublished', 'unrealistic', 'unreasonable', 'unreferenced', 'unreliable', 'unremarkable', 'unrepresented', 'unrolled', 'unsalted', 'unsatisfactory', 'unseen', 'unskilled', 'unsolved', 'unsorted', 'unspecified', 'unsu', 'unsuccessful', 'unsuitable', 'unsuper', 'unsupervised', 'unsure', 'unsurprising', 'untenable', 'untersuchungen', 'unterthiner', 'until', 'untying', 'unusual', 'unveiled', 'unweighted', 'uones', 'uous', 'up', 'upchurch', 'upcoming', 'update', 'updated', 'updates', 'updating', 'upjohn', 'uploading', 'upon', 'upper', 'upsampled', 'upscal', 'upscale', 'upvoted', 'upwards', 'upwork', 'urakhia45', 'ural', 'ured', 'urgent', 'urinary', 'url', 'urological', 'urothelial', 'urtasun', 'urxvt', 'urxvtbackgroundbackground', 'urxvtfontxftyour', 'us', 'usa', 'usa1066', 'usa108', 'usa10department', 'usa117', 'usa118', 'usa11department', 'usa12molecular', 'usa13department', 'usa21department', 'usa23howard', 'usa2institute', 'usa2new', 'usa3faculty', 'usa3stanford', 'usa4john', 'usa5eugene', 'usa6department', 'usa7howard', 'usa8department', 'usability', 'usage', 'usavol', 'usbased', 'use', 'useable', 'useanexistingpreprocessedversionofgad', 'used', 'used52', 'useful', 'usefulness', 'useless', 'uselftrained', 'usenixsymposium', 'user', 'username', 'users', 'userspecified', 'userspeciﬁed', 'uses', 'using', 'usingbytepair', 'usled', 'usmexico', 'usual', 'usually', 'usunier', 'usuyama', 'uszgooglecom', 'uszkoreit', 'ut', 'utc', 'ute', 'uted', 'uterine', 'utes', 'utf8', 'utility', 'utilize', 'utilized', 'utilizing', 'utrecht', 'uveal', 'uvm', 'uvw', 'uw', 'uyo', 'uzeroes', 'uzeros', 'v', 'v0', 'v004', 'v0111', 'v0122', 'v0169', 'v06', 'v080', 'v092', 'v1', 'v100', 'v11', 'v115', 'v1164', 'v120', 'v121', 'v125', 'v140', 'v190', 'v1p161162', 'v1w195006', 'v2', 'v200', 'v2018_08', 'v2018_12', 'v20190822', 'v2020_01', 'v230', 'v3', 'v30beta3', 'v33', 'v334', 'v36', 'v371', 'v38', 'v3813', 'v431', 'v5', 'v731', 'v73169', 'v8', 'v925af', 'v930', 'va', 'vague', 'vahj', 'vahkð', 'vaidya12348', 'vaillant', 'val', 'valencia', 'valenciaproteins', 'vali', 'valid', 'validate', 'validated', 'validating', 'validation', 'vallentync', 'valley', 'vallonpontdarc', 'valod', 'valuable', 'value', 'valued', 'values', 'van', 'vances', 'vancouver', 'vanderplas', 'vanguri', 'vanhoucke', 'vanish', 'vanish7778', 'vanishing', 'vanishingexploding', 'vanishinggradient', 'vanni', 'vannier', 'vantage', 'vantagesovermixeddomainpretrainingbeitcontinualpretrainingofgeneraldomainlanguagemodelsorpre', 'varadarajan', 'varadiet', 'vari', 'variabil', 'variability', 'variable', 'variablelength', 'variableresolution', 'variables', 'variance', 'variances', 'variant', 'variants', 'variation', 'variational', 'variations', 'varied', 'varies', 'variety', 'various', 'variously', 'varoquaux', 'varun', 'vary', 'varying', 'vasculature', 'vasilache', 'vast', 'vastly', 'vasudevan', 'vaswani', 'vaswaniet', 'vatanian', 'vation', 'vations', 'vbow149150', 'vd142m', 'vd142m22', 'vec', 'vector', 'vectors', 'vectors101', 'vectorskiros', 'vedaldi', 'vedantam', 'veeling', 'vegetables', 'vehicles', 'velcheti', 'velocity', 'velopment', 'ven', 'veness', 'vent', 'ventilation', 'ventional', 'ventral', 'ventric', 'ventricu', 'ventricular', 'ventured', 'ventures', 'venugopalan', 'verbatim', 'verbeek', 'verbs', 'verging', 'verify', 'veriﬁcation', 'veriﬁed', 'veriﬁes', 'verkuil1', 'vernet', 'versa', 'versal', 'versatile', 'versatility', 'version', 'versions', 'versus', 'vert', 'vertical', 'verticals', 'very', 'ves', 'veselin', 'vessel', 'vessels', 'vestigate', 'veteran', 'vf', 'vg', 'vgg', 'vgg16', 'vgg1619', 'vgg19', 'vggnet', 'vgrg', 'vi', 'vi0if', 'vi1', 'via', 'viable', 'viation', 'vice', 'victoria', 'victory', 'vid', 'vide', 'vided', 'video', 'videos', 'vider', 'vides', 'vienna89', 'view', 'viewed', 'viewing', 'viewrequest', 'views', 'vig', 'vignetting', 'vijay', 'vikram', 'vilbert', 'village', 'villain', 'vincent', 'vinyals', 'vinyals1', 'viola', 'violate', 'violation', 'violations', 'viously', 'virchow', 'virginia', 'virion', 'virtanen', 'virtex', 'virus', 'viruses', 'vis', 'vised', 'visible', 'visibly', 'visiolinguistic', 'vision', 'vision2275', 'vision2829', 'vision3153', 'visionandlanguage', 'visioneccv', 'visionlanguage', 'visiononly', 'visionspecific', 'visit', 'visitable', 'visited', 'visual', 'visualcentric', 'visualization', 'visualizations', 'visualize', 'visualized', 'visualizes', 'visualizing', 'visuallanguage', 'visually', 'vit', 'vit73', 'vitadapter', 'vitadatper', 'vitavg', 'vitb', 'vitb16', 'vitb32', 'vitbase', 'vitbased', 'vitgiant', 'vitl', 'vitl14', 'vitl14336px', 'vitl16', 'vitl75', 'vitlarge', 'vits', 'vitvjtaverage', 'vival', 'vj', 'vj2vj1', 'vjtavg', 'vlad', 'vlfeat', 'vn', 'vnet', 'vnets', 'voc', 'vocab', 'vocabu', 'vocabularies', 'vocabulary', 'vocabularyfrom', 'vocabularyinsteadofusingaconstantmaskingrateof15astandardapproachistograduallyincreaseitfrom', 'void', 'vol', 'voltage', 'volts', 'volume', 'volumetric', 'voluminous', 'volutional', 'vorontsov', 'vortex', 'vos', 'vote', 'voting', 'vqa', 'vs', 'vshren', 'vt', 'vtab', 'vtventricular', 'vu', 'vulnerabilities', 'vv', 'vw', 'vxiangz', 'w', 'w007', 'w1', 'w10', 'w14', 'w18', 'w1x', 'w1zk', 'w2', 'w200w204', 'w26', 'w2ð', 'w2σw1x', 'w38', 'w408w413', 'w50', 'w_i', 'w_id_i', 'w_t', 'w_td_t', 'wa', 'wa1', 'wagner', 'wai', 'waibel', 'wait', 'waiting', 'waived', 'wakesleep', 'wall', 'wallace', 'wallach', 'wallner', 'wallnerplos', 'walls', 'walt', 'walter', 'wan', 'wandering', 'wang', 'wang1', 'wang12', 'wanget', 'wannier', 'want', 'wanted', 'wants', 'war', 'ward', 'wardefarley', 'wards', 'ware', 'warm', 'warmed', 'warmup', 'warmup_steps', 'warmup_stepstraining', 'warping', 'warranty', 'wars', 'wart', 'was', 'wash', 'washington', 'wasnt', 'wasteful', 'watching', 'watchvfmym_bkwqzk', 'water', 'waterhouseet', 'wave', 'waveform', 'wavelengths', 'wavelet', 'way', 'way2', 'way44', 'wayne', 'ways', 'wc1', 'wci', 'wcihslidei', 'wck', 'wckfk', 'wcn', 'wdr', 'we', 'weak', 'weaker', 'weakly', 'weaklysupervised', 'weaksupervised', 'wealsoinvestigatethetaggingschemeusedinnerthestandardtaggingschemedistinguisheswordsbytheir', 'wealthy', 'wear', 'wearing', 'web', 'webbased', 'webber', 'weber', 'webimagetext', 'webly', 'weblysupervised', 'webpages', 'webscale', 'webserver', 'website', 'websites', 'webtext', 'weckesser', 'weconducthyperparametersearchusingthedevelopmentsetbasedontaskspecificmetricssimilartoprevious', 'wed', 'wednesday', 'week', 'weeks', 'wefocusonhyperparametertuningusingasubsetofrepresentativemodelssuchasbertandbiobertanduse', 'wefollowthestandardpretrainingprocedurebasedonthetensorflowimplementationreleasedbynvidia', 'wehmiller', 'wei', 'weiet', 'weighing', 'weighs', 'weight', 'weightdecay', 'weighted', 'weighting', 'weights', 'weightsd', 'weightspace', 'weigt', 'weihung', 'weikl', 'weinberg', 'weinberger', 'weinstein', 'weishaupt12348', 'weiss', 'weissenborn', 'weissenow', 'weizhu', 'welch', 'welcome', 'welcoming', 'well', 'well43139142', 'wellaligned', 'wellcalibrated', 'wellcome', 'wellcurated', 'welldefined', 'wellens', 'wellestablished', 'welling', 'wellknown', 'wellposed', 'wellpredicted', 'wellrecognized', 'wellsuited', 'wenckebach', 'weng', 'went', 'wenting', 'werbos', 'were', 'weshoulddistinguishdifferenttypesoftransferlearningandseparatelyassesstheirutilityinvarioussituations', 'west', 'western', 'weston', 'wexner', 'weyand', 'wgm2', 'whale', 'what', 'whatever', 'wheelwright', 'when', 'whenever', 'where', 'whereas', 'whereby', 'wherein', 'whether', 'whetheronesentencefollowstheotherintheoriginaltexttheutilityofnsphasbeencalledintoquestion', 'which', 'whichdeterminesforagivensentencepair', 'while', 'whilst', 'whipped', 'whiskers', 'white', 'whites', 'who', 'whole', 'wholesight90', 'wholeslide', 'wholeslidelevel', 'whom', 'whose', 'why', 'wi', 'wi1', 'wich', 'wickyet', 'wide', 'widely', 'widelyused', 'widening', 'wider', 'widerange', 'wideresnet', 'widescreen', 'widespread', 'widner', 'width', 'widthdepthresolution', 'wiebe', 'wiener', 'wierstra', 'wiesel', 'wieser', 'wieting', 'wife', 'wii', 'wij', 'wiki', 'wikipedia', 'wikitext103', 'wikitext2', 'wilamowski', 'wilber', 'wilbur', 'wild', 'wildlife', 'wilds', 'wilds164', 'wildtype', 'wiley', 'will', 'willets', 'william', 'williams', 'williams12345', 'williams16', 'williamson', 'willing', 'willis', 'willows', 'willwacher', 'wilson', 'win', 'window', 'windows', 'winkens', 'winn', 'winner', 'winning', 'winograd', 'wins', 'winsti', 'winstiehb', 'winstiehkbþb', 'winstihk', 'winter', 'wiping', 'wire', 'wisconsinmadison', 'wise', 'wiseman', 'wish', 'wishes', 'wit', 'witbraad', 'with', 'within', 'withinanentityispotentiallyadvantageouscomparedtotheminimalioschemethatonlydistinguishesbetween', 'withl', 'without', 'withstandardpretrainingtable', 'withtheadventofbertmodelsandtheselfattentionmechanismtheutilityofexplicitsequentialmodeling', 'withthetotalpretrainingtextincreasedsubstantiallyto168billionwords107gbsurprisingly', 'witnessed', 'wizard', 'wji', 'wjk', 'wk', 'wkl', 'wl', 'wlodawer', 'wmt', 'wmt14', 'wns', 'wntnonsonic', 'wo', 'wojna', 'wolf', 'wolfgang', 'wolflike', 'wolfsberg', 'wolves', 'woman', 'women', 'womens', 'won', 'wonderful', 'wonderfully', 'wong', 'wont', 'woo', 'wook', 'wool', 'woolly', 'word', 'wordbyword', 'wordclouds', 'wordlevel', 'wordnet', 'wordpiece', 'words', 'words16', 'words21', 'words37', 'words4748', 'words71', 'wore', 'work', 'work115116', 'work30', 'work38', 'work56', 'work90', 'workable', 'workdata', 'worked', 'workflow', 'workflows', 'working', 'works', 'works25', 'works536', 'workshop', 'workshop7278', 'workshops', 'workstation', 'workstationgrade', 'workstations', 'world', 'worldleading', 'worlds', 'worldwide', 'worrall', 'worry', 'worryingly', 'worse', 'worst', 'worstperforming', 'worth', 'worx', 'would', 'writ', 'write', 'writes', 'writeup', 'writing', 'written', 'wrote', 'ws', 'wsi', 'wsi35', 'wsis', 'wsisa', 'wsisa145', 'wsitowsi', 'wsss4luad', 'wsss4luad85', 'wsx', 'wt', 'wu', 'wuet', 'wulan', 'wulczyn', 'wv', 'ww', 'wwas', 'wwm', 'wwmenforces', 'wwpdb', 'www', 'wwwaaaiorg', 'wwwbracsicarcnrit', 'wwwgithubcommahmoodlabuni', 'wwwimage', 'wwwnaturecom', 'wwwnaturecomreprints', 'wwwscipyorg', 'wy', 'wüthrich', 'x', 'x0', 'x1', 'x1x', 'x2', 'x3', 'x4', 'xa', 'xaany', 'xaxis', 'xb', 'xc', 'xception', 'xeon', 'xft', 'xhiwici', 'xi', 'xia', 'xiang', 'xiangyu', 'xiaodan', 'xiaodl', 'xiaodong', 'xiaosong', 'xie', 'xiii', 'xing', 'xinghua', 'xiong', 'xiv', 'xiwi', 'xizi', 'xj', 'xk', 'xkl', 'xla', 'xmonad', 'xmonadurxvt', 'xn', 'xon', 'xplore', 'xproﬁle', 'xray', 'xrays', 'xresources', 'xrˆhirˆwiwˆci', 'xsessionerrors', 'xsessionrc', 'xt', 'xt1x', 'xtxt1', 'xtʹ', 'xu', 'xue', 'xuguang', 'xuplos', 'xx', 'xxx', 'xy', 'xy2', 'xyib', 'xyig', 'xyt', 'xℓ', 'y', 'y1', 'y1y', 'ya', 'yacht', 'yad', 'yada', 'yan', 'yang', 'yang1', 'yaniv', 'yann', 'yannakoudakis', 'yanncsnyuedu', 'yanofsky', 'yanqi', 'yao', 'yarats', 'yarowsky', 'yaxis', 'yb', 'yc', 'ye', 'year', 'years', 'years9', 'yeates', 'yellen', 'yellow', 'yelong', 'yemen', 'yemeni', 'yes', 'yesmaybeno', 'yesno', 'yet', 'yeung', 'yeyi', 'yfcc100m', 'yi', 'yichong', 'yield', 'yielded', 'yielding', 'yields', 'yifan', 'yijia', 'yim', 'yin', 'ying', 'yinhan', 'yip19', 'yj', 'yjcdjc2', 'yk', 'yl', 'ylog', 'ymoma', 'ymphatic', 'yn', 'ynfyg', 'yo', 'yogatama', 'yolk', 'yolks', 'yolov571', 'yonezawa', 'yonghui', 'yoo', 'yoon', 'york', 'york11', 'york20', 'york28', 'yoshimasa', 'yoshua', 'yosinski', 'yottixel', 'yottixelan', 'you', 'youlong', 'young', 'youngest', 'your', 'youre', 'yours', 'ypes', 'ys', 'ysis', 'yt', 'yu', 'yu1', 'yuan', 'yue', 'yueping', 'yufan', 'yuguroberttinnhaochengmichaellucasnaotousuyamaxiaodongliutristannaumannjianfenggaoandhoifung', 'yuille', 'yuka', 'yuksekgonul', 'yukun', 'yung', 'yuqi', 'yuqing', 'yw', 'yx', 'yy', 'yyb', 'y¼', 'y¼f', 'yþ¼', 'z', 'z17z', 'z1z', 'zadrozny', 'zagoruyko', 'zak', 'zamir', 'zbigniew', 'zeiler', 'zemel', 'zeming', 'zemla', 'zen', 'zeng', 'zenodo', 'zenodoorgrecord5889558', 'zero', 'zerodata', 'zeromean', 'zeropadded', 'zeropadding', 'zeroshot', 'zeroshots', 'zesch', 'zettlemoyer', 'zhai', 'zhang', 'zhangcurr', 'zhangserver', 'zhao', 'zhavoronkov', 'zheng', 'zhengping', 'zhifeng', 'zhihao', 'zhiyong', 'zhmoginov', 'zhong', 'zhongkai', 'zhou', 'zhouhan', 'zhu', 'zhu1', 'zhuang', 'zhukov', 'zielinski1', 'zihlmann', 'zilles', 'zimmet', 'zinc', 'zincbinding', 'zio', 'zip', 'zis', 'zisser', 'zisserman', 'zitnick', 'zj', 'zk', 'zl', 'zn', 'zong', 'zons', 'zoomedin', 'zoomin', 'zoph', 'zou', 'zscores_finalcgi', 'zu', 'zuboff', 'zumberge', 'zx', 'zz', 'zzcentered', 'µm', '¼', '¼c0', '¼ðehk', '½2', 'à', 'àlexbravojanetpiñeronúriaqueraltrosinachmichaelrautschkaandlauraifurlong2015extractionofrelationsbetweengenes', 'á', 'å', 'çaglar', 'ð1þ', 'ð2þ', 'ð3þ', 'ð4þ', 'ð5þ', 'ð6þ', 'ð7þ', 'ðeh1', 'ðehk', 'ðhk', 'ðw1zk', 'ððh1', 'özgür', 'öztürk', 'þ', 'þb2', 'þ¼', 'þð', 'þðþ', 'þþ', 'ł', 'łukasz', 'šali', 'ž', 'žídek', 'žídek14', 'ˆci', 'ˆfdˆli', 'ˆfi', 'ˆfiˆli', 'ˆhi', 'ˆhiˆwi', 'ˆli', 'ˆwi', 'α', 'α1ðjyþþ', 'α1β', 'αi', 'αn', 'αβ2', 'αβγ', 'αφ', 'β', 'β1', 'β2', 'βare', 'βn', 'βparameters', 'βφ', 'γ2', 'γ2φ', 'γn', 'γφ', 'δ', 'δx', 'δy', 'δz', 'ε', 'θ', 'θcaption', 'θcontrast', 'θm', 'θrot', 'κ', 'λ', 'λi', 'ξ', 'σ', 'σj', 'σy', 'τ', 'τlog', 'φ', 'φin', 'φis', 'φusing', 'χ', 'ψ', 'ϕ', 'ϵ', 'ϵis', 'ϵls', 'ϵwi', 'ಯhorseರ', 'ℒ', 'ℒclmξ', 'ℓ', 'ℓ2', 'ℓ2normalization', 'ℓ2normalized', 'ℓth', 'ﬁ', 'ﬁcation', 'ﬁcientnet', 'ﬁction', 'ﬁddle', 'ﬁeld', 'ﬁelds', 'ﬁer', 'ﬁers', 'ﬁes', 'ﬁfth', 'ﬁftyfour', 'ﬁght', 'ﬁgure', 'ﬁl', 'ﬁle', 'ﬁlenames', 'ﬁles', 'ﬁll', 'ﬁlls', 'ﬁlm', 'ﬁlms', 'ﬁlter', 'ﬁltering', 'ﬁlters', 'ﬁnal', 'ﬁnally', 'ﬁnd', 'ﬁnder', 'ﬁnding', 'ﬁndings', 'ﬁne', 'ﬁnegrained', 'ﬁnely', 'ﬁner', 'ﬁnetune', 'ﬁnetuned', 'ﬁnetuning', 'ﬁnished', 'ﬁrm', 'ﬁrmly', 'ﬁrst', 'ﬁt', 'ﬁts', 'ﬁtted', 'ﬁtting', 'ﬁve', 'ﬁx', 'ﬁxed', 'ﬁxedlength', 'ﬁxing', 'ﬂame', 'ﬂashy', 'ﬂeeing', 'ﬂexible', 'ﬂip', 'ﬂipping', 'ﬂoating', 'ﬂoatingpoint', 'ﬂops', 'ﬂour', 'ﬂow', 'ﬂower', 'ﬂuid']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnOBq2D0FKIC",
        "outputId": "c34fd504-0ef2-4c58-b323-25b6049d685f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                0  00  000  0000  00000  00001  00005  0001  0003  0004  ...  \\\n",
            "Paper Name                                                               ...   \n",
            "neural-nets    23   0    0     3      1      0      0     0     0     0  ...   \n",
            "backprop       12   2    0     0      0      0      0     0     0     0  ...   \n",
            "cnn             2   0    0     0      0      0      2     0     0     0  ...   \n",
            "deep-learning   2   0    0     0      0      0      0     0     0     0  ...   \n",
            "resnet          8   0    0     0      0      2      0     0     0     0  ...   \n",
            "attention       2   2    0     0      0      0      0     0     0     0  ...   \n",
            "chexnet         1   0    0     0      0      0      0     1     0     0  ...   \n",
            "densenet        9   0    1     0      0      0      0     0     0     0  ...   \n",
            "ecg             2  14    0     0      0      0      0     0     0     0  ...   \n",
            "gpt-2           0   0    0     0      0      0      0     0     0     0  ...   \n",
            "\n",
            "               ﬂexible  ﬂip  ﬂipping  ﬂoating  ﬂoatingpoint  ﬂops  ﬂour  ﬂow  \\\n",
            "Paper Name                                                                     \n",
            "neural-nets          0    0        0        0             0     0     0    0   \n",
            "backprop             0    0        0        0             0     0     0    0   \n",
            "cnn                  0    0        0        0             0     0     0    0   \n",
            "deep-learning        0    0        0        0             0     0     0    0   \n",
            "resnet               1    2        0        0             0     0     0    0   \n",
            "attention            0    0        0        1             1     0     0    2   \n",
            "chexnet              0    0        1        0             0     0     0    1   \n",
            "densenet             0    0        0        0             0     0     0    8   \n",
            "ecg                  0    0        0        0             0     0     0    0   \n",
            "gpt-2                2    0        0        0             0     0     1    0   \n",
            "\n",
            "               ﬂower  ﬂuid  \n",
            "Paper Name                  \n",
            "neural-nets        0     0  \n",
            "backprop           0     0  \n",
            "cnn                0     0  \n",
            "deep-learning      0     0  \n",
            "resnet             0     0  \n",
            "attention          0     0  \n",
            "chexnet            0     1  \n",
            "densenet           0     0  \n",
            "ecg                0     0  \n",
            "gpt-2              0     0  \n",
            "\n",
            "[10 rows x 20276 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the arranged word count chart in csv\n",
        "#df.to_csv('hw1/word_counts.csv', index=True)"
      ],
      "metadata": {
        "id": "2xH8PzktJNLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TfIdf()"
      ],
      "metadata": {
        "id": "Nnjo4uIGKk5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}