{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMSC 35440 Machine Learning in Biology and Medicine\n",
        "## Homework 2: Molecular Subtyping of Lung Cancer\n",
        "**Released**: Jan 28, 2025\n",
        "\n",
        "**Due**: Feb 7, 2025 at 11:59 PM Chicago Time on Gradescope\n",
        "\n",
        "**In the second homework, you'll explore embeddings of genomic expression data for two related cancer types.**\n",
        "\n",
        "Lung adenocarcinoma and lung squamous cell carcinoma are the 2 most prevalent non-small cell lung cancer (NSCLC) types. They are related but distinct cancer types. Lung squamous cell carcinoma is the most common tumor in male smokers and occurs more centrally in the lung (closer to the root of the lung, nearer to the bronchi). Lung adenocarcinoma is the most common tumor in nonsmokers and female smokers and occurs more peripherally in the lung (closer to the sides of the lung, further from the bronchi). Hopefully this diagram of lung anatomy helps clarify the locations mentioned: https://en.wikipedia.org/wiki/Lung#/media/File:Illu_bronchi_lungs.jpg.\n",
        "\n",
        "However, there are often exceptions to these epidemiological and anatomical patterns. This is why obtaining a histological (tissue slides) or molecular (e.g. gene expression) profile of a patient's specific cancer is vital for accurate diagnosis and subsequent treatment. But even molecular patterns of cancer can be heterogeneous. In this homework, you'll explore some of that heterogeneity and observe how models can get things wrong.\n",
        "\n",
        "Last, you'll practice a vital step for biomedical machine learning: expert review. Before these models can ever be deployed in real patient settings, they must undergo rigorous review. In the US, any medical products intended for patient usage must be approved by the FDA. An excellent historical case of demonstrating why we need such review is Thalidomide in the late 1950s. It was originally marketed in Europe as a treatment for morning sickness, especially during pregnancy. However, the drug was blocked in the US by an expert reviewer at the FDA, [Dr. Frances Kelsey](https://en.wikipedia.org/wiki/Frances_Oldham_Kelsey) (a UChicago MD/PhD alum!), who was concerned over the lack of evidence concerning the drug's safety. She was of course right to be concerned, as Thalidomide was shown to cause severe birth defects, leading to its removal from European markets. Suffice to say, expert review is crucial to patient safety, especially as we dive into this new age of AI/ML in medicine.\n",
        "\n",
        "The starter notebook for this homework can be downloaded from GitHub:\n",
        "\n",
        "https://github.com/StevenSong/CMSC-35440-Source/blob/main/hw2/CMSC_35440_HW2_Student_Version.ipynb\n"
      ],
      "metadata": {
        "id": "K-gV2pmJcU79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "vfHBZQl0rauN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Download and open the starter notebook. No need for any GPUs for this homework.\n",
        "1. Download and unzip the data. We've provided gene expression data spanning 2 TCGA projects: `TCGA-LUAD` (lung adenocarcinoma) and `TCGA-LUSC` (lung squamous cell carcinoma). For simplicity, we'll use the project ID to distinguish the cancer type.\n",
        "  * We've provided the data as a tarball that be downloaded from [https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw2/hw2.tar.gz](https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw2/hw2.tar.gz).\n",
        "  * After unzipping the data, there should be a CSV of metadata, a folder of expression TSV files, and the code that was used to originally download the data. You don't need the download code but it can be a good template if you want to pull other data from the NCI GDC.\n",
        "1. Using the expression data, derive one expression vector per unique patient (given by the `case_id`). We'll treat this as our patient embedding.\n",
        "  * **Only use the `protein_coding` genes within each expression file.**\n",
        "  * **Only use the count columns which contain the string `unstranded`.**\n",
        "  * One challenge of working with real biomedical data is that each patient may contribute a variable number of samples, for example if multiple biopsies are taken. For this homework, we're looking for one patient embedding aggregated from all of the patient's samples. **The exact aggregation method is up to you.**\n",
        "  * Don't forget to normalize the counts. You can refer to the slides from lecture on popular normalization methods. The normalizations that are precomputed by the GDC are also documented [here](https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/#mrna-expression-transformation). **The exact normalization method is up to you.**\n",
        "  * Include in your writeup a brief description and justification of the methods you used for normalization & aggregation and in what order you applied them.\n",
        "  * Tips:\n",
        "    * Beware the extra rows present in each count file e.g. `N_unmapped`, `N_multimapping`, etc.\n",
        "    * Gene expression counts are naturally stored as a matrix where the columns are genes, the rows are individual samples, and the matrix values are the counts. In python, `scanpy` and `anndata` are packages used to handle and transform such data. You do not have to use either, but these may be useful.\n",
        "1. Cluster your embeddings to 2 clusters. We're trying to derive a model which can distinguish the different lung cancer types. Simple `KMeans` from `sklearn` is fine for this. Derive cluster IDs for each sample.\n",
        "1. Use PCA to reduce your embeddings to 2 dimensions.\n",
        "1. Visualize your embeddings. We're looking for a scatter plot where the color of the points differ by the TCGA project (`LUAD` vs `LUSC`) and the shape of the points differ by the cluster ID assigned by clustering.\n",
        "1. Consider these questions (you should probably address some of these in your writeup):\n",
        "  * In the 2D projection of the embeddings, is there a natural decision boundary you would be able to draw to classify the different cancer types?\n",
        "  * Are there samples which are misclassified by this decision boundary?\n",
        "  * How close is clustering to this decision boundary?\n",
        "  * Are there samples which are misclassified by clustering?\n",
        "  * Are misclassifications by the model (either clustering or the imagined decision boundary) actually mislabeled data? For example, if a sample is labeled as lung adenocarcinoma but the model thinks it's lung squamous cell carcinoma, is the model right or is the label right?\n",
        "1. Review the misclassifications:\n",
        "  * We'll use another data modality to double check if the samples are labeled correctly. The imaging data for select TCGA lung cancer cases are available The Cancer Imaging Archive (TCIA).\n",
        "  * Lung cancer patients often undergo computed tomography (CT) scans to identify the tumor. Additionally, these CTs are done in parallel to positron emission tomography (PET) scans. PET scans work by introducing a radioactive tracer that is taken up by metabolically active tissues, such as tumors. As a result, **tumors light up brighter white on PET scans**.\n",
        "  * CT images are slices through the body going from head to toe. To understand the way each image is oriented, imagine you're looking through the feet of a person lying facing up on a table. This means that in each image, the top of the image is the patient's front, the bottom is their back, and the left of the image is the patient's right and vice versa. The images start at the patient's head so as you scroll through the images, you're looking further down the patient's body.\n",
        "    * It's recommended to use the up/down arrow keys to scroll through the images.\n",
        "  * For this exercise, we'll provide two cases which were misclassified by our implementation. You're welcome to check cases from your implementation, however not all cases have paired imaging data available ([LUAD cases](https://nbia.cancerimagingarchive.net/nbia-search/?CollectionCriteria=TCGA-LUAD), [LUSC cases](https://nbia.cancerimagingarchive.net/nbia-search/?CollectionCriteria=TCGA-LUSC)).\n",
        "    * `TCGA-60-2715`: labeled as `LUSC` but classified by our model as `LUAD`.\n",
        "      * Scroll through images 109 through 113. Look for the bright white spot in these images on the PET scan.\n",
        "      * [CT scan](https://nbia.cancerimagingarchive.net/viewer/?study=1.3.6.1.4.1.14519.5.2.1.3023.4012.507148485748821590204034796320&series=1.3.6.1.4.1.14519.5.2.1.3023.4012.313155987490130625808038798781)\n",
        "      * [PET scan](https://nbia.cancerimagingarchive.net/viewer/?study=1.3.6.1.4.1.14519.5.2.1.3023.4012.507148485748821590204034796320&series=1.3.6.1.4.1.14519.5.2.1.3023.4012.613169434607414222857186346352)\n",
        "    * `TCGA-50-6590`: labeled as `LUAD` but classified by our model as `LUSC`.\n",
        "      * Scroll through images 78 through 85. Look for the bright white spot in these images on the PET scan.\n",
        "      * [CT scan](https://nbia.cancerimagingarchive.net/viewer/?study=1.3.6.1.4.1.14519.5.2.1.6450.9002.125969062420301466106414902377&series=1.3.6.1.4.1.14519.5.2.1.6450.9002.216176897913679442475013148754)\n",
        "      * [PET scan](https://nbia.cancerimagingarchive.net/viewer/?study=1.3.6.1.4.1.14519.5.2.1.6450.9002.125969062420301466106414902377&series=1.3.6.1.4.1.14519.5.2.1.6450.9002.321022540475237033558410330699)\n",
        "  * Without much background, it is probably much easier to see the tumor as the brightly lit up white spot on the PET scan. However, it's easier to appreciate finer detail on the CT scan. Try to find the tumor on the CT using the PET scan to get the rough location of the tumor. The image indices are the same on both.\n",
        "  * Using the anatomical descriptions of lung adenocarcinoma and lung squamous cell carcinoma provided in the intro of this assignment, does it look like the labels for these cases are correct?\n",
        "1. Writeup your work, your writeup should be 1 to 2 pages long, excluding figures. 12pt font, single space, 1 inch margins, letter size paper. Please submit either a PDF or a Word document. Make sure to include the following:\n",
        "  * Your embedding visualization.\n",
        "  * A brief justification for the normalization & aggregation method and the order in which you applied them.\n",
        "  * A discussion of some of the above questions regarding identified misclassifications via visualization and clustering.\n",
        "  * A discussion of manual review of the the misclassified cases.\n",
        "  * A discussion on the question: Why is review by an expert important? Phrased another way, why should someone with domain knowledge review models?\n",
        "1. Submit your homework. Make sure to include:\n",
        "  1. Your writeup containing a figure with your embedding visualization.\n",
        "  1. Your notebook with your code.\n"
      ],
      "metadata": {
        "id": "qW_R-TgIriQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "w4aMfZ0krmzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy\n",
        "!wget https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw2/hw2.tar.gz\n",
        "!tar -xzf hw2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLCvyGgskxHa",
        "outputId": "f442f0dd-be35-49fc-c85f-68249f7b784c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.11.3-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.12.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->scanpy) (3.5.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->scanpy) (1.17.0)\n",
            "Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.3-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.10.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=c0a5db18fa2e3d2b40ae1237ebe5c78bb5f746a335e8ee774a59988fe0bc0b96\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/56/35/a748fc57279a4b84d0b332879445fed1ad8478e7257986b015\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, array-api-compat, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.11.3 array-api-compat-1.10.0 legacy-api-wrap-1.4.1 pynndescent-0.5.13 scanpy-1.10.4 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.7\n",
            "--2025-01-26 03:31:07--  https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw2/hw2.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/915385537/964dec5e-0cc7-412e-9b72-1bcec02a61af?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250126T033107Z&X-Amz-Expires=300&X-Amz-Signature=6ebbec498f9937e9cd7638bb482ed2527f99526442e413597081a2bb5bbf82f6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dhw2.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-01-26 03:31:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/915385537/964dec5e-0cc7-412e-9b72-1bcec02a61af?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250126T033107Z&X-Amz-Expires=300&X-Amz-Signature=6ebbec498f9937e9cd7638bb482ed2527f99526442e413597081a2bb5bbf82f6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dhw2.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1107219891 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘hw2.tar.gz’\n",
            "\n",
            "hw2.tar.gz          100%[===================>]   1.03G  89.1MB/s    in 13s     \n",
            "\n",
            "2025-01-26 03:31:21 (79.5 MB/s) - ‘hw2.tar.gz’ saved [1107219891/1107219891]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XM8OiAB4uCSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}